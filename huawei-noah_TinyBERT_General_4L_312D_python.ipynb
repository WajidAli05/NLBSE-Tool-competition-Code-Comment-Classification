{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":37271,"status":"ok","timestamp":1733068364094,"user":{"displayName":"i200981 Wajid Ali","userId":"09984756062104537001"},"user_tz":480},"id":"sHWJIUgTy7qd","outputId":"390cb789-c6b0-4c04-969c-e8cc92a98c90"},"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.9/67.9 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m33.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m897.5/897.5 kB\u001b[0m \u001b[31m35.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","sentence-transformers 3.2.1 requires transformers<5.0.0,>=4.41.0, but you have transformers 4.17.0 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m364.4/364.4 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m233.5/233.5 kB\u001b[0m \u001b[31m14.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.6/78.6 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting datasets\n","  Downloading datasets-3.1.0-py3-none-any.whl.metadata (20 kB)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.16.1)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.26.4)\n","Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (17.0.0)\n","Collecting dill<0.3.9,>=0.3.0 (from datasets)\n","  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.2)\n","Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.32.3)\n","Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.6)\n","Collecting xxhash (from datasets)\n","  Downloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n","Collecting multiprocess<0.70.17 (from datasets)\n","  Downloading multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n","Collecting fsspec<=2024.9.0,>=2023.1.0 (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets)\n","  Downloading fsspec-2024.9.0-py3-none-any.whl.metadata (11 kB)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.11.2)\n","Requirement already satisfied: huggingface-hub>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.26.2)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.2)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.2)\n","Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.3)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.2.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.5.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\n","Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (0.2.0)\n","Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.17.2)\n","Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.23.0->datasets) (4.12.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.4.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.2.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2024.8.30)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n","Downloading datasets-3.1.0-py3-none-any.whl (480 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m480.6/480.6 kB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading fsspec-2024.9.0-py3-none-any.whl (179 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m179.3/179.3 kB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m15.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: xxhash, fsspec, dill, multiprocess, datasets\n","  Attempting uninstall: fsspec\n","    Found existing installation: fsspec 2024.10.0\n","    Uninstalling fsspec-2024.10.0:\n","      Successfully uninstalled fsspec-2024.10.0\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","gcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2024.9.0 which is incompatible.\n","sentence-transformers 3.2.1 requires transformers<5.0.0,>=4.41.0, but you have transformers 4.17.0 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed datasets-3.1.0 dill-0.3.8 fsspec-2024.9.0 multiprocess-0.70.16 xxhash-3.5.0\n"]}],"source":["!pip -q install transformers==4.17\n","!pip -q install optuna\n","!pip install datasets"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"P2E2HOAZzM-8"},"outputs":[],"source":["from transformers import Trainer, TrainingArguments, AutoModel, AutoTokenizer, AutoModelForSequenceClassification\n","from datasets import load_dataset\n","import torch\n","import pandas as pd\n","import nltk\n","import re\n","import ast\n","import numpy as np\n","from torch.utils.data import Dataset\n","from sklearn.metrics import precision_score, recall_score, f1_score, precision_recall_fscore_support, accuracy_score\n","from sklearn.model_selection import train_test_split\n","import torch.nn as nn\n","import optuna\n","import time"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":519},"executionInfo":{"elapsed":3272,"status":"ok","timestamp":1733068388984,"user":{"displayName":"i200981 Wajid Ali","userId":"09984756062104537001"},"user_tz":480},"id":"_b1pJXwqzbkx","outputId":"3b8df785-943f-4070-d8ee-73e684bf229b"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n","The secret `HF_TOKEN` does not exist in your Colab secrets.\n","To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n","You will be able to reuse this secret in all of your notebooks.\n","Please note that authentication is recommended but still optional to access public models or datasets.\n","  warnings.warn(\n"]},{"output_type":"display_data","data":{"text/plain":["   index        class                                   comment_sentence  \\\n","0      1  AccessMixin                                     functionality.   \n","1      8       Atomic  when it s used as a decorator, call wraps the ...   \n","2     10       Atomic  when it s used as a context manager, enter cre...   \n","3     12       Atomic  exit commits the transaction or releases the s...   \n","4     14       Atomic  it s possible to disable the creation of savep...   \n","\n","   partition                                              combo  \\\n","0          0                       functionality. | AccessMixin   \n","1          0  when it s used as a decorator, call wraps the ...   \n","2          0  when it s used as a context manager, enter cre...   \n","3          0  exit commits the transaction or releases the s...   \n","4          0  it s possible to disable the creation of savep...   \n","\n","            labels  \n","0  [0, 0, 0, 0, 1]  \n","1  [1, 0, 0, 0, 0]  \n","2  [1, 0, 0, 0, 0]  \n","3  [1, 0, 0, 0, 0]  \n","4  [1, 0, 0, 0, 0]  "],"text/html":["\n","  <div id=\"df-a451bf41-94e2-4ffb-bb67-b18c5cb4a6ba\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>index</th>\n","      <th>class</th>\n","      <th>comment_sentence</th>\n","      <th>partition</th>\n","      <th>combo</th>\n","      <th>labels</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1</td>\n","      <td>AccessMixin</td>\n","      <td>functionality.</td>\n","      <td>0</td>\n","      <td>functionality. | AccessMixin</td>\n","      <td>[0, 0, 0, 0, 1]</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>8</td>\n","      <td>Atomic</td>\n","      <td>when it s used as a decorator, call wraps the ...</td>\n","      <td>0</td>\n","      <td>when it s used as a decorator, call wraps the ...</td>\n","      <td>[1, 0, 0, 0, 0]</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>10</td>\n","      <td>Atomic</td>\n","      <td>when it s used as a context manager, enter cre...</td>\n","      <td>0</td>\n","      <td>when it s used as a context manager, enter cre...</td>\n","      <td>[1, 0, 0, 0, 0]</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>12</td>\n","      <td>Atomic</td>\n","      <td>exit commits the transaction or releases the s...</td>\n","      <td>0</td>\n","      <td>exit commits the transaction or releases the s...</td>\n","      <td>[1, 0, 0, 0, 0]</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>14</td>\n","      <td>Atomic</td>\n","      <td>it s possible to disable the creation of savep...</td>\n","      <td>0</td>\n","      <td>it s possible to disable the creation of savep...</td>\n","      <td>[1, 0, 0, 0, 0]</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-a451bf41-94e2-4ffb-bb67-b18c5cb4a6ba')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-a451bf41-94e2-4ffb-bb67-b18c5cb4a6ba button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-a451bf41-94e2-4ffb-bb67-b18c5cb4a6ba');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","<div id=\"df-3e3f2812-78c1-4cd1-8e16-bdaf98e118d4\">\n","  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-3e3f2812-78c1-4cd1-8e16-bdaf98e118d4')\"\n","            title=\"Suggest charts\"\n","            style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","  </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","  <script>\n","    async function quickchart(key) {\n","      const quickchartButtonEl =\n","        document.querySelector('#' + key + ' button');\n","      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","      quickchartButtonEl.classList.add('colab-df-spinner');\n","      try {\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      } catch (error) {\n","        console.error('Error during call to suggestCharts:', error);\n","      }\n","      quickchartButtonEl.classList.remove('colab-df-spinner');\n","      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","    }\n","    (() => {\n","      let quickchartButtonEl =\n","        document.querySelector('#df-3e3f2812-78c1-4cd1-8e16-bdaf98e118d4 button');\n","      quickchartButtonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","    })();\n","  </script>\n","</div>\n","\n","    </div>\n","  </div>\n"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"dataframe","summary":"{\n  \"name\": \"display(test_df\",\n  \"rows\": 5,\n  \"fields\": [\n    {\n      \"column\": \"index\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 5,\n        \"min\": 1,\n        \"max\": 14,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          8,\n          14,\n          10\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"class\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"Atomic\",\n          \"AccessMixin\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"comment_sentence\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"when it s used as a decorator, call wraps the execution of the\",\n          \"it s possible to disable the creation of savepoints if the goal is to\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"partition\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 0,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"combo\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"when it s used as a decorator, call wraps the execution of the | Atomic\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"labels\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["   index           class                                   comment_sentence  \\\n","0      0     AccessMixin  abstract cbv mixin that gives access mixins th...   \n","1      2  AmbiguityError     more than one migration matches a name prefix.   \n","2      3   AppConfigStub                              stub of an appconfig.   \n","3      4   AppConfigStub        only provides a label and a dict of models.   \n","4      5         Archive  the external api class that encapsulates an ar...   \n","\n","   partition                                              combo  \\\n","0          1  abstract cbv mixin that gives access mixins th...   \n","1          1  more than one migration matches a name prefix....   \n","2          1              stub of an appconfig. | AppConfigStub   \n","3          1  only provides a label and a dict of models. | ...   \n","4          1  the external api class that encapsulates an ar...   \n","\n","            labels  \n","0  [0, 0, 0, 0, 1]  \n","1  [0, 0, 0, 0, 1]  \n","2  [0, 0, 0, 0, 1]  \n","3  [0, 1, 0, 0, 0]  \n","4  [0, 0, 1, 0, 1]  "],"text/html":["\n","  <div id=\"df-aef71fd9-1172-4cae-87d4-37ec3d706b80\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>index</th>\n","      <th>class</th>\n","      <th>comment_sentence</th>\n","      <th>partition</th>\n","      <th>combo</th>\n","      <th>labels</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>AccessMixin</td>\n","      <td>abstract cbv mixin that gives access mixins th...</td>\n","      <td>1</td>\n","      <td>abstract cbv mixin that gives access mixins th...</td>\n","      <td>[0, 0, 0, 0, 1]</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>2</td>\n","      <td>AmbiguityError</td>\n","      <td>more than one migration matches a name prefix.</td>\n","      <td>1</td>\n","      <td>more than one migration matches a name prefix....</td>\n","      <td>[0, 0, 0, 0, 1]</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>3</td>\n","      <td>AppConfigStub</td>\n","      <td>stub of an appconfig.</td>\n","      <td>1</td>\n","      <td>stub of an appconfig. | AppConfigStub</td>\n","      <td>[0, 0, 0, 0, 1]</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>4</td>\n","      <td>AppConfigStub</td>\n","      <td>only provides a label and a dict of models.</td>\n","      <td>1</td>\n","      <td>only provides a label and a dict of models. | ...</td>\n","      <td>[0, 1, 0, 0, 0]</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>5</td>\n","      <td>Archive</td>\n","      <td>the external api class that encapsulates an ar...</td>\n","      <td>1</td>\n","      <td>the external api class that encapsulates an ar...</td>\n","      <td>[0, 0, 1, 0, 1]</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-aef71fd9-1172-4cae-87d4-37ec3d706b80')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-aef71fd9-1172-4cae-87d4-37ec3d706b80 button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-aef71fd9-1172-4cae-87d4-37ec3d706b80');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","<div id=\"df-d3e15f0d-60ff-440c-b208-5bda7355eaa8\">\n","  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-d3e15f0d-60ff-440c-b208-5bda7355eaa8')\"\n","            title=\"Suggest charts\"\n","            style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","  </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","  <script>\n","    async function quickchart(key) {\n","      const quickchartButtonEl =\n","        document.querySelector('#' + key + ' button');\n","      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","      quickchartButtonEl.classList.add('colab-df-spinner');\n","      try {\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      } catch (error) {\n","        console.error('Error during call to suggestCharts:', error);\n","      }\n","      quickchartButtonEl.classList.remove('colab-df-spinner');\n","      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","    }\n","    (() => {\n","      let quickchartButtonEl =\n","        document.querySelector('#df-d3e15f0d-60ff-440c-b208-5bda7355eaa8 button');\n","      quickchartButtonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","    })();\n","  </script>\n","</div>\n","\n","    </div>\n","  </div>\n"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"dataframe","summary":"{\n  \"name\": \"display(test_df\",\n  \"rows\": 5,\n  \"fields\": [\n    {\n      \"column\": \"index\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1,\n        \"min\": 0,\n        \"max\": 5,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          2,\n          5,\n          3\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"class\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 4,\n        \"samples\": [\n          \"AmbiguityError\",\n          \"Archive\",\n          \"AccessMixin\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"comment_sentence\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"more than one migration matches a name prefix.\",\n          \"the external api class that encapsulates an archive implementation.\",\n          \"stub of an appconfig.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"partition\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 1,\n        \"max\": 1,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"combo\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"more than one migration matches a name prefix. | AmbiguityError\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"labels\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"}},"metadata":{}}],"source":["splits = {'python_train': 'data/python_train-00000-of-00001.parquet', 'python_test': 'data/python_test-00000-of-00001.parquet'}\n","train_df = pd.read_parquet(\"hf://datasets/NLBSE/nlbse25-code-comment-classification/\" + splits[\"python_train\"])\n","test_df = pd.read_parquet(\"hf://datasets/NLBSE/nlbse25-code-comment-classification/\" + splits[\"python_test\"])\n","\n","display(train_df.head())\n","display(test_df.head())"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":465},"executionInfo":{"elapsed":25,"status":"ok","timestamp":1733068388985,"user":{"displayName":"i200981 Wajid Ali","userId":"09984756062104537001"},"user_tz":480},"id":"9x93EZXwzo-f","outputId":"dc69d54e-d190-4151-b04c-4eca586321c4"},"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-4-df4392ae2e99>:9: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n","  train_str_df = train_df[[\"class\", \"comment_sentence\", \"combo\"]].applymap(\n","<ipython-input-4-df4392ae2e99>:12: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n","  test_str_df = test_df[[\"class\", \"comment_sentence\", \"combo\"]].applymap(\n"]},{"output_type":"display_data","data":{"text/plain":["   index        class                                   comment_sentence  \\\n","0      1  accessmixin                                     functionality.   \n","1      8       atomic  when it s used as a decorator, call wraps the ...   \n","2     10       atomic  when it s used as a context manager, enter cre...   \n","3     12       atomic  exit commits the transaction or releases the s...   \n","4     14       atomic  it s possible to disable the creation of savep...   \n","\n","   partition                                              combo  \\\n","0          0                        functionality | accessmixin   \n","1          0  when it s used as a decorator call wraps the e...   \n","2          0  when it s used as a context manager enter crea...   \n","3          0  exit commits the transaction or releases the s...   \n","4          0  it s possible to disable the creation of savep...   \n","\n","            labels  \n","0  [0, 0, 0, 0, 1]  \n","1  [1, 0, 0, 0, 0]  \n","2  [1, 0, 0, 0, 0]  \n","3  [1, 0, 0, 0, 0]  \n","4  [1, 0, 0, 0, 0]  "],"text/html":["\n","  <div id=\"df-b8e0c911-25fd-4afc-ba13-897354d49ad5\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>index</th>\n","      <th>class</th>\n","      <th>comment_sentence</th>\n","      <th>partition</th>\n","      <th>combo</th>\n","      <th>labels</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1</td>\n","      <td>accessmixin</td>\n","      <td>functionality.</td>\n","      <td>0</td>\n","      <td>functionality | accessmixin</td>\n","      <td>[0, 0, 0, 0, 1]</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>8</td>\n","      <td>atomic</td>\n","      <td>when it s used as a decorator, call wraps the ...</td>\n","      <td>0</td>\n","      <td>when it s used as a decorator call wraps the e...</td>\n","      <td>[1, 0, 0, 0, 0]</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>10</td>\n","      <td>atomic</td>\n","      <td>when it s used as a context manager, enter cre...</td>\n","      <td>0</td>\n","      <td>when it s used as a context manager enter crea...</td>\n","      <td>[1, 0, 0, 0, 0]</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>12</td>\n","      <td>atomic</td>\n","      <td>exit commits the transaction or releases the s...</td>\n","      <td>0</td>\n","      <td>exit commits the transaction or releases the s...</td>\n","      <td>[1, 0, 0, 0, 0]</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>14</td>\n","      <td>atomic</td>\n","      <td>it s possible to disable the creation of savep...</td>\n","      <td>0</td>\n","      <td>it s possible to disable the creation of savep...</td>\n","      <td>[1, 0, 0, 0, 0]</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-b8e0c911-25fd-4afc-ba13-897354d49ad5')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-b8e0c911-25fd-4afc-ba13-897354d49ad5 button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-b8e0c911-25fd-4afc-ba13-897354d49ad5');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","<div id=\"df-af1adbdf-612f-49d6-9382-0b09cd6156de\">\n","  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-af1adbdf-612f-49d6-9382-0b09cd6156de')\"\n","            title=\"Suggest charts\"\n","            style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","  </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","  <script>\n","    async function quickchart(key) {\n","      const quickchartButtonEl =\n","        document.querySelector('#' + key + ' button');\n","      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","      quickchartButtonEl.classList.add('colab-df-spinner');\n","      try {\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      } catch (error) {\n","        console.error('Error during call to suggestCharts:', error);\n","      }\n","      quickchartButtonEl.classList.remove('colab-df-spinner');\n","      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","    }\n","    (() => {\n","      let quickchartButtonEl =\n","        document.querySelector('#df-af1adbdf-612f-49d6-9382-0b09cd6156de button');\n","      quickchartButtonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","    })();\n","  </script>\n","</div>\n","\n","    </div>\n","  </div>\n"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"dataframe","summary":"{\n  \"name\": \"display(test_df\",\n  \"rows\": 5,\n  \"fields\": [\n    {\n      \"column\": \"index\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 5,\n        \"min\": 1,\n        \"max\": 14,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          8,\n          14,\n          10\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"class\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"atomic\",\n          \"accessmixin\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"comment_sentence\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"when it s used as a decorator, call wraps the execution of the\",\n          \"it s possible to disable the creation of savepoints if the goal is to\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"partition\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 0,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"combo\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"when it s used as a decorator call wraps the execution of the | atomic\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"labels\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["   index           class                                   comment_sentence  \\\n","0      0     accessmixin  abstract cbv mixin that gives access mixins th...   \n","1      2  ambiguityerror     more than one migration matches a name prefix.   \n","2      3   appconfigstub                              stub of an appconfig.   \n","3      4   appconfigstub        only provides a label and a dict of models.   \n","4      5         archive  the external api class that encapsulates an ar...   \n","\n","   partition                                              combo  \\\n","0          1  abstract cbv mixin that gives access mixins th...   \n","1          1  more than one migration matches a name prefix ...   \n","2          1               stub of an appconfig | appconfigstub   \n","3          1  only provides a label and a dict of models | a...   \n","4          1  the external api class that encapsulates an ar...   \n","\n","            labels  \n","0  [0, 0, 0, 0, 1]  \n","1  [0, 0, 0, 0, 1]  \n","2  [0, 0, 0, 0, 1]  \n","3  [0, 1, 0, 0, 0]  \n","4  [0, 0, 1, 0, 1]  "],"text/html":["\n","  <div id=\"df-8c5cd766-1a70-4bcd-94f1-c9b4d792e46e\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>index</th>\n","      <th>class</th>\n","      <th>comment_sentence</th>\n","      <th>partition</th>\n","      <th>combo</th>\n","      <th>labels</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>accessmixin</td>\n","      <td>abstract cbv mixin that gives access mixins th...</td>\n","      <td>1</td>\n","      <td>abstract cbv mixin that gives access mixins th...</td>\n","      <td>[0, 0, 0, 0, 1]</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>2</td>\n","      <td>ambiguityerror</td>\n","      <td>more than one migration matches a name prefix.</td>\n","      <td>1</td>\n","      <td>more than one migration matches a name prefix ...</td>\n","      <td>[0, 0, 0, 0, 1]</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>3</td>\n","      <td>appconfigstub</td>\n","      <td>stub of an appconfig.</td>\n","      <td>1</td>\n","      <td>stub of an appconfig | appconfigstub</td>\n","      <td>[0, 0, 0, 0, 1]</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>4</td>\n","      <td>appconfigstub</td>\n","      <td>only provides a label and a dict of models.</td>\n","      <td>1</td>\n","      <td>only provides a label and a dict of models | a...</td>\n","      <td>[0, 1, 0, 0, 0]</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>5</td>\n","      <td>archive</td>\n","      <td>the external api class that encapsulates an ar...</td>\n","      <td>1</td>\n","      <td>the external api class that encapsulates an ar...</td>\n","      <td>[0, 0, 1, 0, 1]</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-8c5cd766-1a70-4bcd-94f1-c9b4d792e46e')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-8c5cd766-1a70-4bcd-94f1-c9b4d792e46e button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-8c5cd766-1a70-4bcd-94f1-c9b4d792e46e');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","<div id=\"df-ab64b7f2-6df9-4c52-aff6-3c5c80199142\">\n","  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-ab64b7f2-6df9-4c52-aff6-3c5c80199142')\"\n","            title=\"Suggest charts\"\n","            style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","  </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","  <script>\n","    async function quickchart(key) {\n","      const quickchartButtonEl =\n","        document.querySelector('#' + key + ' button');\n","      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","      quickchartButtonEl.classList.add('colab-df-spinner');\n","      try {\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      } catch (error) {\n","        console.error('Error during call to suggestCharts:', error);\n","      }\n","      quickchartButtonEl.classList.remove('colab-df-spinner');\n","      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","    }\n","    (() => {\n","      let quickchartButtonEl =\n","        document.querySelector('#df-ab64b7f2-6df9-4c52-aff6-3c5c80199142 button');\n","      quickchartButtonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","    })();\n","  </script>\n","</div>\n","\n","    </div>\n","  </div>\n"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"dataframe","summary":"{\n  \"name\": \"display(test_df\",\n  \"rows\": 5,\n  \"fields\": [\n    {\n      \"column\": \"index\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1,\n        \"min\": 0,\n        \"max\": 5,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          2,\n          5,\n          3\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"class\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 4,\n        \"samples\": [\n          \"ambiguityerror\",\n          \"archive\",\n          \"accessmixin\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"comment_sentence\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"more than one migration matches a name prefix.\",\n          \"the external api class that encapsulates an archive implementation.\",\n          \"stub of an appconfig.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"partition\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 1,\n        \"max\": 1,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"combo\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"more than one migration matches a name prefix | ambiguityerror\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"labels\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"}},"metadata":{}}],"source":["def remove_punctuation_except_pipe(text):\n","    return re.sub(r\"[^\\w\\s\\|]\", \"\", text)\n","\n","# Apply the function to the 'combo' column\n","train_df['combo'] = train_df['combo'].apply(remove_punctuation_except_pipe)\n","test_df['combo'] = test_df['combo'].apply(remove_punctuation_except_pipe)\n","\n","# Convert data to lowercase\n","train_str_df = train_df[[\"class\", \"comment_sentence\", \"combo\"]].applymap(\n","    lambda x: x.lower() if isinstance(x, str) else str(x)\n",")\n","test_str_df = test_df[[\"class\", \"comment_sentence\", \"combo\"]].applymap(\n","    lambda x: x.lower() if isinstance(x, str) else str(x)\n",")\n","\n","train_df[[\"class\", \"comment_sentence\", \"combo\"]] = train_str_df[[\"class\", \"comment_sentence\", \"combo\"]]\n","test_df[[\"class\", \"comment_sentence\", \"combo\"]] = test_str_df[[\"class\", \"comment_sentence\", \"combo\"]]\n","\n","display(train_df.head())\n","display(test_df.head())"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"20n8nIz2zr-2"},"outputs":[],"source":["X = list(train_df['combo'])\n","y = list(train_df['labels'])\n","\n","X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":272,"referenced_widgets":["efb7b34e5fcd407ab2f81e45b8197319","334c2d58d36c4989803bb315cb5a7d0f","26d7d8620d584aa78f38eb5ed9d86dcf","30705faac2a34642b50c13af042b242e","e59a83763ea1411fb55c8c909ab1c713","23032f1fdba54fdb8ea4ee2f561c16a3","066b9ff6d12d40d9928a2d8c1c97eadf","e9ee6e85b36241949816fe50d76fbeab","b1be8b2321854cfe8848a871931544cd","e379dc26b73f46bd850932cbe89e38ed","ae8e8fda121c48249707680efc5e3282","bdffab7bfd73431191b0982603de044f","de7a06340b93489fb7a5cd743b96eb22","74cfdb87914b478892d20d36e6761ecf","b6b6682a1f3e469d84779c5c0b0a6a56","1247a74ac7344f9b8cfe98af8c59ce7a","5c8df202f00a411c9b1fe2a21fceddd4","fb57b3c8e21c4b20a8fb63aa92a9e583","464d31353c884215b65b1088406cd07e","623ce299f5f84eb2944e89e5472492ac","efd8e939ec6e4fecb2cd222f6fa3ad75","1f4f42bb1993482d946a655ab1ed8f9b","7ffe5037639a4d259145135ddb29f74a","11a88252c70f4e668645a9598d1d2c5c","ad9bd499bace4be5842929eda2e20e86","e34077a3c7a142fcbb85eb9fed83fd6f","1cb9b80524d64fb8b2b35849d270feeb","876bcacfc9694512ad24604300dc2c36","9ccd3d9d9e7d4d86b38e58c68d145d40","bcf02cdf5fc64ebab523c6df7d969ec0","cf63badb6d2740079ab81d42e8eac42f","8da47d52503242dc9c98fcab497acf07","368cb507daca42efacf52eb727d816ed"]},"executionInfo":{"elapsed":14667,"status":"ok","timestamp":1733068403635,"user":{"displayName":"i200981 Wajid Ali","userId":"09984756062104537001"},"user_tz":480},"id":"4lJ7iGgPzwDz","outputId":"195e1f8f-d323-43b7-846a-3abd47fbb4d5"},"outputs":[{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/409 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"efb7b34e5fcd407ab2f81e45b8197319"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/226k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bdffab7bfd73431191b0982603de044f"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"]},{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/59.8M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7ffe5037639a4d259145135ddb29f74a"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py:1439: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  state_dict = torch.load(resolved_archive_file, map_location=\"cpu\")\n","Some weights of the model checkpoint at huawei-noah/TinyBERT_General_4L_312D were not used when initializing BertForSequenceClassification: ['fit_denses.3.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'fit_denses.0.bias', 'cls.predictions.transform.dense.weight', 'fit_denses.2.bias', 'fit_denses.4.weight', 'fit_denses.3.bias', 'fit_denses.1.bias', 'cls.predictions.decoder.weight', 'fit_denses.1.weight', 'cls.predictions.bias', 'fit_denses.0.weight', 'fit_denses.2.weight', 'fit_denses.4.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias']\n","- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at huawei-noah/TinyBERT_General_4L_312D and are newly initialized: ['classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]}],"source":["tokenizer = AutoTokenizer.from_pretrained('huawei-noah/TinyBERT_General_4L_312D')\n","model = AutoModelForSequenceClassification.from_pretrained('huawei-noah/TinyBERT_General_4L_312D', num_labels=5)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BLFFkwfl0j2W"},"outputs":[],"source":["class TextClassificationDataset(Dataset):\n","    def __init__(self, texts, labels, tokenizer, device=\"cpu\"):\n","        self.texts = texts\n","        self.labels = labels\n","        self.tokenizer = tokenizer\n","        self.device = device\n","\n","    def __len__(self):\n","        return len(self.texts)\n","\n","    def __getitem__(self, index):\n","        text = self.texts[index]\n","        label = self.labels[index]\n","\n","        encoded_text = self.tokenizer.encode_plus(\n","            text,\n","            add_special_tokens=True,\n","            max_length=256,\n","            padding='max_length',\n","            truncation=True,\n","            return_token_type_ids=False,\n","            return_attention_mask=True,\n","            return_tensors='pt',\n","        )\n","\n","        input_ids = encoded_text['input_ids'].squeeze().to(self.device)\n","        attention_mask = encoded_text['attention_mask'].squeeze().to(self.device)\n","        label = torch.tensor(label, dtype=torch.float).to(self.device)\n","\n","        return {\n","            'input_ids': input_ids,\n","            'attention_mask': attention_mask,\n","            'labels': label\n","        }\n","\n","train_dataset = TextClassificationDataset(X_train, y_train, tokenizer)\n","eval_dataset = TextClassificationDataset(X_val, y_val, tokenizer)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aO311Hhf0rKO"},"outputs":[],"source":["class OneHotTrainer(Trainer):\n","    def compute_loss(self, model, inputs, return_outputs=False):\n","        labels = inputs.get(\"labels\")\n","        model_inputs = {k: v for k, v in inputs.items() if k != \"labels\"}\n","        outputs = model(**model_inputs)\n","        logits = outputs.get(\"logits\")\n","\n","        if logits is None:\n","            raise ValueError(\"Logits are missing from the model output.\")\n","\n","        loss_fct = nn.BCEWithLogitsLoss()\n","        loss = loss_fct(logits, labels.float())\n","\n","        # Log training loss\n","        if self.state.global_step % self.args.logging_steps == 0:\n","            print(f\"Step {self.state.global_step}, Training Loss: {loss.item()}\")\n","\n","        return (loss, outputs) if return_outputs else loss\n","\n","\n","# class OneHotTrainer(Trainer):\n","#     def compute_loss(self, model, inputs, return_outputs=False):\n","#         labels = inputs.get(\"labels\")\n","#         # Remove 'labels' from inputs before passing to model\n","#         model_inputs = {k: v for k, v in inputs.items() if k != \"labels\"}\n","#         outputs = model(**model_inputs)  # Pass only the required inputs to the model\n","#         logits = outputs.get(\"logits\")\n","\n","#         if logits is None:\n","#             # Handle the case where logits are missing, e.g., raise an exception or return a default loss\n","#             raise ValueError(\"Logits are missing from the model output.\")  # Or return a default loss\n","\n","#         # Use BCEWithLogitsLoss for multi-label classification\n","#         loss_fct = nn.BCEWithLogitsLoss()\n","#         loss = loss_fct(logits, labels.float())\n","\n","#         return (loss, outputs) if return_outputs else loss"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-pVPFv7O0ut3"},"outputs":[],"source":["def compute_metrics(pred):\n","    logits = pred.predictions\n","    preds = (logits > 0).astype(int)\n","    labels = pred.label_ids\n","\n","    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='weighted', zero_division=1)\n","    accuracy = accuracy_score(labels, preds)\n","\n","    return {\n","        'accuracy': accuracy,\n","        'precision': precision,\n","        'recall': recall,\n","        'f1': f1\n","    }\n","\n","\n","# def compute_metrics(pred):\n","#     logits = pred.predictions\n","#     preds = (logits > 0).astype(int)\n","\n","#     labels = pred.label_ids\n","\n","#     precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='weighted')\n","#     accuracy = (preds == labels).mean()\n","\n","#     return {\n","#         'accuracy': accuracy,\n","#         'precision': precision,\n","#         'recall': recall,\n","#         'f1': f1\n","#     }\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nOK9HTwq9wuM"},"outputs":[],"source":["def objective(trial):\n","    learning_rate = trial.suggest_loguniform(\"learning_rate\", 1e-6, 5e-5)\n","    batch_size = trial.suggest_categorical(\"batch_size\", [4, 8, 16])\n","    num_train_epochs = trial.suggest_int(\"num_train_epochs\", 3, 20)\n","    weight_decay = trial.suggest_loguniform(\"weight_decay\", 1e-4, 0.1)\n","\n","    # training_args = TrainingArguments(\n","    #     output_dir=\"./results\",\n","    #     num_train_epochs=num_train_epochs,\n","    #     per_device_train_batch_size=batch_size,\n","    #     per_device_eval_batch_size=batch_size * 2,\n","    #     warmup_steps=10,\n","    #     weight_decay=weight_decay,\n","    #     evaluation_strategy=\"epoch\",\n","    #     learning_rate=learning_rate,\n","    #     gradient_accumulation_steps=2,\n","    #     report_to=\"none\",\n","    # )\n","\n","    training_args = TrainingArguments(\n","        output_dir=\"./results\",\n","        num_train_epochs=num_train_epochs,\n","        per_device_train_batch_size=batch_size,\n","        per_device_eval_batch_size=batch_size * 2,\n","        warmup_steps=10,\n","        weight_decay=weight_decay,\n","        evaluation_strategy=\"epoch\",\n","        logging_dir=\"./logs\",  # Directory for logging\n","        logging_steps=10,  # Log every 10 steps\n","        save_steps=500,\n","        learning_rate=learning_rate,\n","        gradient_accumulation_steps=2,\n","        report_to=\"none\",  # Avoid reporting to third-party loggers like WandB\n","        logging_first_step=True,  # Log the first step\n","    )\n","\n","    trainer = OneHotTrainer(\n","        model=model,\n","        args=training_args,\n","        train_dataset=train_dataset,\n","        eval_dataset=eval_dataset,\n","        compute_metrics=compute_metrics,\n","    )\n","    print(\"Starting training...\")\n","    trainer.train()\n","    eval_results = trainer.evaluate()\n","\n","    return eval_results[\"eval_f1\"]"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"-12Jc6Vq-FNz","executionInfo":{"status":"ok","timestamp":1733070206964,"user_tz":480,"elapsed":1802869,"user":{"displayName":"i200981 Wajid Ali","userId":"09984756062104537001"}},"outputId":"ee1cb702-3434-4abd-a7d7-678bca25f129"},"outputs":[{"output_type":"stream","name":"stderr","text":["[I 2024-12-01 15:53:23,644] A new study created in memory with name: no-name-8c45ec8b-ae4f-47d6-808a-54d8ed54529c\n","<ipython-input-10-747f1883d862>:2: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n","  learning_rate = trial.suggest_loguniform(\"learning_rate\", 1e-6, 5e-5)\n","<ipython-input-10-747f1883d862>:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n","  weight_decay = trial.suggest_loguniform(\"weight_decay\", 1e-4, 0.1)\n"]},{"output_type":"stream","name":"stdout","text":["Starting training...\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n","***** Running training *****\n","  Num examples = 1507\n","  Num Epochs = 10\n","  Instantaneous batch size per device = 8\n","  Total train batch size (w. parallel, distributed & accumulation) = 16\n","  Gradient Accumulation steps = 2\n","  Total optimization steps = 940\n"]},{"output_type":"stream","name":"stdout","text":["Step 0, Training Loss: 0.6907334923744202\n","Step 0, Training Loss: 0.6918603181838989\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='940' max='940' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [940/940 01:25, Epoch 9/10]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","      <th>F1</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>0</td>\n","      <td>0.582200</td>\n","      <td>0.571854</td>\n","      <td>0.000000</td>\n","      <td>1.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>1</td>\n","      <td>0.533400</td>\n","      <td>0.521292</td>\n","      <td>0.000000</td>\n","      <td>1.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>0.523300</td>\n","      <td>0.509048</td>\n","      <td>0.000000</td>\n","      <td>1.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>0.507800</td>\n","      <td>0.505790</td>\n","      <td>0.000000</td>\n","      <td>1.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>4</td>\n","      <td>0.509200</td>\n","      <td>0.501989</td>\n","      <td>0.000000</td>\n","      <td>1.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>5</td>\n","      <td>0.499500</td>\n","      <td>0.496842</td>\n","      <td>0.000000</td>\n","      <td>1.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>6</td>\n","      <td>0.486400</td>\n","      <td>0.489918</td>\n","      <td>0.000000</td>\n","      <td>1.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>7</td>\n","      <td>0.475900</td>\n","      <td>0.485373</td>\n","      <td>0.000000</td>\n","      <td>1.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>8</td>\n","      <td>0.486800</td>\n","      <td>0.483023</td>\n","      <td>0.000000</td>\n","      <td>1.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>9</td>\n","      <td>0.481300</td>\n","      <td>0.481083</td>\n","      <td>0.000000</td>\n","      <td>1.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Step 10, Training Loss: 0.6879746317863464\n","Step 10, Training Loss: 0.6876989603042603\n","Step 20, Training Loss: 0.6709696650505066\n","Step 20, Training Loss: 0.6713778972625732\n","Step 30, Training Loss: 0.6622553467750549\n","Step 30, Training Loss: 0.6505895853042603\n","Step 40, Training Loss: 0.6449963450431824\n","Step 40, Training Loss: 0.6385287642478943\n","Step 50, Training Loss: 0.6322230696678162\n","Step 50, Training Loss: 0.6226707100868225\n","Step 60, Training Loss: 0.6147946715354919\n","Step 60, Training Loss: 0.6085945963859558\n","Step 70, Training Loss: 0.5933279395103455\n","Step 70, Training Loss: 0.5962795615196228\n","Step 80, Training Loss: 0.5925130248069763\n","Step 80, Training Loss: 0.5825238227844238\n","Step 90, Training Loss: 0.5604209899902344\n","Step 90, Training Loss: 0.5666843056678772\n"]},{"output_type":"stream","name":"stderr","text":["***** Running Evaluation *****\n","  Num examples = 377\n","  Batch size = 16\n"]},{"output_type":"stream","name":"stdout","text":["Step 100, Training Loss: 0.5645327568054199\n","Step 100, Training Loss: 0.5597255825996399\n","Step 110, Training Loss: 0.5623728632926941\n","Step 110, Training Loss: 0.5643550753593445\n","Step 120, Training Loss: 0.5655815005302429\n","Step 120, Training Loss: 0.541921854019165\n","Step 130, Training Loss: 0.5354541540145874\n","Step 130, Training Loss: 0.5470213890075684\n","Step 140, Training Loss: 0.53826504945755\n","Step 140, Training Loss: 0.5820143818855286\n","Step 150, Training Loss: 0.5675742626190186\n","Step 150, Training Loss: 0.5395730137825012\n","Step 160, Training Loss: 0.4966323971748352\n","Step 160, Training Loss: 0.5620198249816895\n","Step 170, Training Loss: 0.5045308470726013\n","Step 170, Training Loss: 0.5093820095062256\n","Step 180, Training Loss: 0.4987434446811676\n","Step 180, Training Loss: 0.4806446135044098\n"]},{"output_type":"stream","name":"stderr","text":["***** Running Evaluation *****\n","  Num examples = 377\n","  Batch size = 16\n"]},{"output_type":"stream","name":"stdout","text":["Step 190, Training Loss: 0.5416064858436584\n","Step 190, Training Loss: 0.49959680438041687\n","Step 200, Training Loss: 0.5287346243858337\n","Step 200, Training Loss: 0.4764552712440491\n","Step 210, Training Loss: 0.5465919971466064\n","Step 210, Training Loss: 0.509050190448761\n","Step 220, Training Loss: 0.4779624044895172\n","Step 220, Training Loss: 0.5161886811256409\n","Step 230, Training Loss: 0.5033215880393982\n","Step 230, Training Loss: 0.4914112687110901\n","Step 240, Training Loss: 0.5180352330207825\n","Step 240, Training Loss: 0.4895005226135254\n","Step 250, Training Loss: 0.542292594909668\n","Step 250, Training Loss: 0.47364184260368347\n","Step 260, Training Loss: 0.5316621661186218\n","Step 260, Training Loss: 0.5138059854507446\n","Step 270, Training Loss: 0.5517176985740662\n","Step 270, Training Loss: 0.5078423619270325\n","Step 280, Training Loss: 0.5583656430244446\n","Step 280, Training Loss: 0.5303906202316284\n"]},{"output_type":"stream","name":"stderr","text":["***** Running Evaluation *****\n","  Num examples = 377\n","  Batch size = 16\n"]},{"output_type":"stream","name":"stdout","text":["Step 290, Training Loss: 0.49446558952331543\n","Step 290, Training Loss: 0.5884785056114197\n","Step 300, Training Loss: 0.5319215059280396\n","Step 300, Training Loss: 0.5096115469932556\n","Step 310, Training Loss: 0.49529728293418884\n","Step 310, Training Loss: 0.46902474761009216\n","Step 320, Training Loss: 0.5067219138145447\n","Step 320, Training Loss: 0.5264977812767029\n","Step 330, Training Loss: 0.48869743943214417\n","Step 330, Training Loss: 0.49194908142089844\n","Step 340, Training Loss: 0.4990873336791992\n","Step 340, Training Loss: 0.5407105684280396\n","Step 350, Training Loss: 0.4656493663787842\n","Step 350, Training Loss: 0.4948163628578186\n","Step 360, Training Loss: 0.6065379977226257\n","Step 360, Training Loss: 0.5486618876457214\n","Step 370, Training Loss: 0.5043326020240784\n","Step 370, Training Loss: 0.5005491375923157\n"]},{"output_type":"stream","name":"stderr","text":["***** Running Evaluation *****\n","  Num examples = 377\n","  Batch size = 16\n"]},{"output_type":"stream","name":"stdout","text":["Step 380, Training Loss: 0.5421192049980164\n","Step 380, Training Loss: 0.49929314851760864\n","Step 390, Training Loss: 0.4638807475566864\n","Step 390, Training Loss: 0.5003483891487122\n","Step 400, Training Loss: 0.49127283692359924\n","Step 400, Training Loss: 0.5084178447723389\n","Step 410, Training Loss: 0.49265676736831665\n","Step 410, Training Loss: 0.47392377257347107\n","Step 420, Training Loss: 0.48944997787475586\n","Step 420, Training Loss: 0.49966755509376526\n","Step 430, Training Loss: 0.47014695405960083\n","Step 430, Training Loss: 0.5622682571411133\n","Step 440, Training Loss: 0.49654847383499146\n","Step 440, Training Loss: 0.481564998626709\n","Step 450, Training Loss: 0.590924859046936\n","Step 450, Training Loss: 0.4517286419868469\n","Step 460, Training Loss: 0.48718294501304626\n","Step 460, Training Loss: 0.4460635781288147\n"]},{"output_type":"stream","name":"stderr","text":["***** Running Evaluation *****\n","  Num examples = 377\n","  Batch size = 16\n"]},{"output_type":"stream","name":"stdout","text":["Step 470, Training Loss: 0.4729989469051361\n","Step 470, Training Loss: 0.4742017686367035\n","Step 470, Training Loss: 0.5847257971763611\n","Step 470, Training Loss: 0.5060647130012512\n","Step 470, Training Loss: 0.5194761157035828\n","Step 470, Training Loss: 0.4994642734527588\n","Step 470, Training Loss: 0.48744040727615356\n","Step 470, Training Loss: 0.44866490364074707\n","Step 470, Training Loss: 0.5290685892105103\n","Step 470, Training Loss: 0.4962773323059082\n","Step 470, Training Loss: 0.48540669679641724\n","Step 470, Training Loss: 0.47377750277519226\n","Step 470, Training Loss: 0.526543378829956\n","Step 470, Training Loss: 0.5370855927467346\n","Step 470, Training Loss: 0.5086618065834045\n","Step 470, Training Loss: 0.5025133490562439\n","Step 470, Training Loss: 0.47953739762306213\n","Step 470, Training Loss: 0.5390931367874146\n","Step 470, Training Loss: 0.5009843707084656\n","Step 470, Training Loss: 0.46025189757347107\n","Step 470, Training Loss: 0.48164159059524536\n","Step 470, Training Loss: 0.4996141493320465\n","Step 470, Training Loss: 0.4833022654056549\n","Step 470, Training Loss: 0.5257596373558044\n","Step 470, Training Loss: 0.4952374994754791\n","Step 470, Training Loss: 0.49224787950515747\n","Step 470, Training Loss: 0.4845273494720459\n","Step 480, Training Loss: 0.5634267926216125\n","Step 480, Training Loss: 0.5079190135002136\n","Step 490, Training Loss: 0.5341883897781372\n","Step 490, Training Loss: 0.5684971213340759\n"]},{"output_type":"stream","name":"stderr","text":["Saving model checkpoint to ./results/checkpoint-500\n","Configuration saved in ./results/checkpoint-500/config.json\n","Model weights saved in ./results/checkpoint-500/pytorch_model.bin\n"]},{"output_type":"stream","name":"stdout","text":["Step 500, Training Loss: 0.5063234567642212\n","Step 500, Training Loss: 0.49506378173828125\n","Step 510, Training Loss: 0.5676097869873047\n","Step 510, Training Loss: 0.45536550879478455\n","Step 520, Training Loss: 0.5144946575164795\n","Step 520, Training Loss: 0.5449985861778259\n","Step 530, Training Loss: 0.46465644240379333\n","Step 530, Training Loss: 0.5055258274078369\n","Step 540, Training Loss: 0.5363339781761169\n","Step 540, Training Loss: 0.6180896759033203\n","Step 550, Training Loss: 0.46524763107299805\n","Step 550, Training Loss: 0.501067578792572\n","Step 560, Training Loss: 0.5257135629653931\n","Step 560, Training Loss: 0.4615213871002197\n"]},{"output_type":"stream","name":"stderr","text":["***** Running Evaluation *****\n","  Num examples = 377\n","  Batch size = 16\n"]},{"output_type":"stream","name":"stdout","text":["Step 570, Training Loss: 0.5327291488647461\n","Step 570, Training Loss: 0.543739914894104\n","Step 580, Training Loss: 0.49129319190979004\n","Step 580, Training Loss: 0.5166996717453003\n","Step 590, Training Loss: 0.5192492008209229\n","Step 590, Training Loss: 0.5437281727790833\n","Step 600, Training Loss: 0.4905203878879547\n","Step 600, Training Loss: 0.5073189735412598\n","Step 610, Training Loss: 0.4760243594646454\n","Step 610, Training Loss: 0.5391536355018616\n","Step 620, Training Loss: 0.48407718539237976\n","Step 620, Training Loss: 0.5153955221176147\n","Step 630, Training Loss: 0.5187620520591736\n","Step 630, Training Loss: 0.4431534707546234\n","Step 640, Training Loss: 0.4806240499019623\n","Step 640, Training Loss: 0.5110552906990051\n","Step 650, Training Loss: 0.4474352300167084\n","Step 650, Training Loss: 0.48434996604919434\n"]},{"output_type":"stream","name":"stderr","text":["***** Running Evaluation *****\n","  Num examples = 377\n","  Batch size = 16\n"]},{"output_type":"stream","name":"stdout","text":["Step 660, Training Loss: 0.4751320481300354\n","Step 660, Training Loss: 0.4763510227203369\n","Step 670, Training Loss: 0.43637940287590027\n","Step 670, Training Loss: 0.4979022145271301\n","Step 680, Training Loss: 0.5400350093841553\n","Step 680, Training Loss: 0.5365294814109802\n","Step 690, Training Loss: 0.5191911458969116\n","Step 690, Training Loss: 0.43644481897354126\n","Step 700, Training Loss: 0.45236024260520935\n","Step 700, Training Loss: 0.5319591760635376\n","Step 710, Training Loss: 0.4912841022014618\n","Step 710, Training Loss: 0.4477628171443939\n","Step 720, Training Loss: 0.49110308289527893\n","Step 720, Training Loss: 0.4560639560222626\n","Step 730, Training Loss: 0.488360732793808\n","Step 730, Training Loss: 0.5045949816703796\n","Step 740, Training Loss: 0.46107837557792664\n","Step 740, Training Loss: 0.4891173839569092\n","Step 750, Training Loss: 0.44510617852211\n","Step 750, Training Loss: 0.4512630105018616\n"]},{"output_type":"stream","name":"stderr","text":["***** Running Evaluation *****\n","  Num examples = 377\n","  Batch size = 16\n"]},{"output_type":"stream","name":"stdout","text":["Step 760, Training Loss: 0.4518684446811676\n","Step 760, Training Loss: 0.49451571702957153\n","Step 770, Training Loss: 0.49886274337768555\n","Step 770, Training Loss: 0.5221221446990967\n","Step 780, Training Loss: 0.4407375454902649\n","Step 780, Training Loss: 0.4515303671360016\n","Step 790, Training Loss: 0.4631277024745941\n","Step 790, Training Loss: 0.4804862439632416\n","Step 800, Training Loss: 0.4945129454135895\n","Step 800, Training Loss: 0.4759669899940491\n","Step 810, Training Loss: 0.5549730658531189\n","Step 810, Training Loss: 0.556285560131073\n","Step 820, Training Loss: 0.600342333316803\n","Step 820, Training Loss: 0.4817364811897278\n","Step 830, Training Loss: 0.5257759094238281\n","Step 830, Training Loss: 0.4956853985786438\n","Step 840, Training Loss: 0.49158892035484314\n","Step 840, Training Loss: 0.4714427888393402\n"]},{"output_type":"stream","name":"stderr","text":["***** Running Evaluation *****\n","  Num examples = 377\n","  Batch size = 16\n"]},{"output_type":"stream","name":"stdout","text":["Step 850, Training Loss: 0.5399824976921082\n","Step 850, Training Loss: 0.4483472406864166\n","Step 860, Training Loss: 0.45337343215942383\n","Step 860, Training Loss: 0.47929152846336365\n","Step 870, Training Loss: 0.466815710067749\n","Step 870, Training Loss: 0.46586570143699646\n","Step 880, Training Loss: 0.439498633146286\n","Step 880, Training Loss: 0.5015830397605896\n","Step 890, Training Loss: 0.4544604420661926\n","Step 890, Training Loss: 0.46792760491371155\n","Step 900, Training Loss: 0.4350549876689911\n","Step 900, Training Loss: 0.4801730811595917\n","Step 910, Training Loss: 0.4180680215358734\n","Step 910, Training Loss: 0.4498126208782196\n","Step 920, Training Loss: 0.40043574571609497\n","Step 920, Training Loss: 0.49233198165893555\n","Step 930, Training Loss: 0.5066179633140564\n","Step 930, Training Loss: 0.48334333300590515\n"]},{"output_type":"stream","name":"stderr","text":["***** Running Evaluation *****\n","  Num examples = 377\n","  Batch size = 16\n"]},{"output_type":"stream","name":"stdout","text":["Step 940, Training Loss: 0.4637364447116852\n","Step 940, Training Loss: 0.578174352645874\n","Step 940, Training Loss: 0.48733434081077576\n","Step 940, Training Loss: 0.4956437647342682\n","Step 940, Training Loss: 0.46596136689186096\n","Step 940, Training Loss: 0.4695104658603668\n","Step 940, Training Loss: 0.4126923680305481\n","Step 940, Training Loss: 0.48931702971458435\n","Step 940, Training Loss: 0.4885431230068207\n","Step 940, Training Loss: 0.4500087797641754\n","Step 940, Training Loss: 0.44057461619377136\n","Step 940, Training Loss: 0.5041033625602722\n","Step 940, Training Loss: 0.5233299732208252\n","Step 940, Training Loss: 0.5068055987358093\n","Step 940, Training Loss: 0.4875808358192444\n","Step 940, Training Loss: 0.471489816904068\n","Step 940, Training Loss: 0.5332536101341248\n","Step 940, Training Loss: 0.47575074434280396\n","Step 940, Training Loss: 0.4331855773925781\n","Step 940, Training Loss: 0.4573636054992676\n","Step 940, Training Loss: 0.47895270586013794\n","Step 940, Training Loss: 0.45821309089660645\n","Step 940, Training Loss: 0.4987313449382782\n","Step 940, Training Loss: 0.4715630114078522\n"]},{"output_type":"stream","name":"stderr","text":["\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","***** Running Evaluation *****\n","  Num examples = 377\n","  Batch size = 16\n"]},{"output_type":"stream","name":"stdout","text":["Step 940, Training Loss: 0.4637364447116852\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='24' max='24' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [24/24 00:00]\n","    </div>\n","    "]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Step 940, Training Loss: 0.578174352645874\n","Step 940, Training Loss: 0.48733434081077576\n","Step 940, Training Loss: 0.4956437647342682\n","Step 940, Training Loss: 0.46596136689186096\n","Step 940, Training Loss: 0.4695104658603668\n","Step 940, Training Loss: 0.4126923680305481\n","Step 940, Training Loss: 0.48931702971458435\n","Step 940, Training Loss: 0.4885431230068207\n","Step 940, Training Loss: 0.4500087797641754\n","Step 940, Training Loss: 0.44057461619377136\n","Step 940, Training Loss: 0.5041033625602722\n","Step 940, Training Loss: 0.5233299732208252\n","Step 940, Training Loss: 0.5068055987358093\n","Step 940, Training Loss: 0.4875808358192444\n","Step 940, Training Loss: 0.471489816904068\n","Step 940, Training Loss: 0.5332536101341248\n","Step 940, Training Loss: 0.47575074434280396\n","Step 940, Training Loss: 0.4331855773925781\n","Step 940, Training Loss: 0.4573636054992676\n","Step 940, Training Loss: 0.47895270586013794\n","Step 940, Training Loss: 0.45821309089660645\n","Step 940, Training Loss: 0.4987313449382782\n","Step 940, Training Loss: 0.4715630114078522\n"]},{"output_type":"stream","name":"stderr","text":["[I 2024-12-01 15:55:00,417] Trial 0 finished with value: 0.0 and parameters: {'learning_rate': 1.5152299534954738e-05, 'batch_size': 8, 'num_train_epochs': 10, 'weight_decay': 0.000726413052968185}. Best is trial 0 with value: 0.0.\n","<ipython-input-10-747f1883d862>:2: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n","  learning_rate = trial.suggest_loguniform(\"learning_rate\", 1e-6, 5e-5)\n","<ipython-input-10-747f1883d862>:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n","  weight_decay = trial.suggest_loguniform(\"weight_decay\", 1e-4, 0.1)\n","PyTorch: setting up devices\n"]},{"output_type":"stream","name":"stdout","text":["Starting training...\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n","***** Running training *****\n","  Num examples = 1507\n","  Num Epochs = 19\n","  Instantaneous batch size per device = 16\n","  Total train batch size (w. parallel, distributed & accumulation) = 32\n","  Gradient Accumulation steps = 2\n","  Total optimization steps = 893\n"]},{"output_type":"stream","name":"stdout","text":["Step 0, Training Loss: 0.4784404933452606\n","Step 0, Training Loss: 0.4880254864692688\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='893' max='893' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [893/893 02:23, Epoch 18/19]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","      <th>F1</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>0</td>\n","      <td>0.465500</td>\n","      <td>0.476637</td>\n","      <td>0.000000</td>\n","      <td>1.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>1</td>\n","      <td>0.466800</td>\n","      <td>0.470993</td>\n","      <td>0.042440</td>\n","      <td>1.000000</td>\n","      <td>0.041872</td>\n","      <td>0.073352</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>0.467800</td>\n","      <td>0.466430</td>\n","      <td>0.079576</td>\n","      <td>0.990764</td>\n","      <td>0.076355</td>\n","      <td>0.120560</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>0.455100</td>\n","      <td>0.460746</td>\n","      <td>0.111406</td>\n","      <td>0.986864</td>\n","      <td>0.105911</td>\n","      <td>0.154053</td>\n","    </tr>\n","    <tr>\n","      <td>4</td>\n","      <td>0.451200</td>\n","      <td>0.456145</td>\n","      <td>0.124668</td>\n","      <td>0.988177</td>\n","      <td>0.118227</td>\n","      <td>0.166908</td>\n","    </tr>\n","    <tr>\n","      <td>5</td>\n","      <td>0.445800</td>\n","      <td>0.453019</td>\n","      <td>0.129973</td>\n","      <td>0.988409</td>\n","      <td>0.123153</td>\n","      <td>0.174267</td>\n","    </tr>\n","    <tr>\n","      <td>6</td>\n","      <td>0.429200</td>\n","      <td>0.447599</td>\n","      <td>0.177719</td>\n","      <td>0.935627</td>\n","      <td>0.169951</td>\n","      <td>0.240746</td>\n","    </tr>\n","    <tr>\n","      <td>7</td>\n","      <td>0.425800</td>\n","      <td>0.448234</td>\n","      <td>0.212202</td>\n","      <td>0.953925</td>\n","      <td>0.206897</td>\n","      <td>0.292594</td>\n","    </tr>\n","    <tr>\n","      <td>8</td>\n","      <td>0.426700</td>\n","      <td>0.441469</td>\n","      <td>0.262599</td>\n","      <td>0.880434</td>\n","      <td>0.253695</td>\n","      <td>0.338336</td>\n","    </tr>\n","    <tr>\n","      <td>9</td>\n","      <td>0.404500</td>\n","      <td>0.438284</td>\n","      <td>0.286472</td>\n","      <td>0.890535</td>\n","      <td>0.275862</td>\n","      <td>0.357461</td>\n","    </tr>\n","    <tr>\n","      <td>10</td>\n","      <td>0.415500</td>\n","      <td>0.436641</td>\n","      <td>0.318302</td>\n","      <td>0.872226</td>\n","      <td>0.310345</td>\n","      <td>0.371882</td>\n","    </tr>\n","    <tr>\n","      <td>11</td>\n","      <td>0.414400</td>\n","      <td>0.433061</td>\n","      <td>0.323607</td>\n","      <td>0.856951</td>\n","      <td>0.312808</td>\n","      <td>0.381054</td>\n","    </tr>\n","    <tr>\n","      <td>12</td>\n","      <td>0.408000</td>\n","      <td>0.435014</td>\n","      <td>0.320955</td>\n","      <td>0.863097</td>\n","      <td>0.315271</td>\n","      <td>0.384614</td>\n","    </tr>\n","    <tr>\n","      <td>13</td>\n","      <td>0.404300</td>\n","      <td>0.431482</td>\n","      <td>0.336870</td>\n","      <td>0.843823</td>\n","      <td>0.327586</td>\n","      <td>0.386315</td>\n","    </tr>\n","    <tr>\n","      <td>14</td>\n","      <td>0.398500</td>\n","      <td>0.432225</td>\n","      <td>0.334218</td>\n","      <td>0.848052</td>\n","      <td>0.327586</td>\n","      <td>0.384810</td>\n","    </tr>\n","    <tr>\n","      <td>15</td>\n","      <td>0.403200</td>\n","      <td>0.430187</td>\n","      <td>0.355438</td>\n","      <td>0.856376</td>\n","      <td>0.347291</td>\n","      <td>0.402423</td>\n","    </tr>\n","    <tr>\n","      <td>16</td>\n","      <td>0.406100</td>\n","      <td>0.429717</td>\n","      <td>0.352785</td>\n","      <td>0.854426</td>\n","      <td>0.344828</td>\n","      <td>0.399329</td>\n","    </tr>\n","    <tr>\n","      <td>17</td>\n","      <td>0.392000</td>\n","      <td>0.429362</td>\n","      <td>0.347480</td>\n","      <td>0.838776</td>\n","      <td>0.337438</td>\n","      <td>0.389652</td>\n","    </tr>\n","    <tr>\n","      <td>18</td>\n","      <td>0.391700</td>\n","      <td>0.429510</td>\n","      <td>0.347480</td>\n","      <td>0.838776</td>\n","      <td>0.337438</td>\n","      <td>0.389652</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Step 10, Training Loss: 0.4441073536872864\n","Step 10, Training Loss: 0.4469880759716034\n","Step 20, Training Loss: 0.48325586318969727\n","Step 20, Training Loss: 0.4939540922641754\n","Step 30, Training Loss: 0.48912134766578674\n","Step 30, Training Loss: 0.45046958327293396\n","Step 40, Training Loss: 0.4435022473335266\n","Step 40, Training Loss: 0.430385559797287\n"]},{"output_type":"stream","name":"stderr","text":["***** Running Evaluation *****\n","  Num examples = 377\n","  Batch size = 32\n"]},{"output_type":"stream","name":"stdout","text":["Step 50, Training Loss: 0.46223387122154236\n","Step 50, Training Loss: 0.4190467894077301\n","Step 60, Training Loss: 0.4584740698337555\n","Step 60, Training Loss: 0.517063558101654\n","Step 70, Training Loss: 0.49811407923698425\n","Step 70, Training Loss: 0.44949886202812195\n","Step 80, Training Loss: 0.4727723300457001\n","Step 80, Training Loss: 0.4885809123516083\n","Step 90, Training Loss: 0.4068003296852112\n","Step 90, Training Loss: 0.45662060379981995\n"]},{"output_type":"stream","name":"stderr","text":["***** Running Evaluation *****\n","  Num examples = 377\n","  Batch size = 32\n"]},{"output_type":"stream","name":"stdout","text":["Step 100, Training Loss: 0.4576101005077362\n","Step 100, Training Loss: 0.43633508682250977\n","Step 110, Training Loss: 0.44734692573547363\n","Step 110, Training Loss: 0.47674161195755005\n","Step 120, Training Loss: 0.4610505998134613\n","Step 120, Training Loss: 0.4997108578681946\n","Step 130, Training Loss: 0.4941297471523285\n","Step 130, Training Loss: 0.48351699113845825\n","Step 140, Training Loss: 0.5161568522453308\n","Step 140, Training Loss: 0.3916967809200287\n"]},{"output_type":"stream","name":"stderr","text":["***** Running Evaluation *****\n","  Num examples = 377\n","  Batch size = 32\n"]},{"output_type":"stream","name":"stdout","text":["Step 150, Training Loss: 0.4870852530002594\n","Step 150, Training Loss: 0.37875881791114807\n","Step 160, Training Loss: 0.45923957228660583\n","Step 160, Training Loss: 0.434336394071579\n","Step 170, Training Loss: 0.4465401768684387\n","Step 170, Training Loss: 0.4933198094367981\n","Step 180, Training Loss: 0.5162195563316345\n","Step 180, Training Loss: 0.4495178163051605\n"]},{"output_type":"stream","name":"stderr","text":["***** Running Evaluation *****\n","  Num examples = 377\n","  Batch size = 32\n"]},{"output_type":"stream","name":"stdout","text":["Step 190, Training Loss: 0.4613204896450043\n","Step 190, Training Loss: 0.4448660910129547\n","Step 200, Training Loss: 0.4371315538883209\n","Step 200, Training Loss: 0.42123302817344666\n","Step 210, Training Loss: 0.4282991588115692\n","Step 210, Training Loss: 0.41603538393974304\n","Step 220, Training Loss: 0.423922061920166\n","Step 220, Training Loss: 0.42686596512794495\n","Step 230, Training Loss: 0.40397071838378906\n","Step 230, Training Loss: 0.5045045614242554\n"]},{"output_type":"stream","name":"stderr","text":["***** Running Evaluation *****\n","  Num examples = 377\n","  Batch size = 32\n"]},{"output_type":"stream","name":"stdout","text":["Step 240, Training Loss: 0.4783114492893219\n","Step 240, Training Loss: 0.4575665593147278\n","Step 250, Training Loss: 0.4407094419002533\n","Step 250, Training Loss: 0.40212973952293396\n","Step 260, Training Loss: 0.472532719373703\n","Step 260, Training Loss: 0.4523443877696991\n","Step 270, Training Loss: 0.5162310600280762\n","Step 270, Training Loss: 0.4654636085033417\n","Step 280, Training Loss: 0.4483925998210907\n","Step 280, Training Loss: 0.4040379226207733\n"]},{"output_type":"stream","name":"stderr","text":["***** Running Evaluation *****\n","  Num examples = 377\n","  Batch size = 32\n"]},{"output_type":"stream","name":"stdout","text":["Step 290, Training Loss: 0.44110894203186035\n","Step 290, Training Loss: 0.449441522359848\n","Step 300, Training Loss: 0.4608229100704193\n","Step 300, Training Loss: 0.4152931272983551\n","Step 310, Training Loss: 0.47782060503959656\n","Step 310, Training Loss: 0.4028148651123047\n","Step 320, Training Loss: 0.4371945858001709\n","Step 320, Training Loss: 0.4405089020729065\n"]},{"output_type":"stream","name":"stderr","text":["***** Running Evaluation *****\n","  Num examples = 377\n","  Batch size = 32\n"]},{"output_type":"stream","name":"stdout","text":["Step 330, Training Loss: 0.4205656051635742\n","Step 330, Training Loss: 0.40412279963493347\n","Step 340, Training Loss: 0.49120068550109863\n","Step 340, Training Loss: 0.3888033926486969\n","Step 350, Training Loss: 0.42132148146629333\n","Step 350, Training Loss: 0.5376360416412354\n","Step 360, Training Loss: 0.4050309360027313\n","Step 360, Training Loss: 0.3801077604293823\n","Step 370, Training Loss: 0.43100452423095703\n","Step 370, Training Loss: 0.40099143981933594\n"]},{"output_type":"stream","name":"stderr","text":["***** Running Evaluation *****\n","  Num examples = 377\n","  Batch size = 32\n"]},{"output_type":"stream","name":"stdout","text":["Step 380, Training Loss: 0.42450419068336487\n","Step 380, Training Loss: 0.47066211700439453\n","Step 390, Training Loss: 0.38639602065086365\n","Step 390, Training Loss: 0.3807215392589569\n","Step 400, Training Loss: 0.42062973976135254\n","Step 400, Training Loss: 0.3571441173553467\n","Step 410, Training Loss: 0.4680660367012024\n","Step 410, Training Loss: 0.4233337938785553\n","Step 420, Training Loss: 0.4318850636482239\n","Step 420, Training Loss: 0.4185600280761719\n"]},{"output_type":"stream","name":"stderr","text":["***** Running Evaluation *****\n","  Num examples = 377\n","  Batch size = 32\n"]},{"output_type":"stream","name":"stdout","text":["Step 430, Training Loss: 0.41960570216178894\n","Step 430, Training Loss: 0.46559903025627136\n","Step 440, Training Loss: 0.39782509207725525\n","Step 440, Training Loss: 0.4097665846347809\n","Step 450, Training Loss: 0.41294828057289124\n","Step 450, Training Loss: 0.3963266611099243\n","Step 460, Training Loss: 0.37690111994743347\n","Step 460, Training Loss: 0.36334118247032166\n"]},{"output_type":"stream","name":"stderr","text":["***** Running Evaluation *****\n","  Num examples = 377\n","  Batch size = 32\n"]},{"output_type":"stream","name":"stdout","text":["Step 470, Training Loss: 0.39406898617744446\n","Step 470, Training Loss: 0.5042787790298462\n","Step 470, Training Loss: 0.44125768542289734\n","Step 470, Training Loss: 0.4106390178203583\n","Step 470, Training Loss: 0.38823676109313965\n","Step 470, Training Loss: 0.42969703674316406\n","Step 470, Training Loss: 0.41997766494750977\n","Step 470, Training Loss: 0.4896523654460907\n","Step 470, Training Loss: 0.4492776095867157\n","Step 470, Training Loss: 0.4835112690925598\n","Step 470, Training Loss: 0.3910931944847107\n","Step 470, Training Loss: 0.41988736391067505\n","Step 470, Training Loss: 0.4301120936870575\n","Step 470, Training Loss: 0.40298891067504883\n","Step 470, Training Loss: 0.4083671271800995\n","Step 480, Training Loss: 0.47117820382118225\n","Step 480, Training Loss: 0.47434720396995544\n","Step 490, Training Loss: 0.3361845910549164\n","Step 490, Training Loss: 0.41033217310905457\n"]},{"output_type":"stream","name":"stderr","text":["Saving model checkpoint to ./results/checkpoint-500\n","Configuration saved in ./results/checkpoint-500/config.json\n","Model weights saved in ./results/checkpoint-500/pytorch_model.bin\n"]},{"output_type":"stream","name":"stdout","text":["Step 500, Training Loss: 0.38553592562675476\n","Step 500, Training Loss: 0.44969987869262695\n","Step 510, Training Loss: 0.4091077744960785\n","Step 510, Training Loss: 0.43227139115333557\n"]},{"output_type":"stream","name":"stderr","text":["***** Running Evaluation *****\n","  Num examples = 377\n","  Batch size = 32\n"]},{"output_type":"stream","name":"stdout","text":["Step 520, Training Loss: 0.4103271961212158\n","Step 520, Training Loss: 0.35424870252609253\n","Step 530, Training Loss: 0.4206030070781708\n","Step 530, Training Loss: 0.4107479155063629\n","Step 540, Training Loss: 0.36970531940460205\n","Step 540, Training Loss: 0.4292783737182617\n","Step 550, Training Loss: 0.41055184602737427\n","Step 550, Training Loss: 0.48223647475242615\n","Step 560, Training Loss: 0.38049760460853577\n","Step 560, Training Loss: 0.4494995176792145\n"]},{"output_type":"stream","name":"stderr","text":["***** Running Evaluation *****\n","  Num examples = 377\n","  Batch size = 32\n"]},{"output_type":"stream","name":"stdout","text":["Step 570, Training Loss: 0.37858307361602783\n","Step 570, Training Loss: 0.4066358208656311\n","Step 580, Training Loss: 0.4205434024333954\n","Step 580, Training Loss: 0.38745346665382385\n","Step 590, Training Loss: 0.41275960206985474\n","Step 590, Training Loss: 0.399122953414917\n","Step 600, Training Loss: 0.43201103806495667\n","Step 600, Training Loss: 0.40690404176712036\n","Step 610, Training Loss: 0.4373633861541748\n","Step 610, Training Loss: 0.3698769509792328\n"]},{"output_type":"stream","name":"stderr","text":["***** Running Evaluation *****\n","  Num examples = 377\n","  Batch size = 32\n"]},{"output_type":"stream","name":"stdout","text":["Step 620, Training Loss: 0.3734576106071472\n","Step 620, Training Loss: 0.457144170999527\n","Step 630, Training Loss: 0.439719021320343\n","Step 630, Training Loss: 0.4706120193004608\n","Step 640, Training Loss: 0.430163711309433\n","Step 640, Training Loss: 0.3696315288543701\n","Step 650, Training Loss: 0.3844103515148163\n","Step 650, Training Loss: 0.3631878197193146\n"]},{"output_type":"stream","name":"stderr","text":["***** Running Evaluation *****\n","  Num examples = 377\n","  Batch size = 32\n"]},{"output_type":"stream","name":"stdout","text":["Step 660, Training Loss: 0.41012588143348694\n","Step 660, Training Loss: 0.41533365845680237\n","Step 670, Training Loss: 0.4287734031677246\n","Step 670, Training Loss: 0.37894487380981445\n","Step 680, Training Loss: 0.4017728865146637\n","Step 680, Training Loss: 0.3592158854007721\n","Step 690, Training Loss: 0.34852612018585205\n","Step 690, Training Loss: 0.3690485656261444\n","Step 700, Training Loss: 0.4305679500102997\n","Step 700, Training Loss: 0.35505393147468567\n"]},{"output_type":"stream","name":"stderr","text":["***** Running Evaluation *****\n","  Num examples = 377\n","  Batch size = 32\n"]},{"output_type":"stream","name":"stdout","text":["Step 710, Training Loss: 0.4324208199977875\n","Step 710, Training Loss: 0.4189443290233612\n","Step 720, Training Loss: 0.3149876892566681\n","Step 720, Training Loss: 0.41614553332328796\n","Step 730, Training Loss: 0.4053981900215149\n","Step 730, Training Loss: 0.3481031358242035\n","Step 740, Training Loss: 0.3803376257419586\n","Step 740, Training Loss: 0.3960215151309967\n","Step 750, Training Loss: 0.3826398551464081\n","Step 750, Training Loss: 0.43741416931152344\n"]},{"output_type":"stream","name":"stderr","text":["***** Running Evaluation *****\n","  Num examples = 377\n","  Batch size = 32\n"]},{"output_type":"stream","name":"stdout","text":["Step 760, Training Loss: 0.37221500277519226\n","Step 760, Training Loss: 0.4304787814617157\n","Step 770, Training Loss: 0.3543831408023834\n","Step 770, Training Loss: 0.40129175782203674\n","Step 780, Training Loss: 0.34434428811073303\n","Step 780, Training Loss: 0.4317512512207031\n","Step 790, Training Loss: 0.36174890398979187\n","Step 790, Training Loss: 0.37596338987350464\n"]},{"output_type":"stream","name":"stderr","text":["***** Running Evaluation *****\n","  Num examples = 377\n","  Batch size = 32\n"]},{"output_type":"stream","name":"stdout","text":["Step 800, Training Loss: 0.3985251784324646\n","Step 800, Training Loss: 0.41025781631469727\n","Step 810, Training Loss: 0.40557166934013367\n","Step 810, Training Loss: 0.45077037811279297\n","Step 820, Training Loss: 0.33399665355682373\n","Step 820, Training Loss: 0.4151587188243866\n","Step 830, Training Loss: 0.32855066657066345\n","Step 830, Training Loss: 0.37760958075523376\n","Step 840, Training Loss: 0.4112780690193176\n","Step 840, Training Loss: 0.39483460783958435\n"]},{"output_type":"stream","name":"stderr","text":["***** Running Evaluation *****\n","  Num examples = 377\n","  Batch size = 32\n"]},{"output_type":"stream","name":"stdout","text":["Step 850, Training Loss: 0.3507697284221649\n","Step 850, Training Loss: 0.37433376908302307\n","Step 860, Training Loss: 0.41136646270751953\n","Step 860, Training Loss: 0.4412544369697571\n","Step 870, Training Loss: 0.44887858629226685\n","Step 870, Training Loss: 0.45440927147865295\n","Step 880, Training Loss: 0.36060044169425964\n","Step 880, Training Loss: 0.4162479043006897\n","Step 890, Training Loss: 0.3761312663555145\n","Step 890, Training Loss: 0.39906883239746094\n"]},{"output_type":"stream","name":"stderr","text":["***** Running Evaluation *****\n","  Num examples = 377\n","  Batch size = 32\n","\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","***** Running Evaluation *****\n","  Num examples = 377\n","  Batch size = 32\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='12' max='12' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [12/12 00:00]\n","    </div>\n","    "]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["[I 2024-12-01 15:57:25,016] Trial 1 finished with value: 0.38965160277004357 and parameters: {'learning_rate': 6.928853470926913e-06, 'batch_size': 16, 'num_train_epochs': 19, 'weight_decay': 0.003232874455291587}. Best is trial 1 with value: 0.38965160277004357.\n","<ipython-input-10-747f1883d862>:2: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n","  learning_rate = trial.suggest_loguniform(\"learning_rate\", 1e-6, 5e-5)\n","<ipython-input-10-747f1883d862>:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n","  weight_decay = trial.suggest_loguniform(\"weight_decay\", 1e-4, 0.1)\n","PyTorch: setting up devices\n","/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n","***** Running training *****\n","  Num examples = 1507\n","  Num Epochs = 18\n","  Instantaneous batch size per device = 4\n","  Total train batch size (w. parallel, distributed & accumulation) = 8\n","  Gradient Accumulation steps = 2\n","  Total optimization steps = 3384\n"]},{"output_type":"stream","name":"stdout","text":["Starting training...\n","Step 0, Training Loss: 0.34883221983909607\n","Step 0, Training Loss: 0.42961379885673523\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='3384' max='3384' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [3384/3384 03:01, Epoch 17/18]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","      <th>F1</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>0</td>\n","      <td>0.379100</td>\n","      <td>0.399563</td>\n","      <td>0.400531</td>\n","      <td>0.867578</td>\n","      <td>0.396552</td>\n","      <td>0.404933</td>\n","    </tr>\n","    <tr>\n","      <td>1</td>\n","      <td>0.333500</td>\n","      <td>0.388065</td>\n","      <td>0.435013</td>\n","      <td>0.827925</td>\n","      <td>0.428571</td>\n","      <td>0.481954</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>0.333100</td>\n","      <td>0.375435</td>\n","      <td>0.482759</td>\n","      <td>0.823317</td>\n","      <td>0.472906</td>\n","      <td>0.504246</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>0.286900</td>\n","      <td>0.378212</td>\n","      <td>0.435013</td>\n","      <td>0.783575</td>\n","      <td>0.428571</td>\n","      <td>0.427800</td>\n","    </tr>\n","    <tr>\n","      <td>4</td>\n","      <td>0.314800</td>\n","      <td>0.407961</td>\n","      <td>0.461538</td>\n","      <td>0.751151</td>\n","      <td>0.463054</td>\n","      <td>0.496689</td>\n","    </tr>\n","    <tr>\n","      <td>5</td>\n","      <td>0.260700</td>\n","      <td>0.386793</td>\n","      <td>0.543767</td>\n","      <td>0.728397</td>\n","      <td>0.554187</td>\n","      <td>0.581578</td>\n","    </tr>\n","    <tr>\n","      <td>6</td>\n","      <td>0.282100</td>\n","      <td>0.395893</td>\n","      <td>0.559682</td>\n","      <td>0.737450</td>\n","      <td>0.568966</td>\n","      <td>0.594198</td>\n","    </tr>\n","    <tr>\n","      <td>7</td>\n","      <td>0.242100</td>\n","      <td>0.375567</td>\n","      <td>0.572944</td>\n","      <td>0.730922</td>\n","      <td>0.573892</td>\n","      <td>0.596342</td>\n","    </tr>\n","    <tr>\n","      <td>8</td>\n","      <td>0.161000</td>\n","      <td>0.376063</td>\n","      <td>0.594164</td>\n","      <td>0.678835</td>\n","      <td>0.608374</td>\n","      <td>0.623670</td>\n","    </tr>\n","    <tr>\n","      <td>9</td>\n","      <td>0.187200</td>\n","      <td>0.379486</td>\n","      <td>0.580902</td>\n","      <td>0.661532</td>\n","      <td>0.603448</td>\n","      <td>0.619796</td>\n","    </tr>\n","    <tr>\n","      <td>10</td>\n","      <td>0.165500</td>\n","      <td>0.379281</td>\n","      <td>0.599469</td>\n","      <td>0.663117</td>\n","      <td>0.608374</td>\n","      <td>0.622996</td>\n","    </tr>\n","    <tr>\n","      <td>11</td>\n","      <td>0.169100</td>\n","      <td>0.371807</td>\n","      <td>0.591512</td>\n","      <td>0.680150</td>\n","      <td>0.605911</td>\n","      <td>0.632093</td>\n","    </tr>\n","    <tr>\n","      <td>12</td>\n","      <td>0.165500</td>\n","      <td>0.389169</td>\n","      <td>0.591512</td>\n","      <td>0.656928</td>\n","      <td>0.600985</td>\n","      <td>0.614947</td>\n","    </tr>\n","    <tr>\n","      <td>13</td>\n","      <td>0.165700</td>\n","      <td>0.384190</td>\n","      <td>0.604775</td>\n","      <td>0.662380</td>\n","      <td>0.623153</td>\n","      <td>0.632760</td>\n","    </tr>\n","    <tr>\n","      <td>14</td>\n","      <td>0.154400</td>\n","      <td>0.402677</td>\n","      <td>0.580902</td>\n","      <td>0.679715</td>\n","      <td>0.600985</td>\n","      <td>0.616678</td>\n","    </tr>\n","    <tr>\n","      <td>15</td>\n","      <td>0.132800</td>\n","      <td>0.399587</td>\n","      <td>0.588859</td>\n","      <td>0.677274</td>\n","      <td>0.610837</td>\n","      <td>0.621272</td>\n","    </tr>\n","    <tr>\n","      <td>16</td>\n","      <td>0.142200</td>\n","      <td>0.412435</td>\n","      <td>0.578249</td>\n","      <td>0.670675</td>\n","      <td>0.615764</td>\n","      <td>0.619867</td>\n","    </tr>\n","    <tr>\n","      <td>17</td>\n","      <td>0.140900</td>\n","      <td>0.399850</td>\n","      <td>0.588859</td>\n","      <td>0.672791</td>\n","      <td>0.610837</td>\n","      <td>0.624204</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Step 10, Training Loss: 0.42806631326675415\n","Step 10, Training Loss: 0.41748857498168945\n","Step 20, Training Loss: 0.49853500723838806\n","Step 20, Training Loss: 0.35576677322387695\n","Step 30, Training Loss: 0.34165847301483154\n","Step 30, Training Loss: 0.46740278601646423\n","Step 40, Training Loss: 0.38288208842277527\n","Step 40, Training Loss: 0.31573230028152466\n","Step 50, Training Loss: 0.37337419390678406\n","Step 50, Training Loss: 0.37491127848625183\n","Step 60, Training Loss: 0.39428722858428955\n","Step 60, Training Loss: 0.7335578203201294\n","Step 70, Training Loss: 0.47268277406692505\n","Step 70, Training Loss: 0.3203963041305542\n","Step 80, Training Loss: 0.4153439700603485\n","Step 80, Training Loss: 0.46763715147972107\n","Step 90, Training Loss: 0.4107036292552948\n","Step 90, Training Loss: 0.5939322113990784\n","Step 100, Training Loss: 0.37154603004455566\n","Step 100, Training Loss: 0.3984920084476471\n","Step 110, Training Loss: 0.32665395736694336\n","Step 110, Training Loss: 0.24874423444271088\n","Step 120, Training Loss: 0.29140159487724304\n","Step 120, Training Loss: 0.518756091594696\n","Step 130, Training Loss: 0.27565842866897583\n","Step 130, Training Loss: 0.4616363048553467\n","Step 140, Training Loss: 0.4710833728313446\n","Step 140, Training Loss: 0.30094727873802185\n","Step 150, Training Loss: 0.4195355474948883\n","Step 150, Training Loss: 0.3031691312789917\n","Step 160, Training Loss: 0.30575981736183167\n","Step 160, Training Loss: 0.38885340094566345\n","Step 170, Training Loss: 0.4192209839820862\n","Step 170, Training Loss: 0.4089716970920563\n","Step 180, Training Loss: 0.3383047580718994\n","Step 180, Training Loss: 0.292826384305954\n"]},{"output_type":"stream","name":"stderr","text":["***** Running Evaluation *****\n","  Num examples = 377\n","  Batch size = 8\n"]},{"output_type":"stream","name":"stdout","text":["Step 190, Training Loss: 0.47326430678367615\n","Step 190, Training Loss: 0.6954504251480103\n","Step 200, Training Loss: 0.3487928509712219\n","Step 200, Training Loss: 0.45783287286758423\n","Step 210, Training Loss: 0.3465500771999359\n","Step 210, Training Loss: 0.3382950723171234\n","Step 220, Training Loss: 0.35763683915138245\n","Step 220, Training Loss: 0.42346009612083435\n","Step 230, Training Loss: 0.24529047310352325\n","Step 230, Training Loss: 0.2814400792121887\n","Step 240, Training Loss: 0.3977111577987671\n","Step 240, Training Loss: 0.45465776324272156\n","Step 250, Training Loss: 0.45072221755981445\n","Step 250, Training Loss: 0.3442583382129669\n","Step 260, Training Loss: 0.32650360465049744\n","Step 260, Training Loss: 0.33628174662590027\n","Step 270, Training Loss: 0.4521125257015228\n","Step 270, Training Loss: 0.38297179341316223\n","Step 280, Training Loss: 0.39588767290115356\n","Step 280, Training Loss: 0.43214449286460876\n","Step 290, Training Loss: 0.4586661458015442\n","Step 290, Training Loss: 0.25349292159080505\n","Step 300, Training Loss: 0.41354379057884216\n","Step 300, Training Loss: 0.6425920128822327\n","Step 310, Training Loss: 0.24871502816677094\n","Step 310, Training Loss: 0.555878758430481\n","Step 320, Training Loss: 0.2620832026004791\n","Step 320, Training Loss: 0.23130841553211212\n","Step 330, Training Loss: 0.2889493703842163\n","Step 330, Training Loss: 0.4435139298439026\n","Step 340, Training Loss: 0.12960858643054962\n","Step 340, Training Loss: 0.2851349413394928\n","Step 350, Training Loss: 0.21958433091640472\n","Step 350, Training Loss: 0.46188923716545105\n","Step 360, Training Loss: 0.20809152722358704\n","Step 360, Training Loss: 0.36302557587623596\n","Step 370, Training Loss: 0.20764413475990295\n","Step 370, Training Loss: 0.5577918887138367\n"]},{"output_type":"stream","name":"stderr","text":["***** Running Evaluation *****\n","  Num examples = 377\n","  Batch size = 8\n"]},{"output_type":"stream","name":"stdout","text":["Step 380, Training Loss: 0.40936514735221863\n","Step 380, Training Loss: 0.32643139362335205\n","Step 390, Training Loss: 0.13625231385231018\n","Step 390, Training Loss: 0.43663451075553894\n","Step 400, Training Loss: 0.34889522194862366\n","Step 400, Training Loss: 0.3447906970977783\n","Step 410, Training Loss: 0.33588871359825134\n","Step 410, Training Loss: 0.4114561080932617\n","Step 420, Training Loss: 0.4481127858161926\n","Step 420, Training Loss: 0.4060768187046051\n","Step 430, Training Loss: 0.1604420393705368\n","Step 430, Training Loss: 0.11826598644256592\n","Step 440, Training Loss: 0.1198650375008583\n","Step 440, Training Loss: 0.29136523604393005\n","Step 450, Training Loss: 0.4067234694957733\n","Step 450, Training Loss: 0.29749980568885803\n","Step 460, Training Loss: 0.29790574312210083\n","Step 460, Training Loss: 0.2477789968252182\n","Step 470, Training Loss: 0.2680325508117676\n","Step 470, Training Loss: 0.3062903583049774\n","Step 480, Training Loss: 0.31817442178726196\n","Step 480, Training Loss: 0.33628031611442566\n","Step 490, Training Loss: 0.45823660492897034\n","Step 490, Training Loss: 0.2557046711444855\n"]},{"output_type":"stream","name":"stderr","text":["Saving model checkpoint to ./results/checkpoint-500\n","Configuration saved in ./results/checkpoint-500/config.json\n","Model weights saved in ./results/checkpoint-500/pytorch_model.bin\n"]},{"output_type":"stream","name":"stdout","text":["Step 500, Training Loss: 0.2847095727920532\n","Step 500, Training Loss: 0.47700396180152893\n","Step 510, Training Loss: 0.4019308090209961\n","Step 510, Training Loss: 0.23329758644104004\n","Step 520, Training Loss: 0.3658970594406128\n","Step 520, Training Loss: 0.5446311831474304\n","Step 530, Training Loss: 0.2591191828250885\n","Step 530, Training Loss: 0.5600654482841492\n","Step 540, Training Loss: 0.4696950912475586\n","Step 540, Training Loss: 0.4587073028087616\n","Step 550, Training Loss: 0.3394399583339691\n","Step 550, Training Loss: 0.308773010969162\n","Step 560, Training Loss: 0.35457512736320496\n","Step 560, Training Loss: 0.35222315788269043\n"]},{"output_type":"stream","name":"stderr","text":["***** Running Evaluation *****\n","  Num examples = 377\n","  Batch size = 8\n"]},{"output_type":"stream","name":"stdout","text":["Step 570, Training Loss: 0.3591131865978241\n","Step 570, Training Loss: 0.34893742203712463\n","Step 580, Training Loss: 0.16736005246639252\n","Step 580, Training Loss: 0.3307572603225708\n","Step 590, Training Loss: 0.1965988129377365\n","Step 590, Training Loss: 0.37439796328544617\n","Step 600, Training Loss: 0.21753299236297607\n","Step 600, Training Loss: 0.4719008505344391\n","Step 610, Training Loss: 0.4011509418487549\n","Step 610, Training Loss: 0.23353305459022522\n","Step 620, Training Loss: 0.14409230649471283\n","Step 620, Training Loss: 0.3586183488368988\n","Step 630, Training Loss: 0.41727563738822937\n","Step 630, Training Loss: 0.45078736543655396\n","Step 640, Training Loss: 0.23494358360767365\n","Step 640, Training Loss: 0.300966739654541\n","Step 650, Training Loss: 0.4624410569667816\n","Step 650, Training Loss: 0.3635241687297821\n","Step 660, Training Loss: 0.15484924614429474\n","Step 660, Training Loss: 0.4584232270717621\n","Step 670, Training Loss: 0.4904756546020508\n","Step 670, Training Loss: 0.16009573638439178\n","Step 680, Training Loss: 0.22158317267894745\n","Step 680, Training Loss: 0.2622871696949005\n","Step 690, Training Loss: 0.3885782063007355\n","Step 690, Training Loss: 0.18016380071640015\n","Step 700, Training Loss: 0.21743391454219818\n","Step 700, Training Loss: 0.2346329241991043\n","Step 710, Training Loss: 0.1904279738664627\n","Step 710, Training Loss: 0.2660195529460907\n","Step 720, Training Loss: 0.22924871742725372\n","Step 720, Training Loss: 0.4968155026435852\n","Step 730, Training Loss: 0.21222691237926483\n","Step 730, Training Loss: 0.3217630684375763\n","Step 740, Training Loss: 0.5305696725845337\n","Step 740, Training Loss: 0.26144349575042725\n","Step 750, Training Loss: 0.2553160488605499\n","Step 750, Training Loss: 0.21970818936824799\n"]},{"output_type":"stream","name":"stderr","text":["***** Running Evaluation *****\n","  Num examples = 377\n","  Batch size = 8\n"]},{"output_type":"stream","name":"stdout","text":["Step 760, Training Loss: 0.21272774040699005\n","Step 760, Training Loss: 0.32522520422935486\n","Step 770, Training Loss: 0.19297783076763153\n","Step 770, Training Loss: 0.2629316747188568\n","Step 780, Training Loss: 0.1477128118276596\n","Step 780, Training Loss: 0.13542059063911438\n","Step 790, Training Loss: 0.47280094027519226\n","Step 790, Training Loss: 0.3144403398036957\n","Step 800, Training Loss: 0.5699553489685059\n","Step 800, Training Loss: 0.1850229799747467\n","Step 810, Training Loss: 0.0738215297460556\n","Step 810, Training Loss: 0.15269553661346436\n","Step 820, Training Loss: 0.17876076698303223\n","Step 820, Training Loss: 0.323625385761261\n","Step 830, Training Loss: 0.5733885169029236\n","Step 830, Training Loss: 0.5133291482925415\n","Step 840, Training Loss: 0.26574090123176575\n","Step 840, Training Loss: 0.14665836095809937\n","Step 850, Training Loss: 0.31998372077941895\n","Step 850, Training Loss: 0.06622067838907242\n","Step 860, Training Loss: 0.13573408126831055\n","Step 860, Training Loss: 0.21272125840187073\n","Step 870, Training Loss: 0.47208085656166077\n","Step 870, Training Loss: 0.42938119173049927\n","Step 880, Training Loss: 0.25772711634635925\n","Step 880, Training Loss: 0.40582379698753357\n","Step 890, Training Loss: 0.2784847617149353\n","Step 890, Training Loss: 0.36970266699790955\n","Step 900, Training Loss: 0.3687155544757843\n","Step 900, Training Loss: 0.3656242787837982\n","Step 910, Training Loss: 0.38985008001327515\n","Step 910, Training Loss: 0.29161950945854187\n","Step 920, Training Loss: 0.21533949673175812\n","Step 920, Training Loss: 0.19732771813869476\n","Step 930, Training Loss: 0.27481234073638916\n","Step 930, Training Loss: 0.501161515712738\n"]},{"output_type":"stream","name":"stderr","text":["***** Running Evaluation *****\n","  Num examples = 377\n","  Batch size = 8\n"]},{"output_type":"stream","name":"stdout","text":["Step 940, Training Loss: 0.171397402882576\n","Step 940, Training Loss: 0.5363752841949463\n","Step 940, Training Loss: 0.5698155760765076\n","Step 940, Training Loss: 0.39603859186172485\n","Step 940, Training Loss: 0.8976143002510071\n","Step 940, Training Loss: 0.5040852427482605\n","Step 940, Training Loss: 0.437002569437027\n","Step 940, Training Loss: 0.2641752064228058\n","Step 940, Training Loss: 0.37427908182144165\n","Step 940, Training Loss: 0.18831156194210052\n","Step 940, Training Loss: 0.2916729152202606\n","Step 940, Training Loss: 0.5283047556877136\n","Step 940, Training Loss: 0.2807368338108063\n","Step 940, Training Loss: 0.3259529769420624\n","Step 940, Training Loss: 0.11568167060613632\n","Step 940, Training Loss: 0.292428582906723\n","Step 940, Training Loss: 0.4800871014595032\n","Step 940, Training Loss: 0.5467602610588074\n","Step 940, Training Loss: 0.5499116778373718\n","Step 940, Training Loss: 0.23489737510681152\n","Step 940, Training Loss: 0.2927248477935791\n","Step 940, Training Loss: 0.21767745912075043\n","Step 940, Training Loss: 0.21570296585559845\n","Step 940, Training Loss: 0.6133518815040588\n","Step 940, Training Loss: 0.3603731095790863\n","Step 940, Training Loss: 0.40071025490760803\n","Step 940, Training Loss: 0.408152312040329\n","Step 940, Training Loss: 0.5986025929450989\n","Step 940, Training Loss: 0.39717957377433777\n","Step 940, Training Loss: 0.4547243118286133\n","Step 940, Training Loss: 0.41541701555252075\n","Step 940, Training Loss: 0.43699178099632263\n","Step 940, Training Loss: 0.5272337198257446\n","Step 940, Training Loss: 0.8249271512031555\n","Step 940, Training Loss: 0.4815864562988281\n","Step 940, Training Loss: 0.2798127830028534\n","Step 940, Training Loss: 0.47474366426467896\n","Step 940, Training Loss: 0.2071150839328766\n","Step 940, Training Loss: 0.3132372796535492\n","Step 940, Training Loss: 0.5145738124847412\n","Step 940, Training Loss: 0.434821218252182\n","Step 940, Training Loss: 0.4880095422267914\n","Step 940, Training Loss: 0.39383354783058167\n","Step 940, Training Loss: 0.28867676854133606\n","Step 940, Training Loss: 0.27274033427238464\n","Step 940, Training Loss: 0.4016207754611969\n","Step 940, Training Loss: 0.3433787524700165\n","Step 940, Training Loss: 0.3452330529689789\n","Step 940, Training Loss: 0.06307544559240341\n","Step 940, Training Loss: 0.2982747554779053\n","Step 940, Training Loss: 0.29488590359687805\n","Step 950, Training Loss: 0.3263495862483978\n","Step 950, Training Loss: 0.31876420974731445\n","Step 960, Training Loss: 0.3916611969470978\n","Step 960, Training Loss: 0.25358888506889343\n","Step 970, Training Loss: 0.1800999492406845\n","Step 970, Training Loss: 0.37807634472846985\n","Step 980, Training Loss: 0.33340421319007874\n","Step 980, Training Loss: 0.18408513069152832\n","Step 990, Training Loss: 0.07749368995428085\n","Step 990, Training Loss: 0.11461097002029419\n"]},{"output_type":"stream","name":"stderr","text":["Saving model checkpoint to ./results/checkpoint-1000\n","Configuration saved in ./results/checkpoint-1000/config.json\n","Model weights saved in ./results/checkpoint-1000/pytorch_model.bin\n"]},{"output_type":"stream","name":"stdout","text":["Step 1000, Training Loss: 0.2539416551589966\n","Step 1000, Training Loss: 0.29708439111709595\n","Step 1010, Training Loss: 0.2254633754491806\n","Step 1010, Training Loss: 0.059202443808317184\n","Step 1020, Training Loss: 0.2514216899871826\n","Step 1020, Training Loss: 0.7167226672172546\n","Step 1030, Training Loss: 0.1813966929912567\n","Step 1030, Training Loss: 0.3491484224796295\n","Step 1040, Training Loss: 0.17805111408233643\n","Step 1040, Training Loss: 0.5115050077438354\n","Step 1050, Training Loss: 0.28884702920913696\n","Step 1050, Training Loss: 0.4572940766811371\n","Step 1060, Training Loss: 0.1368369162082672\n","Step 1060, Training Loss: 0.16705618798732758\n","Step 1070, Training Loss: 0.08006389439105988\n","Step 1070, Training Loss: 0.2526634931564331\n","Step 1080, Training Loss: 0.24655799567699432\n","Step 1080, Training Loss: 0.2581411302089691\n","Step 1090, Training Loss: 0.16677522659301758\n","Step 1090, Training Loss: 0.3876698613166809\n","Step 1100, Training Loss: 0.124536894261837\n","Step 1100, Training Loss: 0.15846948325634003\n","Step 1110, Training Loss: 0.1199565902352333\n","Step 1110, Training Loss: 0.1840653419494629\n","Step 1120, Training Loss: 0.3920876681804657\n","Step 1120, Training Loss: 0.4791801869869232\n"]},{"output_type":"stream","name":"stderr","text":["***** Running Evaluation *****\n","  Num examples = 377\n","  Batch size = 8\n"]},{"output_type":"stream","name":"stdout","text":["Step 1130, Training Loss: 0.16498462855815887\n","Step 1130, Training Loss: 0.33361726999282837\n","Step 1140, Training Loss: 0.25967761874198914\n","Step 1140, Training Loss: 0.20399408042430878\n","Step 1150, Training Loss: 0.17811807990074158\n","Step 1150, Training Loss: 0.31701019406318665\n","Step 1160, Training Loss: 0.4633023738861084\n","Step 1160, Training Loss: 0.3006818890571594\n","Step 1170, Training Loss: 0.1870468407869339\n","Step 1170, Training Loss: 0.3327349126338959\n","Step 1180, Training Loss: 0.3683207929134369\n","Step 1180, Training Loss: 0.2119799107313156\n","Step 1190, Training Loss: 0.06300535053014755\n","Step 1190, Training Loss: 0.2621665894985199\n","Step 1200, Training Loss: 0.2316979616880417\n","Step 1200, Training Loss: 0.11131396144628525\n","Step 1210, Training Loss: 0.20306296646595\n","Step 1210, Training Loss: 0.10879351943731308\n","Step 1220, Training Loss: 0.14443783462047577\n","Step 1220, Training Loss: 0.17592553794384003\n","Step 1230, Training Loss: 0.3036794662475586\n","Step 1230, Training Loss: 0.48361945152282715\n","Step 1240, Training Loss: 0.2001677006483078\n","Step 1240, Training Loss: 0.22993826866149902\n","Step 1250, Training Loss: 0.19016246497631073\n","Step 1250, Training Loss: 0.22215695679187775\n","Step 1260, Training Loss: 0.5272287726402283\n","Step 1260, Training Loss: 0.2644325792789459\n","Step 1270, Training Loss: 0.2876990735530853\n","Step 1270, Training Loss: 0.17898251116275787\n","Step 1280, Training Loss: 0.18505434691905975\n","Step 1280, Training Loss: 0.17572999000549316\n","Step 1290, Training Loss: 0.17470739781856537\n","Step 1290, Training Loss: 0.3314298391342163\n","Step 1300, Training Loss: 0.18337984383106232\n","Step 1300, Training Loss: 0.09642010927200317\n","Step 1310, Training Loss: 0.11940906196832657\n","Step 1310, Training Loss: 0.45426374673843384\n"]},{"output_type":"stream","name":"stderr","text":["***** Running Evaluation *****\n","  Num examples = 377\n","  Batch size = 8\n"]},{"output_type":"stream","name":"stdout","text":["Step 1320, Training Loss: 0.3538637161254883\n","Step 1320, Training Loss: 0.07885151356458664\n","Step 1330, Training Loss: 0.18690286576747894\n","Step 1330, Training Loss: 0.27590081095695496\n","Step 1340, Training Loss: 0.05106133967638016\n","Step 1340, Training Loss: 0.1602848768234253\n","Step 1350, Training Loss: 0.24961905181407928\n","Step 1350, Training Loss: 0.24682073295116425\n","Step 1360, Training Loss: 0.3851495385169983\n","Step 1360, Training Loss: 0.14061219990253448\n","Step 1370, Training Loss: 0.40774622559547424\n","Step 1370, Training Loss: 0.09909459203481674\n","Step 1380, Training Loss: 0.2449704259634018\n","Step 1380, Training Loss: 0.24695070087909698\n","Step 1390, Training Loss: 0.07532655447721481\n","Step 1390, Training Loss: 0.10357611626386642\n","Step 1400, Training Loss: 0.04487103968858719\n","Step 1400, Training Loss: 0.1764906793832779\n","Step 1410, Training Loss: 0.17056970298290253\n","Step 1410, Training Loss: 0.20263110101222992\n","Step 1420, Training Loss: 0.23426485061645508\n","Step 1420, Training Loss: 0.2879272401332855\n","Step 1430, Training Loss: 0.4949234127998352\n","Step 1430, Training Loss: 0.18045106530189514\n","Step 1440, Training Loss: 0.28525957465171814\n","Step 1440, Training Loss: 0.19280032813549042\n","Step 1450, Training Loss: 0.39491161704063416\n","Step 1450, Training Loss: 0.13932029902935028\n","Step 1460, Training Loss: 0.2183123379945755\n","Step 1460, Training Loss: 0.19025668501853943\n","Step 1470, Training Loss: 0.618781328201294\n","Step 1470, Training Loss: 0.2459607869386673\n","Step 1480, Training Loss: 0.20190882682800293\n","Step 1480, Training Loss: 0.20170597732067108\n","Step 1490, Training Loss: 0.13869509100914001\n","Step 1490, Training Loss: 0.07134245336055756\n"]},{"output_type":"stream","name":"stderr","text":["Saving model checkpoint to ./results/checkpoint-1500\n","Configuration saved in ./results/checkpoint-1500/config.json\n","Model weights saved in ./results/checkpoint-1500/pytorch_model.bin\n"]},{"output_type":"stream","name":"stdout","text":["Step 1500, Training Loss: 0.07282375544309616\n","Step 1500, Training Loss: 0.19151662290096283\n"]},{"output_type":"stream","name":"stderr","text":["***** Running Evaluation *****\n","  Num examples = 377\n","  Batch size = 8\n"]},{"output_type":"stream","name":"stdout","text":["Step 1510, Training Loss: 0.15167070925235748\n","Step 1510, Training Loss: 0.07166662067174911\n","Step 1520, Training Loss: 0.15513284504413605\n","Step 1520, Training Loss: 0.07014669477939606\n","Step 1530, Training Loss: 0.36608418822288513\n","Step 1530, Training Loss: 0.23779907822608948\n","Step 1540, Training Loss: 0.16099055111408234\n","Step 1540, Training Loss: 0.19236990809440613\n","Step 1550, Training Loss: 0.12031365931034088\n","Step 1550, Training Loss: 0.10742007941007614\n","Step 1560, Training Loss: 0.35006266832351685\n","Step 1560, Training Loss: 0.16398334503173828\n","Step 1570, Training Loss: 0.10712804645299911\n","Step 1570, Training Loss: 0.13204990327358246\n","Step 1580, Training Loss: 0.13619600236415863\n","Step 1580, Training Loss: 0.07237092405557632\n","Step 1590, Training Loss: 0.12070963531732559\n","Step 1590, Training Loss: 0.04067875072360039\n","Step 1600, Training Loss: 0.2862260043621063\n","Step 1600, Training Loss: 0.16396674513816833\n","Step 1610, Training Loss: 0.24452553689479828\n","Step 1610, Training Loss: 0.1988387554883957\n","Step 1620, Training Loss: 0.2589626908302307\n","Step 1620, Training Loss: 0.2506473958492279\n","Step 1630, Training Loss: 0.19332024455070496\n","Step 1630, Training Loss: 0.23208501935005188\n","Step 1640, Training Loss: 0.2813124358654022\n","Step 1640, Training Loss: 0.41598063707351685\n","Step 1650, Training Loss: 0.23240284621715546\n","Step 1650, Training Loss: 0.1232578307390213\n","Step 1660, Training Loss: 0.46088477969169617\n","Step 1660, Training Loss: 0.19868917763233185\n","Step 1670, Training Loss: 0.07238959521055222\n","Step 1670, Training Loss: 0.08038073033094406\n","Step 1680, Training Loss: 0.12512648105621338\n","Step 1680, Training Loss: 0.21379749476909637\n","Step 1690, Training Loss: 0.24401359260082245\n","Step 1690, Training Loss: 0.08731520175933838\n"]},{"output_type":"stream","name":"stderr","text":["***** Running Evaluation *****\n","  Num examples = 377\n","  Batch size = 8\n"]},{"output_type":"stream","name":"stdout","text":["Step 1700, Training Loss: 0.3435279130935669\n","Step 1700, Training Loss: 0.21238718926906586\n","Step 1710, Training Loss: 0.21809063851833344\n","Step 1710, Training Loss: 0.312883585691452\n","Step 1720, Training Loss: 0.1862144023180008\n","Step 1720, Training Loss: 0.10058198124170303\n","Step 1730, Training Loss: 0.3158019483089447\n","Step 1730, Training Loss: 0.0862068235874176\n","Step 1740, Training Loss: 0.08371713757514954\n","Step 1740, Training Loss: 0.09037082642316818\n","Step 1750, Training Loss: 0.2273954600095749\n","Step 1750, Training Loss: 0.4428470730781555\n","Step 1760, Training Loss: 0.271270751953125\n","Step 1760, Training Loss: 0.04287115857005119\n","Step 1770, Training Loss: 0.21588587760925293\n","Step 1770, Training Loss: 0.1403530240058899\n","Step 1780, Training Loss: 0.08883299678564072\n","Step 1780, Training Loss: 0.1594630926847458\n","Step 1790, Training Loss: 0.09253522008657455\n","Step 1790, Training Loss: 0.06545209884643555\n","Step 1800, Training Loss: 0.08656760305166245\n","Step 1800, Training Loss: 0.22116100788116455\n","Step 1810, Training Loss: 0.055756427347660065\n","Step 1810, Training Loss: 0.06429478526115417\n","Step 1820, Training Loss: 0.0962839424610138\n","Step 1820, Training Loss: 0.0823206678032875\n","Step 1830, Training Loss: 0.07646221667528152\n","Step 1830, Training Loss: 0.1395951360464096\n","Step 1840, Training Loss: 0.07994332164525986\n","Step 1840, Training Loss: 0.04665039852261543\n","Step 1850, Training Loss: 0.042471498250961304\n","Step 1850, Training Loss: 0.18122981488704681\n","Step 1860, Training Loss: 0.37283089756965637\n","Step 1860, Training Loss: 0.2711124122142792\n","Step 1870, Training Loss: 0.08132427930831909\n","Step 1870, Training Loss: 0.22598198056221008\n"]},{"output_type":"stream","name":"stderr","text":["***** Running Evaluation *****\n","  Num examples = 377\n","  Batch size = 8\n"]},{"output_type":"stream","name":"stdout","text":["Step 1880, Training Loss: 0.21160544455051422\n","Step 1880, Training Loss: 0.33369630575180054\n","Step 1880, Training Loss: 0.566461980342865\n","Step 1880, Training Loss: 0.3981517553329468\n","Step 1880, Training Loss: 0.9812980890274048\n","Step 1880, Training Loss: 0.4346885681152344\n","Step 1880, Training Loss: 0.34640899300575256\n","Step 1880, Training Loss: 0.5228856205940247\n","Step 1880, Training Loss: 0.3207922875881195\n","Step 1880, Training Loss: 0.08226585388183594\n","Step 1880, Training Loss: 0.4315832257270813\n","Step 1880, Training Loss: 0.5029610991477966\n","Step 1880, Training Loss: 0.24117107689380646\n","Step 1880, Training Loss: 0.24333219230175018\n","Step 1880, Training Loss: 0.06488265842199326\n","Step 1880, Training Loss: 0.24406909942626953\n","Step 1880, Training Loss: 0.41224566102027893\n","Step 1880, Training Loss: 0.4745190143585205\n","Step 1880, Training Loss: 0.3954208791255951\n","Step 1880, Training Loss: 0.22635586559772491\n","Step 1880, Training Loss: 0.20349843800067902\n","Step 1880, Training Loss: 0.1862727254629135\n","Step 1880, Training Loss: 0.29220229387283325\n","Step 1880, Training Loss: 0.6585485935211182\n","Step 1880, Training Loss: 0.374807745218277\n","Step 1880, Training Loss: 0.5032938122749329\n","Step 1880, Training Loss: 0.33450207114219666\n","Step 1880, Training Loss: 0.4788270890712738\n","Step 1880, Training Loss: 0.420992374420166\n","Step 1880, Training Loss: 0.5977858901023865\n","Step 1880, Training Loss: 0.4349834620952606\n","Step 1880, Training Loss: 0.39389270544052124\n","Step 1880, Training Loss: 0.5176028609275818\n","Step 1880, Training Loss: 0.8775684237480164\n","Step 1880, Training Loss: 0.4189468026161194\n","Step 1880, Training Loss: 0.27386483550071716\n","Step 1880, Training Loss: 0.39697492122650146\n","Step 1880, Training Loss: 0.22667789459228516\n","Step 1880, Training Loss: 0.31499069929122925\n","Step 1880, Training Loss: 0.621646523475647\n","Step 1880, Training Loss: 0.31421396136283875\n","Step 1880, Training Loss: 0.4173703193664551\n","Step 1880, Training Loss: 0.32046735286712646\n","Step 1880, Training Loss: 0.31645193696022034\n","Step 1880, Training Loss: 0.10354458540678024\n","Step 1880, Training Loss: 0.21228495240211487\n","Step 1880, Training Loss: 0.19000184535980225\n","Step 1880, Training Loss: 0.25408825278282166\n","Step 1880, Training Loss: 0.03008345328271389\n","Step 1880, Training Loss: 0.2132788747549057\n","Step 1880, Training Loss: 0.16400735080242157\n","Step 1890, Training Loss: 0.4211249053478241\n","Step 1890, Training Loss: 0.06495390087366104\n","Step 1900, Training Loss: 0.33123064041137695\n","Step 1900, Training Loss: 0.08191511780023575\n","Step 1910, Training Loss: 0.11414871364831924\n","Step 1910, Training Loss: 0.09220730513334274\n","Step 1920, Training Loss: 0.32166150212287903\n","Step 1920, Training Loss: 0.5539076328277588\n","Step 1930, Training Loss: 0.236398383975029\n","Step 1930, Training Loss: 0.18203683197498322\n","Step 1940, Training Loss: 0.1903078556060791\n","Step 1940, Training Loss: 0.3576027452945709\n","Step 1950, Training Loss: 0.06289830058813095\n","Step 1950, Training Loss: 0.27422794699668884\n","Step 1960, Training Loss: 0.12782810628414154\n","Step 1960, Training Loss: 0.09130368381738663\n","Step 1970, Training Loss: 0.119046151638031\n","Step 1970, Training Loss: 0.21917720139026642\n","Step 1980, Training Loss: 0.058692481368780136\n","Step 1980, Training Loss: 0.1419042944908142\n","Step 1990, Training Loss: 0.05222881957888603\n","Step 1990, Training Loss: 0.07332900911569595\n"]},{"output_type":"stream","name":"stderr","text":["Saving model checkpoint to ./results/checkpoint-2000\n","Configuration saved in ./results/checkpoint-2000/config.json\n","Model weights saved in ./results/checkpoint-2000/pytorch_model.bin\n"]},{"output_type":"stream","name":"stdout","text":["Step 2000, Training Loss: 0.1517118662595749\n","Step 2000, Training Loss: 0.2947445809841156\n","Step 2010, Training Loss: 0.06871479749679565\n","Step 2010, Training Loss: 0.09602265805006027\n","Step 2020, Training Loss: 0.12116167694330215\n","Step 2020, Training Loss: 0.18663333356380463\n","Step 2030, Training Loss: 0.03829561918973923\n","Step 2030, Training Loss: 0.2639608085155487\n","Step 2040, Training Loss: 0.2679555118083954\n","Step 2040, Training Loss: 0.09228327125310898\n","Step 2050, Training Loss: 0.25844332575798035\n","Step 2050, Training Loss: 0.13519302010536194\n","Step 2060, Training Loss: 0.1806306093931198\n","Step 2060, Training Loss: 0.20102572441101074\n"]},{"output_type":"stream","name":"stderr","text":["***** Running Evaluation *****\n","  Num examples = 377\n","  Batch size = 8\n"]},{"output_type":"stream","name":"stdout","text":["Step 2070, Training Loss: 0.22673049569129944\n","Step 2070, Training Loss: 0.14398424327373505\n","Step 2080, Training Loss: 0.10217521339654922\n","Step 2080, Training Loss: 0.07144182920455933\n","Step 2090, Training Loss: 0.07638204842805862\n","Step 2090, Training Loss: 0.1053723618388176\n","Step 2100, Training Loss: 0.1591736525297165\n","Step 2100, Training Loss: 0.3328703045845032\n","Step 2110, Training Loss: 0.07993340492248535\n","Step 2110, Training Loss: 0.20315995812416077\n","Step 2120, Training Loss: 0.1275293231010437\n","Step 2120, Training Loss: 0.08486372977495193\n","Step 2130, Training Loss: 0.14645230770111084\n","Step 2130, Training Loss: 0.14210234582424164\n","Step 2140, Training Loss: 0.1791345328092575\n","Step 2140, Training Loss: 0.11509270966053009\n","Step 2150, Training Loss: 0.1195339560508728\n","Step 2150, Training Loss: 0.1444060206413269\n","Step 2160, Training Loss: 0.04414207860827446\n","Step 2160, Training Loss: 0.051851239055395126\n","Step 2170, Training Loss: 0.09832262247800827\n","Step 2170, Training Loss: 0.09517254680395126\n","Step 2180, Training Loss: 0.4581277072429657\n","Step 2180, Training Loss: 0.038447536528110504\n","Step 2190, Training Loss: 0.18835124373435974\n","Step 2190, Training Loss: 0.35415831208229065\n","Step 2200, Training Loss: 0.07966553419828415\n","Step 2200, Training Loss: 0.15847253799438477\n","Step 2210, Training Loss: 0.449893057346344\n","Step 2210, Training Loss: 0.4079035818576813\n","Step 2220, Training Loss: 0.04745807498693466\n","Step 2220, Training Loss: 0.1979798525571823\n","Step 2230, Training Loss: 0.04617675021290779\n","Step 2230, Training Loss: 0.1399068832397461\n","Step 2240, Training Loss: 0.027570432052016258\n","Step 2240, Training Loss: 0.34368130564689636\n","Step 2250, Training Loss: 0.14162075519561768\n","Step 2250, Training Loss: 0.28020811080932617\n"]},{"output_type":"stream","name":"stderr","text":["***** Running Evaluation *****\n","  Num examples = 377\n","  Batch size = 8\n"]},{"output_type":"stream","name":"stdout","text":["Step 2260, Training Loss: 0.373555988073349\n","Step 2260, Training Loss: 0.2563818395137787\n","Step 2270, Training Loss: 0.03189128264784813\n","Step 2270, Training Loss: 0.12952052056789398\n","Step 2280, Training Loss: 0.21865224838256836\n","Step 2280, Training Loss: 0.07719095796346664\n","Step 2290, Training Loss: 0.18649731576442719\n","Step 2290, Training Loss: 0.27366891503334045\n","Step 2300, Training Loss: 0.3782808482646942\n","Step 2300, Training Loss: 0.23874430358409882\n","Step 2310, Training Loss: 0.050751667469739914\n","Step 2310, Training Loss: 0.04532218724489212\n","Step 2320, Training Loss: 0.23676030337810516\n","Step 2320, Training Loss: 0.14937788248062134\n","Step 2330, Training Loss: 0.19260458648204803\n","Step 2330, Training Loss: 0.11120333522558212\n","Step 2340, Training Loss: 0.1203879714012146\n","Step 2340, Training Loss: 0.16063861548900604\n","Step 2350, Training Loss: 0.11069267988204956\n","Step 2350, Training Loss: 0.062381960451602936\n","Step 2360, Training Loss: 0.19536522030830383\n","Step 2360, Training Loss: 0.2444169521331787\n","Step 2370, Training Loss: 0.08260860294103622\n","Step 2370, Training Loss: 0.07925989478826523\n","Step 2380, Training Loss: 0.1230078935623169\n","Step 2380, Training Loss: 0.31847378611564636\n","Step 2390, Training Loss: 0.04798901453614235\n","Step 2390, Training Loss: 0.1948259323835373\n","Step 2400, Training Loss: 0.16391335427761078\n","Step 2400, Training Loss: 0.26377102732658386\n","Step 2410, Training Loss: 0.49433261156082153\n","Step 2410, Training Loss: 0.16461460292339325\n","Step 2420, Training Loss: 0.12163612991571426\n","Step 2420, Training Loss: 0.13005027174949646\n","Step 2430, Training Loss: 0.07072241604328156\n","Step 2430, Training Loss: 0.435007780790329\n","Step 2440, Training Loss: 0.23121635615825653\n","Step 2440, Training Loss: 0.11619456857442856\n"]},{"output_type":"stream","name":"stderr","text":["***** Running Evaluation *****\n","  Num examples = 377\n","  Batch size = 8\n"]},{"output_type":"stream","name":"stdout","text":["Step 2450, Training Loss: 0.17493467032909393\n","Step 2450, Training Loss: 0.07768622785806656\n","Step 2460, Training Loss: 0.1602117270231247\n","Step 2460, Training Loss: 0.19365666806697845\n","Step 2470, Training Loss: 0.15731097757816315\n","Step 2470, Training Loss: 0.04142530634999275\n","Step 2480, Training Loss: 0.027653729543089867\n","Step 2480, Training Loss: 0.06958339363336563\n","Step 2490, Training Loss: 0.1842539757490158\n","Step 2490, Training Loss: 0.24321119487285614\n"]},{"output_type":"stream","name":"stderr","text":["Saving model checkpoint to ./results/checkpoint-2500\n","Configuration saved in ./results/checkpoint-2500/config.json\n","Model weights saved in ./results/checkpoint-2500/pytorch_model.bin\n"]},{"output_type":"stream","name":"stdout","text":["Step 2500, Training Loss: 0.0707458108663559\n","Step 2500, Training Loss: 0.23494242131710052\n","Step 2510, Training Loss: 0.15353265404701233\n","Step 2510, Training Loss: 0.028771761804819107\n","Step 2520, Training Loss: 0.11453763395547867\n","Step 2520, Training Loss: 0.18496285378932953\n","Step 2530, Training Loss: 0.5331632494926453\n","Step 2530, Training Loss: 0.04073556140065193\n","Step 2540, Training Loss: 0.2436658889055252\n","Step 2540, Training Loss: 0.03571571037173271\n","Step 2550, Training Loss: 0.3549257218837738\n","Step 2550, Training Loss: 0.06573739647865295\n","Step 2560, Training Loss: 0.23874531686306\n","Step 2560, Training Loss: 0.03736765310168266\n","Step 2570, Training Loss: 0.06752412766218185\n","Step 2570, Training Loss: 0.15286339819431305\n","Step 2580, Training Loss: 0.2979319989681244\n","Step 2580, Training Loss: 0.0922435000538826\n","Step 2590, Training Loss: 0.03973257169127464\n","Step 2590, Training Loss: 0.4007388651371002\n","Step 2600, Training Loss: 0.15604142844676971\n","Step 2600, Training Loss: 0.17510294914245605\n","Step 2610, Training Loss: 0.027568621560931206\n","Step 2610, Training Loss: 0.1648881733417511\n","Step 2620, Training Loss: 0.07375945895910263\n","Step 2620, Training Loss: 0.11026406288146973\n","Step 2630, Training Loss: 0.40920716524124146\n","Step 2630, Training Loss: 0.22445975244045258\n"]},{"output_type":"stream","name":"stderr","text":["***** Running Evaluation *****\n","  Num examples = 377\n","  Batch size = 8\n"]},{"output_type":"stream","name":"stdout","text":["Step 2640, Training Loss: 0.14282257854938507\n","Step 2640, Training Loss: 0.18101274967193604\n","Step 2650, Training Loss: 0.09177646785974503\n","Step 2650, Training Loss: 0.17074887454509735\n","Step 2660, Training Loss: 0.13200907409191132\n","Step 2660, Training Loss: 0.11699718236923218\n","Step 2670, Training Loss: 0.05787121132016182\n","Step 2670, Training Loss: 0.39362040162086487\n","Step 2680, Training Loss: 0.06799595803022385\n","Step 2680, Training Loss: 0.14667688310146332\n","Step 2690, Training Loss: 0.029502231627702713\n","Step 2690, Training Loss: 0.045720119029283524\n","Step 2700, Training Loss: 0.11997878551483154\n","Step 2700, Training Loss: 0.20174400508403778\n","Step 2710, Training Loss: 0.07660802453756332\n","Step 2710, Training Loss: 0.0744495838880539\n","Step 2720, Training Loss: 0.06354286521673203\n","Step 2720, Training Loss: 0.3145105838775635\n","Step 2730, Training Loss: 0.04480363801121712\n","Step 2730, Training Loss: 0.3198598027229309\n","Step 2740, Training Loss: 0.03537486493587494\n","Step 2740, Training Loss: 0.0913674458861351\n","Step 2750, Training Loss: 0.23553338646888733\n","Step 2750, Training Loss: 0.06416477262973785\n","Step 2760, Training Loss: 0.07094398885965347\n","Step 2760, Training Loss: 0.04043764993548393\n","Step 2770, Training Loss: 0.11293181031942368\n","Step 2770, Training Loss: 0.04232082888484001\n","Step 2780, Training Loss: 0.07895632088184357\n","Step 2780, Training Loss: 0.07969900220632553\n","Step 2790, Training Loss: 0.10736923664808273\n","Step 2790, Training Loss: 0.049908366054296494\n","Step 2800, Training Loss: 0.22457651793956757\n","Step 2800, Training Loss: 0.0479801669716835\n","Step 2810, Training Loss: 0.296625018119812\n","Step 2810, Training Loss: 0.22203032672405243\n"]},{"output_type":"stream","name":"stderr","text":["***** Running Evaluation *****\n","  Num examples = 377\n","  Batch size = 8\n"]},{"output_type":"stream","name":"stdout","text":["Step 2820, Training Loss: 0.025986842811107635\n","Step 2820, Training Loss: 0.4413203299045563\n","Step 2820, Training Loss: 0.4848000705242157\n","Step 2820, Training Loss: 0.45196184515953064\n","Step 2820, Training Loss: 0.9735922813415527\n","Step 2820, Training Loss: 0.554288387298584\n","Step 2820, Training Loss: 0.30001184344291687\n","Step 2820, Training Loss: 0.6545891165733337\n","Step 2820, Training Loss: 0.33808648586273193\n","Step 2820, Training Loss: 0.0588463731110096\n","Step 2820, Training Loss: 0.4631598889827728\n","Step 2820, Training Loss: 0.5790621638298035\n","Step 2820, Training Loss: 0.3299403786659241\n","Step 2820, Training Loss: 0.266269326210022\n","Step 2820, Training Loss: 0.049605563282966614\n","Step 2820, Training Loss: 0.2318502515554428\n","Step 2820, Training Loss: 0.42370739579200745\n","Step 2820, Training Loss: 0.4743497967720032\n","Step 2820, Training Loss: 0.3486740291118622\n","Step 2820, Training Loss: 0.22570236027240753\n","Step 2820, Training Loss: 0.19112686812877655\n","Step 2820, Training Loss: 0.2362145632505417\n","Step 2820, Training Loss: 0.3000465929508209\n","Step 2820, Training Loss: 0.672778308391571\n","Step 2820, Training Loss: 0.3928508460521698\n","Step 2820, Training Loss: 0.5539942979812622\n","Step 2820, Training Loss: 0.3567938506603241\n","Step 2820, Training Loss: 0.43457719683647156\n","Step 2820, Training Loss: 0.375436931848526\n","Step 2820, Training Loss: 0.613524854183197\n","Step 2820, Training Loss: 0.49175891280174255\n","Step 2820, Training Loss: 0.595841109752655\n","Step 2820, Training Loss: 0.6376528739929199\n","Step 2820, Training Loss: 0.9383415579795837\n","Step 2820, Training Loss: 0.4467620551586151\n","Step 2820, Training Loss: 0.31597983837127686\n","Step 2820, Training Loss: 0.43576571345329285\n","Step 2820, Training Loss: 0.22368863224983215\n","Step 2820, Training Loss: 0.3358590304851532\n","Step 2820, Training Loss: 0.6182348132133484\n","Step 2820, Training Loss: 0.4203285276889801\n","Step 2820, Training Loss: 0.3989141583442688\n","Step 2820, Training Loss: 0.26598894596099854\n","Step 2820, Training Loss: 0.29580792784690857\n","Step 2820, Training Loss: 0.07827810943126678\n","Step 2820, Training Loss: 0.22855864465236664\n","Step 2820, Training Loss: 0.1931765079498291\n","Step 2820, Training Loss: 0.27512508630752563\n","Step 2820, Training Loss: 0.023410433903336525\n","Step 2820, Training Loss: 0.21619342267513275\n","Step 2820, Training Loss: 0.027125606313347816\n","Step 2830, Training Loss: 0.20623400807380676\n","Step 2830, Training Loss: 0.0668623074889183\n","Step 2840, Training Loss: 0.15902821719646454\n","Step 2840, Training Loss: 0.5233137011528015\n","Step 2850, Training Loss: 0.1780233085155487\n","Step 2850, Training Loss: 0.1090778112411499\n","Step 2860, Training Loss: 0.036709800362586975\n","Step 2860, Training Loss: 0.18888823688030243\n","Step 2870, Training Loss: 0.2611258029937744\n","Step 2870, Training Loss: 0.05951184034347534\n","Step 2880, Training Loss: 0.03956058248877525\n","Step 2880, Training Loss: 0.06295853108167648\n","Step 2890, Training Loss: 0.35300981998443604\n","Step 2890, Training Loss: 0.2687431275844574\n","Step 2900, Training Loss: 0.05711669474840164\n","Step 2900, Training Loss: 0.03664807975292206\n","Step 2910, Training Loss: 0.09885666519403458\n","Step 2910, Training Loss: 0.3866277039051056\n","Step 2920, Training Loss: 0.049892082810401917\n","Step 2920, Training Loss: 0.05063481256365776\n","Step 2930, Training Loss: 0.09173337370157242\n","Step 2930, Training Loss: 0.15879280865192413\n","Step 2940, Training Loss: 0.06857164949178696\n","Step 2940, Training Loss: 0.14163847267627716\n","Step 2950, Training Loss: 0.1090720072388649\n","Step 2950, Training Loss: 0.22862215340137482\n","Step 2960, Training Loss: 0.03043738380074501\n","Step 2960, Training Loss: 0.15695276856422424\n","Step 2970, Training Loss: 0.31220418214797974\n","Step 2970, Training Loss: 0.30821606516838074\n","Step 2980, Training Loss: 0.20594003796577454\n","Step 2980, Training Loss: 0.07853462547063828\n","Step 2990, Training Loss: 0.1801532506942749\n","Step 2990, Training Loss: 0.03753642365336418\n"]},{"output_type":"stream","name":"stderr","text":["Saving model checkpoint to ./results/checkpoint-3000\n","Configuration saved in ./results/checkpoint-3000/config.json\n","Model weights saved in ./results/checkpoint-3000/pytorch_model.bin\n"]},{"output_type":"stream","name":"stdout","text":["Step 3000, Training Loss: 0.040393125265836716\n","Step 3000, Training Loss: 0.22796893119812012\n"]},{"output_type":"stream","name":"stderr","text":["***** Running Evaluation *****\n","  Num examples = 377\n","  Batch size = 8\n"]},{"output_type":"stream","name":"stdout","text":["Step 3010, Training Loss: 0.16177092492580414\n","Step 3010, Training Loss: 0.2769795358181\n","Step 3020, Training Loss: 0.039063990116119385\n","Step 3020, Training Loss: 0.1370905488729477\n","Step 3030, Training Loss: 0.1543341726064682\n","Step 3030, Training Loss: 0.1130274087190628\n","Step 3040, Training Loss: 0.03601454570889473\n","Step 3040, Training Loss: 0.3849963843822479\n","Step 3050, Training Loss: 0.05318150669336319\n","Step 3050, Training Loss: 0.2580876052379608\n","Step 3060, Training Loss: 0.26175156235694885\n","Step 3060, Training Loss: 0.035172559320926666\n","Step 3070, Training Loss: 0.04007693752646446\n","Step 3070, Training Loss: 0.18458674848079681\n","Step 3080, Training Loss: 0.12490870803594589\n","Step 3080, Training Loss: 0.03669334575533867\n","Step 3090, Training Loss: 0.20990656316280365\n","Step 3090, Training Loss: 0.14285969734191895\n","Step 3100, Training Loss: 0.032505180686712265\n","Step 3100, Training Loss: 0.02404649555683136\n","Step 3110, Training Loss: 0.04691258445382118\n","Step 3110, Training Loss: 0.08912891894578934\n","Step 3120, Training Loss: 0.06058084964752197\n","Step 3120, Training Loss: 0.18661890923976898\n","Step 3130, Training Loss: 0.048851873725652695\n","Step 3130, Training Loss: 0.34485265612602234\n","Step 3140, Training Loss: 0.03259927034378052\n","Step 3140, Training Loss: 0.04113193228840828\n","Step 3150, Training Loss: 0.1662645787000656\n","Step 3150, Training Loss: 0.038561802357435226\n","Step 3160, Training Loss: 0.06455589830875397\n","Step 3160, Training Loss: 0.05597952753305435\n","Step 3170, Training Loss: 0.14444135129451752\n","Step 3170, Training Loss: 0.13425198197364807\n","Step 3180, Training Loss: 0.07759382575750351\n","Step 3180, Training Loss: 0.07893817871809006\n","Step 3190, Training Loss: 0.06681054085493088\n","Step 3190, Training Loss: 0.18978764116764069\n"]},{"output_type":"stream","name":"stderr","text":["***** Running Evaluation *****\n","  Num examples = 377\n","  Batch size = 8\n"]},{"output_type":"stream","name":"stdout","text":["Step 3200, Training Loss: 0.15490248799324036\n","Step 3200, Training Loss: 0.028060102835297585\n","Step 3210, Training Loss: 0.12783236801624298\n","Step 3210, Training Loss: 0.3652755320072174\n","Step 3220, Training Loss: 0.034159619361162186\n","Step 3220, Training Loss: 0.08004578948020935\n","Step 3230, Training Loss: 0.21129603683948517\n","Step 3230, Training Loss: 0.03471225127577782\n","Step 3240, Training Loss: 0.09661289304494858\n","Step 3240, Training Loss: 0.02514994516968727\n","Step 3250, Training Loss: 0.24914760887622833\n","Step 3250, Training Loss: 0.07243800908327103\n","Step 3260, Training Loss: 0.18082217872142792\n","Step 3260, Training Loss: 0.14868034422397614\n","Step 3270, Training Loss: 0.02633730135858059\n","Step 3270, Training Loss: 0.023453328758478165\n","Step 3280, Training Loss: 0.11144782602787018\n","Step 3280, Training Loss: 0.02852424420416355\n","Step 3290, Training Loss: 0.11820922046899796\n","Step 3290, Training Loss: 0.08829902857542038\n","Step 3300, Training Loss: 0.07442065328359604\n","Step 3300, Training Loss: 0.06319387257099152\n","Step 3310, Training Loss: 0.055688198655843735\n","Step 3310, Training Loss: 0.10589311271905899\n","Step 3320, Training Loss: 0.035460375249385834\n","Step 3320, Training Loss: 0.061063457280397415\n","Step 3330, Training Loss: 0.2679309844970703\n","Step 3330, Training Loss: 0.025454143062233925\n","Step 3340, Training Loss: 0.06867233663797379\n","Step 3340, Training Loss: 0.049868691712617874\n","Step 3350, Training Loss: 0.036336030811071396\n","Step 3350, Training Loss: 0.4806032180786133\n","Step 3360, Training Loss: 0.04650263115763664\n","Step 3360, Training Loss: 0.1406625509262085\n","Step 3370, Training Loss: 0.0973399356007576\n","Step 3370, Training Loss: 0.3427574634552002\n","Step 3380, Training Loss: 0.026349950581789017\n","Step 3380, Training Loss: 0.22116351127624512\n"]},{"output_type":"stream","name":"stderr","text":["***** Running Evaluation *****\n","  Num examples = 377\n","  Batch size = 8\n","\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","***** Running Evaluation *****\n","  Num examples = 377\n","  Batch size = 8\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='48' max='48' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [48/48 00:00]\n","    </div>\n","    "]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["[I 2024-12-01 16:00:27,583] Trial 2 finished with value: 0.6242044891831583 and parameters: {'learning_rate': 2.9926416523466886e-05, 'batch_size': 4, 'num_train_epochs': 18, 'weight_decay': 0.022070810773856874}. Best is trial 2 with value: 0.6242044891831583.\n","<ipython-input-10-747f1883d862>:2: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n","  learning_rate = trial.suggest_loguniform(\"learning_rate\", 1e-6, 5e-5)\n","<ipython-input-10-747f1883d862>:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n","  weight_decay = trial.suggest_loguniform(\"weight_decay\", 1e-4, 0.1)\n","PyTorch: setting up devices\n","/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n","***** Running training *****\n","  Num examples = 1507\n","  Num Epochs = 11\n","  Instantaneous batch size per device = 4\n","  Total train batch size (w. parallel, distributed & accumulation) = 8\n","  Gradient Accumulation steps = 2\n","  Total optimization steps = 2068\n"]},{"output_type":"stream","name":"stdout","text":["Starting training...\n","Step 0, Training Loss: 0.12033867835998535\n","Step 0, Training Loss: 0.11454232037067413\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='2068' max='2068' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [2068/2068 01:50, Epoch 10/11]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","      <th>F1</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>0</td>\n","      <td>0.147800</td>\n","      <td>0.436665</td>\n","      <td>0.580902</td>\n","      <td>0.681533</td>\n","      <td>0.613300</td>\n","      <td>0.618427</td>\n","    </tr>\n","    <tr>\n","      <td>1</td>\n","      <td>0.132000</td>\n","      <td>0.404587</td>\n","      <td>0.604775</td>\n","      <td>0.681000</td>\n","      <td>0.645320</td>\n","      <td>0.651990</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>0.132300</td>\n","      <td>0.416773</td>\n","      <td>0.572944</td>\n","      <td>0.677827</td>\n","      <td>0.623153</td>\n","      <td>0.629762</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>0.130200</td>\n","      <td>0.430204</td>\n","      <td>0.580902</td>\n","      <td>0.667393</td>\n","      <td>0.628079</td>\n","      <td>0.632399</td>\n","    </tr>\n","    <tr>\n","      <td>4</td>\n","      <td>0.146600</td>\n","      <td>0.398293</td>\n","      <td>0.594164</td>\n","      <td>0.686446</td>\n","      <td>0.628079</td>\n","      <td>0.643709</td>\n","    </tr>\n","    <tr>\n","      <td>5</td>\n","      <td>0.109800</td>\n","      <td>0.428626</td>\n","      <td>0.572944</td>\n","      <td>0.670336</td>\n","      <td>0.618227</td>\n","      <td>0.626914</td>\n","    </tr>\n","    <tr>\n","      <td>6</td>\n","      <td>0.101600</td>\n","      <td>0.427467</td>\n","      <td>0.588859</td>\n","      <td>0.665775</td>\n","      <td>0.630542</td>\n","      <td>0.633950</td>\n","    </tr>\n","    <tr>\n","      <td>7</td>\n","      <td>0.130200</td>\n","      <td>0.409054</td>\n","      <td>0.586207</td>\n","      <td>0.678125</td>\n","      <td>0.640394</td>\n","      <td>0.649664</td>\n","    </tr>\n","    <tr>\n","      <td>8</td>\n","      <td>0.065800</td>\n","      <td>0.427781</td>\n","      <td>0.580902</td>\n","      <td>0.668866</td>\n","      <td>0.620690</td>\n","      <td>0.631052</td>\n","    </tr>\n","    <tr>\n","      <td>9</td>\n","      <td>0.095100</td>\n","      <td>0.422212</td>\n","      <td>0.580902</td>\n","      <td>0.677070</td>\n","      <td>0.642857</td>\n","      <td>0.646799</td>\n","    </tr>\n","    <tr>\n","      <td>10</td>\n","      <td>0.075200</td>\n","      <td>0.424076</td>\n","      <td>0.586207</td>\n","      <td>0.668003</td>\n","      <td>0.642857</td>\n","      <td>0.644375</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Step 10, Training Loss: 0.1813405156135559\n","Step 10, Training Loss: 0.38022375106811523\n","Step 20, Training Loss: 0.16461174190044403\n","Step 20, Training Loss: 0.03801910579204559\n","Step 30, Training Loss: 0.0793585255742073\n","Step 30, Training Loss: 0.28822779655456543\n","Step 40, Training Loss: 0.14245471358299255\n","Step 40, Training Loss: 0.09556658565998077\n","Step 50, Training Loss: 0.0797577053308487\n","Step 50, Training Loss: 0.023456964641809464\n","Step 60, Training Loss: 0.077391617000103\n","Step 60, Training Loss: 1.1212916374206543\n","Step 70, Training Loss: 0.118415467441082\n","Step 70, Training Loss: 0.14070867002010345\n","Step 80, Training Loss: 0.16892889142036438\n","Step 80, Training Loss: 0.2419516146183014\n","Step 90, Training Loss: 0.2508567273616791\n","Step 90, Training Loss: 0.06612788885831833\n","Step 100, Training Loss: 0.08709823340177536\n","Step 100, Training Loss: 0.2284155935049057\n","Step 110, Training Loss: 0.14710481464862823\n","Step 110, Training Loss: 0.035168830305337906\n","Step 120, Training Loss: 0.0684523954987526\n","Step 120, Training Loss: 0.25206050276756287\n","Step 130, Training Loss: 0.0336730033159256\n","Step 130, Training Loss: 0.11050184071063995\n","Step 140, Training Loss: 0.17425115406513214\n","Step 140, Training Loss: 0.02406841516494751\n","Step 150, Training Loss: 0.16920816898345947\n","Step 150, Training Loss: 0.1083189994096756\n","Step 160, Training Loss: 0.04791802912950516\n","Step 160, Training Loss: 0.09191440045833588\n","Step 170, Training Loss: 0.1435975283384323\n","Step 170, Training Loss: 0.2109319269657135\n","Step 180, Training Loss: 0.0622020848095417\n","Step 180, Training Loss: 0.05800195410847664\n"]},{"output_type":"stream","name":"stderr","text":["***** Running Evaluation *****\n","  Num examples = 377\n","  Batch size = 8\n"]},{"output_type":"stream","name":"stdout","text":["Step 190, Training Loss: 0.14349551498889923\n","Step 190, Training Loss: 0.30787771940231323\n","Step 200, Training Loss: 0.12196695804595947\n","Step 200, Training Loss: 0.17716553807258606\n","Step 210, Training Loss: 0.08866294473409653\n","Step 210, Training Loss: 0.15143680572509766\n","Step 220, Training Loss: 0.12586800754070282\n","Step 220, Training Loss: 0.26695242524147034\n","Step 230, Training Loss: 0.14335612952709198\n","Step 230, Training Loss: 0.069656603038311\n","Step 240, Training Loss: 0.08813942223787308\n","Step 240, Training Loss: 0.219684436917305\n","Step 250, Training Loss: 0.29882392287254333\n","Step 250, Training Loss: 0.067605160176754\n","Step 260, Training Loss: 0.035336773842573166\n","Step 260, Training Loss: 0.14321203529834747\n","Step 270, Training Loss: 0.17966477572917938\n","Step 270, Training Loss: 0.12326079607009888\n","Step 280, Training Loss: 0.15993432700634003\n","Step 280, Training Loss: 0.09207374602556229\n","Step 290, Training Loss: 0.44109201431274414\n","Step 290, Training Loss: 0.020910311490297318\n","Step 300, Training Loss: 0.2753969728946686\n","Step 300, Training Loss: 0.291985422372818\n","Step 310, Training Loss: 0.11838262528181076\n","Step 310, Training Loss: 0.2203817367553711\n","Step 320, Training Loss: 0.15469709038734436\n","Step 320, Training Loss: 0.023340051993727684\n","Step 330, Training Loss: 0.07114952057600021\n","Step 330, Training Loss: 0.5945512652397156\n","Step 340, Training Loss: 0.03643558546900749\n","Step 340, Training Loss: 0.07748081535100937\n","Step 350, Training Loss: 0.05725071579217911\n","Step 350, Training Loss: 0.173387810587883\n","Step 360, Training Loss: 0.08566483110189438\n","Step 360, Training Loss: 0.061799515038728714\n","Step 370, Training Loss: 0.022539347410202026\n","Step 370, Training Loss: 0.08111244440078735\n"]},{"output_type":"stream","name":"stderr","text":["***** Running Evaluation *****\n","  Num examples = 377\n","  Batch size = 8\n"]},{"output_type":"stream","name":"stdout","text":["Step 380, Training Loss: 0.11726772785186768\n","Step 380, Training Loss: 0.12833385169506073\n","Step 390, Training Loss: 0.020514916628599167\n","Step 390, Training Loss: 0.1067107543349266\n","Step 400, Training Loss: 0.2088879644870758\n","Step 400, Training Loss: 0.2792316973209381\n","Step 410, Training Loss: 0.24964241683483124\n","Step 410, Training Loss: 0.323010116815567\n","Step 420, Training Loss: 0.23200960457324982\n","Step 420, Training Loss: 0.09085241705179214\n","Step 430, Training Loss: 0.02401375211775303\n","Step 430, Training Loss: 0.02548806183040142\n","Step 440, Training Loss: 0.021763024851679802\n","Step 440, Training Loss: 0.08720974624156952\n","Step 450, Training Loss: 0.48372459411621094\n","Step 450, Training Loss: 0.1820981353521347\n","Step 460, Training Loss: 0.18978199362754822\n","Step 460, Training Loss: 0.10752582550048828\n","Step 470, Training Loss: 0.09939693659543991\n","Step 470, Training Loss: 0.08698204904794693\n","Step 480, Training Loss: 0.1410164088010788\n","Step 480, Training Loss: 0.04648273065686226\n","Step 490, Training Loss: 0.20698487758636475\n","Step 490, Training Loss: 0.10452891886234283\n"]},{"output_type":"stream","name":"stderr","text":["Saving model checkpoint to ./results/checkpoint-500\n","Configuration saved in ./results/checkpoint-500/config.json\n","Model weights saved in ./results/checkpoint-500/pytorch_model.bin\n"]},{"output_type":"stream","name":"stdout","text":["Step 500, Training Loss: 0.07801201194524765\n","Step 500, Training Loss: 0.26413068175315857\n","Step 510, Training Loss: 0.2125512808561325\n","Step 510, Training Loss: 0.19630765914916992\n","Step 520, Training Loss: 0.28417834639549255\n","Step 520, Training Loss: 0.32398098707199097\n","Step 530, Training Loss: 0.11396465450525284\n","Step 530, Training Loss: 0.20347057282924652\n","Step 540, Training Loss: 0.15706929564476013\n","Step 540, Training Loss: 0.15191130340099335\n","Step 550, Training Loss: 0.14003871381282806\n","Step 550, Training Loss: 0.18209335207939148\n","Step 560, Training Loss: 0.06638956815004349\n","Step 560, Training Loss: 0.15480469167232513\n"]},{"output_type":"stream","name":"stderr","text":["***** Running Evaluation *****\n","  Num examples = 377\n","  Batch size = 8\n"]},{"output_type":"stream","name":"stdout","text":["Step 570, Training Loss: 0.13332399725914001\n","Step 570, Training Loss: 0.1313270777463913\n","Step 580, Training Loss: 0.028732532635331154\n","Step 580, Training Loss: 0.06398826837539673\n","Step 590, Training Loss: 0.05484673008322716\n","Step 590, Training Loss: 0.18934236466884613\n","Step 600, Training Loss: 0.0489354133605957\n","Step 600, Training Loss: 0.2566552758216858\n","Step 610, Training Loss: 0.2688029408454895\n","Step 610, Training Loss: 0.06457594782114029\n","Step 620, Training Loss: 0.027741968631744385\n","Step 620, Training Loss: 0.2178422510623932\n","Step 630, Training Loss: 0.08779488503932953\n","Step 630, Training Loss: 0.2773225009441376\n","Step 640, Training Loss: 0.07261063903570175\n","Step 640, Training Loss: 0.054502010345458984\n","Step 650, Training Loss: 0.23769955337047577\n","Step 650, Training Loss: 0.1866474598646164\n","Step 660, Training Loss: 0.023243874311447144\n","Step 660, Training Loss: 0.24657554924488068\n","Step 670, Training Loss: 0.19981703162193298\n","Step 670, Training Loss: 0.03970537334680557\n","Step 680, Training Loss: 0.12396024912595749\n","Step 680, Training Loss: 0.032084498554468155\n","Step 690, Training Loss: 0.10765824466943741\n","Step 690, Training Loss: 0.06443043798208237\n","Step 700, Training Loss: 0.05589940771460533\n","Step 700, Training Loss: 0.020018789917230606\n","Step 710, Training Loss: 0.050231266766786575\n","Step 710, Training Loss: 0.14689743518829346\n","Step 720, Training Loss: 0.07588399946689606\n","Step 720, Training Loss: 0.4263138771057129\n","Step 730, Training Loss: 0.05154243856668472\n","Step 730, Training Loss: 0.028976012021303177\n","Step 740, Training Loss: 0.08838476985692978\n","Step 740, Training Loss: 0.11322528123855591\n","Step 750, Training Loss: 0.0882665291428566\n","Step 750, Training Loss: 0.05189692601561546\n"]},{"output_type":"stream","name":"stderr","text":["***** Running Evaluation *****\n","  Num examples = 377\n","  Batch size = 8\n"]},{"output_type":"stream","name":"stdout","text":["Step 760, Training Loss: 0.1270790845155716\n","Step 760, Training Loss: 0.18313352763652802\n","Step 770, Training Loss: 0.1323099136352539\n","Step 770, Training Loss: 0.07786235213279724\n","Step 780, Training Loss: 0.026259252801537514\n","Step 780, Training Loss: 0.04643763229250908\n","Step 790, Training Loss: 0.043583955615758896\n","Step 790, Training Loss: 0.10581906884908676\n","Step 800, Training Loss: 0.12113364040851593\n","Step 800, Training Loss: 0.027526307851076126\n","Step 810, Training Loss: 0.019642477855086327\n","Step 810, Training Loss: 0.05730250105261803\n","Step 820, Training Loss: 0.053508829325437546\n","Step 820, Training Loss: 0.03900638222694397\n","Step 830, Training Loss: 0.4010814130306244\n","Step 830, Training Loss: 0.3224467933177948\n","Step 840, Training Loss: 0.12815676629543304\n","Step 840, Training Loss: 0.05342337116599083\n","Step 850, Training Loss: 0.133420929312706\n","Step 850, Training Loss: 0.17164035141468048\n","Step 860, Training Loss: 0.04292977228760719\n","Step 860, Training Loss: 0.06370498239994049\n","Step 870, Training Loss: 0.1710699051618576\n","Step 870, Training Loss: 0.20090118050575256\n","Step 880, Training Loss: 0.16719070076942444\n","Step 880, Training Loss: 0.3430328369140625\n","Step 890, Training Loss: 0.03065021149814129\n","Step 890, Training Loss: 0.054867591708898544\n","Step 900, Training Loss: 0.14865238964557648\n","Step 900, Training Loss: 0.025896532461047173\n","Step 910, Training Loss: 0.2718532979488373\n","Step 910, Training Loss: 0.08350720256567001\n","Step 920, Training Loss: 0.048699747771024704\n","Step 920, Training Loss: 0.029742974787950516\n","Step 930, Training Loss: 0.15256695449352264\n","Step 930, Training Loss: 0.21794764697551727\n"]},{"output_type":"stream","name":"stderr","text":["***** Running Evaluation *****\n","  Num examples = 377\n","  Batch size = 8\n"]},{"output_type":"stream","name":"stdout","text":["Step 940, Training Loss: 0.05610334128141403\n","Step 940, Training Loss: 0.2845795452594757\n","Step 940, Training Loss: 0.5400256514549255\n","Step 940, Training Loss: 0.4731287956237793\n","Step 940, Training Loss: 1.1283677816390991\n","Step 940, Training Loss: 0.4638438820838928\n","Step 940, Training Loss: 0.2690320909023285\n","Step 940, Training Loss: 0.7504009008407593\n","Step 940, Training Loss: 0.3239571750164032\n","Step 940, Training Loss: 0.051673758774995804\n","Step 940, Training Loss: 0.5189547538757324\n","Step 940, Training Loss: 0.6564796566963196\n","Step 940, Training Loss: 0.28535252809524536\n","Step 940, Training Loss: 0.24426327645778656\n","Step 940, Training Loss: 0.040428128093481064\n","Step 940, Training Loss: 0.24820759892463684\n","Step 940, Training Loss: 0.4385969340801239\n","Step 940, Training Loss: 0.5234535336494446\n","Step 940, Training Loss: 0.3227992653846741\n","Step 940, Training Loss: 0.17010429501533508\n","Step 940, Training Loss: 0.15976202487945557\n","Step 940, Training Loss: 0.2233513593673706\n","Step 940, Training Loss: 0.31775563955307007\n","Step 940, Training Loss: 0.6984629034996033\n","Step 940, Training Loss: 0.3304596245288849\n","Step 940, Training Loss: 0.5257055163383484\n","Step 940, Training Loss: 0.3526933193206787\n","Step 940, Training Loss: 0.3029763102531433\n","Step 940, Training Loss: 0.4577968120574951\n","Step 940, Training Loss: 0.6552945971488953\n","Step 940, Training Loss: 0.5096842050552368\n","Step 940, Training Loss: 0.5138716101646423\n","Step 940, Training Loss: 0.730604887008667\n","Step 940, Training Loss: 0.9752232432365417\n","Step 940, Training Loss: 0.4680938422679901\n","Step 940, Training Loss: 0.25852566957473755\n","Step 940, Training Loss: 0.34963589906692505\n","Step 940, Training Loss: 0.23492267727851868\n","Step 940, Training Loss: 0.16905395686626434\n","Step 940, Training Loss: 0.670670211315155\n","Step 940, Training Loss: 0.322579950094223\n","Step 940, Training Loss: 0.46093231439590454\n","Step 940, Training Loss: 0.285785049200058\n","Step 940, Training Loss: 0.33042994141578674\n","Step 940, Training Loss: 0.11572892963886261\n","Step 940, Training Loss: 0.16548621654510498\n","Step 940, Training Loss: 0.18044598400592804\n","Step 940, Training Loss: 0.2678951025009155\n","Step 940, Training Loss: 0.016583772376179695\n","Step 940, Training Loss: 0.24134092032909393\n","Step 940, Training Loss: 0.12670312821865082\n","Step 950, Training Loss: 0.0749591663479805\n","Step 950, Training Loss: 0.12260770052671432\n","Step 960, Training Loss: 0.14854101836681366\n","Step 960, Training Loss: 0.07527583092451096\n","Step 970, Training Loss: 0.15662816166877747\n","Step 970, Training Loss: 0.18568937480449677\n","Step 980, Training Loss: 0.12687571346759796\n","Step 980, Training Loss: 0.08564119786024094\n","Step 990, Training Loss: 0.018877459689974785\n","Step 990, Training Loss: 0.026422858238220215\n"]},{"output_type":"stream","name":"stderr","text":["Saving model checkpoint to ./results/checkpoint-1000\n","Configuration saved in ./results/checkpoint-1000/config.json\n","Model weights saved in ./results/checkpoint-1000/pytorch_model.bin\n"]},{"output_type":"stream","name":"stdout","text":["Step 1000, Training Loss: 0.09334620088338852\n","Step 1000, Training Loss: 0.22067712247371674\n","Step 1010, Training Loss: 0.026372376829385757\n","Step 1010, Training Loss: 0.020100608468055725\n","Step 1020, Training Loss: 0.09115123003721237\n","Step 1020, Training Loss: 0.6105890274047852\n","Step 1030, Training Loss: 0.1006636843085289\n","Step 1030, Training Loss: 0.10234387964010239\n","Step 1040, Training Loss: 0.06614737957715988\n","Step 1040, Training Loss: 0.13317164778709412\n","Step 1050, Training Loss: 0.11097631603479385\n","Step 1050, Training Loss: 0.21453623473644257\n","Step 1060, Training Loss: 0.2296537458896637\n","Step 1060, Training Loss: 0.05589892342686653\n","Step 1070, Training Loss: 0.018752535805106163\n","Step 1070, Training Loss: 0.06079960614442825\n","Step 1080, Training Loss: 0.12138360738754272\n","Step 1080, Training Loss: 0.1037369966506958\n","Step 1090, Training Loss: 0.06381570547819138\n","Step 1090, Training Loss: 0.4264495372772217\n","Step 1100, Training Loss: 0.03856702148914337\n","Step 1100, Training Loss: 0.04465198144316673\n","Step 1110, Training Loss: 0.04536141827702522\n","Step 1110, Training Loss: 0.10280594974756241\n","Step 1120, Training Loss: 0.09195910394191742\n","Step 1120, Training Loss: 0.2195100337266922\n"]},{"output_type":"stream","name":"stderr","text":["***** Running Evaluation *****\n","  Num examples = 377\n","  Batch size = 8\n"]},{"output_type":"stream","name":"stdout","text":["Step 1130, Training Loss: 0.0927344411611557\n","Step 1130, Training Loss: 0.0472693108022213\n","Step 1140, Training Loss: 0.0714464783668518\n","Step 1140, Training Loss: 0.11228833347558975\n","Step 1150, Training Loss: 0.15269172191619873\n","Step 1150, Training Loss: 0.35218557715415955\n","Step 1160, Training Loss: 0.14517685770988464\n","Step 1160, Training Loss: 0.03868138790130615\n","Step 1170, Training Loss: 0.03896902874112129\n","Step 1170, Training Loss: 0.17763783037662506\n","Step 1180, Training Loss: 0.22937551140785217\n","Step 1180, Training Loss: 0.06639498472213745\n","Step 1190, Training Loss: 0.022429384291172028\n","Step 1190, Training Loss: 0.03536916524171829\n","Step 1200, Training Loss: 0.0634935274720192\n","Step 1200, Training Loss: 0.04312789812684059\n","Step 1210, Training Loss: 0.08945820480585098\n","Step 1210, Training Loss: 0.02296331338584423\n","Step 1220, Training Loss: 0.04757779464125633\n","Step 1220, Training Loss: 0.06547686457633972\n","Step 1230, Training Loss: 0.10759520530700684\n","Step 1230, Training Loss: 0.16877435147762299\n","Step 1240, Training Loss: 0.057134974747896194\n","Step 1240, Training Loss: 0.046163011342287064\n","Step 1250, Training Loss: 0.10093088448047638\n","Step 1250, Training Loss: 0.05305379256606102\n","Step 1260, Training Loss: 0.6262850761413574\n","Step 1260, Training Loss: 0.08943536877632141\n","Step 1270, Training Loss: 0.19324862957000732\n","Step 1270, Training Loss: 0.09354044497013092\n","Step 1280, Training Loss: 0.02726542390882969\n","Step 1280, Training Loss: 0.13904552161693573\n","Step 1290, Training Loss: 0.1140250489115715\n","Step 1290, Training Loss: 0.14469651877880096\n","Step 1300, Training Loss: 0.09691625833511353\n","Step 1300, Training Loss: 0.023689502850174904\n","Step 1310, Training Loss: 0.023165781050920486\n","Step 1310, Training Loss: 0.4147071838378906\n"]},{"output_type":"stream","name":"stderr","text":["***** Running Evaluation *****\n","  Num examples = 377\n","  Batch size = 8\n"]},{"output_type":"stream","name":"stdout","text":["Step 1320, Training Loss: 0.19381891191005707\n","Step 1320, Training Loss: 0.022421209141612053\n","Step 1330, Training Loss: 0.06920772045850754\n","Step 1330, Training Loss: 0.17807617783546448\n","Step 1340, Training Loss: 0.018317630514502525\n","Step 1340, Training Loss: 0.047420434653759\n","Step 1350, Training Loss: 0.11665766686201096\n","Step 1350, Training Loss: 0.12905699014663696\n","Step 1360, Training Loss: 0.1570061296224594\n","Step 1360, Training Loss: 0.06467043608427048\n","Step 1370, Training Loss: 0.2857498526573181\n","Step 1370, Training Loss: 0.01872374303638935\n","Step 1380, Training Loss: 0.10823176056146622\n","Step 1380, Training Loss: 0.2762439250946045\n","Step 1390, Training Loss: 0.021683678030967712\n","Step 1390, Training Loss: 0.04024756699800491\n","Step 1400, Training Loss: 0.019828123971819878\n","Step 1400, Training Loss: 0.07670791447162628\n","Step 1410, Training Loss: 0.046135127544403076\n","Step 1410, Training Loss: 0.03766490891575813\n","Step 1420, Training Loss: 0.06740115582942963\n","Step 1420, Training Loss: 0.23542284965515137\n","Step 1430, Training Loss: 0.24812066555023193\n","Step 1430, Training Loss: 0.12310280650854111\n","Step 1440, Training Loss: 0.07124944776296616\n","Step 1440, Training Loss: 0.09373104572296143\n","Step 1450, Training Loss: 0.19170044362545013\n","Step 1450, Training Loss: 0.04770940542221069\n","Step 1460, Training Loss: 0.14169524610042572\n","Step 1460, Training Loss: 0.0987018570303917\n","Step 1470, Training Loss: 0.41951656341552734\n","Step 1470, Training Loss: 0.11853008717298508\n","Step 1480, Training Loss: 0.1513233631849289\n","Step 1480, Training Loss: 0.062169548124074936\n","Step 1490, Training Loss: 0.047507889568805695\n","Step 1490, Training Loss: 0.02234727516770363\n"]},{"output_type":"stream","name":"stderr","text":["Saving model checkpoint to ./results/checkpoint-1500\n","Configuration saved in ./results/checkpoint-1500/config.json\n","Model weights saved in ./results/checkpoint-1500/pytorch_model.bin\n"]},{"output_type":"stream","name":"stdout","text":["Step 1500, Training Loss: 0.02321689762175083\n","Step 1500, Training Loss: 0.13370372354984283\n"]},{"output_type":"stream","name":"stderr","text":["***** Running Evaluation *****\n","  Num examples = 377\n","  Batch size = 8\n"]},{"output_type":"stream","name":"stdout","text":["Step 1510, Training Loss: 0.04685162752866745\n","Step 1510, Training Loss: 0.02469339594244957\n","Step 1520, Training Loss: 0.09356007725000381\n","Step 1520, Training Loss: 0.022400274872779846\n","Step 1530, Training Loss: 0.09756801277399063\n","Step 1530, Training Loss: 0.07238548994064331\n","Step 1540, Training Loss: 0.09570532292127609\n","Step 1540, Training Loss: 0.14022846519947052\n","Step 1550, Training Loss: 0.05064528062939644\n","Step 1550, Training Loss: 0.03784945607185364\n","Step 1560, Training Loss: 0.42417246103286743\n","Step 1560, Training Loss: 0.04789203405380249\n","Step 1570, Training Loss: 0.029615093022584915\n","Step 1570, Training Loss: 0.041876330971717834\n","Step 1580, Training Loss: 0.043409354984760284\n","Step 1580, Training Loss: 0.02201571688055992\n","Step 1590, Training Loss: 0.03305242583155632\n","Step 1590, Training Loss: 0.016887415200471878\n","Step 1600, Training Loss: 0.08815949410200119\n","Step 1600, Training Loss: 0.08705601841211319\n","Step 1610, Training Loss: 0.1914123147726059\n","Step 1610, Training Loss: 0.026072267442941666\n","Step 1620, Training Loss: 0.16504445672035217\n","Step 1620, Training Loss: 0.08831261843442917\n","Step 1630, Training Loss: 0.15412457287311554\n","Step 1630, Training Loss: 0.1902996003627777\n","Step 1640, Training Loss: 0.10348232090473175\n","Step 1640, Training Loss: 0.31823036074638367\n","Step 1650, Training Loss: 0.09934806823730469\n","Step 1650, Training Loss: 0.047458160668611526\n","Step 1660, Training Loss: 0.3984108865261078\n","Step 1660, Training Loss: 0.12655551731586456\n","Step 1670, Training Loss: 0.024165546521544456\n","Step 1670, Training Loss: 0.03494396433234215\n","Step 1680, Training Loss: 0.05359629541635513\n","Step 1680, Training Loss: 0.07695285230875015\n","Step 1690, Training Loss: 0.03154420480132103\n","Step 1690, Training Loss: 0.04194388911128044\n"]},{"output_type":"stream","name":"stderr","text":["***** Running Evaluation *****\n","  Num examples = 377\n","  Batch size = 8\n"]},{"output_type":"stream","name":"stdout","text":["Step 1700, Training Loss: 0.16001224517822266\n","Step 1700, Training Loss: 0.14141957461833954\n","Step 1710, Training Loss: 0.06516184657812119\n","Step 1710, Training Loss: 0.19588088989257812\n","Step 1720, Training Loss: 0.1262587606906891\n","Step 1720, Training Loss: 0.042302943766117096\n","Step 1730, Training Loss: 0.07160034775733948\n","Step 1730, Training Loss: 0.03727279603481293\n","Step 1740, Training Loss: 0.039044830948114395\n","Step 1740, Training Loss: 0.029414713382720947\n","Step 1750, Training Loss: 0.1124420166015625\n","Step 1750, Training Loss: 0.056781210005283356\n","Step 1760, Training Loss: 0.15716134011745453\n","Step 1760, Training Loss: 0.01879083178937435\n","Step 1770, Training Loss: 0.09432800859212875\n","Step 1770, Training Loss: 0.02499169483780861\n","Step 1780, Training Loss: 0.0379706509411335\n","Step 1780, Training Loss: 0.06498302519321442\n","Step 1790, Training Loss: 0.025857914239168167\n","Step 1790, Training Loss: 0.02298036776483059\n","Step 1800, Training Loss: 0.021787244826555252\n","Step 1800, Training Loss: 0.13003841042518616\n","Step 1810, Training Loss: 0.0178521778434515\n","Step 1810, Training Loss: 0.018398476764559746\n","Step 1820, Training Loss: 0.03956208750605583\n","Step 1820, Training Loss: 0.03735385835170746\n","Step 1830, Training Loss: 0.034395162016153336\n","Step 1830, Training Loss: 0.06740786135196686\n","Step 1840, Training Loss: 0.03413311392068863\n","Step 1840, Training Loss: 0.0264133308082819\n","Step 1850, Training Loss: 0.01691329851746559\n","Step 1850, Training Loss: 0.12773682177066803\n","Step 1860, Training Loss: 0.16880308091640472\n","Step 1860, Training Loss: 0.17195844650268555\n","Step 1870, Training Loss: 0.03473317250609398\n","Step 1870, Training Loss: 0.1971220076084137\n"]},{"output_type":"stream","name":"stderr","text":["***** Running Evaluation *****\n","  Num examples = 377\n","  Batch size = 8\n"]},{"output_type":"stream","name":"stdout","text":["Step 1880, Training Loss: 0.06542324274778366\n","Step 1880, Training Loss: 0.2735946774482727\n","Step 1880, Training Loss: 0.5981193780899048\n","Step 1880, Training Loss: 0.47294503450393677\n","Step 1880, Training Loss: 1.147378921508789\n","Step 1880, Training Loss: 0.55115807056427\n","Step 1880, Training Loss: 0.29016226530075073\n","Step 1880, Training Loss: 0.7861324548721313\n","Step 1880, Training Loss: 0.3269614279270172\n","Step 1880, Training Loss: 0.04381387308239937\n","Step 1880, Training Loss: 0.5116862654685974\n","Step 1880, Training Loss: 0.7414970993995667\n","Step 1880, Training Loss: 0.29535314440727234\n","Step 1880, Training Loss: 0.3083414137363434\n","Step 1880, Training Loss: 0.03150054067373276\n","Step 1880, Training Loss: 0.2496597021818161\n","Step 1880, Training Loss: 0.46960964798927307\n","Step 1880, Training Loss: 0.5505350232124329\n","Step 1880, Training Loss: 0.36677122116088867\n","Step 1880, Training Loss: 0.1626841276884079\n","Step 1880, Training Loss: 0.16725564002990723\n","Step 1880, Training Loss: 0.22752223908901215\n","Step 1880, Training Loss: 0.32949694991111755\n","Step 1880, Training Loss: 0.7252659797668457\n","Step 1880, Training Loss: 0.3358829915523529\n","Step 1880, Training Loss: 0.5251588821411133\n","Step 1880, Training Loss: 0.35878515243530273\n","Step 1880, Training Loss: 0.31778421998023987\n","Step 1880, Training Loss: 0.46328282356262207\n","Step 1880, Training Loss: 0.6616884469985962\n","Step 1880, Training Loss: 0.5560122728347778\n","Step 1880, Training Loss: 0.603095531463623\n","Step 1880, Training Loss: 0.7677616477012634\n","Step 1880, Training Loss: 1.0768213272094727\n","Step 1880, Training Loss: 0.4609018862247467\n","Step 1880, Training Loss: 0.2633056342601776\n","Step 1880, Training Loss: 0.4009447991847992\n","Step 1880, Training Loss: 0.24485540390014648\n","Step 1880, Training Loss: 0.2211657613515854\n","Step 1880, Training Loss: 0.7019709944725037\n","Step 1880, Training Loss: 0.4514966905117035\n","Step 1880, Training Loss: 0.5110016465187073\n","Step 1880, Training Loss: 0.25664231181144714\n","Step 1880, Training Loss: 0.31432437896728516\n","Step 1880, Training Loss: 0.10328219085931778\n","Step 1880, Training Loss: 0.1485469490289688\n","Step 1880, Training Loss: 0.22920022904872894\n","Step 1880, Training Loss: 0.29352983832359314\n","Step 1880, Training Loss: 0.014687304385006428\n","Step 1880, Training Loss: 0.11024314165115356\n","Step 1880, Training Loss: 0.08766448497772217\n","Step 1890, Training Loss: 0.13728740811347961\n","Step 1890, Training Loss: 0.026748646050691605\n","Step 1900, Training Loss: 0.2196810096502304\n","Step 1900, Training Loss: 0.03289521485567093\n","Step 1910, Training Loss: 0.08991822600364685\n","Step 1910, Training Loss: 0.02590825967490673\n","Step 1920, Training Loss: 0.17196033895015717\n","Step 1920, Training Loss: 0.11200235038995743\n","Step 1930, Training Loss: 0.13246841728687286\n","Step 1930, Training Loss: 0.09319987148046494\n","Step 1940, Training Loss: 0.12437639385461807\n","Step 1940, Training Loss: 0.27766352891921997\n","Step 1950, Training Loss: 0.020520253106951714\n","Step 1950, Training Loss: 0.20962432026863098\n","Step 1960, Training Loss: 0.06655890494585037\n","Step 1960, Training Loss: 0.03284553438425064\n","Step 1970, Training Loss: 0.04504043981432915\n","Step 1970, Training Loss: 0.07825682312250137\n","Step 1980, Training Loss: 0.024572081863880157\n","Step 1980, Training Loss: 0.055468231439590454\n","Step 1990, Training Loss: 0.02169504575431347\n","Step 1990, Training Loss: 0.023088356480002403\n"]},{"output_type":"stream","name":"stderr","text":["Saving model checkpoint to ./results/checkpoint-2000\n","Configuration saved in ./results/checkpoint-2000/config.json\n","Model weights saved in ./results/checkpoint-2000/pytorch_model.bin\n"]},{"output_type":"stream","name":"stdout","text":["Step 2000, Training Loss: 0.06044290214776993\n","Step 2000, Training Loss: 0.09859967231750488\n","Step 2010, Training Loss: 0.024593165144324303\n","Step 2010, Training Loss: 0.04259026423096657\n","Step 2020, Training Loss: 0.05565422400832176\n","Step 2020, Training Loss: 0.11386727541685104\n","Step 2030, Training Loss: 0.01727920025587082\n","Step 2030, Training Loss: 0.0829264298081398\n","Step 2040, Training Loss: 0.18029527366161346\n","Step 2040, Training Loss: 0.04212930053472519\n","Step 2050, Training Loss: 0.04501919075846672\n","Step 2050, Training Loss: 0.07960418611764908\n","Step 2060, Training Loss: 0.04630095884203911\n","Step 2060, Training Loss: 0.09411084651947021\n"]},{"output_type":"stream","name":"stderr","text":["***** Running Evaluation *****\n","  Num examples = 377\n","  Batch size = 8\n","\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","***** Running Evaluation *****\n","  Num examples = 377\n","  Batch size = 8\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='48' max='48' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [48/48 00:00]\n","    </div>\n","    "]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["[I 2024-12-01 16:02:18,537] Trial 3 finished with value: 0.6443748273236536 and parameters: {'learning_rate': 1.0830097629363162e-05, 'batch_size': 4, 'num_train_epochs': 11, 'weight_decay': 0.002780227984984073}. Best is trial 3 with value: 0.6443748273236536.\n","<ipython-input-10-747f1883d862>:2: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n","  learning_rate = trial.suggest_loguniform(\"learning_rate\", 1e-6, 5e-5)\n","<ipython-input-10-747f1883d862>:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n","  weight_decay = trial.suggest_loguniform(\"weight_decay\", 1e-4, 0.1)\n","PyTorch: setting up devices\n","/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n","***** Running training *****\n","  Num examples = 1507\n","  Num Epochs = 11\n","  Instantaneous batch size per device = 8\n","  Total train batch size (w. parallel, distributed & accumulation) = 16\n","  Gradient Accumulation steps = 2\n","  Total optimization steps = 1034\n"]},{"output_type":"stream","name":"stdout","text":["Starting training...\n","Step 0, Training Loss: 0.06814213842153549\n","Step 0, Training Loss: 0.1091352254152298\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='1034' max='1034' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [1034/1034 01:30, Epoch 10/11]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","      <th>F1</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>0</td>\n","      <td>0.097600</td>\n","      <td>0.417915</td>\n","      <td>0.583554</td>\n","      <td>0.681162</td>\n","      <td>0.633005</td>\n","      <td>0.645666</td>\n","    </tr>\n","    <tr>\n","      <td>1</td>\n","      <td>0.105500</td>\n","      <td>0.412588</td>\n","      <td>0.602122</td>\n","      <td>0.690884</td>\n","      <td>0.645320</td>\n","      <td>0.659094</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>0.116800</td>\n","      <td>0.424021</td>\n","      <td>0.575597</td>\n","      <td>0.678940</td>\n","      <td>0.645320</td>\n","      <td>0.650137</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>0.114800</td>\n","      <td>0.434278</td>\n","      <td>0.588859</td>\n","      <td>0.683092</td>\n","      <td>0.647783</td>\n","      <td>0.654669</td>\n","    </tr>\n","    <tr>\n","      <td>4</td>\n","      <td>0.119700</td>\n","      <td>0.428400</td>\n","      <td>0.588859</td>\n","      <td>0.678236</td>\n","      <td>0.642857</td>\n","      <td>0.650079</td>\n","    </tr>\n","    <tr>\n","      <td>5</td>\n","      <td>0.072300</td>\n","      <td>0.430998</td>\n","      <td>0.578249</td>\n","      <td>0.673612</td>\n","      <td>0.645320</td>\n","      <td>0.648178</td>\n","    </tr>\n","    <tr>\n","      <td>6</td>\n","      <td>0.088400</td>\n","      <td>0.418469</td>\n","      <td>0.599469</td>\n","      <td>0.688754</td>\n","      <td>0.657635</td>\n","      <td>0.667227</td>\n","    </tr>\n","    <tr>\n","      <td>7</td>\n","      <td>0.090400</td>\n","      <td>0.421224</td>\n","      <td>0.596817</td>\n","      <td>0.683009</td>\n","      <td>0.660099</td>\n","      <td>0.665840</td>\n","    </tr>\n","    <tr>\n","      <td>8</td>\n","      <td>0.078100</td>\n","      <td>0.422168</td>\n","      <td>0.604775</td>\n","      <td>0.684594</td>\n","      <td>0.657635</td>\n","      <td>0.667014</td>\n","    </tr>\n","    <tr>\n","      <td>9</td>\n","      <td>0.113000</td>\n","      <td>0.423251</td>\n","      <td>0.604775</td>\n","      <td>0.685857</td>\n","      <td>0.660099</td>\n","      <td>0.667499</td>\n","    </tr>\n","    <tr>\n","      <td>10</td>\n","      <td>0.075700</td>\n","      <td>0.424681</td>\n","      <td>0.599469</td>\n","      <td>0.680339</td>\n","      <td>0.655172</td>\n","      <td>0.661454</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Step 10, Training Loss: 0.05521576479077339\n","Step 10, Training Loss: 0.0851610079407692\n","Step 20, Training Loss: 0.07000089436769485\n","Step 20, Training Loss: 0.036058951169252396\n","Step 30, Training Loss: 0.5977150797843933\n","Step 30, Training Loss: 0.03201698139309883\n","Step 40, Training Loss: 0.14237940311431885\n","Step 40, Training Loss: 0.035550858825445175\n","Step 50, Training Loss: 0.12419722229242325\n","Step 50, Training Loss: 0.09090732783079147\n","Step 60, Training Loss: 0.1759028285741806\n","Step 60, Training Loss: 0.10759427398443222\n","Step 70, Training Loss: 0.06853964179754257\n","Step 70, Training Loss: 0.13054513931274414\n","Step 80, Training Loss: 0.06602423638105392\n","Step 80, Training Loss: 0.1211419478058815\n","Step 90, Training Loss: 0.04154011607170105\n","Step 90, Training Loss: 0.14777818322181702\n"]},{"output_type":"stream","name":"stderr","text":["***** Running Evaluation *****\n","  Num examples = 377\n","  Batch size = 16\n"]},{"output_type":"stream","name":"stdout","text":["Step 100, Training Loss: 0.1241132989525795\n","Step 100, Training Loss: 0.06880076974630356\n","Step 110, Training Loss: 0.1396845430135727\n","Step 110, Training Loss: 0.09929203242063522\n","Step 120, Training Loss: 0.1594882309436798\n","Step 120, Training Loss: 0.05343497917056084\n","Step 130, Training Loss: 0.06317110359668732\n","Step 130, Training Loss: 0.17225013673305511\n","Step 140, Training Loss: 0.07899617403745651\n","Step 140, Training Loss: 0.23974435031414032\n","Step 150, Training Loss: 0.2960437834262848\n","Step 150, Training Loss: 0.09740865230560303\n","Step 160, Training Loss: 0.08991396427154541\n","Step 160, Training Loss: 0.10867420583963394\n","Step 170, Training Loss: 0.03508099541068077\n","Step 170, Training Loss: 0.03436001017689705\n","Step 180, Training Loss: 0.03787451609969139\n","Step 180, Training Loss: 0.019294917583465576\n"]},{"output_type":"stream","name":"stderr","text":["***** Running Evaluation *****\n","  Num examples = 377\n","  Batch size = 16\n"]},{"output_type":"stream","name":"stdout","text":["Step 190, Training Loss: 0.1539827287197113\n","Step 190, Training Loss: 0.027661988511681557\n","Step 200, Training Loss: 0.17411121726036072\n","Step 200, Training Loss: 0.04430041462182999\n","Step 210, Training Loss: 0.10030779987573624\n","Step 210, Training Loss: 0.09129282087087631\n","Step 220, Training Loss: 0.030566489323973656\n","Step 220, Training Loss: 0.20151185989379883\n","Step 230, Training Loss: 0.1083766371011734\n","Step 230, Training Loss: 0.0650956779718399\n","Step 240, Training Loss: 0.07404106855392456\n","Step 240, Training Loss: 0.08230917155742645\n","Step 250, Training Loss: 0.1882147341966629\n","Step 250, Training Loss: 0.05173918604850769\n","Step 260, Training Loss: 0.13922874629497528\n","Step 260, Training Loss: 0.2231842577457428\n","Step 270, Training Loss: 0.20200760662555695\n","Step 270, Training Loss: 0.10165047645568848\n","Step 280, Training Loss: 0.10655512660741806\n","Step 280, Training Loss: 0.13627345860004425\n"]},{"output_type":"stream","name":"stderr","text":["***** Running Evaluation *****\n","  Num examples = 377\n","  Batch size = 16\n"]},{"output_type":"stream","name":"stdout","text":["Step 290, Training Loss: 0.03933343663811684\n","Step 290, Training Loss: 0.14283950626850128\n","Step 300, Training Loss: 0.15895560383796692\n","Step 300, Training Loss: 0.07093892246484756\n","Step 310, Training Loss: 0.05888805538415909\n","Step 310, Training Loss: 0.024244174361228943\n","Step 320, Training Loss: 0.046855878084897995\n","Step 320, Training Loss: 0.13268589973449707\n","Step 330, Training Loss: 0.09949912130832672\n","Step 330, Training Loss: 0.05795727297663689\n","Step 340, Training Loss: 0.10291609913110733\n","Step 340, Training Loss: 0.12797653675079346\n","Step 350, Training Loss: 0.02909575216472149\n","Step 350, Training Loss: 0.08170925825834274\n","Step 360, Training Loss: 0.1871175318956375\n","Step 360, Training Loss: 0.1681695282459259\n","Step 370, Training Loss: 0.12235777825117111\n","Step 370, Training Loss: 0.13195890188217163\n"]},{"output_type":"stream","name":"stderr","text":["***** Running Evaluation *****\n","  Num examples = 377\n","  Batch size = 16\n"]},{"output_type":"stream","name":"stdout","text":["Step 380, Training Loss: 0.09360091388225555\n","Step 380, Training Loss: 0.08726980537176132\n","Step 390, Training Loss: 0.024820750579237938\n","Step 390, Training Loss: 0.06135792285203934\n","Step 400, Training Loss: 0.06080674007534981\n","Step 400, Training Loss: 0.15801547467708588\n","Step 410, Training Loss: 0.03193319961428642\n","Step 410, Training Loss: 0.023679284378886223\n","Step 420, Training Loss: 0.0792195275425911\n","Step 420, Training Loss: 0.035775285214185715\n","Step 430, Training Loss: 0.04273092374205589\n","Step 430, Training Loss: 0.3910985291004181\n","Step 440, Training Loss: 0.2501574158668518\n","Step 440, Training Loss: 0.07664870470762253\n","Step 450, Training Loss: 0.055082596838474274\n","Step 450, Training Loss: 0.042548876255750656\n","Step 460, Training Loss: 0.0325092077255249\n","Step 460, Training Loss: 0.03219113498926163\n"]},{"output_type":"stream","name":"stderr","text":["***** Running Evaluation *****\n","  Num examples = 377\n","  Batch size = 16\n"]},{"output_type":"stream","name":"stdout","text":["Step 470, Training Loss: 0.03714892640709877\n","Step 470, Training Loss: 0.42131900787353516\n","Step 470, Training Loss: 0.782885730266571\n","Step 470, Training Loss: 0.4302945137023926\n","Step 470, Training Loss: 0.5798630714416504\n","Step 470, Training Loss: 0.2798263132572174\n","Step 470, Training Loss: 0.5090978741645813\n","Step 470, Training Loss: 0.16689138114452362\n","Step 470, Training Loss: 0.3588431477546692\n","Step 470, Training Loss: 0.4615955352783203\n","Step 470, Training Loss: 0.11403360217809677\n","Step 470, Training Loss: 0.2991470396518707\n","Step 470, Training Loss: 0.5004319548606873\n","Step 470, Training Loss: 0.5104007720947266\n","Step 470, Training Loss: 0.4008118212223053\n","Step 470, Training Loss: 0.6010586023330688\n","Step 470, Training Loss: 0.7108294367790222\n","Step 470, Training Loss: 0.7893348336219788\n","Step 470, Training Loss: 0.3635036051273346\n","Step 470, Training Loss: 0.25872305035591125\n","Step 470, Training Loss: 0.5438148975372314\n","Step 470, Training Loss: 0.39147207140922546\n","Step 470, Training Loss: 0.25537681579589844\n","Step 470, Training Loss: 0.18312297761440277\n","Step 470, Training Loss: 0.32266169786453247\n","Step 470, Training Loss: 0.07365178316831589\n","Step 470, Training Loss: 0.03592491149902344\n","Step 480, Training Loss: 0.11868786066770554\n","Step 480, Training Loss: 0.20328174531459808\n","Step 490, Training Loss: 0.06764185428619385\n","Step 490, Training Loss: 0.1921447366476059\n"]},{"output_type":"stream","name":"stderr","text":["Saving model checkpoint to ./results/checkpoint-500\n","Configuration saved in ./results/checkpoint-500/config.json\n","Model weights saved in ./results/checkpoint-500/pytorch_model.bin\n"]},{"output_type":"stream","name":"stdout","text":["Step 500, Training Loss: 0.1053764596581459\n","Step 500, Training Loss: 0.037959422916173935\n","Step 510, Training Loss: 0.28238025307655334\n","Step 510, Training Loss: 0.01941036991775036\n","Step 520, Training Loss: 0.11180669069290161\n","Step 520, Training Loss: 0.09290410578250885\n","Step 530, Training Loss: 0.06851458549499512\n","Step 530, Training Loss: 0.0820668637752533\n","Step 540, Training Loss: 0.08464854210615158\n","Step 540, Training Loss: 0.11727068573236465\n","Step 550, Training Loss: 0.035615868866443634\n","Step 550, Training Loss: 0.05608231574296951\n","Step 560, Training Loss: 0.17758986353874207\n","Step 560, Training Loss: 0.027687394991517067\n"]},{"output_type":"stream","name":"stderr","text":["***** Running Evaluation *****\n","  Num examples = 377\n","  Batch size = 16\n"]},{"output_type":"stream","name":"stdout","text":["Step 570, Training Loss: 0.0675622969865799\n","Step 570, Training Loss: 0.13987931609153748\n","Step 580, Training Loss: 0.059046484529972076\n","Step 580, Training Loss: 0.06426920741796494\n","Step 590, Training Loss: 0.10439745336771011\n","Step 590, Training Loss: 0.10229308903217316\n","Step 600, Training Loss: 0.03984174132347107\n","Step 600, Training Loss: 0.04656706377863884\n","Step 610, Training Loss: 0.03878991678357124\n","Step 610, Training Loss: 0.09251754730939865\n","Step 620, Training Loss: 0.033003076910972595\n","Step 620, Training Loss: 0.13091541826725006\n","Step 630, Training Loss: 0.40569114685058594\n","Step 630, Training Loss: 0.10716766119003296\n","Step 640, Training Loss: 0.09171398729085922\n","Step 640, Training Loss: 0.09025166928768158\n","Step 650, Training Loss: 0.044773686677217484\n","Step 650, Training Loss: 0.07582493871450424\n"]},{"output_type":"stream","name":"stderr","text":["***** Running Evaluation *****\n","  Num examples = 377\n","  Batch size = 16\n"]},{"output_type":"stream","name":"stdout","text":["Step 660, Training Loss: 0.10395496338605881\n","Step 660, Training Loss: 0.13481196761131287\n","Step 670, Training Loss: 0.02481807954609394\n","Step 670, Training Loss: 0.04991045221686363\n","Step 680, Training Loss: 0.09167889505624771\n","Step 680, Training Loss: 0.09217574447393417\n","Step 690, Training Loss: 0.140053853392601\n","Step 690, Training Loss: 0.02829720638692379\n","Step 700, Training Loss: 0.045328669250011444\n","Step 700, Training Loss: 0.09137701243162155\n","Step 710, Training Loss: 0.14468201994895935\n","Step 710, Training Loss: 0.05704161152243614\n","Step 720, Training Loss: 0.08554816246032715\n","Step 720, Training Loss: 0.02903272584080696\n","Step 730, Training Loss: 0.16446441411972046\n","Step 730, Training Loss: 0.12314309924840927\n","Step 740, Training Loss: 0.10501869767904282\n","Step 740, Training Loss: 0.08297335356473923\n","Step 750, Training Loss: 0.11876215785741806\n","Step 750, Training Loss: 0.02901519276201725\n"]},{"output_type":"stream","name":"stderr","text":["***** Running Evaluation *****\n","  Num examples = 377\n","  Batch size = 16\n"]},{"output_type":"stream","name":"stdout","text":["Step 760, Training Loss: 0.07748159021139145\n","Step 760, Training Loss: 0.03876141831278801\n","Step 770, Training Loss: 0.1402413696050644\n","Step 770, Training Loss: 0.1942940056324005\n","Step 780, Training Loss: 0.038382645696401596\n","Step 780, Training Loss: 0.018243417143821716\n","Step 790, Training Loss: 0.03465723991394043\n","Step 790, Training Loss: 0.17849300801753998\n","Step 800, Training Loss: 0.08542750030755997\n","Step 800, Training Loss: 0.061707254499197006\n","Step 810, Training Loss: 0.10741443932056427\n","Step 810, Training Loss: 0.07976289093494415\n","Step 820, Training Loss: 0.23349180817604065\n","Step 820, Training Loss: 0.06134920194745064\n","Step 830, Training Loss: 0.23025953769683838\n","Step 830, Training Loss: 0.05185030773282051\n","Step 840, Training Loss: 0.04969390109181404\n","Step 840, Training Loss: 0.044789869338274\n"]},{"output_type":"stream","name":"stderr","text":["***** Running Evaluation *****\n","  Num examples = 377\n","  Batch size = 16\n"]},{"output_type":"stream","name":"stdout","text":["Step 850, Training Loss: 0.12299523502588272\n","Step 850, Training Loss: 0.024905644357204437\n","Step 860, Training Loss: 0.06287366151809692\n","Step 860, Training Loss: 0.03629211708903313\n","Step 870, Training Loss: 0.03468712791800499\n","Step 870, Training Loss: 0.038116514682769775\n","Step 880, Training Loss: 0.0587826743721962\n","Step 880, Training Loss: 0.0740474984049797\n","Step 890, Training Loss: 0.04848716780543327\n","Step 890, Training Loss: 0.09169479459524155\n","Step 900, Training Loss: 0.10225845873355865\n","Step 900, Training Loss: 0.0922040343284607\n","Step 910, Training Loss: 0.043622858822345734\n","Step 910, Training Loss: 0.04931752756237984\n","Step 920, Training Loss: 0.023455986753106117\n","Step 920, Training Loss: 0.06407543271780014\n","Step 930, Training Loss: 0.12122396379709244\n","Step 930, Training Loss: 0.21693480014801025\n"]},{"output_type":"stream","name":"stderr","text":["***** Running Evaluation *****\n","  Num examples = 377\n","  Batch size = 16\n"]},{"output_type":"stream","name":"stdout","text":["Step 940, Training Loss: 0.025910740718245506\n","Step 940, Training Loss: 0.4115520119667053\n","Step 940, Training Loss: 0.7786180377006531\n","Step 940, Training Loss: 0.4400508403778076\n","Step 940, Training Loss: 0.583422839641571\n","Step 940, Training Loss: 0.2961171567440033\n","Step 940, Training Loss: 0.5371419191360474\n","Step 940, Training Loss: 0.16190992295742035\n","Step 940, Training Loss: 0.3672521114349365\n","Step 940, Training Loss: 0.42668476700782776\n","Step 940, Training Loss: 0.12249298393726349\n","Step 940, Training Loss: 0.29054009914398193\n","Step 940, Training Loss: 0.508283793926239\n","Step 940, Training Loss: 0.48758766055107117\n","Step 940, Training Loss: 0.4048751890659332\n","Step 940, Training Loss: 0.5714167952537537\n","Step 940, Training Loss: 0.6951950192451477\n","Step 940, Training Loss: 0.7774558067321777\n","Step 940, Training Loss: 0.34060990810394287\n","Step 940, Training Loss: 0.24542789161205292\n","Step 940, Training Loss: 0.5334817171096802\n","Step 940, Training Loss: 0.402818888425827\n","Step 940, Training Loss: 0.25513237714767456\n","Step 940, Training Loss: 0.18336354196071625\n","Step 940, Training Loss: 0.2691832184791565\n","Step 940, Training Loss: 0.10989325493574142\n","Step 940, Training Loss: 0.18169963359832764\n","Step 950, Training Loss: 0.10444008558988571\n","Step 950, Training Loss: 0.09708809107542038\n","Step 960, Training Loss: 0.07139386981725693\n","Step 960, Training Loss: 0.08361118286848068\n","Step 970, Training Loss: 0.1656941920518875\n","Step 970, Training Loss: 0.01809546910226345\n","Step 980, Training Loss: 0.08465977758169174\n","Step 980, Training Loss: 0.01448312122374773\n","Step 990, Training Loss: 0.034968309104442596\n","Step 990, Training Loss: 0.04995293542742729\n"]},{"output_type":"stream","name":"stderr","text":["Saving model checkpoint to ./results/checkpoint-1000\n","Configuration saved in ./results/checkpoint-1000/config.json\n","Model weights saved in ./results/checkpoint-1000/pytorch_model.bin\n"]},{"output_type":"stream","name":"stdout","text":["Step 1000, Training Loss: 0.08204815536737442\n","Step 1000, Training Loss: 0.1050681620836258\n","Step 1010, Training Loss: 0.06442528963088989\n","Step 1010, Training Loss: 0.028130996972322464\n","Step 1020, Training Loss: 0.14469245076179504\n","Step 1020, Training Loss: 0.08843523263931274\n","Step 1030, Training Loss: 0.06615543365478516\n","Step 1030, Training Loss: 0.15126295387744904\n"]},{"output_type":"stream","name":"stderr","text":["***** Running Evaluation *****\n","  Num examples = 377\n","  Batch size = 16\n","\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","***** Running Evaluation *****\n","  Num examples = 377\n","  Batch size = 16\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='24' max='24' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [24/24 00:00]\n","    </div>\n","    "]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["[I 2024-12-01 16:03:50,274] Trial 4 finished with value: 0.6614536686040812 and parameters: {'learning_rate': 6.444007461706811e-06, 'batch_size': 8, 'num_train_epochs': 11, 'weight_decay': 0.034169848749039756}. Best is trial 4 with value: 0.6614536686040812.\n","<ipython-input-10-747f1883d862>:2: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n","  learning_rate = trial.suggest_loguniform(\"learning_rate\", 1e-6, 5e-5)\n","<ipython-input-10-747f1883d862>:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n","  weight_decay = trial.suggest_loguniform(\"weight_decay\", 1e-4, 0.1)\n","PyTorch: setting up devices\n","/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n","***** Running training *****\n","  Num examples = 1507\n","  Num Epochs = 18\n","  Instantaneous batch size per device = 16\n","  Total train batch size (w. parallel, distributed & accumulation) = 32\n","  Gradient Accumulation steps = 2\n","  Total optimization steps = 846\n"]},{"output_type":"stream","name":"stdout","text":["Starting training...\n","Step 0, Training Loss: 0.07255639880895615\n","Step 0, Training Loss: 0.100864939391613\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='846' max='846' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [846/846 02:17, Epoch 17/18]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","      <th>F1</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>0</td>\n","      <td>0.094200</td>\n","      <td>0.437973</td>\n","      <td>0.588859</td>\n","      <td>0.686092</td>\n","      <td>0.635468</td>\n","      <td>0.647679</td>\n","    </tr>\n","    <tr>\n","      <td>1</td>\n","      <td>0.092100</td>\n","      <td>0.438969</td>\n","      <td>0.596817</td>\n","      <td>0.677438</td>\n","      <td>0.657635</td>\n","      <td>0.661135</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>0.087300</td>\n","      <td>0.431855</td>\n","      <td>0.588859</td>\n","      <td>0.686937</td>\n","      <td>0.660099</td>\n","      <td>0.662905</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>0.084900</td>\n","      <td>0.425455</td>\n","      <td>0.602122</td>\n","      <td>0.689980</td>\n","      <td>0.655172</td>\n","      <td>0.667478</td>\n","    </tr>\n","    <tr>\n","      <td>4</td>\n","      <td>0.090200</td>\n","      <td>0.422904</td>\n","      <td>0.604775</td>\n","      <td>0.702773</td>\n","      <td>0.672414</td>\n","      <td>0.681689</td>\n","    </tr>\n","    <tr>\n","      <td>5</td>\n","      <td>0.077900</td>\n","      <td>0.433118</td>\n","      <td>0.594164</td>\n","      <td>0.683163</td>\n","      <td>0.662562</td>\n","      <td>0.662793</td>\n","    </tr>\n","    <tr>\n","      <td>6</td>\n","      <td>0.094500</td>\n","      <td>0.429691</td>\n","      <td>0.604775</td>\n","      <td>0.689610</td>\n","      <td>0.677340</td>\n","      <td>0.674082</td>\n","    </tr>\n","    <tr>\n","      <td>7</td>\n","      <td>0.073500</td>\n","      <td>0.433357</td>\n","      <td>0.594164</td>\n","      <td>0.687578</td>\n","      <td>0.665025</td>\n","      <td>0.668472</td>\n","    </tr>\n","    <tr>\n","      <td>8</td>\n","      <td>0.077100</td>\n","      <td>0.424031</td>\n","      <td>0.604775</td>\n","      <td>0.687641</td>\n","      <td>0.687192</td>\n","      <td>0.681224</td>\n","    </tr>\n","    <tr>\n","      <td>9</td>\n","      <td>0.071800</td>\n","      <td>0.430315</td>\n","      <td>0.602122</td>\n","      <td>0.686778</td>\n","      <td>0.667488</td>\n","      <td>0.669967</td>\n","    </tr>\n","    <tr>\n","      <td>10</td>\n","      <td>0.080300</td>\n","      <td>0.431089</td>\n","      <td>0.604775</td>\n","      <td>0.683601</td>\n","      <td>0.684729</td>\n","      <td>0.678988</td>\n","    </tr>\n","    <tr>\n","      <td>11</td>\n","      <td>0.079300</td>\n","      <td>0.437836</td>\n","      <td>0.580902</td>\n","      <td>0.681326</td>\n","      <td>0.660099</td>\n","      <td>0.660300</td>\n","    </tr>\n","    <tr>\n","      <td>12</td>\n","      <td>0.066600</td>\n","      <td>0.436392</td>\n","      <td>0.602122</td>\n","      <td>0.683510</td>\n","      <td>0.674877</td>\n","      <td>0.672621</td>\n","    </tr>\n","    <tr>\n","      <td>13</td>\n","      <td>0.064900</td>\n","      <td>0.429430</td>\n","      <td>0.594164</td>\n","      <td>0.680229</td>\n","      <td>0.674877</td>\n","      <td>0.670733</td>\n","    </tr>\n","    <tr>\n","      <td>14</td>\n","      <td>0.067200</td>\n","      <td>0.436126</td>\n","      <td>0.596817</td>\n","      <td>0.678689</td>\n","      <td>0.667488</td>\n","      <td>0.664938</td>\n","    </tr>\n","    <tr>\n","      <td>15</td>\n","      <td>0.074900</td>\n","      <td>0.439086</td>\n","      <td>0.599469</td>\n","      <td>0.678199</td>\n","      <td>0.672414</td>\n","      <td>0.668209</td>\n","    </tr>\n","    <tr>\n","      <td>16</td>\n","      <td>0.070600</td>\n","      <td>0.439763</td>\n","      <td>0.588859</td>\n","      <td>0.677250</td>\n","      <td>0.669951</td>\n","      <td>0.665101</td>\n","    </tr>\n","    <tr>\n","      <td>17</td>\n","      <td>0.068500</td>\n","      <td>0.436374</td>\n","      <td>0.591512</td>\n","      <td>0.680885</td>\n","      <td>0.677340</td>\n","      <td>0.671381</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Step 10, Training Loss: 0.04715413600206375\n","Step 10, Training Loss: 0.0646246001124382\n","Step 20, Training Loss: 0.08011233806610107\n","Step 20, Training Loss: 0.05076848343014717\n","Step 30, Training Loss: 0.09585021436214447\n","Step 30, Training Loss: 0.06852389872074127\n","Step 40, Training Loss: 0.06952721625566483\n","Step 40, Training Loss: 0.031415220350027084\n"]},{"output_type":"stream","name":"stderr","text":["***** Running Evaluation *****\n","  Num examples = 377\n","  Batch size = 32\n"]},{"output_type":"stream","name":"stdout","text":["Step 50, Training Loss: 0.0889730378985405\n","Step 50, Training Loss: 0.029019450768828392\n","Step 60, Training Loss: 0.06703777611255646\n","Step 60, Training Loss: 0.056936826556921005\n","Step 70, Training Loss: 0.16180992126464844\n","Step 70, Training Loss: 0.06607713550329208\n","Step 80, Training Loss: 0.06654497236013412\n","Step 80, Training Loss: 0.09653869271278381\n","Step 90, Training Loss: 0.023636773228645325\n","Step 90, Training Loss: 0.07432105392217636\n"]},{"output_type":"stream","name":"stderr","text":["***** Running Evaluation *****\n","  Num examples = 377\n","  Batch size = 32\n"]},{"output_type":"stream","name":"stdout","text":["Step 100, Training Loss: 0.11121853440999985\n","Step 100, Training Loss: 0.07483461499214172\n","Step 110, Training Loss: 0.06973537802696228\n","Step 110, Training Loss: 0.05593779310584068\n","Step 120, Training Loss: 0.06234569102525711\n","Step 120, Training Loss: 0.16700764000415802\n","Step 130, Training Loss: 0.08192344009876251\n","Step 130, Training Loss: 0.06717179715633392\n","Step 140, Training Loss: 0.10141655057668686\n","Step 140, Training Loss: 0.02982604317367077\n"]},{"output_type":"stream","name":"stderr","text":["***** Running Evaluation *****\n","  Num examples = 377\n","  Batch size = 32\n"]},{"output_type":"stream","name":"stdout","text":["Step 150, Training Loss: 0.12011346966028214\n","Step 150, Training Loss: 0.027550214901566505\n","Step 160, Training Loss: 0.06878741830587387\n","Step 160, Training Loss: 0.043801646679639816\n","Step 170, Training Loss: 0.06993178278207779\n","Step 170, Training Loss: 0.13338366150856018\n","Step 180, Training Loss: 0.13646075129508972\n","Step 180, Training Loss: 0.12005176395177841\n"]},{"output_type":"stream","name":"stderr","text":["***** Running Evaluation *****\n","  Num examples = 377\n","  Batch size = 32\n"]},{"output_type":"stream","name":"stdout","text":["Step 190, Training Loss: 0.180905282497406\n","Step 190, Training Loss: 0.07923100143671036\n","Step 200, Training Loss: 0.06505447626113892\n","Step 200, Training Loss: 0.03842081129550934\n","Step 210, Training Loss: 0.04911601170897484\n","Step 210, Training Loss: 0.07177354395389557\n","Step 220, Training Loss: 0.0915340706706047\n","Step 220, Training Loss: 0.12354221194982529\n","Step 230, Training Loss: 0.03171948343515396\n","Step 230, Training Loss: 0.13815100491046906\n"]},{"output_type":"stream","name":"stderr","text":["***** Running Evaluation *****\n","  Num examples = 377\n","  Batch size = 32\n"]},{"output_type":"stream","name":"stdout","text":["Step 240, Training Loss: 0.125509575009346\n","Step 240, Training Loss: 0.08527671545743942\n","Step 250, Training Loss: 0.06606369465589523\n","Step 250, Training Loss: 0.10170113295316696\n","Step 260, Training Loss: 0.09579525142908096\n","Step 260, Training Loss: 0.07588788121938705\n","Step 270, Training Loss: 0.08750353008508682\n","Step 270, Training Loss: 0.08881095796823502\n","Step 280, Training Loss: 0.08005034178495407\n","Step 280, Training Loss: 0.07068884372711182\n"]},{"output_type":"stream","name":"stderr","text":["***** Running Evaluation *****\n","  Num examples = 377\n","  Batch size = 32\n"]},{"output_type":"stream","name":"stdout","text":["Step 290, Training Loss: 0.06250704824924469\n","Step 290, Training Loss: 0.08945386856794357\n","Step 300, Training Loss: 0.03478557989001274\n","Step 300, Training Loss: 0.08306864649057388\n","Step 310, Training Loss: 0.06296728551387787\n","Step 310, Training Loss: 0.1163475438952446\n","Step 320, Training Loss: 0.0775521919131279\n","Step 320, Training Loss: 0.0662011057138443\n"]},{"output_type":"stream","name":"stderr","text":["***** Running Evaluation *****\n","  Num examples = 377\n","  Batch size = 32\n"]},{"output_type":"stream","name":"stdout","text":["Step 330, Training Loss: 0.15165790915489197\n","Step 330, Training Loss: 0.04629194736480713\n","Step 340, Training Loss: 0.07248903065919876\n","Step 340, Training Loss: 0.035898834466934204\n","Step 350, Training Loss: 0.06872875243425369\n","Step 350, Training Loss: 0.13016048073768616\n","Step 360, Training Loss: 0.04125909134745598\n","Step 360, Training Loss: 0.04471394419670105\n","Step 370, Training Loss: 0.14183013141155243\n","Step 370, Training Loss: 0.03339260444045067\n"]},{"output_type":"stream","name":"stderr","text":["***** Running Evaluation *****\n","  Num examples = 377\n","  Batch size = 32\n"]},{"output_type":"stream","name":"stdout","text":["Step 380, Training Loss: 0.04071866348385811\n","Step 380, Training Loss: 0.0631968155503273\n","Step 390, Training Loss: 0.02521386742591858\n","Step 390, Training Loss: 0.08807972818613052\n","Step 400, Training Loss: 0.055617399513721466\n","Step 400, Training Loss: 0.042622145265340805\n","Step 410, Training Loss: 0.14921867847442627\n","Step 410, Training Loss: 0.1093987375497818\n","Step 420, Training Loss: 0.052665647119283676\n","Step 420, Training Loss: 0.08030302822589874\n"]},{"output_type":"stream","name":"stderr","text":["***** Running Evaluation *****\n","  Num examples = 377\n","  Batch size = 32\n"]},{"output_type":"stream","name":"stdout","text":["Step 430, Training Loss: 0.06183952838182449\n","Step 430, Training Loss: 0.18792258203029633\n","Step 440, Training Loss: 0.05378328636288643\n","Step 440, Training Loss: 0.08396530151367188\n","Step 450, Training Loss: 0.14111986756324768\n","Step 450, Training Loss: 0.04417884349822998\n","Step 460, Training Loss: 0.04368570074439049\n","Step 460, Training Loss: 0.02844645455479622\n"]},{"output_type":"stream","name":"stderr","text":["***** Running Evaluation *****\n","  Num examples = 377\n","  Batch size = 32\n"]},{"output_type":"stream","name":"stdout","text":["Step 470, Training Loss: 0.01360572874546051\n","Step 470, Training Loss: 0.5791217088699341\n","Step 470, Training Loss: 0.553854763507843\n","Step 470, Training Loss: 0.4377147853374481\n","Step 470, Training Loss: 0.2819662392139435\n","Step 470, Training Loss: 0.24450623989105225\n","Step 470, Training Loss: 0.384571373462677\n","Step 470, Training Loss: 0.46103739738464355\n","Step 470, Training Loss: 0.6329044699668884\n","Step 470, Training Loss: 0.5655752420425415\n","Step 470, Training Loss: 0.41537028551101685\n","Step 470, Training Loss: 0.34089621901512146\n","Step 470, Training Loss: 0.22033363580703735\n","Step 470, Training Loss: 0.05796215683221817\n","Step 470, Training Loss: 0.05182121321558952\n","Step 480, Training Loss: 0.0646287351846695\n","Step 480, Training Loss: 0.1803220510482788\n","Step 490, Training Loss: 0.05416175350546837\n","Step 490, Training Loss: 0.05139734968543053\n"]},{"output_type":"stream","name":"stderr","text":["Saving model checkpoint to ./results/checkpoint-500\n","Configuration saved in ./results/checkpoint-500/config.json\n","Model weights saved in ./results/checkpoint-500/pytorch_model.bin\n"]},{"output_type":"stream","name":"stdout","text":["Step 500, Training Loss: 0.08850549906492233\n","Step 500, Training Loss: 0.11729339510202408\n","Step 510, Training Loss: 0.09907866269350052\n","Step 510, Training Loss: 0.05329188331961632\n"]},{"output_type":"stream","name":"stderr","text":["***** Running Evaluation *****\n","  Num examples = 377\n","  Batch size = 32\n"]},{"output_type":"stream","name":"stdout","text":["Step 520, Training Loss: 0.04745451360940933\n","Step 520, Training Loss: 0.030347350984811783\n","Step 530, Training Loss: 0.07537495344877243\n","Step 530, Training Loss: 0.07058162987232208\n","Step 540, Training Loss: 0.02824471890926361\n","Step 540, Training Loss: 0.04941783472895622\n","Step 550, Training Loss: 0.04465179517865181\n","Step 550, Training Loss: 0.12132413685321808\n","Step 560, Training Loss: 0.08424834161996841\n","Step 560, Training Loss: 0.0804009959101677\n"]},{"output_type":"stream","name":"stderr","text":["***** Running Evaluation *****\n","  Num examples = 377\n","  Batch size = 32\n"]},{"output_type":"stream","name":"stdout","text":["Step 570, Training Loss: 0.038577161729335785\n","Step 570, Training Loss: 0.058439839631319046\n","Step 580, Training Loss: 0.06770366430282593\n","Step 580, Training Loss: 0.036394938826560974\n","Step 590, Training Loss: 0.10704236477613449\n","Step 590, Training Loss: 0.0690988302230835\n","Step 600, Training Loss: 0.09355195611715317\n","Step 600, Training Loss: 0.06846893578767776\n","Step 610, Training Loss: 0.06616202741861343\n","Step 610, Training Loss: 0.04437391832470894\n"]},{"output_type":"stream","name":"stderr","text":["***** Running Evaluation *****\n","  Num examples = 377\n","  Batch size = 32\n"]},{"output_type":"stream","name":"stdout","text":["Step 620, Training Loss: 0.024839775636792183\n","Step 620, Training Loss: 0.13716886937618256\n","Step 630, Training Loss: 0.07608320564031601\n","Step 630, Training Loss: 0.08129259943962097\n","Step 640, Training Loss: 0.12004544585943222\n","Step 640, Training Loss: 0.04319165274500847\n","Step 650, Training Loss: 0.04221831262111664\n","Step 650, Training Loss: 0.031572308391332626\n"]},{"output_type":"stream","name":"stderr","text":["***** Running Evaluation *****\n","  Num examples = 377\n","  Batch size = 32\n"]},{"output_type":"stream","name":"stdout","text":["Step 660, Training Loss: 0.05129735544323921\n","Step 660, Training Loss: 0.07660191506147385\n","Step 670, Training Loss: 0.05090527608990669\n","Step 670, Training Loss: 0.038775116205215454\n","Step 680, Training Loss: 0.04669598862528801\n","Step 680, Training Loss: 0.04312654212117195\n","Step 690, Training Loss: 0.09749855846166611\n","Step 690, Training Loss: 0.04852885380387306\n","Step 700, Training Loss: 0.09162499010562897\n","Step 700, Training Loss: 0.030902672559022903\n"]},{"output_type":"stream","name":"stderr","text":["***** Running Evaluation *****\n","  Num examples = 377\n","  Batch size = 32\n"]},{"output_type":"stream","name":"stdout","text":["Step 710, Training Loss: 0.06096911430358887\n","Step 710, Training Loss: 0.09798093885183334\n","Step 720, Training Loss: 0.017332008108496666\n","Step 720, Training Loss: 0.03573503717780113\n","Step 730, Training Loss: 0.07076181471347809\n","Step 730, Training Loss: 0.04182983189821243\n","Step 740, Training Loss: 0.044784314930438995\n","Step 740, Training Loss: 0.028880983591079712\n","Step 750, Training Loss: 0.041819699108600616\n","Step 750, Training Loss: 0.06390649080276489\n"]},{"output_type":"stream","name":"stderr","text":["***** Running Evaluation *****\n","  Num examples = 377\n","  Batch size = 32\n"]},{"output_type":"stream","name":"stdout","text":["Step 760, Training Loss: 0.048024170100688934\n","Step 760, Training Loss: 0.08174493908882141\n","Step 770, Training Loss: 0.056523121893405914\n","Step 770, Training Loss: 0.05919307470321655\n","Step 780, Training Loss: 0.027694815769791603\n","Step 780, Training Loss: 0.06264853477478027\n","Step 790, Training Loss: 0.03888508304953575\n","Step 790, Training Loss: 0.03341225907206535\n"]},{"output_type":"stream","name":"stderr","text":["***** Running Evaluation *****\n","  Num examples = 377\n","  Batch size = 32\n"]},{"output_type":"stream","name":"stdout","text":["Step 800, Training Loss: 0.08441021293401718\n","Step 800, Training Loss: 0.06643334776163101\n","Step 810, Training Loss: 0.06353084743022919\n","Step 810, Training Loss: 0.08208970725536346\n","Step 820, Training Loss: 0.040600258857011795\n","Step 820, Training Loss: 0.06438768655061722\n","Step 830, Training Loss: 0.019281793385744095\n","Step 830, Training Loss: 0.06673195213079453\n","Step 840, Training Loss: 0.1183050200343132\n","Step 840, Training Loss: 0.050585515797138214\n"]},{"output_type":"stream","name":"stderr","text":["***** Running Evaluation *****\n","  Num examples = 377\n","  Batch size = 32\n","\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","***** Running Evaluation *****\n","  Num examples = 377\n","  Batch size = 32\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='12' max='12' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [12/12 00:00]\n","    </div>\n","    "]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["[I 2024-12-01 16:06:08,310] Trial 5 finished with value: 0.6713814671739401 and parameters: {'learning_rate': 9.480517573334494e-06, 'batch_size': 16, 'num_train_epochs': 18, 'weight_decay': 0.00011796123940234114}. Best is trial 5 with value: 0.6713814671739401.\n","<ipython-input-10-747f1883d862>:2: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n","  learning_rate = trial.suggest_loguniform(\"learning_rate\", 1e-6, 5e-5)\n","<ipython-input-10-747f1883d862>:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n","  weight_decay = trial.suggest_loguniform(\"weight_decay\", 1e-4, 0.1)\n","PyTorch: setting up devices\n","/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n","***** Running training *****\n","  Num examples = 1507\n","  Num Epochs = 10\n","  Instantaneous batch size per device = 8\n","  Total train batch size (w. parallel, distributed & accumulation) = 16\n","  Gradient Accumulation steps = 2\n","  Total optimization steps = 940\n"]},{"output_type":"stream","name":"stdout","text":["Starting training...\n","Step 0, Training Loss: 0.07198842614889145\n","Step 0, Training Loss: 0.056254711002111435\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='940' max='940' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [940/940 01:22, Epoch 9/10]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","      <th>F1</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>0</td>\n","      <td>0.050700</td>\n","      <td>0.434066</td>\n","      <td>0.607427</td>\n","      <td>0.683723</td>\n","      <td>0.674877</td>\n","      <td>0.673540</td>\n","    </tr>\n","    <tr>\n","      <td>1</td>\n","      <td>0.054800</td>\n","      <td>0.431742</td>\n","      <td>0.594164</td>\n","      <td>0.684778</td>\n","      <td>0.667488</td>\n","      <td>0.667951</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>0.055300</td>\n","      <td>0.444191</td>\n","      <td>0.594164</td>\n","      <td>0.690167</td>\n","      <td>0.672414</td>\n","      <td>0.670145</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>0.079400</td>\n","      <td>0.448063</td>\n","      <td>0.599469</td>\n","      <td>0.686365</td>\n","      <td>0.662562</td>\n","      <td>0.662705</td>\n","    </tr>\n","    <tr>\n","      <td>4</td>\n","      <td>0.078700</td>\n","      <td>0.442704</td>\n","      <td>0.607427</td>\n","      <td>0.693845</td>\n","      <td>0.665025</td>\n","      <td>0.671161</td>\n","    </tr>\n","    <tr>\n","      <td>5</td>\n","      <td>0.046400</td>\n","      <td>0.445806</td>\n","      <td>0.594164</td>\n","      <td>0.678021</td>\n","      <td>0.672414</td>\n","      <td>0.666309</td>\n","    </tr>\n","    <tr>\n","      <td>6</td>\n","      <td>0.059800</td>\n","      <td>0.439198</td>\n","      <td>0.607427</td>\n","      <td>0.684625</td>\n","      <td>0.672414</td>\n","      <td>0.673113</td>\n","    </tr>\n","    <tr>\n","      <td>7</td>\n","      <td>0.057700</td>\n","      <td>0.442629</td>\n","      <td>0.596817</td>\n","      <td>0.680906</td>\n","      <td>0.674877</td>\n","      <td>0.671574</td>\n","    </tr>\n","    <tr>\n","      <td>8</td>\n","      <td>0.053800</td>\n","      <td>0.442060</td>\n","      <td>0.610080</td>\n","      <td>0.686605</td>\n","      <td>0.677340</td>\n","      <td>0.676172</td>\n","    </tr>\n","    <tr>\n","      <td>9</td>\n","      <td>0.080500</td>\n","      <td>0.444424</td>\n","      <td>0.602122</td>\n","      <td>0.679748</td>\n","      <td>0.672414</td>\n","      <td>0.669129</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Step 10, Training Loss: 0.048860158771276474\n","Step 10, Training Loss: 0.04896586015820503\n","Step 20, Training Loss: 0.03186840936541557\n","Step 20, Training Loss: 0.024768147617578506\n","Step 30, Training Loss: 0.5052036046981812\n","Step 30, Training Loss: 0.022491399198770523\n","Step 40, Training Loss: 0.0737471729516983\n","Step 40, Training Loss: 0.02619541995227337\n","Step 50, Training Loss: 0.05577637627720833\n","Step 50, Training Loss: 0.04225822910666466\n","Step 60, Training Loss: 0.09233196824789047\n","Step 60, Training Loss: 0.055741455405950546\n","Step 70, Training Loss: 0.035744670778512955\n","Step 70, Training Loss: 0.057548489421606064\n","Step 80, Training Loss: 0.04248213395476341\n","Step 80, Training Loss: 0.05608699470758438\n","Step 90, Training Loss: 0.027716880664229393\n","Step 90, Training Loss: 0.08286821097135544\n"]},{"output_type":"stream","name":"stderr","text":["***** Running Evaluation *****\n","  Num examples = 377\n","  Batch size = 16\n"]},{"output_type":"stream","name":"stdout","text":["Step 100, Training Loss: 0.06295173615217209\n","Step 100, Training Loss: 0.03667618706822395\n","Step 110, Training Loss: 0.06398376077413559\n","Step 110, Training Loss: 0.04344914108514786\n","Step 120, Training Loss: 0.06745334714651108\n","Step 120, Training Loss: 0.029665885493159294\n","Step 130, Training Loss: 0.03287026658654213\n","Step 130, Training Loss: 0.06207708269357681\n","Step 140, Training Loss: 0.04930783063173294\n","Step 140, Training Loss: 0.11689050495624542\n","Step 150, Training Loss: 0.1835123747587204\n","Step 150, Training Loss: 0.047555796802043915\n","Step 160, Training Loss: 0.03774412348866463\n","Step 160, Training Loss: 0.07121052592992783\n","Step 170, Training Loss: 0.02244366705417633\n","Step 170, Training Loss: 0.022567933425307274\n","Step 180, Training Loss: 0.024226896464824677\n","Step 180, Training Loss: 0.015055092982947826\n"]},{"output_type":"stream","name":"stderr","text":["***** Running Evaluation *****\n","  Num examples = 377\n","  Batch size = 16\n"]},{"output_type":"stream","name":"stdout","text":["Step 190, Training Loss: 0.11036723107099533\n","Step 190, Training Loss: 0.01974000222980976\n","Step 200, Training Loss: 0.0832897499203682\n","Step 200, Training Loss: 0.03096570447087288\n","Step 210, Training Loss: 0.061354584991931915\n","Step 210, Training Loss: 0.03838738799095154\n","Step 220, Training Loss: 0.01975155621767044\n","Step 220, Training Loss: 0.10133751481771469\n","Step 230, Training Loss: 0.10689181089401245\n","Step 230, Training Loss: 0.031998783349990845\n","Step 240, Training Loss: 0.03619487211108208\n","Step 240, Training Loss: 0.044142819941043854\n","Step 250, Training Loss: 0.08109711855649948\n","Step 250, Training Loss: 0.030260175466537476\n","Step 260, Training Loss: 0.08580256998538971\n","Step 260, Training Loss: 0.18689289689064026\n","Step 270, Training Loss: 0.1767488569021225\n","Step 270, Training Loss: 0.053078543394804\n","Step 280, Training Loss: 0.05882180854678154\n","Step 280, Training Loss: 0.10516351461410522\n"]},{"output_type":"stream","name":"stderr","text":["***** Running Evaluation *****\n","  Num examples = 377\n","  Batch size = 16\n"]},{"output_type":"stream","name":"stdout","text":["Step 290, Training Loss: 0.026509631425142288\n","Step 290, Training Loss: 0.1019810363650322\n","Step 300, Training Loss: 0.10641076415777206\n","Step 300, Training Loss: 0.037775106728076935\n","Step 310, Training Loss: 0.035503506660461426\n","Step 310, Training Loss: 0.018453270196914673\n","Step 320, Training Loss: 0.029895737767219543\n","Step 320, Training Loss: 0.08007524907588959\n","Step 330, Training Loss: 0.05230962112545967\n","Step 330, Training Loss: 0.034287355840206146\n","Step 340, Training Loss: 0.03474239632487297\n","Step 340, Training Loss: 0.0815875455737114\n","Step 350, Training Loss: 0.01962418295443058\n","Step 350, Training Loss: 0.04550744965672493\n","Step 360, Training Loss: 0.084621861577034\n","Step 360, Training Loss: 0.08772190660238266\n","Step 370, Training Loss: 0.06284593790769577\n","Step 370, Training Loss: 0.0741930678486824\n"]},{"output_type":"stream","name":"stderr","text":["***** Running Evaluation *****\n","  Num examples = 377\n","  Batch size = 16\n"]},{"output_type":"stream","name":"stdout","text":["Step 380, Training Loss: 0.06999971717596054\n","Step 380, Training Loss: 0.0680481418967247\n","Step 390, Training Loss: 0.024377113208174706\n","Step 390, Training Loss: 0.03605835512280464\n","Step 400, Training Loss: 0.03704530745744705\n","Step 400, Training Loss: 0.07737564295530319\n","Step 410, Training Loss: 0.022275784984230995\n","Step 410, Training Loss: 0.017423806712031364\n","Step 420, Training Loss: 0.04104961082339287\n","Step 420, Training Loss: 0.025739211589097977\n","Step 430, Training Loss: 0.02598828636109829\n","Step 430, Training Loss: 0.34770700335502625\n","Step 440, Training Loss: 0.10211010277271271\n","Step 440, Training Loss: 0.05470305681228638\n","Step 450, Training Loss: 0.03894495591521263\n","Step 450, Training Loss: 0.02435803785920143\n","Step 460, Training Loss: 0.022107018157839775\n","Step 460, Training Loss: 0.01973157934844494\n"]},{"output_type":"stream","name":"stderr","text":["***** Running Evaluation *****\n","  Num examples = 377\n","  Batch size = 16\n"]},{"output_type":"stream","name":"stdout","text":["Step 470, Training Loss: 0.024411143735051155\n","Step 470, Training Loss: 0.4679093360900879\n","Step 470, Training Loss: 0.7970593571662903\n","Step 470, Training Loss: 0.49143633246421814\n","Step 470, Training Loss: 0.6172800064086914\n","Step 470, Training Loss: 0.31681570410728455\n","Step 470, Training Loss: 0.5654427409172058\n","Step 470, Training Loss: 0.1827901154756546\n","Step 470, Training Loss: 0.4040440022945404\n","Step 470, Training Loss: 0.39022600650787354\n","Step 470, Training Loss: 0.10982479155063629\n","Step 470, Training Loss: 0.30984625220298767\n","Step 470, Training Loss: 0.488251656293869\n","Step 470, Training Loss: 0.45612141489982605\n","Step 470, Training Loss: 0.4244444966316223\n","Step 470, Training Loss: 0.628402054309845\n","Step 470, Training Loss: 0.7126713395118713\n","Step 470, Training Loss: 0.8472070693969727\n","Step 470, Training Loss: 0.36260986328125\n","Step 470, Training Loss: 0.2635980546474457\n","Step 470, Training Loss: 0.5711230635643005\n","Step 470, Training Loss: 0.42278724908828735\n","Step 470, Training Loss: 0.30248695611953735\n","Step 470, Training Loss: 0.14317671954631805\n","Step 470, Training Loss: 0.2767218053340912\n","Step 470, Training Loss: 0.0412905178964138\n","Step 470, Training Loss: 0.024522684514522552\n","Step 480, Training Loss: 0.08580183237791061\n","Step 480, Training Loss: 0.10630948841571808\n","Step 490, Training Loss: 0.040755391120910645\n","Step 490, Training Loss: 0.11094582080841064\n"]},{"output_type":"stream","name":"stderr","text":["Saving model checkpoint to ./results/checkpoint-500\n","Configuration saved in ./results/checkpoint-500/config.json\n","Model weights saved in ./results/checkpoint-500/pytorch_model.bin\n"]},{"output_type":"stream","name":"stdout","text":["Step 500, Training Loss: 0.05848100408911705\n","Step 500, Training Loss: 0.026021182537078857\n","Step 510, Training Loss: 0.0878322497010231\n","Step 510, Training Loss: 0.014674133621156216\n","Step 520, Training Loss: 0.04891473427414894\n","Step 520, Training Loss: 0.0550510473549366\n","Step 530, Training Loss: 0.042475923895835876\n","Step 530, Training Loss: 0.04709121212363243\n","Step 540, Training Loss: 0.0688256099820137\n","Step 540, Training Loss: 0.06943196058273315\n","Step 550, Training Loss: 0.022745642811059952\n","Step 550, Training Loss: 0.04286925122141838\n","Step 560, Training Loss: 0.1180775910615921\n","Step 560, Training Loss: 0.020471978932619095\n"]},{"output_type":"stream","name":"stderr","text":["***** Running Evaluation *****\n","  Num examples = 377\n","  Batch size = 16\n"]},{"output_type":"stream","name":"stdout","text":["Step 570, Training Loss: 0.04050789028406143\n","Step 570, Training Loss: 0.07106639444828033\n","Step 580, Training Loss: 0.03493683785200119\n","Step 580, Training Loss: 0.03778448328375816\n","Step 590, Training Loss: 0.05296240374445915\n","Step 590, Training Loss: 0.055140893906354904\n","Step 600, Training Loss: 0.03137805685400963\n","Step 600, Training Loss: 0.02946675382554531\n","Step 610, Training Loss: 0.025885120034217834\n","Step 610, Training Loss: 0.052134592086076736\n","Step 620, Training Loss: 0.02186567150056362\n","Step 620, Training Loss: 0.05519945174455643\n","Step 630, Training Loss: 0.3325934410095215\n","Step 630, Training Loss: 0.04032178595662117\n","Step 640, Training Loss: 0.05816904455423355\n","Step 640, Training Loss: 0.050673749297857285\n","Step 650, Training Loss: 0.027050748467445374\n","Step 650, Training Loss: 0.040101032704114914\n"]},{"output_type":"stream","name":"stderr","text":["***** Running Evaluation *****\n","  Num examples = 377\n","  Batch size = 16\n"]},{"output_type":"stream","name":"stdout","text":["Step 660, Training Loss: 0.09914635866880417\n","Step 660, Training Loss: 0.06654100865125656\n","Step 670, Training Loss: 0.01804972067475319\n","Step 670, Training Loss: 0.033120013773441315\n","Step 680, Training Loss: 0.09273774176836014\n","Step 680, Training Loss: 0.05236165598034859\n","Step 690, Training Loss: 0.07784842699766159\n","Step 690, Training Loss: 0.020167676731944084\n","Step 700, Training Loss: 0.024860868230462074\n","Step 700, Training Loss: 0.053039707243442535\n","Step 710, Training Loss: 0.06962423771619797\n","Step 710, Training Loss: 0.04172424599528313\n","Step 720, Training Loss: 0.05112817510962486\n","Step 720, Training Loss: 0.018673228099942207\n","Step 730, Training Loss: 0.1083936020731926\n","Step 730, Training Loss: 0.05957294628024101\n","Step 740, Training Loss: 0.06349290162324905\n","Step 740, Training Loss: 0.07799644768238068\n","Step 750, Training Loss: 0.08637287467718124\n","Step 750, Training Loss: 0.021428918465971947\n"]},{"output_type":"stream","name":"stderr","text":["***** Running Evaluation *****\n","  Num examples = 377\n","  Batch size = 16\n"]},{"output_type":"stream","name":"stdout","text":["Step 760, Training Loss: 0.0450555682182312\n","Step 760, Training Loss: 0.028189552947878838\n","Step 770, Training Loss: 0.10840650647878647\n","Step 770, Training Loss: 0.11278944462537766\n","Step 780, Training Loss: 0.02203839085996151\n","Step 780, Training Loss: 0.014343584887683392\n","Step 790, Training Loss: 0.025350233539938927\n","Step 790, Training Loss: 0.07825535535812378\n","Step 800, Training Loss: 0.05081765726208687\n","Step 800, Training Loss: 0.03755408525466919\n","Step 810, Training Loss: 0.06049811840057373\n","Step 810, Training Loss: 0.06064222380518913\n","Step 820, Training Loss: 0.21391211450099945\n","Step 820, Training Loss: 0.03879295662045479\n","Step 830, Training Loss: 0.159395232796669\n","Step 830, Training Loss: 0.03339808061718941\n","Step 840, Training Loss: 0.036597684025764465\n","Step 840, Training Loss: 0.028218237683176994\n"]},{"output_type":"stream","name":"stderr","text":["***** Running Evaluation *****\n","  Num examples = 377\n","  Batch size = 16\n"]},{"output_type":"stream","name":"stdout","text":["Step 850, Training Loss: 0.08166690170764923\n","Step 850, Training Loss: 0.01889871619641781\n","Step 860, Training Loss: 0.03653167933225632\n","Step 860, Training Loss: 0.03114856779575348\n","Step 870, Training Loss: 0.02716653421521187\n","Step 870, Training Loss: 0.02194838784635067\n","Step 880, Training Loss: 0.030449802055954933\n","Step 880, Training Loss: 0.05617348104715347\n","Step 890, Training Loss: 0.0328625850379467\n","Step 890, Training Loss: 0.04903345927596092\n","Step 900, Training Loss: 0.05367351695895195\n","Step 900, Training Loss: 0.04872169345617294\n","Step 910, Training Loss: 0.027090389281511307\n","Step 910, Training Loss: 0.031548481434583664\n","Step 920, Training Loss: 0.016412081196904182\n","Step 920, Training Loss: 0.041557926684617996\n","Step 930, Training Loss: 0.06761500239372253\n","Step 930, Training Loss: 0.21204160153865814\n"]},{"output_type":"stream","name":"stderr","text":["***** Running Evaluation *****\n","  Num examples = 377\n","  Batch size = 16\n"]},{"output_type":"stream","name":"stdout","text":["Step 940, Training Loss: 0.45546460151672363\n","Step 940, Training Loss: 0.7886873483657837\n","Step 940, Training Loss: 0.5028950572013855\n","Step 940, Training Loss: 0.632801353931427\n","Step 940, Training Loss: 0.32101497054100037\n","Step 940, Training Loss: 0.569145679473877\n","Step 940, Training Loss: 0.1814853399991989\n","Step 940, Training Loss: 0.3869548439979553\n","Step 940, Training Loss: 0.40224558115005493\n","Step 940, Training Loss: 0.10732986778020859\n","Step 940, Training Loss: 0.3184102475643158\n","Step 940, Training Loss: 0.4796152114868164\n","Step 940, Training Loss: 0.4862886965274811\n","Step 940, Training Loss: 0.42714324593544006\n","Step 940, Training Loss: 0.6049315333366394\n","Step 940, Training Loss: 0.7046180367469788\n","Step 940, Training Loss: 0.8583353161811829\n","Step 940, Training Loss: 0.3673105239868164\n","Step 940, Training Loss: 0.27763745188713074\n","Step 940, Training Loss: 0.562900722026825\n","Step 940, Training Loss: 0.400094598531723\n","Step 940, Training Loss: 0.3162996768951416\n","Step 940, Training Loss: 0.15318451821804047\n","Step 940, Training Loss: 0.2968023717403412\n"]},{"output_type":"stream","name":"stderr","text":["\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","***** Running Evaluation *****\n","  Num examples = 377\n","  Batch size = 16\n"]},{"output_type":"stream","name":"stdout","text":["Step 940, Training Loss: 0.45546460151672363\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='24' max='24' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [24/24 00:00]\n","    </div>\n","    "]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Step 940, Training Loss: 0.7886873483657837\n","Step 940, Training Loss: 0.5028950572013855\n","Step 940, Training Loss: 0.632801353931427\n","Step 940, Training Loss: 0.32101497054100037\n","Step 940, Training Loss: 0.569145679473877\n","Step 940, Training Loss: 0.1814853399991989\n","Step 940, Training Loss: 0.3869548439979553\n","Step 940, Training Loss: 0.40224558115005493\n","Step 940, Training Loss: 0.10732986778020859\n","Step 940, Training Loss: 0.3184102475643158\n","Step 940, Training Loss: 0.4796152114868164\n","Step 940, Training Loss: 0.4862886965274811\n","Step 940, Training Loss: 0.42714324593544006\n","Step 940, Training Loss: 0.6049315333366394\n","Step 940, Training Loss: 0.7046180367469788\n","Step 940, Training Loss: 0.8583353161811829\n","Step 940, Training Loss: 0.3673105239868164\n","Step 940, Training Loss: 0.27763745188713074\n","Step 940, Training Loss: 0.562900722026825\n","Step 940, Training Loss: 0.400094598531723\n","Step 940, Training Loss: 0.3162996768951416\n","Step 940, Training Loss: 0.15318451821804047\n","Step 940, Training Loss: 0.2968023717403412\n"]},{"output_type":"stream","name":"stderr","text":["[I 2024-12-01 16:07:31,639] Trial 6 finished with value: 0.669129056129354 and parameters: {'learning_rate': 2.5990370273840767e-06, 'batch_size': 8, 'num_train_epochs': 10, 'weight_decay': 0.02617707692143064}. Best is trial 5 with value: 0.6713814671739401.\n","<ipython-input-10-747f1883d862>:2: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n","  learning_rate = trial.suggest_loguniform(\"learning_rate\", 1e-6, 5e-5)\n","<ipython-input-10-747f1883d862>:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n","  weight_decay = trial.suggest_loguniform(\"weight_decay\", 1e-4, 0.1)\n","PyTorch: setting up devices\n","/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n","***** Running training *****\n","  Num examples = 1507\n","  Num Epochs = 18\n","  Instantaneous batch size per device = 16\n","  Total train batch size (w. parallel, distributed & accumulation) = 32\n","  Gradient Accumulation steps = 2\n","  Total optimization steps = 846\n"]},{"output_type":"stream","name":"stdout","text":["Starting training...\n","Step 0, Training Loss: 0.05353266000747681\n","Step 0, Training Loss: 0.05898798629641533\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='846' max='846' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [846/846 02:15, Epoch 17/18]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","      <th>F1</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>0</td>\n","      <td>0.052700</td>\n","      <td>0.444623</td>\n","      <td>0.602122</td>\n","      <td>0.684650</td>\n","      <td>0.674877</td>\n","      <td>0.673535</td>\n","    </tr>\n","    <tr>\n","      <td>1</td>\n","      <td>0.056600</td>\n","      <td>0.445214</td>\n","      <td>0.604775</td>\n","      <td>0.686810</td>\n","      <td>0.672414</td>\n","      <td>0.673097</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>0.053400</td>\n","      <td>0.446229</td>\n","      <td>0.604775</td>\n","      <td>0.687399</td>\n","      <td>0.677340</td>\n","      <td>0.676105</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>0.056900</td>\n","      <td>0.443775</td>\n","      <td>0.604775</td>\n","      <td>0.686622</td>\n","      <td>0.682266</td>\n","      <td>0.676832</td>\n","    </tr>\n","    <tr>\n","      <td>4</td>\n","      <td>0.053700</td>\n","      <td>0.447193</td>\n","      <td>0.596817</td>\n","      <td>0.681073</td>\n","      <td>0.677340</td>\n","      <td>0.671939</td>\n","    </tr>\n","    <tr>\n","      <td>5</td>\n","      <td>0.049400</td>\n","      <td>0.444684</td>\n","      <td>0.602122</td>\n","      <td>0.684138</td>\n","      <td>0.679803</td>\n","      <td>0.675541</td>\n","    </tr>\n","    <tr>\n","      <td>6</td>\n","      <td>0.067200</td>\n","      <td>0.444655</td>\n","      <td>0.602122</td>\n","      <td>0.687961</td>\n","      <td>0.679803</td>\n","      <td>0.677540</td>\n","    </tr>\n","    <tr>\n","      <td>7</td>\n","      <td>0.051900</td>\n","      <td>0.446613</td>\n","      <td>0.602122</td>\n","      <td>0.687018</td>\n","      <td>0.677340</td>\n","      <td>0.674564</td>\n","    </tr>\n","    <tr>\n","      <td>8</td>\n","      <td>0.053900</td>\n","      <td>0.443699</td>\n","      <td>0.607427</td>\n","      <td>0.687818</td>\n","      <td>0.679803</td>\n","      <td>0.677348</td>\n","    </tr>\n","    <tr>\n","      <td>9</td>\n","      <td>0.049300</td>\n","      <td>0.444473</td>\n","      <td>0.607427</td>\n","      <td>0.686522</td>\n","      <td>0.679803</td>\n","      <td>0.677001</td>\n","    </tr>\n","    <tr>\n","      <td>10</td>\n","      <td>0.061100</td>\n","      <td>0.446916</td>\n","      <td>0.602122</td>\n","      <td>0.679782</td>\n","      <td>0.679803</td>\n","      <td>0.673583</td>\n","    </tr>\n","    <tr>\n","      <td>11</td>\n","      <td>0.064400</td>\n","      <td>0.446792</td>\n","      <td>0.602122</td>\n","      <td>0.689980</td>\n","      <td>0.677340</td>\n","      <td>0.675737</td>\n","    </tr>\n","    <tr>\n","      <td>12</td>\n","      <td>0.052200</td>\n","      <td>0.446583</td>\n","      <td>0.602122</td>\n","      <td>0.685088</td>\n","      <td>0.682266</td>\n","      <td>0.676837</td>\n","    </tr>\n","    <tr>\n","      <td>13</td>\n","      <td>0.053700</td>\n","      <td>0.445482</td>\n","      <td>0.604775</td>\n","      <td>0.686757</td>\n","      <td>0.677340</td>\n","      <td>0.675211</td>\n","    </tr>\n","    <tr>\n","      <td>14</td>\n","      <td>0.056100</td>\n","      <td>0.445926</td>\n","      <td>0.599469</td>\n","      <td>0.683967</td>\n","      <td>0.674877</td>\n","      <td>0.672388</td>\n","    </tr>\n","    <tr>\n","      <td>15</td>\n","      <td>0.064300</td>\n","      <td>0.445862</td>\n","      <td>0.602122</td>\n","      <td>0.682483</td>\n","      <td>0.674877</td>\n","      <td>0.672087</td>\n","    </tr>\n","    <tr>\n","      <td>16</td>\n","      <td>0.059200</td>\n","      <td>0.446403</td>\n","      <td>0.599469</td>\n","      <td>0.681690</td>\n","      <td>0.674877</td>\n","      <td>0.671345</td>\n","    </tr>\n","    <tr>\n","      <td>17</td>\n","      <td>0.060900</td>\n","      <td>0.446111</td>\n","      <td>0.602122</td>\n","      <td>0.683164</td>\n","      <td>0.674877</td>\n","      <td>0.672222</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Step 10, Training Loss: 0.027862204238772392\n","Step 10, Training Loss: 0.035177070647478104\n","Step 20, Training Loss: 0.04343225434422493\n","Step 20, Training Loss: 0.029841888695955276\n","Step 30, Training Loss: 0.05287642404437065\n","Step 30, Training Loss: 0.039776165038347244\n","Step 40, Training Loss: 0.03262010216712952\n","Step 40, Training Loss: 0.021054673939943314\n"]},{"output_type":"stream","name":"stderr","text":["***** Running Evaluation *****\n","  Num examples = 377\n","  Batch size = 32\n"]},{"output_type":"stream","name":"stdout","text":["Step 50, Training Loss: 0.04354865103960037\n","Step 50, Training Loss: 0.019527072086930275\n","Step 60, Training Loss: 0.03826247155666351\n","Step 60, Training Loss: 0.038622498512268066\n","Step 70, Training Loss: 0.0672805905342102\n","Step 70, Training Loss: 0.034436799585819244\n","Step 80, Training Loss: 0.04413798451423645\n","Step 80, Training Loss: 0.05924639850854874\n","Step 90, Training Loss: 0.01796981319785118\n","Step 90, Training Loss: 0.04047725722193718\n"]},{"output_type":"stream","name":"stderr","text":["***** Running Evaluation *****\n","  Num examples = 377\n","  Batch size = 32\n"]},{"output_type":"stream","name":"stdout","text":["Step 100, Training Loss: 0.04938775300979614\n","Step 100, Training Loss: 0.05801770091056824\n","Step 110, Training Loss: 0.04214262217283249\n","Step 110, Training Loss: 0.03638002276420593\n","Step 120, Training Loss: 0.03754960373044014\n","Step 120, Training Loss: 0.08925028890371323\n","Step 130, Training Loss: 0.04564374312758446\n","Step 130, Training Loss: 0.04487903416156769\n","Step 140, Training Loss: 0.06894221156835556\n","Step 140, Training Loss: 0.01838560961186886\n"]},{"output_type":"stream","name":"stderr","text":["***** Running Evaluation *****\n","  Num examples = 377\n","  Batch size = 32\n"]},{"output_type":"stream","name":"stdout","text":["Step 150, Training Loss: 0.08044931292533875\n","Step 150, Training Loss: 0.02027224376797676\n","Step 160, Training Loss: 0.04064280539751053\n","Step 160, Training Loss: 0.02923957072198391\n","Step 170, Training Loss: 0.061664797365665436\n","Step 170, Training Loss: 0.09448583424091339\n","Step 180, Training Loss: 0.085179902613163\n","Step 180, Training Loss: 0.08257947862148285\n"]},{"output_type":"stream","name":"stderr","text":["***** Running Evaluation *****\n","  Num examples = 377\n","  Batch size = 32\n"]},{"output_type":"stream","name":"stdout","text":["Step 190, Training Loss: 0.13483141362667084\n","Step 190, Training Loss: 0.0544489324092865\n","Step 200, Training Loss: 0.04320727661252022\n","Step 200, Training Loss: 0.026304855942726135\n","Step 210, Training Loss: 0.03635798394680023\n","Step 210, Training Loss: 0.034020330756902695\n","Step 220, Training Loss: 0.0487058125436306\n","Step 220, Training Loss: 0.08883851021528244\n","Step 230, Training Loss: 0.021042950451374054\n","Step 230, Training Loss: 0.07924982160329819\n"]},{"output_type":"stream","name":"stderr","text":["***** Running Evaluation *****\n","  Num examples = 377\n","  Batch size = 32\n"]},{"output_type":"stream","name":"stdout","text":["Step 240, Training Loss: 0.10460706800222397\n","Step 240, Training Loss: 0.07204349339008331\n","Step 250, Training Loss: 0.04394586384296417\n","Step 250, Training Loss: 0.07468298822641373\n","Step 260, Training Loss: 0.05830884724855423\n","Step 260, Training Loss: 0.04357191547751427\n","Step 270, Training Loss: 0.05905459448695183\n","Step 270, Training Loss: 0.07336000353097916\n","Step 280, Training Loss: 0.04705865681171417\n","Step 280, Training Loss: 0.040009886026382446\n"]},{"output_type":"stream","name":"stderr","text":["***** Running Evaluation *****\n","  Num examples = 377\n","  Batch size = 32\n"]},{"output_type":"stream","name":"stdout","text":["Step 290, Training Loss: 0.04529651999473572\n","Step 290, Training Loss: 0.04953012615442276\n","Step 300, Training Loss: 0.030277913436293602\n","Step 300, Training Loss: 0.055425193160772324\n","Step 310, Training Loss: 0.04102820158004761\n","Step 310, Training Loss: 0.058653730899095535\n","Step 320, Training Loss: 0.038656365126371384\n","Step 320, Training Loss: 0.04491540044546127\n"]},{"output_type":"stream","name":"stderr","text":["***** Running Evaluation *****\n","  Num examples = 377\n","  Batch size = 32\n"]},{"output_type":"stream","name":"stdout","text":["Step 330, Training Loss: 0.09784901887178421\n","Step 330, Training Loss: 0.03123927116394043\n","Step 340, Training Loss: 0.04635206609964371\n","Step 340, Training Loss: 0.028355926275253296\n","Step 350, Training Loss: 0.04305919632315636\n","Step 350, Training Loss: 0.08295031636953354\n","Step 360, Training Loss: 0.03248250484466553\n","Step 360, Training Loss: 0.027061522006988525\n","Step 370, Training Loss: 0.10839349031448364\n","Step 370, Training Loss: 0.024578766897320747\n"]},{"output_type":"stream","name":"stderr","text":["***** Running Evaluation *****\n","  Num examples = 377\n","  Batch size = 32\n"]},{"output_type":"stream","name":"stdout","text":["Step 380, Training Loss: 0.030330199748277664\n","Step 380, Training Loss: 0.04550892859697342\n","Step 390, Training Loss: 0.017583269625902176\n","Step 390, Training Loss: 0.07971178740262985\n","Step 400, Training Loss: 0.042670752853155136\n","Step 400, Training Loss: 0.03806982934474945\n","Step 410, Training Loss: 0.09766741842031479\n","Step 410, Training Loss: 0.0582733117043972\n","Step 420, Training Loss: 0.03537523373961449\n","Step 420, Training Loss: 0.06245069578289986\n"]},{"output_type":"stream","name":"stderr","text":["***** Running Evaluation *****\n","  Num examples = 377\n","  Batch size = 32\n"]},{"output_type":"stream","name":"stdout","text":["Step 430, Training Loss: 0.05476513132452965\n","Step 430, Training Loss: 0.13949613273143768\n","Step 440, Training Loss: 0.03509902581572533\n","Step 440, Training Loss: 0.06202467903494835\n","Step 450, Training Loss: 0.09550390392541885\n","Step 450, Training Loss: 0.03446643427014351\n","Step 460, Training Loss: 0.03064105473458767\n","Step 460, Training Loss: 0.022503240033984184\n"]},{"output_type":"stream","name":"stderr","text":["***** Running Evaluation *****\n","  Num examples = 377\n","  Batch size = 32\n"]},{"output_type":"stream","name":"stdout","text":["Step 470, Training Loss: 0.011416024528443813\n","Step 470, Training Loss: 0.6096821427345276\n","Step 470, Training Loss: 0.5737063884735107\n","Step 470, Training Loss: 0.4531024992465973\n","Step 470, Training Loss: 0.28542205691337585\n","Step 470, Training Loss: 0.250876247882843\n","Step 470, Training Loss: 0.3908436894416809\n","Step 470, Training Loss: 0.46889016032218933\n","Step 470, Training Loss: 0.6457986831665039\n","Step 470, Training Loss: 0.6078258156776428\n","Step 470, Training Loss: 0.4246033728122711\n","Step 470, Training Loss: 0.3621797561645508\n","Step 470, Training Loss: 0.20929986238479614\n","Step 470, Training Loss: 0.04083828255534172\n","Step 470, Training Loss: 0.0390465222299099\n","Step 480, Training Loss: 0.056624848395586014\n","Step 480, Training Loss: 0.15969710052013397\n","Step 490, Training Loss: 0.033389266580343246\n","Step 490, Training Loss: 0.03562801703810692\n"]},{"output_type":"stream","name":"stderr","text":["Saving model checkpoint to ./results/checkpoint-500\n","Configuration saved in ./results/checkpoint-500/config.json\n","Model weights saved in ./results/checkpoint-500/pytorch_model.bin\n"]},{"output_type":"stream","name":"stdout","text":["Step 500, Training Loss: 0.058972444385290146\n","Step 500, Training Loss: 0.08909071981906891\n","Step 510, Training Loss: 0.07100599259138107\n","Step 510, Training Loss: 0.03864162787795067\n"]},{"output_type":"stream","name":"stderr","text":["***** Running Evaluation *****\n","  Num examples = 377\n","  Batch size = 32\n"]},{"output_type":"stream","name":"stdout","text":["Step 520, Training Loss: 0.03536940738558769\n","Step 520, Training Loss: 0.025486702099442482\n","Step 530, Training Loss: 0.053467899560928345\n","Step 530, Training Loss: 0.04882093891501427\n","Step 540, Training Loss: 0.023157253861427307\n","Step 540, Training Loss: 0.04209727793931961\n","Step 550, Training Loss: 0.035009149461984634\n","Step 550, Training Loss: 0.09559478610754013\n","Step 560, Training Loss: 0.07118698209524155\n","Step 560, Training Loss: 0.07015477865934372\n"]},{"output_type":"stream","name":"stderr","text":["***** Running Evaluation *****\n","  Num examples = 377\n","  Batch size = 32\n"]},{"output_type":"stream","name":"stdout","text":["Step 570, Training Loss: 0.0287835281342268\n","Step 570, Training Loss: 0.037993792444467545\n","Step 580, Training Loss: 0.05542443320155144\n","Step 580, Training Loss: 0.029604384675621986\n","Step 590, Training Loss: 0.10420861095190048\n","Step 590, Training Loss: 0.04738660529255867\n","Step 600, Training Loss: 0.08567297458648682\n","Step 600, Training Loss: 0.04871562868356705\n","Step 610, Training Loss: 0.05039254575967789\n","Step 610, Training Loss: 0.035207707434892654\n"]},{"output_type":"stream","name":"stderr","text":["***** Running Evaluation *****\n","  Num examples = 377\n","  Batch size = 32\n"]},{"output_type":"stream","name":"stdout","text":["Step 620, Training Loss: 0.02024967037141323\n","Step 620, Training Loss: 0.11574297398328781\n","Step 630, Training Loss: 0.05722513422369957\n","Step 630, Training Loss: 0.06726615130901337\n","Step 640, Training Loss: 0.09736895561218262\n","Step 640, Training Loss: 0.044794436544179916\n","Step 650, Training Loss: 0.03424523398280144\n","Step 650, Training Loss: 0.02558816783130169\n"]},{"output_type":"stream","name":"stderr","text":["***** Running Evaluation *****\n","  Num examples = 377\n","  Batch size = 32\n"]},{"output_type":"stream","name":"stdout","text":["Step 660, Training Loss: 0.03920742869377136\n","Step 660, Training Loss: 0.06920277327299118\n","Step 670, Training Loss: 0.044211406260728836\n","Step 670, Training Loss: 0.030994905158877373\n","Step 680, Training Loss: 0.04115751013159752\n","Step 680, Training Loss: 0.03350253775715828\n","Step 690, Training Loss: 0.08774201571941376\n","Step 690, Training Loss: 0.03626155108213425\n","Step 700, Training Loss: 0.06919658929109573\n","Step 700, Training Loss: 0.026247432455420494\n"]},{"output_type":"stream","name":"stderr","text":["***** Running Evaluation *****\n","  Num examples = 377\n","  Batch size = 32\n"]},{"output_type":"stream","name":"stdout","text":["Step 710, Training Loss: 0.05131247267127037\n","Step 710, Training Loss: 0.08719370514154434\n","Step 720, Training Loss: 0.015281984582543373\n","Step 720, Training Loss: 0.029295533895492554\n","Step 730, Training Loss: 0.056513357907533646\n","Step 730, Training Loss: 0.040756773203611374\n","Step 740, Training Loss: 0.041453320533037186\n","Step 740, Training Loss: 0.025834841653704643\n","Step 750, Training Loss: 0.03455435484647751\n","Step 750, Training Loss: 0.05348493531346321\n"]},{"output_type":"stream","name":"stderr","text":["***** Running Evaluation *****\n","  Num examples = 377\n","  Batch size = 32\n"]},{"output_type":"stream","name":"stdout","text":["Step 760, Training Loss: 0.03905383497476578\n","Step 760, Training Loss: 0.06568784266710281\n","Step 770, Training Loss: 0.049061041325330734\n","Step 770, Training Loss: 0.05367470532655716\n","Step 780, Training Loss: 0.024255288764834404\n","Step 780, Training Loss: 0.0475415363907814\n","Step 790, Training Loss: 0.03142528608441353\n","Step 790, Training Loss: 0.028882477432489395\n"]},{"output_type":"stream","name":"stderr","text":["***** Running Evaluation *****\n","  Num examples = 377\n","  Batch size = 32\n"]},{"output_type":"stream","name":"stdout","text":["Step 800, Training Loss: 0.06992679834365845\n","Step 800, Training Loss: 0.049946267157793045\n","Step 810, Training Loss: 0.05527743324637413\n","Step 810, Training Loss: 0.07320711761713028\n","Step 820, Training Loss: 0.03504825383424759\n","Step 820, Training Loss: 0.05301253870129585\n","Step 830, Training Loss: 0.017335817217826843\n","Step 830, Training Loss: 0.05220770463347435\n","Step 840, Training Loss: 0.1249585896730423\n","Step 840, Training Loss: 0.046747129410505295\n"]},{"output_type":"stream","name":"stderr","text":["***** Running Evaluation *****\n","  Num examples = 377\n","  Batch size = 32\n","\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","***** Running Evaluation *****\n","  Num examples = 377\n","  Batch size = 32\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='12' max='12' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [12/12 00:00]\n","    </div>\n","    "]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["[I 2024-12-01 16:09:48,328] Trial 7 finished with value: 0.6722218369382835 and parameters: {'learning_rate': 1.2468259418680115e-06, 'batch_size': 16, 'num_train_epochs': 18, 'weight_decay': 0.009050834306317597}. Best is trial 7 with value: 0.6722218369382835.\n","<ipython-input-10-747f1883d862>:2: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n","  learning_rate = trial.suggest_loguniform(\"learning_rate\", 1e-6, 5e-5)\n","<ipython-input-10-747f1883d862>:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n","  weight_decay = trial.suggest_loguniform(\"weight_decay\", 1e-4, 0.1)\n","PyTorch: setting up devices\n","/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n","***** Running training *****\n","  Num examples = 1507\n","  Num Epochs = 13\n","  Instantaneous batch size per device = 4\n","  Total train batch size (w. parallel, distributed & accumulation) = 8\n","  Gradient Accumulation steps = 2\n","  Total optimization steps = 2444\n"]},{"output_type":"stream","name":"stdout","text":["Starting training...\n","Step 0, Training Loss: 0.04691531881690025\n","Step 0, Training Loss: 0.019145866855978966\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='2444' max='2444' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [2444/2444 02:16, Epoch 12/13]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","      <th>F1</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>0</td>\n","      <td>0.046000</td>\n","      <td>0.448940</td>\n","      <td>0.599469</td>\n","      <td>0.690036</td>\n","      <td>0.669951</td>\n","      <td>0.672560</td>\n","    </tr>\n","    <tr>\n","      <td>1</td>\n","      <td>0.044000</td>\n","      <td>0.450165</td>\n","      <td>0.599469</td>\n","      <td>0.692754</td>\n","      <td>0.669951</td>\n","      <td>0.671611</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>0.046400</td>\n","      <td>0.452018</td>\n","      <td>0.604775</td>\n","      <td>0.690230</td>\n","      <td>0.669951</td>\n","      <td>0.671416</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>0.043000</td>\n","      <td>0.447438</td>\n","      <td>0.610080</td>\n","      <td>0.693599</td>\n","      <td>0.679803</td>\n","      <td>0.678791</td>\n","    </tr>\n","    <tr>\n","      <td>4</td>\n","      <td>0.067500</td>\n","      <td>0.448656</td>\n","      <td>0.612732</td>\n","      <td>0.698180</td>\n","      <td>0.672414</td>\n","      <td>0.677834</td>\n","    </tr>\n","    <tr>\n","      <td>5</td>\n","      <td>0.041000</td>\n","      <td>0.449061</td>\n","      <td>0.610080</td>\n","      <td>0.697043</td>\n","      <td>0.674877</td>\n","      <td>0.678272</td>\n","    </tr>\n","    <tr>\n","      <td>6</td>\n","      <td>0.047600</td>\n","      <td>0.451221</td>\n","      <td>0.615385</td>\n","      <td>0.693852</td>\n","      <td>0.677340</td>\n","      <td>0.679114</td>\n","    </tr>\n","    <tr>\n","      <td>7</td>\n","      <td>0.060000</td>\n","      <td>0.450825</td>\n","      <td>0.612732</td>\n","      <td>0.694037</td>\n","      <td>0.677340</td>\n","      <td>0.678955</td>\n","    </tr>\n","    <tr>\n","      <td>8</td>\n","      <td>0.038100</td>\n","      <td>0.449279</td>\n","      <td>0.612732</td>\n","      <td>0.688891</td>\n","      <td>0.677340</td>\n","      <td>0.677290</td>\n","    </tr>\n","    <tr>\n","      <td>9</td>\n","      <td>0.037200</td>\n","      <td>0.456592</td>\n","      <td>0.607427</td>\n","      <td>0.684236</td>\n","      <td>0.677340</td>\n","      <td>0.672925</td>\n","    </tr>\n","    <tr>\n","      <td>10</td>\n","      <td>0.036800</td>\n","      <td>0.457147</td>\n","      <td>0.607427</td>\n","      <td>0.687863</td>\n","      <td>0.679803</td>\n","      <td>0.676983</td>\n","    </tr>\n","    <tr>\n","      <td>11</td>\n","      <td>0.051200</td>\n","      <td>0.455059</td>\n","      <td>0.610080</td>\n","      <td>0.692235</td>\n","      <td>0.677340</td>\n","      <td>0.678514</td>\n","    </tr>\n","    <tr>\n","      <td>12</td>\n","      <td>0.036000</td>\n","      <td>0.454091</td>\n","      <td>0.612732</td>\n","      <td>0.690153</td>\n","      <td>0.677340</td>\n","      <td>0.677860</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Step 10, Training Loss: 0.047453779727220535\n","Step 10, Training Loss: 0.07795870304107666\n","Step 20, Training Loss: 0.08404087275266647\n","Step 20, Training Loss: 0.012857034802436829\n","Step 30, Training Loss: 0.023940877988934517\n","Step 30, Training Loss: 0.06038793548941612\n","Step 40, Training Loss: 0.050620127469301224\n","Step 40, Training Loss: 0.015725070610642433\n","Step 50, Training Loss: 0.04620940610766411\n","Step 50, Training Loss: 0.01102670282125473\n","Step 60, Training Loss: 0.0243083368986845\n","Step 60, Training Loss: 0.8502861857414246\n","Step 70, Training Loss: 0.0371229350566864\n","Step 70, Training Loss: 0.06053740531206131\n","Step 80, Training Loss: 0.0550878532230854\n","Step 80, Training Loss: 0.0637691468000412\n","Step 90, Training Loss: 0.037727098912000656\n","Step 90, Training Loss: 0.03237609192728996\n","Step 100, Training Loss: 0.026772215962409973\n","Step 100, Training Loss: 0.15499351918697357\n","Step 110, Training Loss: 0.03672671690583229\n","Step 110, Training Loss: 0.014564146287739277\n","Step 120, Training Loss: 0.019730759784579277\n","Step 120, Training Loss: 0.0943383201956749\n","Step 130, Training Loss: 0.013391489163041115\n","Step 130, Training Loss: 0.03332054615020752\n","Step 140, Training Loss: 0.04275040701031685\n","Step 140, Training Loss: 0.011220671236515045\n","Step 150, Training Loss: 0.018001019954681396\n","Step 150, Training Loss: 0.06720241904258728\n","Step 160, Training Loss: 0.018293961882591248\n","Step 160, Training Loss: 0.029845213517546654\n","Step 170, Training Loss: 0.03139109164476395\n","Step 170, Training Loss: 0.061157744377851486\n","Step 180, Training Loss: 0.03366616740822792\n","Step 180, Training Loss: 0.023008909076452255\n"]},{"output_type":"stream","name":"stderr","text":["***** Running Evaluation *****\n","  Num examples = 377\n","  Batch size = 8\n"]},{"output_type":"stream","name":"stdout","text":["Step 190, Training Loss: 0.2374623864889145\n","Step 190, Training Loss: 0.17120595276355743\n","Step 200, Training Loss: 0.036872994154691696\n","Step 200, Training Loss: 0.04647621512413025\n","Step 210, Training Loss: 0.028417611494660378\n","Step 210, Training Loss: 0.04413089156150818\n","Step 220, Training Loss: 0.03815777972340584\n","Step 220, Training Loss: 0.06808147579431534\n","Step 230, Training Loss: 0.08769740164279938\n","Step 230, Training Loss: 0.026241688057780266\n","Step 240, Training Loss: 0.027946919202804565\n","Step 240, Training Loss: 0.051584769040346146\n","Step 250, Training Loss: 0.12955382466316223\n","Step 250, Training Loss: 0.025331202894449234\n","Step 260, Training Loss: 0.013191401027143002\n","Step 260, Training Loss: 0.04221854731440544\n","Step 270, Training Loss: 0.09407384693622589\n","Step 270, Training Loss: 0.05708816647529602\n","Step 280, Training Loss: 0.03662290796637535\n","Step 280, Training Loss: 0.04160221293568611\n","Step 290, Training Loss: 0.0740356296300888\n","Step 290, Training Loss: 0.013352999463677406\n","Step 300, Training Loss: 0.207784041762352\n","Step 300, Training Loss: 0.07985647022724152\n","Step 310, Training Loss: 0.03524744138121605\n","Step 310, Training Loss: 0.07165143638849258\n","Step 320, Training Loss: 0.038444142788648605\n","Step 320, Training Loss: 0.011063545010983944\n","Step 330, Training Loss: 0.028654640540480614\n","Step 330, Training Loss: 0.28187131881713867\n","Step 340, Training Loss: 0.014546816237270832\n","Step 340, Training Loss: 0.035662561655044556\n","Step 350, Training Loss: 0.018176794052124023\n","Step 350, Training Loss: 0.07824008911848068\n","Step 360, Training Loss: 0.021197650581598282\n","Step 360, Training Loss: 0.021929746493697166\n","Step 370, Training Loss: 0.01162372063845396\n","Step 370, Training Loss: 0.023302702233195305\n"]},{"output_type":"stream","name":"stderr","text":["***** Running Evaluation *****\n","  Num examples = 377\n","  Batch size = 8\n"]},{"output_type":"stream","name":"stdout","text":["Step 380, Training Loss: 0.03854898735880852\n","Step 380, Training Loss: 0.043556295335292816\n","Step 390, Training Loss: 0.01002789381891489\n","Step 390, Training Loss: 0.04375394061207771\n","Step 400, Training Loss: 0.15160603821277618\n","Step 400, Training Loss: 0.04921794682741165\n","Step 410, Training Loss: 0.04056906700134277\n","Step 410, Training Loss: 0.04444928094744682\n","Step 420, Training Loss: 0.09205883741378784\n","Step 420, Training Loss: 0.03745533153414726\n","Step 430, Training Loss: 0.011385411024093628\n","Step 430, Training Loss: 0.011494040489196777\n","Step 440, Training Loss: 0.010935609228909016\n","Step 440, Training Loss: 0.029149532318115234\n","Step 450, Training Loss: 0.04422475025057793\n","Step 450, Training Loss: 0.2010156214237213\n","Step 460, Training Loss: 0.17485058307647705\n","Step 460, Training Loss: 0.02358042262494564\n","Step 470, Training Loss: 0.04660316929221153\n","Step 470, Training Loss: 0.026249656453728676\n","Step 480, Training Loss: 0.06577660888433456\n","Step 480, Training Loss: 0.01806681416928768\n","Step 490, Training Loss: 0.07751340419054031\n","Step 490, Training Loss: 0.02528280019760132\n"]},{"output_type":"stream","name":"stderr","text":["Saving model checkpoint to ./results/checkpoint-500\n","Configuration saved in ./results/checkpoint-500/config.json\n","Model weights saved in ./results/checkpoint-500/pytorch_model.bin\n"]},{"output_type":"stream","name":"stdout","text":["Step 500, Training Loss: 0.03393786773085594\n","Step 500, Training Loss: 0.14745625853538513\n","Step 510, Training Loss: 0.0681544691324234\n","Step 510, Training Loss: 0.10709705203771591\n","Step 520, Training Loss: 0.014803982339799404\n","Step 520, Training Loss: 0.0984787791967392\n","Step 530, Training Loss: 0.048964180052280426\n","Step 530, Training Loss: 0.05470586568117142\n","Step 540, Training Loss: 0.30488094687461853\n","Step 540, Training Loss: 0.04792742058634758\n","Step 550, Training Loss: 0.0639084205031395\n","Step 550, Training Loss: 0.04918448254466057\n","Step 560, Training Loss: 0.0226620901376009\n","Step 560, Training Loss: 0.047047752887010574\n"]},{"output_type":"stream","name":"stderr","text":["***** Running Evaluation *****\n","  Num examples = 377\n","  Batch size = 8\n"]},{"output_type":"stream","name":"stdout","text":["Step 570, Training Loss: 0.04123704507946968\n","Step 570, Training Loss: 0.040485985577106476\n","Step 580, Training Loss: 0.014421028085052967\n","Step 580, Training Loss: 0.026043696328997612\n","Step 590, Training Loss: 0.01875329576432705\n","Step 590, Training Loss: 0.098246730864048\n","Step 600, Training Loss: 0.018916718661785126\n","Step 600, Training Loss: 0.10793554782867432\n","Step 610, Training Loss: 0.04541995748877525\n","Step 610, Training Loss: 0.023459067568182945\n","Step 620, Training Loss: 0.013960130512714386\n","Step 620, Training Loss: 0.050557732582092285\n","Step 630, Training Loss: 0.037762608379125595\n","Step 630, Training Loss: 0.07963421195745468\n","Step 640, Training Loss: 0.0294561218470335\n","Step 640, Training Loss: 0.019395163282752037\n","Step 650, Training Loss: 0.0940609872341156\n","Step 650, Training Loss: 0.11445516347885132\n","Step 660, Training Loss: 0.01221285667270422\n","Step 660, Training Loss: 0.06333896517753601\n","Step 670, Training Loss: 0.04531260207295418\n","Step 670, Training Loss: 0.01599474623799324\n","Step 680, Training Loss: 0.03395511582493782\n","Step 680, Training Loss: 0.016224056482315063\n","Step 690, Training Loss: 0.05075816437602043\n","Step 690, Training Loss: 0.03232550621032715\n","Step 700, Training Loss: 0.022953316569328308\n","Step 700, Training Loss: 0.011166628450155258\n","Step 710, Training Loss: 0.019991958513855934\n","Step 710, Training Loss: 0.04390174522995949\n","Step 720, Training Loss: 0.030047206208109856\n","Step 720, Training Loss: 0.08814790099859238\n","Step 730, Training Loss: 0.02045777067542076\n","Step 730, Training Loss: 0.012910268269479275\n","Step 740, Training Loss: 0.049635808914899826\n","Step 740, Training Loss: 0.03281472995877266\n","Step 750, Training Loss: 0.030840152874588966\n","Step 750, Training Loss: 0.023515308275818825\n"]},{"output_type":"stream","name":"stderr","text":["***** Running Evaluation *****\n","  Num examples = 377\n","  Batch size = 8\n"]},{"output_type":"stream","name":"stdout","text":["Step 760, Training Loss: 0.03368615359067917\n","Step 760, Training Loss: 0.04447302594780922\n","Step 770, Training Loss: 0.03328489512205124\n","Step 770, Training Loss: 0.042324431240558624\n","Step 780, Training Loss: 0.013072534464299679\n","Step 780, Training Loss: 0.01905824802815914\n","Step 790, Training Loss: 0.01828237622976303\n","Step 790, Training Loss: 0.11733784526586533\n","Step 800, Training Loss: 0.04620495066046715\n","Step 800, Training Loss: 0.013702752068638802\n","Step 810, Training Loss: 0.011119766160845757\n","Step 810, Training Loss: 0.023320503532886505\n","Step 820, Training Loss: 0.02149665728211403\n","Step 820, Training Loss: 0.01968122273683548\n","Step 830, Training Loss: 0.11944224685430527\n","Step 830, Training Loss: 0.0681648999452591\n","Step 840, Training Loss: 0.040109675377607346\n","Step 840, Training Loss: 0.020752232521772385\n","Step 850, Training Loss: 0.04410497844219208\n","Step 850, Training Loss: 0.04932825639843941\n","Step 860, Training Loss: 0.02050301991403103\n","Step 860, Training Loss: 0.022428076714277267\n","Step 870, Training Loss: 0.16149276494979858\n","Step 870, Training Loss: 0.05545611307024956\n","Step 880, Training Loss: 0.05700017139315605\n","Step 880, Training Loss: 0.12039339542388916\n","Step 890, Training Loss: 0.014678636565804482\n","Step 890, Training Loss: 0.021536946296691895\n","Step 900, Training Loss: 0.09575637429952621\n","Step 900, Training Loss: 0.014985928311944008\n","Step 910, Training Loss: 0.12293010205030441\n","Step 910, Training Loss: 0.02801426127552986\n","Step 920, Training Loss: 0.020267149433493614\n","Step 920, Training Loss: 0.01611529104411602\n","Step 930, Training Loss: 0.04602888971567154\n","Step 930, Training Loss: 0.03200793266296387\n"]},{"output_type":"stream","name":"stderr","text":["***** Running Evaluation *****\n","  Num examples = 377\n","  Batch size = 8\n"]},{"output_type":"stream","name":"stdout","text":["Step 940, Training Loss: 0.022539585828781128\n","Step 940, Training Loss: 0.29472023248672485\n","Step 940, Training Loss: 0.6150927543640137\n","Step 940, Training Loss: 0.39246875047683716\n","Step 940, Training Loss: 1.23458993434906\n","Step 940, Training Loss: 0.5622827410697937\n","Step 940, Training Loss: 0.4334384500980377\n","Step 940, Training Loss: 0.9367246627807617\n","Step 940, Training Loss: 0.35107967257499695\n","Step 940, Training Loss: 0.038776542991399765\n","Step 940, Training Loss: 0.6346940398216248\n","Step 940, Training Loss: 0.8094450235366821\n","Step 940, Training Loss: 0.362425297498703\n","Step 940, Training Loss: 0.36423370242118835\n","Step 940, Training Loss: 0.014364625327289104\n","Step 940, Training Loss: 0.2566138803958893\n","Step 940, Training Loss: 0.5377515554428101\n","Step 940, Training Loss: 0.48019781708717346\n","Step 940, Training Loss: 0.3562237322330475\n","Step 940, Training Loss: 0.1618461161851883\n","Step 940, Training Loss: 0.05049365758895874\n","Step 940, Training Loss: 0.2334553450345993\n","Step 940, Training Loss: 0.40465855598449707\n","Step 940, Training Loss: 0.6882573962211609\n","Step 940, Training Loss: 0.24823763966560364\n","Step 940, Training Loss: 0.578262984752655\n","Step 940, Training Loss: 0.3959437906742096\n","Step 940, Training Loss: 0.3196798861026764\n","Step 940, Training Loss: 0.5720359086990356\n","Step 940, Training Loss: 0.672188937664032\n","Step 940, Training Loss: 0.565633237361908\n","Step 940, Training Loss: 0.6039286255836487\n","Step 940, Training Loss: 0.8062983751296997\n","Step 940, Training Loss: 1.2286758422851562\n","Step 940, Training Loss: 0.486910343170166\n","Step 940, Training Loss: 0.27020543813705444\n","Step 940, Training Loss: 0.4244095981121063\n","Step 940, Training Loss: 0.26271307468414307\n","Step 940, Training Loss: 0.2963230311870575\n","Step 940, Training Loss: 0.7701901793479919\n","Step 940, Training Loss: 0.4179879128932953\n","Step 940, Training Loss: 0.573257565498352\n","Step 940, Training Loss: 0.2388119250535965\n","Step 940, Training Loss: 0.4500410556793213\n","Step 940, Training Loss: 0.16813310980796814\n","Step 940, Training Loss: 0.11299238353967667\n","Step 940, Training Loss: 0.17178885638713837\n","Step 940, Training Loss: 0.29323098063468933\n","Step 940, Training Loss: 0.00966726616024971\n","Step 940, Training Loss: 0.06579235941171646\n","Step 940, Training Loss: 0.04222091659903526\n","Step 950, Training Loss: 0.026335272938013077\n","Step 950, Training Loss: 0.06010187789797783\n","Step 960, Training Loss: 0.07620058208703995\n","Step 960, Training Loss: 0.03271215036511421\n","Step 970, Training Loss: 0.046138714998960495\n","Step 970, Training Loss: 0.06485988944768906\n","Step 980, Training Loss: 0.04674729332327843\n","Step 980, Training Loss: 0.03785541653633118\n","Step 990, Training Loss: 0.01005244068801403\n","Step 990, Training Loss: 0.015007826499640942\n"]},{"output_type":"stream","name":"stderr","text":["Saving model checkpoint to ./results/checkpoint-1000\n","Configuration saved in ./results/checkpoint-1000/config.json\n","Model weights saved in ./results/checkpoint-1000/pytorch_model.bin\n"]},{"output_type":"stream","name":"stdout","text":["Step 1000, Training Loss: 0.027227280661463737\n","Step 1000, Training Loss: 0.06308116763830185\n","Step 1010, Training Loss: 0.015319532714784145\n","Step 1010, Training Loss: 0.012567630968987942\n","Step 1020, Training Loss: 0.03666146472096443\n","Step 1020, Training Loss: 0.17136870324611664\n","Step 1030, Training Loss: 0.034747976809740067\n","Step 1030, Training Loss: 0.0316801480948925\n","Step 1040, Training Loss: 0.023609107360243797\n","Step 1040, Training Loss: 0.04393267631530762\n","Step 1050, Training Loss: 0.03438931331038475\n","Step 1050, Training Loss: 0.10075531154870987\n","Step 1060, Training Loss: 0.07185885310173035\n","Step 1060, Training Loss: 0.026120910421013832\n","Step 1070, Training Loss: 0.010693791322410107\n","Step 1070, Training Loss: 0.024200191721320152\n","Step 1080, Training Loss: 0.09939794987440109\n","Step 1080, Training Loss: 0.031453341245651245\n","Step 1090, Training Loss: 0.017456412315368652\n","Step 1090, Training Loss: 0.0392141193151474\n","Step 1100, Training Loss: 0.017085731029510498\n","Step 1100, Training Loss: 0.019327238202095032\n","Step 1110, Training Loss: 0.019765248522162437\n","Step 1110, Training Loss: 0.03645653277635574\n","Step 1120, Training Loss: 0.024748409166932106\n","Step 1120, Training Loss: 0.06660410016775131\n"]},{"output_type":"stream","name":"stderr","text":["***** Running Evaluation *****\n","  Num examples = 377\n","  Batch size = 8\n"]},{"output_type":"stream","name":"stdout","text":["Step 1130, Training Loss: 0.08689829707145691\n","Step 1130, Training Loss: 0.019605373963713646\n","Step 1140, Training Loss: 0.028813708573579788\n","Step 1140, Training Loss: 0.038934871554374695\n","Step 1150, Training Loss: 0.037836480885744095\n","Step 1150, Training Loss: 0.33566519618034363\n","Step 1160, Training Loss: 0.04775070771574974\n","Step 1160, Training Loss: 0.017987137660384178\n","Step 1170, Training Loss: 0.01828400231897831\n","Step 1170, Training Loss: 0.0438370406627655\n","Step 1180, Training Loss: 0.06551330536603928\n","Step 1180, Training Loss: 0.06113402917981148\n","Step 1190, Training Loss: 0.013143395073711872\n","Step 1190, Training Loss: 0.023131510242819786\n","Step 1200, Training Loss: 0.031036842614412308\n","Step 1200, Training Loss: 0.017984682694077492\n","Step 1210, Training Loss: 0.03210502490401268\n","Step 1210, Training Loss: 0.0122990682721138\n","Step 1220, Training Loss: 0.019080299884080887\n","Step 1220, Training Loss: 0.023581281304359436\n","Step 1230, Training Loss: 0.03296784684062004\n","Step 1230, Training Loss: 0.09756956249475479\n","Step 1240, Training Loss: 0.0244282279163599\n","Step 1240, Training Loss: 0.017699358984827995\n","Step 1250, Training Loss: 0.03705465793609619\n","Step 1250, Training Loss: 0.02650359272956848\n","Step 1260, Training Loss: 0.48775872588157654\n","Step 1260, Training Loss: 0.04099143296480179\n","Step 1270, Training Loss: 0.05006968975067139\n","Step 1270, Training Loss: 0.0369730107486248\n","Step 1280, Training Loss: 0.015495826490223408\n","Step 1280, Training Loss: 0.07843022793531418\n","Step 1290, Training Loss: 0.051693715155124664\n","Step 1290, Training Loss: 0.06949196010828018\n","Step 1300, Training Loss: 0.035937611013650894\n","Step 1300, Training Loss: 0.014561617746949196\n","Step 1310, Training Loss: 0.013459202833473682\n","Step 1310, Training Loss: 0.10936131328344345\n"]},{"output_type":"stream","name":"stderr","text":["***** Running Evaluation *****\n","  Num examples = 377\n","  Batch size = 8\n"]},{"output_type":"stream","name":"stdout","text":["Step 1320, Training Loss: 0.16922783851623535\n","Step 1320, Training Loss: 0.013112956658005714\n","Step 1330, Training Loss: 0.034715909510850906\n","Step 1330, Training Loss: 0.07359924167394638\n","Step 1340, Training Loss: 0.010712034069001675\n","Step 1340, Training Loss: 0.022260740399360657\n","Step 1350, Training Loss: 0.03745165467262268\n","Step 1350, Training Loss: 0.04308704659342766\n","Step 1360, Training Loss: 0.05246194824576378\n","Step 1360, Training Loss: 0.026410428807139397\n","Step 1370, Training Loss: 0.08226316422224045\n","Step 1370, Training Loss: 0.011376532725989819\n","Step 1380, Training Loss: 0.035289086401462555\n","Step 1380, Training Loss: 0.09751022607088089\n","Step 1390, Training Loss: 0.012564346194267273\n","Step 1390, Training Loss: 0.018065284937620163\n","Step 1400, Training Loss: 0.01170079130679369\n","Step 1400, Training Loss: 0.02760186232626438\n","Step 1410, Training Loss: 0.0306051317602396\n","Step 1410, Training Loss: 0.017921889200806618\n","Step 1420, Training Loss: 0.03283968195319176\n","Step 1420, Training Loss: 0.06995391100645065\n","Step 1430, Training Loss: 0.1315857619047165\n","Step 1430, Training Loss: 0.04110197350382805\n","Step 1440, Training Loss: 0.04146137461066246\n","Step 1440, Training Loss: 0.03534184768795967\n","Step 1450, Training Loss: 0.05198192596435547\n","Step 1450, Training Loss: 0.0230057742446661\n","Step 1460, Training Loss: 0.047703903168439865\n","Step 1460, Training Loss: 0.038174182176589966\n","Step 1470, Training Loss: 0.22810876369476318\n","Step 1470, Training Loss: 0.04774903506040573\n","Step 1480, Training Loss: 0.10774368047714233\n","Step 1480, Training Loss: 0.05194249749183655\n","Step 1490, Training Loss: 0.02944510243833065\n","Step 1490, Training Loss: 0.013489998877048492\n"]},{"output_type":"stream","name":"stderr","text":["Saving model checkpoint to ./results/checkpoint-1500\n","Configuration saved in ./results/checkpoint-1500/config.json\n","Model weights saved in ./results/checkpoint-1500/pytorch_model.bin\n"]},{"output_type":"stream","name":"stdout","text":["Step 1500, Training Loss: 0.013824030756950378\n","Step 1500, Training Loss: 0.03523309528827667\n"]},{"output_type":"stream","name":"stderr","text":["***** Running Evaluation *****\n","  Num examples = 377\n","  Batch size = 8\n"]},{"output_type":"stream","name":"stdout","text":["Step 1510, Training Loss: 0.020877858623862267\n","Step 1510, Training Loss: 0.011476344428956509\n","Step 1520, Training Loss: 0.03549778088927269\n","Step 1520, Training Loss: 0.01370316743850708\n","Step 1530, Training Loss: 0.07600461691617966\n","Step 1530, Training Loss: 0.03603377193212509\n","Step 1540, Training Loss: 0.03337061405181885\n","Step 1540, Training Loss: 0.08602041006088257\n","Step 1550, Training Loss: 0.032293420284986496\n","Step 1550, Training Loss: 0.01801093854010105\n","Step 1560, Training Loss: 0.39127397537231445\n","Step 1560, Training Loss: 0.03628268837928772\n","Step 1570, Training Loss: 0.01802801713347435\n","Step 1570, Training Loss: 0.01921057514846325\n","Step 1580, Training Loss: 0.021054739132523537\n","Step 1580, Training Loss: 0.013175496831536293\n","Step 1590, Training Loss: 0.020198805257678032\n","Step 1590, Training Loss: 0.010080302134156227\n","Step 1600, Training Loss: 0.037351589649915695\n","Step 1600, Training Loss: 0.03128398582339287\n","Step 1610, Training Loss: 0.06983337551355362\n","Step 1610, Training Loss: 0.01668093539774418\n","Step 1620, Training Loss: 0.0665181502699852\n","Step 1620, Training Loss: 0.050612952560186386\n","Step 1630, Training Loss: 0.13449892401695251\n","Step 1630, Training Loss: 0.0818154364824295\n","Step 1640, Training Loss: 0.05179748684167862\n","Step 1640, Training Loss: 0.24163101613521576\n","Step 1650, Training Loss: 0.017245516180992126\n","Step 1650, Training Loss: 0.023718947544693947\n","Step 1660, Training Loss: 0.2343679517507553\n","Step 1660, Training Loss: 0.04300155118107796\n","Step 1670, Training Loss: 0.0155330840498209\n","Step 1670, Training Loss: 0.019292891025543213\n","Step 1680, Training Loss: 0.036579687148332596\n","Step 1680, Training Loss: 0.031388331204652786\n","Step 1690, Training Loss: 0.018380705267190933\n","Step 1690, Training Loss: 0.0192173570394516\n"]},{"output_type":"stream","name":"stderr","text":["***** Running Evaluation *****\n","  Num examples = 377\n","  Batch size = 8\n"]},{"output_type":"stream","name":"stdout","text":["Step 1700, Training Loss: 0.11931406706571579\n","Step 1700, Training Loss: 0.13100650906562805\n","Step 1710, Training Loss: 0.03652995452284813\n","Step 1710, Training Loss: 0.06892960518598557\n","Step 1720, Training Loss: 0.06862259656190872\n","Step 1720, Training Loss: 0.02204173244535923\n","Step 1730, Training Loss: 0.025228172540664673\n","Step 1730, Training Loss: 0.01688467152416706\n","Step 1740, Training Loss: 0.027442295104265213\n","Step 1740, Training Loss: 0.01649651490151882\n","Step 1750, Training Loss: 0.03280703350901604\n","Step 1750, Training Loss: 0.026581628248095512\n","Step 1760, Training Loss: 0.05279356986284256\n","Step 1760, Training Loss: 0.011574654839932919\n","Step 1770, Training Loss: 0.05910830572247505\n","Step 1770, Training Loss: 0.015036714263260365\n","Step 1780, Training Loss: 0.02087889052927494\n","Step 1780, Training Loss: 0.025328202173113823\n","Step 1790, Training Loss: 0.01596308872103691\n","Step 1790, Training Loss: 0.014172126539051533\n","Step 1800, Training Loss: 0.013080300763249397\n","Step 1800, Training Loss: 0.03488459810614586\n","Step 1810, Training Loss: 0.011068984866142273\n","Step 1810, Training Loss: 0.011193160898983479\n","Step 1820, Training Loss: 0.021118393167853355\n","Step 1820, Training Loss: 0.015916690230369568\n","Step 1830, Training Loss: 0.016035785898566246\n","Step 1830, Training Loss: 0.029227405786514282\n","Step 1840, Training Loss: 0.016872167587280273\n","Step 1840, Training Loss: 0.013428292237222195\n","Step 1850, Training Loss: 0.010549728758633137\n","Step 1850, Training Loss: 0.041170135140419006\n","Step 1860, Training Loss: 0.07747763395309448\n","Step 1860, Training Loss: 0.16036537289619446\n","Step 1870, Training Loss: 0.01580294780433178\n","Step 1870, Training Loss: 0.07012671232223511\n"]},{"output_type":"stream","name":"stderr","text":["***** Running Evaluation *****\n","  Num examples = 377\n","  Batch size = 8\n"]},{"output_type":"stream","name":"stdout","text":["Step 1880, Training Loss: 0.01821092888712883\n","Step 1880, Training Loss: 0.32497310638427734\n","Step 1880, Training Loss: 0.6232871413230896\n","Step 1880, Training Loss: 0.3857496380805969\n","Step 1880, Training Loss: 1.2420275211334229\n","Step 1880, Training Loss: 0.5719897150993347\n","Step 1880, Training Loss: 0.45096760988235474\n","Step 1880, Training Loss: 0.9436573386192322\n","Step 1880, Training Loss: 0.3611748218536377\n","Step 1880, Training Loss: 0.03451431915163994\n","Step 1880, Training Loss: 0.6251901984214783\n","Step 1880, Training Loss: 0.8272099494934082\n","Step 1880, Training Loss: 0.37132251262664795\n","Step 1880, Training Loss: 0.3747040927410126\n","Step 1880, Training Loss: 0.013769187033176422\n","Step 1880, Training Loss: 0.24830327928066254\n","Step 1880, Training Loss: 0.5392123460769653\n","Step 1880, Training Loss: 0.5188018679618835\n","Step 1880, Training Loss: 0.3659650981426239\n","Step 1880, Training Loss: 0.15850774943828583\n","Step 1880, Training Loss: 0.06086720898747444\n","Step 1880, Training Loss: 0.23923693597316742\n","Step 1880, Training Loss: 0.40138837695121765\n","Step 1880, Training Loss: 0.6865021586418152\n","Step 1880, Training Loss: 0.25012925267219543\n","Step 1880, Training Loss: 0.5808011889457703\n","Step 1880, Training Loss: 0.37989649176597595\n","Step 1880, Training Loss: 0.32358431816101074\n","Step 1880, Training Loss: 0.5772827863693237\n","Step 1880, Training Loss: 0.6722136735916138\n","Step 1880, Training Loss: 0.5720962882041931\n","Step 1880, Training Loss: 0.6158677339553833\n","Step 1880, Training Loss: 0.8240861296653748\n","Step 1880, Training Loss: 1.2573679685592651\n","Step 1880, Training Loss: 0.5086113214492798\n","Step 1880, Training Loss: 0.273419052362442\n","Step 1880, Training Loss: 0.45766210556030273\n","Step 1880, Training Loss: 0.2666250765323639\n","Step 1880, Training Loss: 0.31885918974876404\n","Step 1880, Training Loss: 0.7794515490531921\n","Step 1880, Training Loss: 0.42435646057128906\n","Step 1880, Training Loss: 0.5754019021987915\n","Step 1880, Training Loss: 0.22389058768749237\n","Step 1880, Training Loss: 0.46130800247192383\n","Step 1880, Training Loss: 0.16643071174621582\n","Step 1880, Training Loss: 0.11286325752735138\n","Step 1880, Training Loss: 0.18596087396144867\n","Step 1880, Training Loss: 0.33823710680007935\n","Step 1880, Training Loss: 0.00942606758326292\n","Step 1880, Training Loss: 0.047936875373125076\n","Step 1880, Training Loss: 0.03703070059418678\n","Step 1890, Training Loss: 0.056109022349119186\n","Step 1890, Training Loss: 0.014760911464691162\n","Step 1900, Training Loss: 0.1288870871067047\n","Step 1900, Training Loss: 0.019944606348872185\n","Step 1910, Training Loss: 0.033438540995121\n","Step 1910, Training Loss: 0.015802912414073944\n","Step 1920, Training Loss: 0.12639400362968445\n","Step 1920, Training Loss: 0.037206996232271194\n","Step 1930, Training Loss: 0.03784758970141411\n","Step 1930, Training Loss: 0.03827160969376564\n","Step 1940, Training Loss: 0.042611803859472275\n","Step 1940, Training Loss: 0.1238272562623024\n","Step 1950, Training Loss: 0.01235202606767416\n","Step 1950, Training Loss: 0.10442515462636948\n","Step 1960, Training Loss: 0.07946489006280899\n","Step 1960, Training Loss: 0.019791696220636368\n","Step 1970, Training Loss: 0.026219118386507034\n","Step 1970, Training Loss: 0.04415734484791756\n","Step 1980, Training Loss: 0.015054640360176563\n","Step 1980, Training Loss: 0.02464442327618599\n","Step 1990, Training Loss: 0.013841277919709682\n","Step 1990, Training Loss: 0.013552293181419373\n"]},{"output_type":"stream","name":"stderr","text":["Saving model checkpoint to ./results/checkpoint-2000\n","Configuration saved in ./results/checkpoint-2000/config.json\n","Model weights saved in ./results/checkpoint-2000/pytorch_model.bin\n"]},{"output_type":"stream","name":"stdout","text":["Step 2000, Training Loss: 0.024680016562342644\n","Step 2000, Training Loss: 0.030788486823439598\n","Step 2010, Training Loss: 0.014533020555973053\n","Step 2010, Training Loss: 0.021007223054766655\n","Step 2020, Training Loss: 0.025319969281554222\n","Step 2020, Training Loss: 0.04955620318651199\n","Step 2030, Training Loss: 0.01052247267216444\n","Step 2030, Training Loss: 0.052029937505722046\n","Step 2040, Training Loss: 0.05644438415765762\n","Step 2040, Training Loss: 0.019007673487067223\n","Step 2050, Training Loss: 0.021570319309830666\n","Step 2050, Training Loss: 0.031132180243730545\n","Step 2060, Training Loss: 0.026528239250183105\n","Step 2060, Training Loss: 0.03487200662493706\n"]},{"output_type":"stream","name":"stderr","text":["***** Running Evaluation *****\n","  Num examples = 377\n","  Batch size = 8\n"]},{"output_type":"stream","name":"stdout","text":["Step 2070, Training Loss: 0.06068810448050499\n","Step 2070, Training Loss: 0.05596434697508812\n","Step 2080, Training Loss: 0.018154246732592583\n","Step 2080, Training Loss: 0.016305288299918175\n","Step 2090, Training Loss: 0.018167739734053612\n","Step 2090, Training Loss: 0.02367577515542507\n","Step 2100, Training Loss: 0.012262108735740185\n","Step 2100, Training Loss: 0.029860887676477432\n","Step 2110, Training Loss: 0.017998291179537773\n","Step 2110, Training Loss: 0.042515236884355545\n","Step 2120, Training Loss: 0.04426286742091179\n","Step 2120, Training Loss: 0.01936887949705124\n","Step 2130, Training Loss: 0.018703216686844826\n","Step 2130, Training Loss: 0.03398020938038826\n","Step 2140, Training Loss: 0.04474922642111778\n","Step 2140, Training Loss: 0.02420026995241642\n","Step 2150, Training Loss: 0.039860036224126816\n","Step 2150, Training Loss: 0.037247877568006516\n","Step 2160, Training Loss: 0.010439529083669186\n","Step 2160, Training Loss: 0.01114631351083517\n","Step 2170, Training Loss: 0.022249361500144005\n","Step 2170, Training Loss: 0.01827358268201351\n","Step 2180, Training Loss: 0.3860195577144623\n","Step 2180, Training Loss: 0.011663096956908703\n","Step 2190, Training Loss: 0.14235897362232208\n","Step 2190, Training Loss: 0.09660952538251877\n","Step 2200, Training Loss: 0.018688511103391647\n","Step 2200, Training Loss: 0.036302726715803146\n","Step 2210, Training Loss: 0.16392119228839874\n","Step 2210, Training Loss: 0.027445048093795776\n","Step 2220, Training Loss: 0.012867189943790436\n","Step 2220, Training Loss: 0.10083246231079102\n","Step 2230, Training Loss: 0.012296594679355621\n","Step 2230, Training Loss: 0.03270609676837921\n","Step 2240, Training Loss: 0.009507718496024609\n","Step 2240, Training Loss: 0.11329370737075806\n","Step 2250, Training Loss: 0.027193671092391014\n","Step 2250, Training Loss: 0.06124836951494217\n"]},{"output_type":"stream","name":"stderr","text":["***** Running Evaluation *****\n","  Num examples = 377\n","  Batch size = 8\n"]},{"output_type":"stream","name":"stdout","text":["Step 2260, Training Loss: 0.21639029681682587\n","Step 2260, Training Loss: 0.32969585061073303\n","Step 2270, Training Loss: 0.01224168948829174\n","Step 2270, Training Loss: 0.02813604846596718\n","Step 2280, Training Loss: 0.04915505647659302\n","Step 2280, Training Loss: 0.01721291057765484\n","Step 2290, Training Loss: 0.040650393813848495\n","Step 2290, Training Loss: 0.11088304966688156\n","Step 2300, Training Loss: 0.07955814152956009\n","Step 2300, Training Loss: 0.07881032675504684\n","Step 2310, Training Loss: 0.01500671822577715\n","Step 2310, Training Loss: 0.014642051421105862\n","Step 2320, Training Loss: 0.052179086953401566\n","Step 2320, Training Loss: 0.03833878040313721\n","Step 2330, Training Loss: 0.04379533603787422\n","Step 2330, Training Loss: 0.025790104642510414\n","Step 2340, Training Loss: 0.01825566031038761\n","Step 2340, Training Loss: 0.0505533292889595\n","Step 2350, Training Loss: 0.02525174617767334\n","Step 2350, Training Loss: 0.01679261028766632\n","Step 2360, Training Loss: 0.04318232461810112\n","Step 2360, Training Loss: 0.2892118990421295\n","Step 2370, Training Loss: 0.020159101113677025\n","Step 2370, Training Loss: 0.015973342582583427\n","Step 2380, Training Loss: 0.02526971697807312\n","Step 2380, Training Loss: 0.05572919920086861\n","Step 2390, Training Loss: 0.013768418692052364\n","Step 2390, Training Loss: 0.06043366715312004\n","Step 2400, Training Loss: 0.037244826555252075\n","Step 2400, Training Loss: 0.011419057846069336\n","Step 2410, Training Loss: 0.1957380771636963\n","Step 2410, Training Loss: 0.16951388120651245\n","Step 2420, Training Loss: 0.024076534435153008\n","Step 2420, Training Loss: 0.02769326977431774\n","Step 2430, Training Loss: 0.0159433763474226\n","Step 2430, Training Loss: 0.037835314869880676\n","Step 2440, Training Loss: 0.05124703049659729\n","Step 2440, Training Loss: 0.03178514540195465\n"]},{"output_type":"stream","name":"stderr","text":["***** Running Evaluation *****\n","  Num examples = 377\n","  Batch size = 8\n","\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","***** Running Evaluation *****\n","  Num examples = 377\n","  Batch size = 8\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='48' max='48' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [48/48 00:00]\n","    </div>\n","    "]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["[I 2024-12-01 16:12:06,117] Trial 8 finished with value: 0.6778603278049378 and parameters: {'learning_rate': 1.1373930355326894e-06, 'batch_size': 4, 'num_train_epochs': 13, 'weight_decay': 0.0025370726106497773}. Best is trial 8 with value: 0.6778603278049378.\n","<ipython-input-10-747f1883d862>:2: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n","  learning_rate = trial.suggest_loguniform(\"learning_rate\", 1e-6, 5e-5)\n","<ipython-input-10-747f1883d862>:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n","  weight_decay = trial.suggest_loguniform(\"weight_decay\", 1e-4, 0.1)\n","PyTorch: setting up devices\n","/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n","***** Running training *****\n","  Num examples = 1507\n","  Num Epochs = 4\n","  Instantaneous batch size per device = 4\n","  Total train batch size (w. parallel, distributed & accumulation) = 8\n","  Gradient Accumulation steps = 2\n","  Total optimization steps = 752\n"]},{"output_type":"stream","name":"stdout","text":["Starting training...\n","Step 0, Training Loss: 0.0438164584338665\n","Step 0, Training Loss: 0.016909776255488396\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='752' max='752' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [752/752 00:46, Epoch 3/4]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","      <th>F1</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>0</td>\n","      <td>0.042300</td>\n","      <td>0.468879</td>\n","      <td>0.599469</td>\n","      <td>0.694324</td>\n","      <td>0.665025</td>\n","      <td>0.670718</td>\n","    </tr>\n","    <tr>\n","      <td>1</td>\n","      <td>0.040400</td>\n","      <td>0.475410</td>\n","      <td>0.596817</td>\n","      <td>0.676657</td>\n","      <td>0.677340</td>\n","      <td>0.670255</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>0.031800</td>\n","      <td>0.483182</td>\n","      <td>0.607427</td>\n","      <td>0.695061</td>\n","      <td>0.669951</td>\n","      <td>0.671670</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>0.029600</td>\n","      <td>0.473590</td>\n","      <td>0.612732</td>\n","      <td>0.698857</td>\n","      <td>0.677340</td>\n","      <td>0.681460</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Step 10, Training Loss: 0.04175420477986336\n","Step 10, Training Loss: 0.07002245634794235\n","Step 20, Training Loss: 0.06780752539634705\n","Step 20, Training Loss: 0.011700581759214401\n","Step 30, Training Loss: 0.02564012072980404\n","Step 30, Training Loss: 0.07722987234592438\n","Step 40, Training Loss: 0.040502846240997314\n","Step 40, Training Loss: 0.014099068008363247\n","Step 50, Training Loss: 0.04600495845079422\n","Step 50, Training Loss: 0.00935643445700407\n","Step 60, Training Loss: 0.021769536659121513\n","Step 60, Training Loss: 0.9513136744499207\n","Step 70, Training Loss: 0.02900303713977337\n","Step 70, Training Loss: 0.06611472368240356\n","Step 80, Training Loss: 0.047855276614427567\n","Step 80, Training Loss: 0.0540604293346405\n","Step 90, Training Loss: 0.03644761070609093\n","Step 90, Training Loss: 0.024684933945536613\n","Step 100, Training Loss: 0.019999543204903603\n","Step 100, Training Loss: 0.19713643193244934\n","Step 110, Training Loss: 0.02954958751797676\n","Step 110, Training Loss: 0.012724757194519043\n","Step 120, Training Loss: 0.01572621800005436\n","Step 120, Training Loss: 0.07253225147724152\n","Step 130, Training Loss: 0.011375938542187214\n","Step 130, Training Loss: 0.02610746957361698\n","Step 140, Training Loss: 0.03716270253062248\n","Step 140, Training Loss: 0.009642966091632843\n","Step 150, Training Loss: 0.015058308839797974\n","Step 150, Training Loss: 0.04221086576581001\n","Step 160, Training Loss: 0.01567363552749157\n","Step 160, Training Loss: 0.02619808353483677\n","Step 170, Training Loss: 0.02408144809305668\n","Step 170, Training Loss: 0.058437880128622055\n","Step 180, Training Loss: 0.03197813779115677\n","Step 180, Training Loss: 0.01780577376484871\n"]},{"output_type":"stream","name":"stderr","text":["***** Running Evaluation *****\n","  Num examples = 377\n","  Batch size = 8\n"]},{"output_type":"stream","name":"stdout","text":["Step 190, Training Loss: 0.28747788071632385\n","Step 190, Training Loss: 0.1838599294424057\n","Step 200, Training Loss: 0.026326585561037064\n","Step 200, Training Loss: 0.03432532772421837\n","Step 210, Training Loss: 0.019616156816482544\n","Step 210, Training Loss: 0.03268274664878845\n","Step 220, Training Loss: 0.03139067813754082\n","Step 220, Training Loss: 0.04849392920732498\n","Step 230, Training Loss: 0.029126524925231934\n","Step 230, Training Loss: 0.02000550925731659\n","Step 240, Training Loss: 0.02260364219546318\n","Step 240, Training Loss: 0.05703042820096016\n","Step 250, Training Loss: 0.13937515020370483\n","Step 250, Training Loss: 0.020004872232675552\n","Step 260, Training Loss: 0.011052938178181648\n","Step 260, Training Loss: 0.030758893117308617\n","Step 270, Training Loss: 0.09822293370962143\n","Step 270, Training Loss: 0.052801765501499176\n","Step 280, Training Loss: 0.028737885877490044\n","Step 280, Training Loss: 0.04357169568538666\n","Step 290, Training Loss: 0.05983106046915054\n","Step 290, Training Loss: 0.008733510971069336\n","Step 300, Training Loss: 0.20083443820476532\n","Step 300, Training Loss: 0.040791112929582596\n","Step 310, Training Loss: 0.024164147675037384\n","Step 310, Training Loss: 0.052277565002441406\n","Step 320, Training Loss: 0.029631126672029495\n","Step 320, Training Loss: 0.00874082650989294\n","Step 330, Training Loss: 0.02419041097164154\n","Step 330, Training Loss: 0.3055141866207123\n","Step 340, Training Loss: 0.013022500090301037\n","Step 340, Training Loss: 0.021208956837654114\n","Step 350, Training Loss: 0.013084507547318935\n","Step 350, Training Loss: 0.047213200479745865\n","Step 360, Training Loss: 0.015685195103287697\n","Step 360, Training Loss: 0.0165000781416893\n","Step 370, Training Loss: 0.009136583656072617\n","Step 370, Training Loss: 0.020229069516062737\n"]},{"output_type":"stream","name":"stderr","text":["***** Running Evaluation *****\n","  Num examples = 377\n","  Batch size = 8\n"]},{"output_type":"stream","name":"stdout","text":["Step 380, Training Loss: 0.0273419376462698\n","Step 380, Training Loss: 0.040202852338552475\n","Step 390, Training Loss: 0.007740852423012257\n","Step 390, Training Loss: 0.05028944090008736\n","Step 400, Training Loss: 0.10256799310445786\n","Step 400, Training Loss: 0.034477557986974716\n","Step 410, Training Loss: 0.02712392248213291\n","Step 410, Training Loss: 0.039489589631557465\n","Step 420, Training Loss: 0.09162118285894394\n","Step 420, Training Loss: 0.02389616146683693\n","Step 430, Training Loss: 0.00837353803217411\n","Step 430, Training Loss: 0.008953160606324673\n","Step 440, Training Loss: 0.008349868468940258\n","Step 440, Training Loss: 0.024154609069228172\n","Step 450, Training Loss: 0.02196567691862583\n","Step 450, Training Loss: 0.20772750675678253\n","Step 460, Training Loss: 0.10803835839033127\n","Step 460, Training Loss: 0.018553996458649635\n","Step 470, Training Loss: 0.038075365126132965\n","Step 470, Training Loss: 0.01745787262916565\n","Step 480, Training Loss: 0.041000958532094955\n","Step 480, Training Loss: 0.013693282380700111\n","Step 490, Training Loss: 0.052390243858098984\n","Step 490, Training Loss: 0.02136361598968506\n"]},{"output_type":"stream","name":"stderr","text":["Saving model checkpoint to ./results/checkpoint-500\n","Configuration saved in ./results/checkpoint-500/config.json\n","Model weights saved in ./results/checkpoint-500/pytorch_model.bin\n"]},{"output_type":"stream","name":"stdout","text":["Step 500, Training Loss: 0.03396991640329361\n","Step 500, Training Loss: 0.15301118791103363\n","Step 510, Training Loss: 0.03975722938776016\n","Step 510, Training Loss: 0.09353449195623398\n","Step 520, Training Loss: 0.0151903061196208\n","Step 520, Training Loss: 0.061729706823825836\n","Step 530, Training Loss: 0.04357753321528435\n","Step 530, Training Loss: 0.041094135493040085\n","Step 540, Training Loss: 0.3797776401042938\n","Step 540, Training Loss: 0.03350800648331642\n","Step 550, Training Loss: 0.03877987340092659\n","Step 550, Training Loss: 0.0313928984105587\n","Step 560, Training Loss: 0.03468671068549156\n","Step 560, Training Loss: 0.029385704547166824\n"]},{"output_type":"stream","name":"stderr","text":["***** Running Evaluation *****\n","  Num examples = 377\n","  Batch size = 8\n"]},{"output_type":"stream","name":"stdout","text":["Step 570, Training Loss: 0.02724101021885872\n","Step 570, Training Loss: 0.029412249103188515\n","Step 580, Training Loss: 0.010665558278560638\n","Step 580, Training Loss: 0.017860902473330498\n","Step 590, Training Loss: 0.012997567653656006\n","Step 590, Training Loss: 0.1697850525379181\n","Step 600, Training Loss: 0.012940498068928719\n","Step 600, Training Loss: 0.0615822970867157\n","Step 610, Training Loss: 0.05546438694000244\n","Step 610, Training Loss: 0.015782907605171204\n","Step 620, Training Loss: 0.010467223823070526\n","Step 620, Training Loss: 0.0333746038377285\n","Step 630, Training Loss: 0.03118431381881237\n","Step 630, Training Loss: 0.06287635862827301\n","Step 640, Training Loss: 0.029039127752184868\n","Step 640, Training Loss: 0.013127326034009457\n","Step 650, Training Loss: 0.05860162526369095\n","Step 650, Training Loss: 0.04590349271893501\n","Step 660, Training Loss: 0.008694787509739399\n","Step 660, Training Loss: 0.04268106445670128\n","Step 670, Training Loss: 0.028339138254523277\n","Step 670, Training Loss: 0.010926710441708565\n","Step 680, Training Loss: 0.021121768280863762\n","Step 680, Training Loss: 0.011909328401088715\n","Step 690, Training Loss: 0.025823140516877174\n","Step 690, Training Loss: 0.022655710577964783\n","Step 700, Training Loss: 0.016126258298754692\n","Step 700, Training Loss: 0.00833222921937704\n","Step 710, Training Loss: 0.013135877437889576\n","Step 710, Training Loss: 0.026450661942362785\n","Step 720, Training Loss: 0.019288575276732445\n","Step 720, Training Loss: 0.08358041197061539\n","Step 730, Training Loss: 0.013535651378333569\n","Step 730, Training Loss: 0.009438978508114815\n","Step 740, Training Loss: 0.04253353923559189\n","Step 740, Training Loss: 0.020186880603432655\n","Step 750, Training Loss: 0.020534802228212357\n","Step 750, Training Loss: 0.014696781523525715\n"]},{"output_type":"stream","name":"stderr","text":["***** Running Evaluation *****\n","  Num examples = 377\n","  Batch size = 8\n","\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","***** Running Evaluation *****\n","  Num examples = 377\n","  Batch size = 8\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='48' max='48' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [48/48 00:01]\n","    </div>\n","    "]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["[I 2024-12-01 16:12:54,041] Trial 9 finished with value: 0.6814604393006292 and parameters: {'learning_rate': 1.3726801946491336e-05, 'batch_size': 4, 'num_train_epochs': 4, 'weight_decay': 0.009315885239302438}. Best is trial 9 with value: 0.6814604393006292.\n","<ipython-input-10-747f1883d862>:2: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n","  learning_rate = trial.suggest_loguniform(\"learning_rate\", 1e-6, 5e-5)\n","<ipython-input-10-747f1883d862>:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n","  weight_decay = trial.suggest_loguniform(\"weight_decay\", 1e-4, 0.1)\n","PyTorch: setting up devices\n","/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n","***** Running training *****\n","  Num examples = 1507\n","  Num Epochs = 3\n","  Instantaneous batch size per device = 4\n","  Total train batch size (w. parallel, distributed & accumulation) = 8\n","  Gradient Accumulation steps = 2\n","  Total optimization steps = 564\n"]},{"output_type":"stream","name":"stdout","text":["Starting training...\n","Step 0, Training Loss: 0.03843189403414726\n","Step 0, Training Loss: 0.01160572748631239\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='564' max='564' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [564/564 00:40, Epoch 2/3]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","      <th>F1</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>0</td>\n","      <td>0.067100</td>\n","      <td>0.526726</td>\n","      <td>0.596817</td>\n","      <td>0.682910</td>\n","      <td>0.682266</td>\n","      <td>0.664825</td>\n","    </tr>\n","    <tr>\n","      <td>1</td>\n","      <td>0.042000</td>\n","      <td>0.506211</td>\n","      <td>0.631300</td>\n","      <td>0.698855</td>\n","      <td>0.692118</td>\n","      <td>0.690141</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>0.023400</td>\n","      <td>0.519160</td>\n","      <td>0.625995</td>\n","      <td>0.718188</td>\n","      <td>0.684729</td>\n","      <td>0.692638</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Step 10, Training Loss: 0.02645835280418396\n","Step 10, Training Loss: 0.024729738011956215\n","Step 20, Training Loss: 0.09428521245718002\n","Step 20, Training Loss: 0.009148949757218361\n","Step 30, Training Loss: 0.03381648287177086\n","Step 30, Training Loss: 0.11197828501462936\n","Step 40, Training Loss: 0.023180393502116203\n","Step 40, Training Loss: 0.012411114759743214\n","Step 50, Training Loss: 0.013274810276925564\n","Step 50, Training Loss: 0.008044078014791012\n","Step 60, Training Loss: 0.01527422945946455\n","Step 60, Training Loss: 0.751141369342804\n","Step 70, Training Loss: 0.08631496876478195\n","Step 70, Training Loss: 0.022725600749254227\n","Step 80, Training Loss: 0.031008219346404076\n","Step 80, Training Loss: 0.027470553293824196\n","Step 90, Training Loss: 0.024598727002739906\n","Step 90, Training Loss: 0.3587874472141266\n","Step 100, Training Loss: 0.01212336029857397\n","Step 100, Training Loss: 0.03850299492478371\n","Step 110, Training Loss: 0.016053952276706696\n","Step 110, Training Loss: 0.008823896758258343\n","Step 120, Training Loss: 0.01043087150901556\n","Step 120, Training Loss: 0.1356395035982132\n","Step 130, Training Loss: 0.008119112811982632\n","Step 130, Training Loss: 0.049104537814855576\n","Step 140, Training Loss: 0.028286606073379517\n","Step 140, Training Loss: 0.006813244428485632\n","Step 150, Training Loss: 0.011613297276198864\n","Step 150, Training Loss: 0.6251155734062195\n","Step 160, Training Loss: 0.011143145151436329\n","Step 160, Training Loss: 0.018076548352837563\n","Step 170, Training Loss: 0.019102824851870537\n","Step 170, Training Loss: 0.13399837911128998\n","Step 180, Training Loss: 0.00924202986061573\n","Step 180, Training Loss: 0.010885089635848999\n"]},{"output_type":"stream","name":"stderr","text":["***** Running Evaluation *****\n","  Num examples = 377\n","  Batch size = 8\n"]},{"output_type":"stream","name":"stdout","text":["Step 190, Training Loss: 0.24147549271583557\n","Step 190, Training Loss: 0.31099724769592285\n","Step 200, Training Loss: 0.01625320315361023\n","Step 200, Training Loss: 0.14943227171897888\n","Step 210, Training Loss: 0.012473351322114468\n","Step 210, Training Loss: 0.01773037575185299\n","Step 220, Training Loss: 0.022477781400084496\n","Step 220, Training Loss: 0.02578813210129738\n","Step 230, Training Loss: 0.37526270747184753\n","Step 230, Training Loss: 0.010252696461975574\n","Step 240, Training Loss: 0.017793508246541023\n","Step 240, Training Loss: 0.02113056741654873\n","Step 250, Training Loss: 0.2122102528810501\n","Step 250, Training Loss: 0.012594047002494335\n","Step 260, Training Loss: 0.007318712770938873\n","Step 260, Training Loss: 0.01697365753352642\n","Step 270, Training Loss: 0.13393928110599518\n","Step 270, Training Loss: 0.08058790117502213\n","Step 280, Training Loss: 0.01880352757871151\n","Step 280, Training Loss: 0.15184473991394043\n","Step 290, Training Loss: 0.10713537037372589\n","Step 290, Training Loss: 0.007431645877659321\n","Step 300, Training Loss: 0.17904353141784668\n","Step 300, Training Loss: 0.03637009486556053\n","Step 310, Training Loss: 0.020899536088109016\n","Step 310, Training Loss: 0.02538296952843666\n","Step 320, Training Loss: 0.01337230484932661\n","Step 320, Training Loss: 0.005317342933267355\n","Step 330, Training Loss: 0.015629589557647705\n","Step 330, Training Loss: 0.30329224467277527\n","Step 340, Training Loss: 0.006269830744713545\n","Step 340, Training Loss: 0.07610190659761429\n","Step 350, Training Loss: 0.007695367094129324\n","Step 350, Training Loss: 0.039937376976013184\n","Step 360, Training Loss: 0.009341496042907238\n","Step 360, Training Loss: 0.00972592644393444\n","Step 370, Training Loss: 0.005474498495459557\n","Step 370, Training Loss: 0.029345327988266945\n"]},{"output_type":"stream","name":"stderr","text":["***** Running Evaluation *****\n","  Num examples = 377\n","  Batch size = 8\n"]},{"output_type":"stream","name":"stdout","text":["Step 380, Training Loss: 0.014328625984489918\n","Step 380, Training Loss: 0.034335892647504807\n","Step 390, Training Loss: 0.004620868247002363\n","Step 390, Training Loss: 0.024476811289787292\n","Step 400, Training Loss: 0.09183084219694138\n","Step 400, Training Loss: 0.01651688665151596\n","Step 410, Training Loss: 0.013917461037635803\n","Step 410, Training Loss: 0.07990402728319168\n","Step 420, Training Loss: 0.19498123228549957\n","Step 420, Training Loss: 0.01725366711616516\n","Step 430, Training Loss: 0.0051745264790952206\n","Step 430, Training Loss: 0.005421135108917952\n","Step 440, Training Loss: 0.0050071957521140575\n","Step 440, Training Loss: 0.01312076486647129\n","Step 450, Training Loss: 0.01171917375177145\n","Step 450, Training Loss: 0.21828202903270721\n","Step 460, Training Loss: 0.13851012289524078\n","Step 460, Training Loss: 0.009356200695037842\n","Step 470, Training Loss: 0.030809594318270683\n","Step 470, Training Loss: 0.00955142080783844\n","Step 480, Training Loss: 0.0209785345941782\n","Step 480, Training Loss: 0.008241675794124603\n","Step 490, Training Loss: 0.06917890161275864\n","Step 490, Training Loss: 0.010042625479400158\n"]},{"output_type":"stream","name":"stderr","text":["Saving model checkpoint to ./results/checkpoint-500\n","Configuration saved in ./results/checkpoint-500/config.json\n","Model weights saved in ./results/checkpoint-500/pytorch_model.bin\n"]},{"output_type":"stream","name":"stdout","text":["Step 500, Training Loss: 0.012894600629806519\n","Step 500, Training Loss: 0.043367139995098114\n","Step 510, Training Loss: 0.035850610584020615\n","Step 510, Training Loss: 0.27405986189842224\n","Step 520, Training Loss: 0.007039033807814121\n","Step 520, Training Loss: 0.05404422804713249\n","Step 530, Training Loss: 0.018340788781642914\n","Step 530, Training Loss: 0.022346073761582375\n","Step 540, Training Loss: 0.5083722472190857\n","Step 540, Training Loss: 0.01609175279736519\n","Step 550, Training Loss: 0.022143660113215446\n","Step 550, Training Loss: 0.016163507476449013\n","Step 560, Training Loss: 0.017547080293297768\n","Step 560, Training Loss: 0.014000819995999336\n"]},{"output_type":"stream","name":"stderr","text":["***** Running Evaluation *****\n","  Num examples = 377\n","  Batch size = 8\n","\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","***** Running Evaluation *****\n","  Num examples = 377\n","  Batch size = 8\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='48' max='48' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [48/48 00:00]\n","    </div>\n","    "]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["[I 2024-12-01 16:13:35,027] Trial 10 finished with value: 0.6926377624336458 and parameters: {'learning_rate': 4.920235545991576e-05, 'batch_size': 4, 'num_train_epochs': 3, 'weight_decay': 0.0692333138867673}. Best is trial 10 with value: 0.6926377624336458.\n","<ipython-input-10-747f1883d862>:2: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n","  learning_rate = trial.suggest_loguniform(\"learning_rate\", 1e-6, 5e-5)\n","<ipython-input-10-747f1883d862>:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n","  weight_decay = trial.suggest_loguniform(\"weight_decay\", 1e-4, 0.1)\n","PyTorch: setting up devices\n","/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n","***** Running training *****\n","  Num examples = 1507\n","  Num Epochs = 3\n","  Instantaneous batch size per device = 4\n","  Total train batch size (w. parallel, distributed & accumulation) = 8\n","  Gradient Accumulation steps = 2\n","  Total optimization steps = 564\n"]},{"output_type":"stream","name":"stdout","text":["Starting training...\n","Step 0, Training Loss: 0.208195760846138\n","Step 0, Training Loss: 0.007140282075852156\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='564' max='564' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [564/564 00:29, Epoch 2/3]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","      <th>F1</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>0</td>\n","      <td>0.052500</td>\n","      <td>0.573373</td>\n","      <td>0.596817</td>\n","      <td>0.697738</td>\n","      <td>0.662562</td>\n","      <td>0.664773</td>\n","    </tr>\n","    <tr>\n","      <td>1</td>\n","      <td>0.040800</td>\n","      <td>0.610593</td>\n","      <td>0.599469</td>\n","      <td>0.678034</td>\n","      <td>0.662562</td>\n","      <td>0.658986</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>0.019900</td>\n","      <td>0.583419</td>\n","      <td>0.615385</td>\n","      <td>0.691987</td>\n","      <td>0.677340</td>\n","      <td>0.676408</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Step 10, Training Loss: 0.012371459975838661\n","Step 10, Training Loss: 0.015118936076760292\n","Step 20, Training Loss: 0.21084387600421906\n","Step 20, Training Loss: 0.006064489018172026\n","Step 30, Training Loss: 0.03169698640704155\n","Step 30, Training Loss: 0.022830171510577202\n","Step 40, Training Loss: 0.010738912969827652\n","Step 40, Training Loss: 0.0073704528622329235\n","Step 50, Training Loss: 0.1946052461862564\n","Step 50, Training Loss: 0.004294444806873798\n","Step 60, Training Loss: 0.011543549597263336\n","Step 60, Training Loss: 1.106592059135437\n","Step 70, Training Loss: 0.5759553909301758\n","Step 70, Training Loss: 0.19798289239406586\n","Step 80, Training Loss: 0.026692643761634827\n","Step 80, Training Loss: 0.04163533076643944\n","Step 90, Training Loss: 0.26198500394821167\n","Step 90, Training Loss: 0.011725084856152534\n","Step 100, Training Loss: 0.012547150254249573\n","Step 100, Training Loss: 0.26703155040740967\n","Step 110, Training Loss: 0.012728363275527954\n","Step 110, Training Loss: 0.005952693987637758\n","Step 120, Training Loss: 0.006573291961103678\n","Step 120, Training Loss: 0.021602803841233253\n","Step 130, Training Loss: 0.0051752426661551\n","Step 130, Training Loss: 0.14113876223564148\n","Step 140, Training Loss: 0.013032901100814342\n","Step 140, Training Loss: 0.0071394476108253\n","Step 150, Training Loss: 0.006549136247485876\n","Step 150, Training Loss: 0.023351961746811867\n","Step 160, Training Loss: 0.007478404324501753\n","Step 160, Training Loss: 0.01228475384414196\n","Step 170, Training Loss: 0.00880575180053711\n","Step 170, Training Loss: 0.022407356649637222\n","Step 180, Training Loss: 0.005687743425369263\n","Step 180, Training Loss: 0.006907112896442413\n"]},{"output_type":"stream","name":"stderr","text":["***** Running Evaluation *****\n","  Num examples = 377\n","  Batch size = 8\n"]},{"output_type":"stream","name":"stdout","text":["Step 190, Training Loss: 0.13533081114292145\n","Step 190, Training Loss: 0.407391756772995\n","Step 200, Training Loss: 0.01291697658598423\n","Step 200, Training Loss: 0.021617643535137177\n","Step 210, Training Loss: 0.007168405689299107\n","Step 210, Training Loss: 0.01069147139787674\n","Step 220, Training Loss: 0.00908667128533125\n","Step 220, Training Loss: 0.014651551842689514\n","Step 230, Training Loss: 0.02194707840681076\n","Step 230, Training Loss: 0.0073625193908810616\n","Step 240, Training Loss: 0.008751763962209225\n","Step 240, Training Loss: 0.013659610413014889\n","Step 250, Training Loss: 0.03691689297556877\n","Step 250, Training Loss: 0.009878444485366344\n","Step 260, Training Loss: 0.004928407724946737\n","Step 260, Training Loss: 0.009874475188553333\n","Step 270, Training Loss: 0.013183441944420338\n","Step 270, Training Loss: 0.04895379766821861\n","Step 280, Training Loss: 0.008348054252564907\n","Step 280, Training Loss: 0.015337360091507435\n","Step 290, Training Loss: 0.013814262114465237\n","Step 290, Training Loss: 0.13820897042751312\n","Step 300, Training Loss: 0.024749796837568283\n","Step 300, Training Loss: 0.01868351362645626\n","Step 310, Training Loss: 0.008414183743298054\n","Step 310, Training Loss: 0.013378113508224487\n","Step 320, Training Loss: 0.00782309751957655\n","Step 320, Training Loss: 0.0035048257559537888\n","Step 330, Training Loss: 0.013775372877717018\n","Step 330, Training Loss: 0.30705326795578003\n","Step 340, Training Loss: 0.004652255214750767\n","Step 340, Training Loss: 0.018884068354964256\n","Step 350, Training Loss: 0.004983565304428339\n","Step 350, Training Loss: 0.08659348636865616\n","Step 360, Training Loss: 0.00550613971427083\n","Step 360, Training Loss: 0.006265861447900534\n","Step 370, Training Loss: 0.0036656104493886232\n","Step 370, Training Loss: 0.006219260394573212\n"]},{"output_type":"stream","name":"stderr","text":["***** Running Evaluation *****\n","  Num examples = 377\n","  Batch size = 8\n"]},{"output_type":"stream","name":"stdout","text":["Step 380, Training Loss: 0.01612730138003826\n","Step 380, Training Loss: 0.030324408784508705\n","Step 390, Training Loss: 0.002927306806668639\n","Step 390, Training Loss: 0.019212206825613976\n","Step 400, Training Loss: 0.013935992494225502\n","Step 400, Training Loss: 0.008916615508496761\n","Step 410, Training Loss: 0.007476312108337879\n","Step 410, Training Loss: 0.007417294662445784\n","Step 420, Training Loss: 0.11438687890768051\n","Step 420, Training Loss: 0.006303964648395777\n","Step 430, Training Loss: 0.003179070306941867\n","Step 430, Training Loss: 0.0034055118449032307\n","Step 440, Training Loss: 0.0031230158638209105\n","Step 440, Training Loss: 0.006990370340645313\n","Step 450, Training Loss: 0.01189502328634262\n","Step 450, Training Loss: 0.17702828347682953\n","Step 460, Training Loss: 0.008876400999724865\n","Step 460, Training Loss: 0.012567780911922455\n","Step 470, Training Loss: 0.06099102646112442\n","Step 470, Training Loss: 0.005981256254017353\n","Step 480, Training Loss: 0.025139400735497475\n","Step 480, Training Loss: 0.005655751563608646\n","Step 490, Training Loss: 0.0438624769449234\n","Step 490, Training Loss: 0.006092147435992956\n"]},{"output_type":"stream","name":"stderr","text":["Saving model checkpoint to ./results/checkpoint-500\n","Configuration saved in ./results/checkpoint-500/config.json\n","Model weights saved in ./results/checkpoint-500/pytorch_model.bin\n"]},{"output_type":"stream","name":"stdout","text":["Step 500, Training Loss: 0.007335841655731201\n","Step 500, Training Loss: 0.015709931030869484\n","Step 510, Training Loss: 0.022222639992833138\n","Step 510, Training Loss: 0.021264059469103813\n","Step 520, Training Loss: 0.012727436609566212\n","Step 520, Training Loss: 0.01667339727282524\n","Step 530, Training Loss: 0.037572573870420456\n","Step 530, Training Loss: 0.015035632066428661\n","Step 540, Training Loss: 0.2575255036354065\n","Step 540, Training Loss: 0.011536200530827045\n","Step 550, Training Loss: 0.015744730830192566\n","Step 550, Training Loss: 0.007801339030265808\n","Step 560, Training Loss: 0.04199998080730438\n","Step 560, Training Loss: 0.04970328137278557\n"]},{"output_type":"stream","name":"stderr","text":["***** Running Evaluation *****\n","  Num examples = 377\n","  Batch size = 8\n","\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","***** Running Evaluation *****\n","  Num examples = 377\n","  Batch size = 8\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='48' max='48' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [48/48 00:00]\n","    </div>\n","    "]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["[I 2024-12-01 16:14:05,711] Trial 11 finished with value: 0.6764083642925016 and parameters: {'learning_rate': 4.5855014245658656e-05, 'batch_size': 4, 'num_train_epochs': 3, 'weight_decay': 0.09623593540273408}. Best is trial 10 with value: 0.6926377624336458.\n","<ipython-input-10-747f1883d862>:2: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n","  learning_rate = trial.suggest_loguniform(\"learning_rate\", 1e-6, 5e-5)\n","<ipython-input-10-747f1883d862>:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n","  weight_decay = trial.suggest_loguniform(\"weight_decay\", 1e-4, 0.1)\n","PyTorch: setting up devices\n","/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n","***** Running training *****\n","  Num examples = 1507\n","  Num Epochs = 3\n","  Instantaneous batch size per device = 4\n","  Total train batch size (w. parallel, distributed & accumulation) = 8\n","  Gradient Accumulation steps = 2\n","  Total optimization steps = 564\n"]},{"output_type":"stream","name":"stdout","text":["Starting training...\n","Step 0, Training Loss: 0.010696799494326115\n","Step 0, Training Loss: 0.004524614196270704\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='564' max='564' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [564/564 00:29, Epoch 2/3]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","      <th>F1</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>0</td>\n","      <td>0.010100</td>\n","      <td>0.577653</td>\n","      <td>0.639257</td>\n","      <td>0.702557</td>\n","      <td>0.692118</td>\n","      <td>0.692663</td>\n","    </tr>\n","    <tr>\n","      <td>1</td>\n","      <td>0.018400</td>\n","      <td>0.617052</td>\n","      <td>0.610080</td>\n","      <td>0.691951</td>\n","      <td>0.679803</td>\n","      <td>0.675456</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>0.026200</td>\n","      <td>0.615089</td>\n","      <td>0.618037</td>\n","      <td>0.694123</td>\n","      <td>0.689655</td>\n","      <td>0.685726</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Step 10, Training Loss: 0.007077857851982117\n","Step 10, Training Loss: 0.007584332022815943\n","Step 20, Training Loss: 0.06106198579072952\n","Step 20, Training Loss: 0.003325957804918289\n","Step 30, Training Loss: 0.004463959950953722\n","Step 30, Training Loss: 0.008597953245043755\n","Step 40, Training Loss: 0.006189025938510895\n","Step 40, Training Loss: 0.004075001925230026\n","Step 50, Training Loss: 0.010287299752235413\n","Step 50, Training Loss: 0.0029041250236332417\n","Step 60, Training Loss: 0.007295147515833378\n","Step 60, Training Loss: 0.2614623010158539\n","Step 70, Training Loss: 0.006220833398401737\n","Step 70, Training Loss: 0.04539727792143822\n","Step 80, Training Loss: 0.014180499128997326\n","Step 80, Training Loss: 0.009166714735329151\n","Step 90, Training Loss: 0.0058890231885015965\n","Step 90, Training Loss: 0.006745034363120794\n","Step 100, Training Loss: 0.004766761790961027\n","Step 100, Training Loss: 0.08001936227083206\n","Step 110, Training Loss: 0.052582185715436935\n","Step 110, Training Loss: 0.003593879286199808\n","Step 120, Training Loss: 0.00408524414524436\n","Step 120, Training Loss: 0.009189662523567677\n","Step 130, Training Loss: 0.0033103853929787874\n","Step 130, Training Loss: 0.008160864934325218\n","Step 140, Training Loss: 0.006700899451971054\n","Step 140, Training Loss: 0.002702443627640605\n","Step 150, Training Loss: 0.004512212239205837\n","Step 150, Training Loss: 0.01798776537179947\n","Step 160, Training Loss: 0.00475960923358798\n","Step 160, Training Loss: 0.007600862067192793\n","Step 170, Training Loss: 0.017286192625761032\n","Step 170, Training Loss: 0.007575449533760548\n","Step 180, Training Loss: 0.004815000109374523\n","Step 180, Training Loss: 0.02933203987777233\n"]},{"output_type":"stream","name":"stderr","text":["***** Running Evaluation *****\n","  Num examples = 377\n","  Batch size = 8\n"]},{"output_type":"stream","name":"stdout","text":["Step 190, Training Loss: 0.4772046208381653\n","Step 190, Training Loss: 0.10433131456375122\n","Step 200, Training Loss: 0.00541186798363924\n","Step 200, Training Loss: 0.0062000504694879055\n","Step 210, Training Loss: 0.004447523504495621\n","Step 210, Training Loss: 0.006251850165426731\n","Step 220, Training Loss: 0.006017191801220179\n","Step 220, Training Loss: 0.00832213182002306\n","Step 230, Training Loss: 0.006844189018011093\n","Step 230, Training Loss: 0.006509293802082539\n","Step 240, Training Loss: 0.00567923579365015\n","Step 240, Training Loss: 0.02495400421321392\n","Step 250, Training Loss: 0.009570143185555935\n","Step 250, Training Loss: 0.005620238371193409\n","Step 260, Training Loss: 0.0031171522568911314\n","Step 260, Training Loss: 0.006214977707713842\n","Step 270, Training Loss: 0.038904059678316116\n","Step 270, Training Loss: 0.13401192426681519\n","Step 280, Training Loss: 0.005141797941178083\n","Step 280, Training Loss: 0.014391000382602215\n","Step 290, Training Loss: 0.008649027906358242\n","Step 290, Training Loss: 0.0023290649987757206\n","Step 300, Training Loss: 0.018025701865553856\n","Step 300, Training Loss: 0.010565310716629028\n","Step 310, Training Loss: 0.004684948828071356\n","Step 310, Training Loss: 0.008480927906930447\n","Step 320, Training Loss: 0.005191972944885492\n","Step 320, Training Loss: 0.002329374197870493\n","Step 330, Training Loss: 0.009213087148964405\n","Step 330, Training Loss: 0.07900204509496689\n","Step 340, Training Loss: 0.002905497094616294\n","Step 340, Training Loss: 0.010579251684248447\n","Step 350, Training Loss: 0.0035130286123603582\n","Step 350, Training Loss: 0.018204761669039726\n","Step 360, Training Loss: 0.003924326039850712\n","Step 360, Training Loss: 0.004108539782464504\n","Step 370, Training Loss: 0.0027094476390630007\n","Step 370, Training Loss: 0.004029422998428345\n"]},{"output_type":"stream","name":"stderr","text":["***** Running Evaluation *****\n","  Num examples = 377\n","  Batch size = 8\n"]},{"output_type":"stream","name":"stdout","text":["Step 380, Training Loss: 0.007665838580578566\n","Step 380, Training Loss: 0.18825702369213104\n","Step 390, Training Loss: 0.0021224392112344503\n","Step 390, Training Loss: 0.00963177066296339\n","Step 400, Training Loss: 0.010458031669259071\n","Step 400, Training Loss: 0.0053575593046844006\n","Step 410, Training Loss: 0.005346645601093769\n","Step 410, Training Loss: 0.005281349178403616\n","Step 420, Training Loss: 0.04726727306842804\n","Step 420, Training Loss: 0.0051672919653356075\n","Step 430, Training Loss: 0.002282746834680438\n","Step 430, Training Loss: 0.0024378416128456593\n","Step 440, Training Loss: 0.0024615032598376274\n","Step 440, Training Loss: 0.004358280450105667\n","Step 450, Training Loss: 0.006306950002908707\n","Step 450, Training Loss: 0.13076834380626678\n","Step 460, Training Loss: 0.012759792618453503\n","Step 460, Training Loss: 0.006162536796182394\n","Step 470, Training Loss: 0.021398192271590233\n","Step 470, Training Loss: 0.004641260020434856\n","Step 480, Training Loss: 0.01174683403223753\n","Step 480, Training Loss: 0.0036938413977622986\n","Step 490, Training Loss: 0.007119897753000259\n","Step 490, Training Loss: 0.0057477341033518314\n"]},{"output_type":"stream","name":"stderr","text":["Saving model checkpoint to ./results/checkpoint-500\n","Configuration saved in ./results/checkpoint-500/config.json\n","Model weights saved in ./results/checkpoint-500/pytorch_model.bin\n"]},{"output_type":"stream","name":"stdout","text":["Step 500, Training Loss: 0.005295561160892248\n","Step 500, Training Loss: 0.009049486368894577\n","Step 510, Training Loss: 0.021406998857855797\n","Step 510, Training Loss: 0.008793069049715996\n","Step 520, Training Loss: 0.003510930109769106\n","Step 520, Training Loss: 0.010509652085602283\n","Step 530, Training Loss: 0.007077075075358152\n","Step 530, Training Loss: 0.007736768573522568\n","Step 540, Training Loss: 0.21513617038726807\n","Step 540, Training Loss: 0.007462506182491779\n","Step 550, Training Loss: 0.03250613808631897\n","Step 550, Training Loss: 0.005329059436917305\n","Step 560, Training Loss: 0.007701487746089697\n","Step 560, Training Loss: 0.09325774013996124\n"]},{"output_type":"stream","name":"stderr","text":["***** Running Evaluation *****\n","  Num examples = 377\n","  Batch size = 8\n","\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","***** Running Evaluation *****\n","  Num examples = 377\n","  Batch size = 8\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='48' max='48' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [48/48 00:00]\n","    </div>\n","    "]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["[I 2024-12-01 16:14:36,431] Trial 12 finished with value: 0.6857255341613768 and parameters: {'learning_rate': 2.4017124564022072e-05, 'batch_size': 4, 'num_train_epochs': 3, 'weight_decay': 0.09067665599904984}. Best is trial 10 with value: 0.6926377624336458.\n","<ipython-input-10-747f1883d862>:2: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n","  learning_rate = trial.suggest_loguniform(\"learning_rate\", 1e-6, 5e-5)\n","<ipython-input-10-747f1883d862>:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n","  weight_decay = trial.suggest_loguniform(\"weight_decay\", 1e-4, 0.1)\n","PyTorch: setting up devices\n","/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n","***** Running training *****\n","  Num examples = 1507\n","  Num Epochs = 6\n","  Instantaneous batch size per device = 4\n","  Total train batch size (w. parallel, distributed & accumulation) = 8\n","  Gradient Accumulation steps = 2\n","  Total optimization steps = 1128\n"]},{"output_type":"stream","name":"stdout","text":["Starting training...\n","Step 0, Training Loss: 0.0068326531909406185\n","Step 0, Training Loss: 0.0032797635067254305\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='1128' max='1128' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [1128/1128 01:00, Epoch 5/6]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","      <th>F1</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>0</td>\n","      <td>0.015600</td>\n","      <td>0.713540</td>\n","      <td>0.578249</td>\n","      <td>0.659470</td>\n","      <td>0.657635</td>\n","      <td>0.637977</td>\n","    </tr>\n","    <tr>\n","      <td>1</td>\n","      <td>0.011400</td>\n","      <td>0.683683</td>\n","      <td>0.596817</td>\n","      <td>0.700682</td>\n","      <td>0.672414</td>\n","      <td>0.669473</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>0.025800</td>\n","      <td>0.714308</td>\n","      <td>0.591512</td>\n","      <td>0.694832</td>\n","      <td>0.667488</td>\n","      <td>0.663605</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>0.013400</td>\n","      <td>0.690495</td>\n","      <td>0.607427</td>\n","      <td>0.676741</td>\n","      <td>0.682266</td>\n","      <td>0.665936</td>\n","    </tr>\n","    <tr>\n","      <td>4</td>\n","      <td>0.018600</td>\n","      <td>0.650375</td>\n","      <td>0.625995</td>\n","      <td>0.701235</td>\n","      <td>0.692118</td>\n","      <td>0.690478</td>\n","    </tr>\n","    <tr>\n","      <td>5</td>\n","      <td>0.007900</td>\n","      <td>0.664943</td>\n","      <td>0.615385</td>\n","      <td>0.686241</td>\n","      <td>0.684729</td>\n","      <td>0.677248</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Step 10, Training Loss: 0.004859598353505135\n","Step 10, Training Loss: 0.0049757459200918674\n","Step 20, Training Loss: 0.15236793458461761\n","Step 20, Training Loss: 0.002451279666274786\n","Step 30, Training Loss: 0.003499640617519617\n","Step 30, Training Loss: 0.005995328072458506\n","Step 40, Training Loss: 0.004431507084518671\n","Step 40, Training Loss: 0.0032166640739887953\n","Step 50, Training Loss: 0.003796488046646118\n","Step 50, Training Loss: 0.0029706244822591543\n","Step 60, Training Loss: 0.0054655480198562145\n","Step 60, Training Loss: 0.22829334437847137\n","Step 70, Training Loss: 0.004419919103384018\n","Step 70, Training Loss: 0.004602780099958181\n","Step 80, Training Loss: 0.006921316962689161\n","Step 80, Training Loss: 0.005952277686446905\n","Step 90, Training Loss: 0.004225901328027248\n","Step 90, Training Loss: 0.0063811736181378365\n","Step 100, Training Loss: 0.003290575696155429\n","Step 100, Training Loss: 0.007995675317943096\n","Step 110, Training Loss: 0.003865795908495784\n","Step 110, Training Loss: 0.00266687641851604\n","Step 120, Training Loss: 0.003433567238971591\n","Step 120, Training Loss: 0.00711089838296175\n","Step 130, Training Loss: 0.0023823436349630356\n","Step 130, Training Loss: 0.0058237346820533276\n","Step 140, Training Loss: 0.0051538036204874516\n","Step 140, Training Loss: 0.002096035284921527\n","Step 150, Training Loss: 0.003246771637350321\n","Step 150, Training Loss: 0.006003411952406168\n","Step 160, Training Loss: 0.0032176796812564135\n","Step 160, Training Loss: 0.004056937992572784\n","Step 170, Training Loss: 0.004274421371519566\n","Step 170, Training Loss: 0.005171040538698435\n","Step 180, Training Loss: 0.0026707386132329702\n","Step 180, Training Loss: 0.003304624231532216\n"]},{"output_type":"stream","name":"stderr","text":["***** Running Evaluation *****\n","  Num examples = 377\n","  Batch size = 8\n"]},{"output_type":"stream","name":"stdout","text":["Step 190, Training Loss: 0.18634428083896637\n","Step 190, Training Loss: 0.29650238156318665\n","Step 200, Training Loss: 0.003840705147013068\n","Step 200, Training Loss: 0.005935795605182648\n","Step 210, Training Loss: 0.0033870849292725325\n","Step 210, Training Loss: 0.004369909409433603\n","Step 220, Training Loss: 0.0041104028932750225\n","Step 220, Training Loss: 0.006761444266885519\n","Step 230, Training Loss: 0.0051607959903776646\n","Step 230, Training Loss: 0.0029785935766994953\n","Step 240, Training Loss: 0.0033097013365477324\n","Step 240, Training Loss: 0.005454745143651962\n","Step 250, Training Loss: 0.006343703251332045\n","Step 250, Training Loss: 0.003955415915697813\n","Step 260, Training Loss: 0.0022871417459100485\n","Step 260, Training Loss: 0.004386998247355223\n","Step 270, Training Loss: 0.006761831697076559\n","Step 270, Training Loss: 0.017195668071508408\n","Step 280, Training Loss: 0.0036239742767065763\n","Step 280, Training Loss: 0.007145564537495375\n","Step 290, Training Loss: 0.005664587486535311\n","Step 290, Training Loss: 0.0019430395914241672\n","Step 300, Training Loss: 0.017209718003869057\n","Step 300, Training Loss: 0.014217840507626534\n","Step 310, Training Loss: 0.0032181795686483383\n","Step 310, Training Loss: 0.006439735181629658\n","Step 320, Training Loss: 0.0038888726849108934\n","Step 320, Training Loss: 0.0017530886689200997\n","Step 330, Training Loss: 0.005133446305990219\n","Step 330, Training Loss: 0.22878718376159668\n","Step 340, Training Loss: 0.0021946378983557224\n","Step 340, Training Loss: 0.02159973606467247\n","Step 350, Training Loss: 0.002450051251798868\n","Step 350, Training Loss: 0.007377345114946365\n","Step 360, Training Loss: 0.0026737458538264036\n","Step 360, Training Loss: 0.0030778800137341022\n","Step 370, Training Loss: 0.0019562996458262205\n","Step 370, Training Loss: 0.0028099443297833204\n"]},{"output_type":"stream","name":"stderr","text":["***** Running Evaluation *****\n","  Num examples = 377\n","  Batch size = 8\n"]},{"output_type":"stream","name":"stdout","text":["Step 380, Training Loss: 0.004810900893062353\n","Step 380, Training Loss: 0.05812494829297066\n","Step 390, Training Loss: 0.001484690117649734\n","Step 390, Training Loss: 0.008156339637935162\n","Step 400, Training Loss: 0.012216913513839245\n","Step 400, Training Loss: 0.0034905862994492054\n","Step 410, Training Loss: 0.003783608553931117\n","Step 410, Training Loss: 0.0035776959266513586\n","Step 420, Training Loss: 0.009179815649986267\n","Step 420, Training Loss: 0.003327032318338752\n","Step 430, Training Loss: 0.0014802695950493217\n","Step 430, Training Loss: 0.0017276033759117126\n","Step 440, Training Loss: 0.0016027465462684631\n","Step 440, Training Loss: 0.0036093853414058685\n","Step 450, Training Loss: 0.004115899559110403\n","Step 450, Training Loss: 0.05815749987959862\n","Step 460, Training Loss: 0.006938414182513952\n","Step 460, Training Loss: 0.007212497293949127\n","Step 470, Training Loss: 0.05725695565342903\n","Step 470, Training Loss: 0.002753679407760501\n","Step 480, Training Loss: 0.045084405690431595\n","Step 480, Training Loss: 0.0025428123772144318\n","Step 490, Training Loss: 0.004037728067487478\n","Step 490, Training Loss: 0.002742845332249999\n"]},{"output_type":"stream","name":"stderr","text":["Saving model checkpoint to ./results/checkpoint-500\n","Configuration saved in ./results/checkpoint-500/config.json\n","Model weights saved in ./results/checkpoint-500/pytorch_model.bin\n"]},{"output_type":"stream","name":"stdout","text":["Step 500, Training Loss: 0.005961320828646421\n","Step 500, Training Loss: 0.005662763956934214\n","Step 510, Training Loss: 0.0072867958806455135\n","Step 510, Training Loss: 0.14368335902690887\n","Step 520, Training Loss: 0.002207643585279584\n","Step 520, Training Loss: 0.010299324057996273\n","Step 530, Training Loss: 0.008834315463900566\n","Step 530, Training Loss: 0.007918029092252254\n","Step 540, Training Loss: 0.4024284780025482\n","Step 540, Training Loss: 0.005831263493746519\n","Step 550, Training Loss: 0.013767309486865997\n","Step 550, Training Loss: 0.0035268717911094427\n","Step 560, Training Loss: 0.06524094194173813\n","Step 560, Training Loss: 0.15237830579280853\n"]},{"output_type":"stream","name":"stderr","text":["***** Running Evaluation *****\n","  Num examples = 377\n","  Batch size = 8\n"]},{"output_type":"stream","name":"stdout","text":["Step 570, Training Loss: 0.003993261139839888\n","Step 570, Training Loss: 0.003936240915209055\n","Step 580, Training Loss: 0.0020071316976100206\n","Step 580, Training Loss: 0.003612726228311658\n","Step 590, Training Loss: 0.0022474396973848343\n","Step 590, Training Loss: 0.01014584582298994\n","Step 600, Training Loss: 0.002531712641939521\n","Step 600, Training Loss: 0.008731099776923656\n","Step 610, Training Loss: 0.13956522941589355\n","Step 610, Training Loss: 0.002633399097248912\n","Step 620, Training Loss: 0.0020406758412718773\n","Step 620, Training Loss: 0.005102755036205053\n","Step 630, Training Loss: 0.004908530041575432\n","Step 630, Training Loss: 0.005752749275416136\n","Step 640, Training Loss: 0.0232015922665596\n","Step 640, Training Loss: 0.0027797650545835495\n","Step 650, Training Loss: 0.005936180241405964\n","Step 650, Training Loss: 0.006774839013814926\n","Step 660, Training Loss: 0.0016026919474825263\n","Step 660, Training Loss: 0.005454584490507841\n","Step 670, Training Loss: 0.0033930984791368246\n","Step 670, Training Loss: 0.0019057721365243196\n","Step 680, Training Loss: 0.0044181435368955135\n","Step 680, Training Loss: 0.002211010782048106\n","Step 690, Training Loss: 0.003949444741010666\n","Step 690, Training Loss: 0.005628400482237339\n","Step 700, Training Loss: 0.0031022571492940187\n","Step 700, Training Loss: 0.40050289034843445\n","Step 710, Training Loss: 0.0044838739559054375\n","Step 710, Training Loss: 0.0033902458380907774\n","Step 720, Training Loss: 0.0046850405633449554\n","Step 720, Training Loss: 0.021626779809594154\n","Step 730, Training Loss: 0.002351893577724695\n","Step 730, Training Loss: 0.0055101895704865456\n","Step 740, Training Loss: 0.00804185401648283\n","Step 740, Training Loss: 0.002408677479252219\n","Step 750, Training Loss: 0.004352179821580648\n","Step 750, Training Loss: 0.002672468079254031\n"]},{"output_type":"stream","name":"stderr","text":["***** Running Evaluation *****\n","  Num examples = 377\n","  Batch size = 8\n"]},{"output_type":"stream","name":"stdout","text":["Step 760, Training Loss: 0.0023763009812682867\n","Step 760, Training Loss: 0.012745735235512257\n","Step 770, Training Loss: 0.0025067615788429976\n","Step 770, Training Loss: 0.006076037418097258\n","Step 780, Training Loss: 0.0016811464447528124\n","Step 780, Training Loss: 0.00187856110278517\n","Step 790, Training Loss: 0.0018027126789093018\n","Step 790, Training Loss: 0.030283382162451744\n","Step 800, Training Loss: 0.0032929766457527876\n","Step 800, Training Loss: 0.0016411092365160584\n","Step 810, Training Loss: 0.0014483857667073607\n","Step 810, Training Loss: 0.003400923451408744\n","Step 820, Training Loss: 0.0025148235727101564\n","Step 820, Training Loss: 0.0024511583615094423\n","Step 830, Training Loss: 0.2177840769290924\n","Step 830, Training Loss: 0.006403565406799316\n","Step 840, Training Loss: 0.0036084146704524755\n","Step 840, Training Loss: 0.002445839811116457\n","Step 850, Training Loss: 0.0029902374371886253\n","Step 850, Training Loss: 0.009107265621423721\n","Step 860, Training Loss: 0.002321158070117235\n","Step 860, Training Loss: 0.002081305254250765\n","Step 870, Training Loss: 0.004088451620191336\n","Step 870, Training Loss: 0.0031307146418839693\n","Step 880, Training Loss: 0.00291244313120842\n","Step 880, Training Loss: 0.003990617115050554\n","Step 890, Training Loss: 0.0018365256255492568\n","Step 890, Training Loss: 0.0023515650536864996\n","Step 900, Training Loss: 0.02174360863864422\n","Step 900, Training Loss: 0.0030057255644351244\n","Step 910, Training Loss: 0.04750239476561546\n","Step 910, Training Loss: 0.02672683633863926\n","Step 920, Training Loss: 0.0021982998587191105\n","Step 920, Training Loss: 0.0021105699706822634\n","Step 930, Training Loss: 0.008914655074477196\n","Step 930, Training Loss: 0.002108830725774169\n"]},{"output_type":"stream","name":"stderr","text":["***** Running Evaluation *****\n","  Num examples = 377\n","  Batch size = 8\n"]},{"output_type":"stream","name":"stdout","text":["Step 940, Training Loss: 0.002407154068350792\n","Step 940, Training Loss: 0.43317294120788574\n","Step 940, Training Loss: 1.1080095767974854\n","Step 940, Training Loss: 0.7119316458702087\n","Step 940, Training Loss: 1.8257135152816772\n","Step 940, Training Loss: 0.8080059289932251\n","Step 940, Training Loss: 0.7161162495613098\n","Step 940, Training Loss: 1.3841594457626343\n","Step 940, Training Loss: 0.5697843432426453\n","Step 940, Training Loss: 0.09848632663488388\n","Step 940, Training Loss: 0.8798537254333496\n","Step 940, Training Loss: 1.0241003036499023\n","Step 940, Training Loss: 0.6430465579032898\n","Step 940, Training Loss: 0.5948354601860046\n","Step 940, Training Loss: 0.0016626203432679176\n","Step 940, Training Loss: 0.2762337327003479\n","Step 940, Training Loss: 0.6807486414909363\n","Step 940, Training Loss: 0.9475049376487732\n","Step 940, Training Loss: 0.36234399676322937\n","Step 940, Training Loss: 0.15688443183898926\n","Step 940, Training Loss: 0.04593518003821373\n","Step 940, Training Loss: 0.7379360198974609\n","Step 940, Training Loss: 0.6477276682853699\n","Step 940, Training Loss: 0.5969104170799255\n","Step 940, Training Loss: 0.21575415134429932\n","Step 940, Training Loss: 0.8790585398674011\n","Step 940, Training Loss: 0.5960187315940857\n","Step 940, Training Loss: 0.6235225200653076\n","Step 940, Training Loss: 0.9452654123306274\n","Step 940, Training Loss: 1.0092651844024658\n","Step 940, Training Loss: 0.8660957217216492\n","Step 940, Training Loss: 1.0199421644210815\n","Step 940, Training Loss: 1.0872515439987183\n","Step 940, Training Loss: 1.9136987924575806\n","Step 940, Training Loss: 0.5897938013076782\n","Step 940, Training Loss: 0.43041619658470154\n","Step 940, Training Loss: 0.4659543037414551\n","Step 940, Training Loss: 0.3511684536933899\n","Step 940, Training Loss: 0.21153120696544647\n","Step 940, Training Loss: 1.3594413995742798\n","Step 940, Training Loss: 0.14374849200248718\n","Step 940, Training Loss: 0.8000012636184692\n","Step 940, Training Loss: 0.19130492210388184\n","Step 940, Training Loss: 0.6205917000770569\n","Step 940, Training Loss: 0.35590746998786926\n","Step 940, Training Loss: 0.23244181275367737\n","Step 940, Training Loss: 0.1447751671075821\n","Step 940, Training Loss: 0.34469273686408997\n","Step 940, Training Loss: 0.001266624080017209\n","Step 940, Training Loss: 0.5394676327705383\n","Step 940, Training Loss: 0.00444430997595191\n","Step 950, Training Loss: 0.0026363602373749018\n","Step 950, Training Loss: 0.0048059141263365746\n","Step 960, Training Loss: 0.004966076463460922\n","Step 960, Training Loss: 0.003997336607426405\n","Step 970, Training Loss: 0.007727773394435644\n","Step 970, Training Loss: 0.010123292915523052\n","Step 980, Training Loss: 0.0037714322097599506\n","Step 980, Training Loss: 0.003958852030336857\n","Step 990, Training Loss: 0.0012382058193907142\n","Step 990, Training Loss: 0.002043579239398241\n"]},{"output_type":"stream","name":"stderr","text":["Saving model checkpoint to ./results/checkpoint-1000\n","Configuration saved in ./results/checkpoint-1000/config.json\n","Model weights saved in ./results/checkpoint-1000/pytorch_model.bin\n"]},{"output_type":"stream","name":"stdout","text":["Step 1000, Training Loss: 0.004253620747476816\n","Step 1000, Training Loss: 0.003869013162329793\n","Step 1010, Training Loss: 0.0021117934957146645\n","Step 1010, Training Loss: 0.0012676798505708575\n","Step 1020, Training Loss: 0.003590670181438327\n","Step 1020, Training Loss: 0.0062255458906292915\n","Step 1030, Training Loss: 0.0025542278308421373\n","Step 1030, Training Loss: 0.15308962762355804\n","Step 1040, Training Loss: 0.0022774047683924437\n","Step 1040, Training Loss: 0.002993171801790595\n","Step 1050, Training Loss: 0.002466139616444707\n","Step 1050, Training Loss: 0.008373257704079151\n","Step 1060, Training Loss: 0.017942393198609352\n","Step 1060, Training Loss: 0.004908238537609577\n","Step 1070, Training Loss: 0.001254042494110763\n","Step 1070, Training Loss: 0.0026023914106190205\n","Step 1080, Training Loss: 0.03327598050236702\n","Step 1080, Training Loss: 0.0038469694554805756\n","Step 1090, Training Loss: 0.0018113506957888603\n","Step 1090, Training Loss: 0.004867976065725088\n","Step 1100, Training Loss: 0.0016853444976732135\n","Step 1100, Training Loss: 0.0020800461061298847\n","Step 1110, Training Loss: 0.0017582383006811142\n","Step 1110, Training Loss: 0.003719884203746915\n","Step 1120, Training Loss: 0.002742278389632702\n","Step 1120, Training Loss: 0.004173942841589451\n"]},{"output_type":"stream","name":"stderr","text":["***** Running Evaluation *****\n","  Num examples = 377\n","  Batch size = 8\n","\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","***** Running Evaluation *****\n","  Num examples = 377\n","  Batch size = 8\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='48' max='48' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [48/48 00:00]\n","    </div>\n","    "]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["[I 2024-12-01 16:15:37,995] Trial 13 finished with value: 0.6772482204712689 and parameters: {'learning_rate': 2.388468961774888e-05, 'batch_size': 4, 'num_train_epochs': 6, 'weight_decay': 0.07944780094228106}. Best is trial 10 with value: 0.6926377624336458.\n","<ipython-input-10-747f1883d862>:2: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n","  learning_rate = trial.suggest_loguniform(\"learning_rate\", 1e-6, 5e-5)\n","<ipython-input-10-747f1883d862>:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n","  weight_decay = trial.suggest_loguniform(\"weight_decay\", 1e-4, 0.1)\n","PyTorch: setting up devices\n","/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n","***** Running training *****\n","  Num examples = 1507\n","  Num Epochs = 7\n","  Instantaneous batch size per device = 4\n","  Total train batch size (w. parallel, distributed & accumulation) = 8\n","  Gradient Accumulation steps = 2\n","  Total optimization steps = 1316\n"]},{"output_type":"stream","name":"stdout","text":["Starting training...\n","Step 0, Training Loss: 0.003938970621675253\n","Step 0, Training Loss: 0.0018824845319613814\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='1316' max='1316' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [1316/1316 01:11, Epoch 6/7]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","      <th>F1</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>0</td>\n","      <td>0.015900</td>\n","      <td>0.754147</td>\n","      <td>0.586207</td>\n","      <td>0.685435</td>\n","      <td>0.684729</td>\n","      <td>0.673641</td>\n","    </tr>\n","    <tr>\n","      <td>1</td>\n","      <td>0.016500</td>\n","      <td>0.813714</td>\n","      <td>0.578249</td>\n","      <td>0.670112</td>\n","      <td>0.665025</td>\n","      <td>0.650519</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>0.024600</td>\n","      <td>0.763063</td>\n","      <td>0.599469</td>\n","      <td>0.699637</td>\n","      <td>0.667488</td>\n","      <td>0.669058</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>0.032400</td>\n","      <td>0.756316</td>\n","      <td>0.623342</td>\n","      <td>0.707657</td>\n","      <td>0.699507</td>\n","      <td>0.693188</td>\n","    </tr>\n","    <tr>\n","      <td>4</td>\n","      <td>0.008600</td>\n","      <td>0.706757</td>\n","      <td>0.636605</td>\n","      <td>0.714883</td>\n","      <td>0.724138</td>\n","      <td>0.717268</td>\n","    </tr>\n","    <tr>\n","      <td>5</td>\n","      <td>0.004200</td>\n","      <td>0.720420</td>\n","      <td>0.623342</td>\n","      <td>0.691529</td>\n","      <td>0.711823</td>\n","      <td>0.699136</td>\n","    </tr>\n","    <tr>\n","      <td>6</td>\n","      <td>0.014300</td>\n","      <td>0.720794</td>\n","      <td>0.639257</td>\n","      <td>0.718280</td>\n","      <td>0.706897</td>\n","      <td>0.706533</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Step 10, Training Loss: 0.002518204739317298\n","Step 10, Training Loss: 0.0026890665758401155\n","Step 20, Training Loss: 0.030610842630267143\n","Step 20, Training Loss: 0.001392978592775762\n","Step 30, Training Loss: 0.0026247594505548477\n","Step 30, Training Loss: 0.004805046133697033\n","Step 40, Training Loss: 0.002674389397725463\n","Step 40, Training Loss: 0.0017712239641696215\n","Step 50, Training Loss: 0.0019251414341852069\n","Step 50, Training Loss: 0.0012745094718411565\n","Step 60, Training Loss: 0.0035152172204107046\n","Step 60, Training Loss: 0.0059218681417405605\n","Step 70, Training Loss: 0.0030620391480624676\n","Step 70, Training Loss: 0.018383117392659187\n","Step 80, Training Loss: 0.006227106787264347\n","Step 80, Training Loss: 0.01403252687305212\n","Step 90, Training Loss: 0.002182544907554984\n","Step 90, Training Loss: 0.002753481036052108\n","Step 100, Training Loss: 0.00212852586992085\n","Step 100, Training Loss: 0.00765552744269371\n","Step 110, Training Loss: 0.0020003155805170536\n","Step 110, Training Loss: 0.0014832461019977927\n","Step 120, Training Loss: 0.00163463840726763\n","Step 120, Training Loss: 0.0035553635098040104\n","Step 130, Training Loss: 0.0013154357438907027\n","Step 130, Training Loss: 0.0029936344362795353\n","Step 140, Training Loss: 0.0024915996473282576\n","Step 140, Training Loss: 0.001207545748911798\n","Step 150, Training Loss: 0.0017677812138572335\n","Step 150, Training Loss: 0.0691416934132576\n","Step 160, Training Loss: 0.0024256440810859203\n","Step 160, Training Loss: 0.004222601652145386\n","Step 170, Training Loss: 0.019374355673789978\n","Step 170, Training Loss: 0.002732465509325266\n","Step 180, Training Loss: 0.0014399661449715495\n","Step 180, Training Loss: 0.0017857339698821306\n"]},{"output_type":"stream","name":"stderr","text":["***** Running Evaluation *****\n","  Num examples = 377\n","  Batch size = 8\n"]},{"output_type":"stream","name":"stdout","text":["Step 190, Training Loss: 0.004083060193806887\n","Step 190, Training Loss: 0.39898642897605896\n","Step 200, Training Loss: 0.001957058208063245\n","Step 200, Training Loss: 0.002493279054760933\n","Step 210, Training Loss: 0.0035922490060329437\n","Step 210, Training Loss: 0.0026282265316694975\n","Step 220, Training Loss: 0.002015551785007119\n","Step 220, Training Loss: 0.003542067250236869\n","Step 230, Training Loss: 0.0022806068882346153\n","Step 230, Training Loss: 0.0018776722718030214\n","Step 240, Training Loss: 0.0020259739831089973\n","Step 240, Training Loss: 0.005140864755958319\n","Step 250, Training Loss: 0.003950804006308317\n","Step 250, Training Loss: 0.0020781324710696936\n","Step 260, Training Loss: 0.004254239145666361\n","Step 260, Training Loss: 0.002521877409890294\n","Step 270, Training Loss: 0.002388852881267667\n","Step 270, Training Loss: 0.40384674072265625\n","Step 280, Training Loss: 0.054955046623945236\n","Step 280, Training Loss: 0.12784838676452637\n","Step 290, Training Loss: 0.2519935071468353\n","Step 290, Training Loss: 0.0008697759476490319\n","Step 300, Training Loss: 0.16018734872341156\n","Step 300, Training Loss: 0.004282613284885883\n","Step 310, Training Loss: 0.0015573956770822406\n","Step 310, Training Loss: 0.002659004647284746\n","Step 320, Training Loss: 0.0019417072180658579\n","Step 320, Training Loss: 0.0008808104321360588\n","Step 330, Training Loss: 0.005541493650525808\n","Step 330, Training Loss: 0.24058210849761963\n","Step 340, Training Loss: 0.0010751544032245874\n","Step 340, Training Loss: 0.01378187071532011\n","Step 350, Training Loss: 0.0016289412742480636\n","Step 350, Training Loss: 0.00481115048751235\n","Step 360, Training Loss: 0.0013024205109104514\n","Step 360, Training Loss: 0.002039255341514945\n","Step 370, Training Loss: 0.001009635510854423\n","Step 370, Training Loss: 0.00921083427965641\n"]},{"output_type":"stream","name":"stderr","text":["***** Running Evaluation *****\n","  Num examples = 377\n","  Batch size = 8\n"]},{"output_type":"stream","name":"stdout","text":["Step 380, Training Loss: 0.04920658469200134\n","Step 380, Training Loss: 0.0136495903134346\n","Step 390, Training Loss: 0.0008323679794557393\n","Step 390, Training Loss: 0.0034785696770995855\n","Step 400, Training Loss: 0.0037974826991558075\n","Step 400, Training Loss: 0.0018587980885058641\n","Step 410, Training Loss: 0.001614241860806942\n","Step 410, Training Loss: 0.0019034894648939371\n","Step 420, Training Loss: 0.03817799687385559\n","Step 420, Training Loss: 0.003564516780897975\n","Step 430, Training Loss: 0.0008593382663093507\n","Step 430, Training Loss: 0.0009259149665012956\n","Step 440, Training Loss: 0.0008653344702906907\n","Step 440, Training Loss: 0.0016106503317132592\n","Step 450, Training Loss: 0.003100872505456209\n","Step 450, Training Loss: 0.004754047375172377\n","Step 460, Training Loss: 0.0024770477320998907\n","Step 460, Training Loss: 0.0015641509089618921\n","Step 470, Training Loss: 0.22802379727363586\n","Step 470, Training Loss: 0.0013623289996758103\n","Step 480, Training Loss: 0.002360396785661578\n","Step 480, Training Loss: 0.001264090882614255\n","Step 490, Training Loss: 0.003338039619848132\n","Step 490, Training Loss: 0.003252063412219286\n"]},{"output_type":"stream","name":"stderr","text":["Saving model checkpoint to ./results/checkpoint-500\n","Configuration saved in ./results/checkpoint-500/config.json\n","Model weights saved in ./results/checkpoint-500/pytorch_model.bin\n"]},{"output_type":"stream","name":"stdout","text":["Step 500, Training Loss: 0.0019119320204481483\n","Step 500, Training Loss: 0.002514875028282404\n","Step 510, Training Loss: 0.05511514097452164\n","Step 510, Training Loss: 0.20635442435741425\n","Step 520, Training Loss: 0.0011196298291906714\n","Step 520, Training Loss: 0.003996145911514759\n","Step 530, Training Loss: 0.0031343132723122835\n","Step 530, Training Loss: 0.0026007858105003834\n","Step 540, Training Loss: 0.04574889317154884\n","Step 540, Training Loss: 0.005747166927903891\n","Step 550, Training Loss: 0.06317725032567978\n","Step 550, Training Loss: 0.002022906905040145\n","Step 560, Training Loss: 0.0018836632370948792\n","Step 560, Training Loss: 0.5675883293151855\n"]},{"output_type":"stream","name":"stderr","text":["***** Running Evaluation *****\n","  Num examples = 377\n","  Batch size = 8\n"]},{"output_type":"stream","name":"stdout","text":["Step 570, Training Loss: 0.0023247243370860815\n","Step 570, Training Loss: 0.0021015384700149298\n","Step 580, Training Loss: 0.0010970932198688388\n","Step 580, Training Loss: 0.001561697805300355\n","Step 590, Training Loss: 0.0016388532239943743\n","Step 590, Training Loss: 0.017407121136784554\n","Step 600, Training Loss: 0.0011413152096793056\n","Step 600, Training Loss: 0.025465110316872597\n","Step 610, Training Loss: 0.0016018396709114313\n","Step 610, Training Loss: 0.0011713335989043117\n","Step 620, Training Loss: 0.0011911204783245921\n","Step 620, Training Loss: 0.0022114503663033247\n","Step 630, Training Loss: 0.002230634680017829\n","Step 630, Training Loss: 0.0030386745929718018\n","Step 640, Training Loss: 0.002825723262503743\n","Step 640, Training Loss: 0.0011416253400966525\n","Step 650, Training Loss: 0.14159895479679108\n","Step 650, Training Loss: 0.0016924896044656634\n","Step 660, Training Loss: 0.0007747471681796014\n","Step 660, Training Loss: 0.002887733979150653\n","Step 670, Training Loss: 0.0018429476767778397\n","Step 670, Training Loss: 0.0009203068912029266\n","Step 680, Training Loss: 0.001736980746500194\n","Step 680, Training Loss: 0.0010682378197088838\n","Step 690, Training Loss: 0.0016445756191387773\n","Step 690, Training Loss: 0.0046842978335917\n","Step 700, Training Loss: 0.002016507089138031\n","Step 700, Training Loss: 0.0006926223868504167\n","Step 710, Training Loss: 0.0036611284594982862\n","Step 710, Training Loss: 0.0020046380814164877\n","Step 720, Training Loss: 0.02686602808535099\n","Step 720, Training Loss: 0.015903739258646965\n","Step 730, Training Loss: 0.0009953512344509363\n","Step 730, Training Loss: 0.0009233402088284492\n","Step 740, Training Loss: 0.01503977831453085\n","Step 740, Training Loss: 0.0016503978986293077\n","Step 750, Training Loss: 0.006963761057704687\n","Step 750, Training Loss: 0.0010990595910698175\n"]},{"output_type":"stream","name":"stderr","text":["***** Running Evaluation *****\n","  Num examples = 377\n","  Batch size = 8\n"]},{"output_type":"stream","name":"stdout","text":["Step 760, Training Loss: 0.0011516984086483717\n","Step 760, Training Loss: 0.0034359225537627935\n","Step 770, Training Loss: 0.0011550429044291377\n","Step 770, Training Loss: 0.01810038462281227\n","Step 780, Training Loss: 0.0008456427021883428\n","Step 780, Training Loss: 0.0009322059340775013\n","Step 790, Training Loss: 0.0009145867079496384\n","Step 790, Training Loss: 0.0987444519996643\n","Step 800, Training Loss: 0.0016933385049924254\n","Step 800, Training Loss: 0.0008278480963781476\n","Step 810, Training Loss: 0.0007225160370580852\n","Step 810, Training Loss: 0.0012406368041411042\n","Step 820, Training Loss: 0.001293892040848732\n","Step 820, Training Loss: 0.0011905089486390352\n","Step 830, Training Loss: 0.0031230347231030464\n","Step 830, Training Loss: 0.3806655704975128\n","Step 840, Training Loss: 0.001816069008782506\n","Step 840, Training Loss: 0.0011477138614282012\n","Step 850, Training Loss: 0.001330096391029656\n","Step 850, Training Loss: 0.0008296877495013177\n","Step 860, Training Loss: 0.0008810685249045491\n","Step 860, Training Loss: 0.0011035160860046744\n","Step 870, Training Loss: 0.00299101322889328\n","Step 870, Training Loss: 0.0016711738426238298\n","Step 880, Training Loss: 0.0015721720410510898\n","Step 880, Training Loss: 0.0013328194618225098\n","Step 890, Training Loss: 0.0009399058180861175\n","Step 890, Training Loss: 0.0011043458944186568\n","Step 900, Training Loss: 0.006589763797819614\n","Step 900, Training Loss: 0.0009289219160564244\n","Step 910, Training Loss: 0.005860581994056702\n","Step 910, Training Loss: 0.001930183614604175\n","Step 920, Training Loss: 0.0010931114666163921\n","Step 920, Training Loss: 0.0011547846952453256\n","Step 930, Training Loss: 0.0018833043286576867\n","Step 930, Training Loss: 0.0011120106792077422\n"]},{"output_type":"stream","name":"stderr","text":["***** Running Evaluation *****\n","  Num examples = 377\n","  Batch size = 8\n"]},{"output_type":"stream","name":"stdout","text":["Step 940, Training Loss: 0.0011193082900717854\n","Step 940, Training Loss: 0.5121462941169739\n","Step 940, Training Loss: 1.1669771671295166\n","Step 940, Training Loss: 0.8705788850784302\n","Step 940, Training Loss: 1.9221709966659546\n","Step 940, Training Loss: 0.82325279712677\n","Step 940, Training Loss: 1.082019329071045\n","Step 940, Training Loss: 1.582686424255371\n","Step 940, Training Loss: 0.5480611324310303\n","Step 940, Training Loss: 0.1491672843694687\n","Step 940, Training Loss: 1.1786872148513794\n","Step 940, Training Loss: 1.5177109241485596\n","Step 940, Training Loss: 0.6325703859329224\n","Step 940, Training Loss: 0.8691225051879883\n","Step 940, Training Loss: 0.10422419756650925\n","Step 940, Training Loss: 0.44271358847618103\n","Step 940, Training Loss: 0.8269090056419373\n","Step 940, Training Loss: 0.9793713688850403\n","Step 940, Training Loss: 0.37794432044029236\n","Step 940, Training Loss: 0.056892942637205124\n","Step 940, Training Loss: 0.10361319035291672\n","Step 940, Training Loss: 0.558562695980072\n","Step 940, Training Loss: 0.4163569509983063\n","Step 940, Training Loss: 0.5722862482070923\n","Step 940, Training Loss: 0.26148805022239685\n","Step 940, Training Loss: 1.1688950061798096\n","Step 940, Training Loss: 0.5960051417350769\n","Step 940, Training Loss: 0.7032533884048462\n","Step 940, Training Loss: 0.8300137519836426\n","Step 940, Training Loss: 1.3213919401168823\n","Step 940, Training Loss: 0.9942125678062439\n","Step 940, Training Loss: 0.9373494386672974\n","Step 940, Training Loss: 0.8900761008262634\n","Step 940, Training Loss: 1.9339832067489624\n","Step 940, Training Loss: 0.3631508946418762\n","Step 940, Training Loss: 0.44456997513771057\n","Step 940, Training Loss: 0.652754008769989\n","Step 940, Training Loss: 0.5121224522590637\n","Step 940, Training Loss: 0.32189157605171204\n","Step 940, Training Loss: 1.3068957328796387\n","Step 940, Training Loss: 0.1574137657880783\n","Step 940, Training Loss: 0.7938143610954285\n","Step 940, Training Loss: 0.2682594656944275\n","Step 940, Training Loss: 0.7780762314796448\n","Step 940, Training Loss: 0.47933539748191833\n","Step 940, Training Loss: 0.16848807036876678\n","Step 940, Training Loss: 0.06064903736114502\n","Step 940, Training Loss: 0.06774785369634628\n","Step 940, Training Loss: 0.0006705311825498939\n","Step 940, Training Loss: 0.002377026481553912\n","Step 940, Training Loss: 0.002305177506059408\n","Step 950, Training Loss: 0.0019702452700585127\n","Step 950, Training Loss: 0.014396420679986477\n","Step 960, Training Loss: 0.002251514233648777\n","Step 960, Training Loss: 0.001689435332082212\n","Step 970, Training Loss: 0.05890733748674393\n","Step 970, Training Loss: 0.002033754251897335\n","Step 980, Training Loss: 0.0020761459600180387\n","Step 980, Training Loss: 0.001752441399730742\n","Step 990, Training Loss: 0.0005920071271248162\n","Step 990, Training Loss: 0.0010725207393988967\n"]},{"output_type":"stream","name":"stderr","text":["Saving model checkpoint to ./results/checkpoint-1000\n","Configuration saved in ./results/checkpoint-1000/config.json\n","Model weights saved in ./results/checkpoint-1000/pytorch_model.bin\n"]},{"output_type":"stream","name":"stdout","text":["Step 1000, Training Loss: 0.0021218922920525074\n","Step 1000, Training Loss: 0.0024398916866630316\n","Step 1010, Training Loss: 0.0009785465663298965\n","Step 1010, Training Loss: 0.0005871571484021842\n","Step 1020, Training Loss: 0.0022689681500196457\n","Step 1020, Training Loss: 0.0027611867990344763\n","Step 1030, Training Loss: 0.0012373042991384864\n","Step 1030, Training Loss: 0.006338360253721476\n","Step 1040, Training Loss: 0.0009918144205585122\n","Step 1040, Training Loss: 0.0020728204399347305\n","Step 1050, Training Loss: 0.0013967623235657811\n","Step 1050, Training Loss: 0.0034739882685244083\n","Step 1060, Training Loss: 0.0029639701824635267\n","Step 1060, Training Loss: 0.002547949319705367\n","Step 1070, Training Loss: 0.000631239905487746\n","Step 1070, Training Loss: 0.001141618238762021\n","Step 1080, Training Loss: 0.0027517881244421005\n","Step 1080, Training Loss: 0.0013819251907989383\n","Step 1090, Training Loss: 0.0008248090161941946\n","Step 1090, Training Loss: 0.0011538586113601923\n","Step 1100, Training Loss: 0.0007169329328462481\n","Step 1100, Training Loss: 0.0008653427357785404\n","Step 1110, Training Loss: 0.0007633281056769192\n","Step 1110, Training Loss: 0.0031742271967232227\n","Step 1120, Training Loss: 0.001280596712604165\n","Step 1120, Training Loss: 0.0018535982817411423\n"]},{"output_type":"stream","name":"stderr","text":["***** Running Evaluation *****\n","  Num examples = 377\n","  Batch size = 8\n"]},{"output_type":"stream","name":"stdout","text":["Step 1130, Training Loss: 0.009167971089482307\n","Step 1130, Training Loss: 0.000999932992272079\n","Step 1140, Training Loss: 0.003887162311002612\n","Step 1140, Training Loss: 0.001281103352084756\n","Step 1150, Training Loss: 0.0009544081403873861\n","Step 1150, Training Loss: 0.0012707309797406197\n","Step 1160, Training Loss: 0.0056611644104123116\n","Step 1160, Training Loss: 0.0008291053818538785\n","Step 1170, Training Loss: 0.000947137305047363\n","Step 1170, Training Loss: 0.0012992831179872155\n","Step 1180, Training Loss: 0.001557527924887836\n","Step 1180, Training Loss: 0.0014186757616698742\n","Step 1190, Training Loss: 0.0007852902635931969\n","Step 1190, Training Loss: 0.0010612442856654525\n","Step 1200, Training Loss: 0.0015707522397860885\n","Step 1200, Training Loss: 0.0009044457110576332\n","Step 1210, Training Loss: 0.0012969886884093285\n","Step 1210, Training Loss: 0.0007030951092019677\n","Step 1220, Training Loss: 0.0009391118073835969\n","Step 1220, Training Loss: 0.0009414483211003244\n","Step 1230, Training Loss: 0.001088007353246212\n","Step 1230, Training Loss: 0.010421044193208218\n","Step 1240, Training Loss: 0.0013241393025964499\n","Step 1240, Training Loss: 0.0007445098017342389\n","Step 1250, Training Loss: 0.00111963611561805\n","Step 1250, Training Loss: 0.0016752980882301927\n","Step 1260, Training Loss: 0.029578914865851402\n","Step 1260, Training Loss: 0.003918834496289492\n","Step 1270, Training Loss: 0.0010663530556485057\n","Step 1270, Training Loss: 0.006817949004471302\n","Step 1280, Training Loss: 0.0008322509238496423\n","Step 1280, Training Loss: 0.0021850939374417067\n","Step 1290, Training Loss: 0.0011728985700756311\n","Step 1290, Training Loss: 0.0202163215726614\n","Step 1300, Training Loss: 0.0010816322173923254\n","Step 1300, Training Loss: 0.0007916517788544297\n","Step 1310, Training Loss: 0.0008536548702977598\n","Step 1310, Training Loss: 0.12551359832286835\n"]},{"output_type":"stream","name":"stderr","text":["***** Running Evaluation *****\n","  Num examples = 377\n","  Batch size = 8\n","\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","***** Running Evaluation *****\n","  Num examples = 377\n","  Batch size = 8\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='48' max='48' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [48/48 00:00]\n","    </div>\n","    "]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["[I 2024-12-01 16:16:49,853] Trial 14 finished with value: 0.7065333724623482 and parameters: {'learning_rate': 4.961806220526654e-05, 'batch_size': 4, 'num_train_epochs': 7, 'weight_decay': 0.09875385997446333}. Best is trial 14 with value: 0.7065333724623482.\n","<ipython-input-10-747f1883d862>:2: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n","  learning_rate = trial.suggest_loguniform(\"learning_rate\", 1e-6, 5e-5)\n","<ipython-input-10-747f1883d862>:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n","  weight_decay = trial.suggest_loguniform(\"weight_decay\", 1e-4, 0.1)\n","PyTorch: setting up devices\n","/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n","***** Running training *****\n","  Num examples = 1507\n","  Num Epochs = 7\n","  Instantaneous batch size per device = 4\n","  Total train batch size (w. parallel, distributed & accumulation) = 8\n","  Gradient Accumulation steps = 2\n","  Total optimization steps = 1316\n"]},{"output_type":"stream","name":"stdout","text":["Starting training...\n","Step 0, Training Loss: 0.005865538492798805\n","Step 0, Training Loss: 0.0008923024288378656\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='1316' max='1316' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [1316/1316 01:10, Epoch 6/7]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","      <th>F1</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>0</td>\n","      <td>0.015100</td>\n","      <td>0.771842</td>\n","      <td>0.612732</td>\n","      <td>0.698275</td>\n","      <td>0.679803</td>\n","      <td>0.678998</td>\n","    </tr>\n","    <tr>\n","      <td>1</td>\n","      <td>0.090000</td>\n","      <td>0.818729</td>\n","      <td>0.610080</td>\n","      <td>0.705568</td>\n","      <td>0.660099</td>\n","      <td>0.667963</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>0.014700</td>\n","      <td>0.827120</td>\n","      <td>0.618037</td>\n","      <td>0.690697</td>\n","      <td>0.679803</td>\n","      <td>0.676459</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>0.008800</td>\n","      <td>0.834728</td>\n","      <td>0.618037</td>\n","      <td>0.709859</td>\n","      <td>0.667488</td>\n","      <td>0.672086</td>\n","    </tr>\n","    <tr>\n","      <td>4</td>\n","      <td>0.001500</td>\n","      <td>0.766781</td>\n","      <td>0.618037</td>\n","      <td>0.698292</td>\n","      <td>0.689655</td>\n","      <td>0.688403</td>\n","    </tr>\n","    <tr>\n","      <td>5</td>\n","      <td>0.017300</td>\n","      <td>0.789255</td>\n","      <td>0.620690</td>\n","      <td>0.693759</td>\n","      <td>0.709360</td>\n","      <td>0.693683</td>\n","    </tr>\n","    <tr>\n","      <td>6</td>\n","      <td>0.001400</td>\n","      <td>0.779290</td>\n","      <td>0.625995</td>\n","      <td>0.715846</td>\n","      <td>0.701970</td>\n","      <td>0.701478</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Step 10, Training Loss: 0.001326808356679976\n","Step 10, Training Loss: 0.001404259353876114\n","Step 20, Training Loss: 0.0056051150895655155\n","Step 20, Training Loss: 0.0006232956657186151\n","Step 30, Training Loss: 0.0009164412622340024\n","Step 30, Training Loss: 0.0018488401547074318\n","Step 40, Training Loss: 0.001154581899754703\n","Step 40, Training Loss: 0.0011689242674037814\n","Step 50, Training Loss: 0.0008864993578754365\n","Step 50, Training Loss: 0.0005263464408926666\n","Step 60, Training Loss: 0.001294415444135666\n","Step 60, Training Loss: 0.14599554240703583\n","Step 70, Training Loss: 0.0012357813538983464\n","Step 70, Training Loss: 0.0023267571814358234\n","Step 80, Training Loss: 0.0021523316390812397\n","Step 80, Training Loss: 0.008348769508302212\n","Step 90, Training Loss: 0.0010778282303363085\n","Step 90, Training Loss: 0.002702833618968725\n","Step 100, Training Loss: 0.0009157780441455543\n","Step 100, Training Loss: 0.02338642254471779\n","Step 110, Training Loss: 0.0023516654036939144\n","Step 110, Training Loss: 0.0006914624245837331\n","Step 120, Training Loss: 0.0007637648959644139\n","Step 120, Training Loss: 0.0027966469060629606\n","Step 130, Training Loss: 0.0005897512892261147\n","Step 130, Training Loss: 0.0012569573009386659\n","Step 140, Training Loss: 0.0017236557323485613\n","Step 140, Training Loss: 0.0009138001478277147\n","Step 150, Training Loss: 0.0008958359248936176\n","Step 150, Training Loss: 0.03607378527522087\n","Step 160, Training Loss: 0.000985657679848373\n","Step 160, Training Loss: 0.001708991709165275\n","Step 170, Training Loss: 0.0009869420900940895\n","Step 170, Training Loss: 0.0015204353258013725\n","Step 180, Training Loss: 0.0007304339669644833\n","Step 180, Training Loss: 0.0011130833299830556\n"]},{"output_type":"stream","name":"stderr","text":["***** Running Evaluation *****\n","  Num examples = 377\n","  Batch size = 8\n"]},{"output_type":"stream","name":"stdout","text":["Step 190, Training Loss: 0.0021155073773115873\n","Step 190, Training Loss: 0.5151779651641846\n","Step 200, Training Loss: 0.007789878640323877\n","Step 200, Training Loss: 0.0018780038226395845\n","Step 210, Training Loss: 0.0009437682456336915\n","Step 210, Training Loss: 0.001193588599562645\n","Step 220, Training Loss: 0.0013428336242213845\n","Step 220, Training Loss: 0.0016458958853036165\n","Step 230, Training Loss: 0.27090975642204285\n","Step 230, Training Loss: 0.0008242913754656911\n","Step 240, Training Loss: 0.0011041099205613136\n","Step 240, Training Loss: 0.006995546165853739\n","Step 250, Training Loss: 0.0018508235225453973\n","Step 250, Training Loss: 0.0013349739601835608\n","Step 260, Training Loss: 0.0006509064696729183\n","Step 260, Training Loss: 0.0011160614667460322\n","Step 270, Training Loss: 0.00400754576548934\n","Step 270, Training Loss: 0.003586587030440569\n","Step 280, Training Loss: 0.0008534021908417344\n","Step 280, Training Loss: 0.025708544999361038\n","Step 290, Training Loss: 0.4194916784763336\n","Step 290, Training Loss: 0.0005496302037499845\n","Step 300, Training Loss: 0.14293812215328217\n","Step 300, Training Loss: 0.002160003874450922\n","Step 310, Training Loss: 0.0010013062274083495\n","Step 310, Training Loss: 0.019363215193152428\n","Step 320, Training Loss: 0.0009320235112681985\n","Step 320, Training Loss: 0.00046893974649719894\n","Step 330, Training Loss: 0.0012353468919172883\n","Step 330, Training Loss: 0.25271448493003845\n","Step 340, Training Loss: 0.0005957083194516599\n","Step 340, Training Loss: 0.001323732198216021\n","Step 350, Training Loss: 0.0005983649170957506\n","Step 350, Training Loss: 0.0018122062319889665\n","Step 360, Training Loss: 0.003237790195271373\n","Step 360, Training Loss: 0.529414713382721\n","Step 370, Training Loss: 0.0005251601105555892\n","Step 370, Training Loss: 0.016178961843252182\n"]},{"output_type":"stream","name":"stderr","text":["***** Running Evaluation *****\n","  Num examples = 377\n","  Batch size = 8\n"]},{"output_type":"stream","name":"stdout","text":["Step 380, Training Loss: 0.0012209913693368435\n","Step 380, Training Loss: 0.00692444434389472\n","Step 390, Training Loss: 0.0004346327332314104\n","Step 390, Training Loss: 0.00736714294180274\n","Step 400, Training Loss: 0.003016140079125762\n","Step 400, Training Loss: 0.004101601894944906\n","Step 410, Training Loss: 0.0014743205392733216\n","Step 410, Training Loss: 0.0026085886638611555\n","Step 420, Training Loss: 0.03597843274474144\n","Step 420, Training Loss: 0.0012479296419769526\n","Step 430, Training Loss: 0.0005009962478652596\n","Step 430, Training Loss: 0.0005421436508186162\n","Step 440, Training Loss: 0.0004821803595405072\n","Step 440, Training Loss: 0.0027175205759704113\n","Step 450, Training Loss: 0.0027841755654662848\n","Step 450, Training Loss: 0.16777516901493073\n","Step 460, Training Loss: 0.024721946567296982\n","Step 460, Training Loss: 0.001071812934242189\n","Step 470, Training Loss: 0.0035113890189677477\n","Step 470, Training Loss: 0.0007250732742249966\n","Step 480, Training Loss: 0.0019430998945608735\n","Step 480, Training Loss: 0.0006739898817613721\n","Step 490, Training Loss: 0.001477132784202695\n","Step 490, Training Loss: 0.003120624227449298\n"]},{"output_type":"stream","name":"stderr","text":["Saving model checkpoint to ./results/checkpoint-500\n","Configuration saved in ./results/checkpoint-500/config.json\n","Model weights saved in ./results/checkpoint-500/pytorch_model.bin\n"]},{"output_type":"stream","name":"stdout","text":["Step 500, Training Loss: 0.010218089446425438\n","Step 500, Training Loss: 0.001356154796667397\n","Step 510, Training Loss: 0.0020186107140034437\n","Step 510, Training Loss: 0.4336494505405426\n","Step 520, Training Loss: 0.3088094890117645\n","Step 520, Training Loss: 0.00449945917353034\n","Step 530, Training Loss: 0.006399116013199091\n","Step 530, Training Loss: 0.0009426367469131947\n","Step 540, Training Loss: 0.002010112628340721\n","Step 540, Training Loss: 0.2934243381023407\n","Step 550, Training Loss: 0.0043372889049351215\n","Step 550, Training Loss: 0.0012597058666869998\n","Step 560, Training Loss: 0.003671261016279459\n","Step 560, Training Loss: 0.0013818020233884454\n"]},{"output_type":"stream","name":"stderr","text":["***** Running Evaluation *****\n","  Num examples = 377\n","  Batch size = 8\n"]},{"output_type":"stream","name":"stdout","text":["Step 570, Training Loss: 0.0012220535427331924\n","Step 570, Training Loss: 0.0008146854233928025\n","Step 580, Training Loss: 0.0006132330745458603\n","Step 580, Training Loss: 0.0011023012921214104\n","Step 590, Training Loss: 0.2793508470058441\n","Step 590, Training Loss: 0.08525427430868149\n","Step 600, Training Loss: 0.0005613484536297619\n","Step 600, Training Loss: 0.0013451090781018138\n","Step 610, Training Loss: 0.0020784796215593815\n","Step 610, Training Loss: 0.0007865687948651612\n","Step 620, Training Loss: 0.0005336941685527563\n","Step 620, Training Loss: 0.03254332393407822\n","Step 630, Training Loss: 0.0013947264524176717\n","Step 630, Training Loss: 0.0017293052515015006\n","Step 640, Training Loss: 0.0014696652069687843\n","Step 640, Training Loss: 0.0006127856904640794\n","Step 650, Training Loss: 0.0010177079821005464\n","Step 650, Training Loss: 0.0019543441012501717\n","Step 660, Training Loss: 0.0005706154624931514\n","Step 660, Training Loss: 0.024789994582533836\n","Step 670, Training Loss: 0.0010829141829162836\n","Step 670, Training Loss: 0.0005310151027515531\n","Step 680, Training Loss: 0.000834491744171828\n","Step 680, Training Loss: 0.0006503874901682138\n","Step 690, Training Loss: 0.0018084595212712884\n","Step 690, Training Loss: 0.0011884609702974558\n","Step 700, Training Loss: 0.0007860909099690616\n","Step 700, Training Loss: 0.00044262417941354215\n","Step 710, Training Loss: 0.0005549319903366268\n","Step 710, Training Loss: 0.0009291536989621818\n","Step 720, Training Loss: 0.0007050007698126137\n","Step 720, Training Loss: 0.012552118860185146\n","Step 730, Training Loss: 0.0005618433351628482\n","Step 730, Training Loss: 0.0006010110373608768\n","Step 740, Training Loss: 0.0037998908665031195\n","Step 740, Training Loss: 0.0007243237341754138\n","Step 750, Training Loss: 0.0014809190761297941\n","Step 750, Training Loss: 0.0006218233029358089\n"]},{"output_type":"stream","name":"stderr","text":["***** Running Evaluation *****\n","  Num examples = 377\n","  Batch size = 8\n"]},{"output_type":"stream","name":"stdout","text":["Step 760, Training Loss: 0.0007006513187661767\n","Step 760, Training Loss: 0.19137802720069885\n","Step 770, Training Loss: 0.0006705630803480744\n","Step 770, Training Loss: 0.0012045744806528091\n","Step 780, Training Loss: 0.0004342988249845803\n","Step 780, Training Loss: 0.0006774883950129151\n","Step 790, Training Loss: 0.0005513398791663349\n","Step 790, Training Loss: 0.03622285649180412\n","Step 800, Training Loss: 0.0008110025082714856\n","Step 800, Training Loss: 0.0004877966421190649\n","Step 810, Training Loss: 0.0004925176035612822\n","Step 810, Training Loss: 0.0009062881581485271\n","Step 820, Training Loss: 0.0022365537006407976\n","Step 820, Training Loss: 0.0007739987340755761\n","Step 830, Training Loss: 0.0016762756276875734\n","Step 830, Training Loss: 0.001581639051437378\n","Step 840, Training Loss: 0.009027755819261074\n","Step 840, Training Loss: 0.000715616624802351\n","Step 850, Training Loss: 0.0007299751159735024\n","Step 850, Training Loss: 0.00039586154161952436\n","Step 860, Training Loss: 0.000489798781927675\n","Step 860, Training Loss: 0.0005328278639353812\n","Step 870, Training Loss: 0.005877665244042873\n","Step 870, Training Loss: 0.0008273363346233964\n","Step 880, Training Loss: 0.05047755315899849\n","Step 880, Training Loss: 0.0007749546784907579\n","Step 890, Training Loss: 0.0005815461627207696\n","Step 890, Training Loss: 0.0006335288635455072\n","Step 900, Training Loss: 0.009639565832912922\n","Step 900, Training Loss: 0.0005018854863010347\n","Step 910, Training Loss: 0.0017159428680315614\n","Step 910, Training Loss: 0.0022232020273804665\n","Step 920, Training Loss: 0.0006357470410875976\n","Step 920, Training Loss: 0.0005680506001226604\n","Step 930, Training Loss: 0.0010252278298139572\n","Step 930, Training Loss: 0.0005584717728197575\n"]},{"output_type":"stream","name":"stderr","text":["***** Running Evaluation *****\n","  Num examples = 377\n","  Batch size = 8\n"]},{"output_type":"stream","name":"stdout","text":["Step 940, Training Loss: 0.0004914701567031443\n","Step 940, Training Loss: 0.8716805577278137\n","Step 940, Training Loss: 0.8258814215660095\n","Step 940, Training Loss: 0.8548334240913391\n","Step 940, Training Loss: 1.972572684288025\n","Step 940, Training Loss: 0.9738669395446777\n","Step 940, Training Loss: 0.918280303478241\n","Step 940, Training Loss: 1.6101665496826172\n","Step 940, Training Loss: 0.6196063160896301\n","Step 940, Training Loss: 0.14146797358989716\n","Step 940, Training Loss: 1.3211902379989624\n","Step 940, Training Loss: 1.2950252294540405\n","Step 940, Training Loss: 0.7202640771865845\n","Step 940, Training Loss: 0.8395920991897583\n","Step 940, Training Loss: 0.0005626908969134092\n","Step 940, Training Loss: 0.3096364140510559\n","Step 940, Training Loss: 0.918878972530365\n","Step 940, Training Loss: 0.9864439964294434\n","Step 940, Training Loss: 0.4979996681213379\n","Step 940, Training Loss: 0.3858644366264343\n","Step 940, Training Loss: 0.00698126433417201\n","Step 940, Training Loss: 0.44779205322265625\n","Step 940, Training Loss: 0.7702648043632507\n","Step 940, Training Loss: 0.4997674524784088\n","Step 940, Training Loss: 0.23940277099609375\n","Step 940, Training Loss: 1.259254813194275\n","Step 940, Training Loss: 0.6077154278755188\n","Step 940, Training Loss: 0.5737385153770447\n","Step 940, Training Loss: 0.832960307598114\n","Step 940, Training Loss: 1.0385929346084595\n","Step 940, Training Loss: 0.9761099219322205\n","Step 940, Training Loss: 1.0715548992156982\n","Step 940, Training Loss: 1.4965132474899292\n","Step 940, Training Loss: 2.073352098464966\n","Step 940, Training Loss: 0.37035879492759705\n","Step 940, Training Loss: 0.5404878258705139\n","Step 940, Training Loss: 0.8300348520278931\n","Step 940, Training Loss: 0.4124889075756073\n","Step 940, Training Loss: 0.4067244529724121\n","Step 940, Training Loss: 1.4527758359909058\n","Step 940, Training Loss: 0.21553806960582733\n","Step 940, Training Loss: 0.8492428660392761\n","Step 940, Training Loss: 0.49149903655052185\n","Step 940, Training Loss: 0.8198873400688171\n","Step 940, Training Loss: 0.2854577302932739\n","Step 940, Training Loss: 0.2258928269147873\n","Step 940, Training Loss: 0.38065704703330994\n","Step 940, Training Loss: 0.8956686854362488\n","Step 940, Training Loss: 0.0003654698084574193\n","Step 940, Training Loss: 0.0019378301221877337\n","Step 940, Training Loss: 0.0010947099654003978\n","Step 950, Training Loss: 0.0006334466743282974\n","Step 950, Training Loss: 0.002166474936529994\n","Step 960, Training Loss: 0.001280766213312745\n","Step 960, Training Loss: 0.0010328148491680622\n","Step 970, Training Loss: 0.14568892121315002\n","Step 970, Training Loss: 0.0013218589592725039\n","Step 980, Training Loss: 0.000928312074393034\n","Step 980, Training Loss: 0.0009289723820984364\n","Step 990, Training Loss: 0.0003967054362874478\n","Step 990, Training Loss: 0.0006673847092315555\n"]},{"output_type":"stream","name":"stderr","text":["Saving model checkpoint to ./results/checkpoint-1000\n","Configuration saved in ./results/checkpoint-1000/config.json\n","Model weights saved in ./results/checkpoint-1000/pytorch_model.bin\n"]},{"output_type":"stream","name":"stdout","text":["Step 1000, Training Loss: 0.0027202030178159475\n","Step 1000, Training Loss: 0.0009631059947423637\n","Step 1010, Training Loss: 0.01972109079360962\n","Step 1010, Training Loss: 0.0004011106793768704\n","Step 1020, Training Loss: 0.0008632458047941327\n","Step 1020, Training Loss: 0.0013527503469958901\n","Step 1030, Training Loss: 0.0006876895786263049\n","Step 1030, Training Loss: 0.0006067663780413568\n","Step 1040, Training Loss: 0.0006129147368483245\n","Step 1040, Training Loss: 0.0009776124497875571\n","Step 1050, Training Loss: 0.00149578379932791\n","Step 1050, Training Loss: 0.0025425523053854704\n","Step 1060, Training Loss: 0.00052369583863765\n","Step 1060, Training Loss: 0.0010231084888800979\n","Step 1070, Training Loss: 0.0003837432886939496\n","Step 1070, Training Loss: 0.0006129622925072908\n","Step 1080, Training Loss: 0.002590584335848689\n","Step 1080, Training Loss: 0.000812905840575695\n","Step 1090, Training Loss: 0.000553873076569289\n","Step 1090, Training Loss: 0.000705735117662698\n","Step 1100, Training Loss: 0.00043832106166519225\n","Step 1100, Training Loss: 0.0005359649658203125\n","Step 1110, Training Loss: 0.0004624405992217362\n","Step 1110, Training Loss: 0.0010108031565323472\n","Step 1120, Training Loss: 0.0028281023260205984\n","Step 1120, Training Loss: 0.0010395789286121726\n"]},{"output_type":"stream","name":"stderr","text":["***** Running Evaluation *****\n","  Num examples = 377\n","  Batch size = 8\n"]},{"output_type":"stream","name":"stdout","text":["Step 1130, Training Loss: 0.013019106350839138\n","Step 1130, Training Loss: 0.0005076277302578092\n","Step 1140, Training Loss: 0.0008107023313641548\n","Step 1140, Training Loss: 0.0008209827356040478\n","Step 1150, Training Loss: 0.0005702679045498371\n","Step 1150, Training Loss: 0.0007645003497600555\n","Step 1160, Training Loss: 0.001048387959599495\n","Step 1160, Training Loss: 0.0005478283273987472\n","Step 1170, Training Loss: 0.0006445134058594704\n","Step 1170, Training Loss: 0.0008247989462688565\n","Step 1180, Training Loss: 0.0009389069746248424\n","Step 1180, Training Loss: 0.009460977278649807\n","Step 1190, Training Loss: 0.0005157456616871059\n","Step 1190, Training Loss: 0.0007265842868946493\n","Step 1200, Training Loss: 0.0009320104727521539\n","Step 1200, Training Loss: 0.0005339881754480302\n","Step 1210, Training Loss: 0.0007142685353755951\n","Step 1210, Training Loss: 0.00042331693111918867\n","Step 1220, Training Loss: 0.0004956941702403128\n","Step 1220, Training Loss: 0.0006368078175000846\n","Step 1230, Training Loss: 0.0006151749403215945\n","Step 1230, Training Loss: 0.0010130776790902019\n","Step 1240, Training Loss: 0.0007632414344698191\n","Step 1240, Training Loss: 0.0004583951085805893\n","Step 1250, Training Loss: 0.0006107782246544957\n","Step 1250, Training Loss: 0.0008478589588776231\n","Step 1260, Training Loss: 0.003776337718591094\n","Step 1260, Training Loss: 0.0012357740197330713\n","Step 1270, Training Loss: 0.000638247060123831\n","Step 1270, Training Loss: 0.0036201965995132923\n","Step 1280, Training Loss: 0.0005277054733596742\n","Step 1280, Training Loss: 0.0008135012467391789\n","Step 1290, Training Loss: 0.0006710723973810673\n","Step 1290, Training Loss: 0.003858718788251281\n","Step 1300, Training Loss: 0.0006431788788177073\n","Step 1300, Training Loss: 0.0004911668947897851\n","Step 1310, Training Loss: 0.0004992351750843227\n","Step 1310, Training Loss: 0.033027876168489456\n"]},{"output_type":"stream","name":"stderr","text":["***** Running Evaluation *****\n","  Num examples = 377\n","  Batch size = 8\n","\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","***** Running Evaluation *****\n","  Num examples = 377\n","  Batch size = 8\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='48' max='48' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [48/48 00:00]\n","    </div>\n","    "]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["[I 2024-12-01 16:18:00,883] Trial 15 finished with value: 0.701477620836453 and parameters: {'learning_rate': 4.922742215752957e-05, 'batch_size': 4, 'num_train_epochs': 7, 'weight_decay': 0.040424444933561204}. Best is trial 14 with value: 0.7065333724623482.\n","<ipython-input-10-747f1883d862>:2: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n","  learning_rate = trial.suggest_loguniform(\"learning_rate\", 1e-6, 5e-5)\n","<ipython-input-10-747f1883d862>:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n","  weight_decay = trial.suggest_loguniform(\"weight_decay\", 1e-4, 0.1)\n","PyTorch: setting up devices\n","/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n","***** Running training *****\n","  Num examples = 1507\n","  Num Epochs = 7\n","  Instantaneous batch size per device = 4\n","  Total train batch size (w. parallel, distributed & accumulation) = 8\n","  Gradient Accumulation steps = 2\n","  Total optimization steps = 1316\n"]},{"output_type":"stream","name":"stdout","text":["Starting training...\n","Step 0, Training Loss: 0.0015130118699744344\n","Step 0, Training Loss: 0.0005043903365731239\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='1316' max='1316' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [1316/1316 01:10, Epoch 6/7]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","      <th>F1</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>0</td>\n","      <td>0.000800</td>\n","      <td>0.807312</td>\n","      <td>0.602122</td>\n","      <td>0.697793</td>\n","      <td>0.687192</td>\n","      <td>0.684140</td>\n","    </tr>\n","    <tr>\n","      <td>1</td>\n","      <td>0.000900</td>\n","      <td>0.812194</td>\n","      <td>0.612732</td>\n","      <td>0.702270</td>\n","      <td>0.697044</td>\n","      <td>0.689336</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>0.000900</td>\n","      <td>0.789362</td>\n","      <td>0.628647</td>\n","      <td>0.705346</td>\n","      <td>0.711823</td>\n","      <td>0.700796</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>0.000700</td>\n","      <td>0.774570</td>\n","      <td>0.625995</td>\n","      <td>0.707037</td>\n","      <td>0.701970</td>\n","      <td>0.698079</td>\n","    </tr>\n","    <tr>\n","      <td>4</td>\n","      <td>0.000700</td>\n","      <td>0.776900</td>\n","      <td>0.620690</td>\n","      <td>0.706298</td>\n","      <td>0.709360</td>\n","      <td>0.702050</td>\n","    </tr>\n","    <tr>\n","      <td>5</td>\n","      <td>0.001000</td>\n","      <td>0.772454</td>\n","      <td>0.623342</td>\n","      <td>0.708134</td>\n","      <td>0.704433</td>\n","      <td>0.699714</td>\n","    </tr>\n","    <tr>\n","      <td>6</td>\n","      <td>0.001000</td>\n","      <td>0.773123</td>\n","      <td>0.628647</td>\n","      <td>0.712904</td>\n","      <td>0.704433</td>\n","      <td>0.702235</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Step 10, Training Loss: 0.0006503383629024029\n","Step 10, Training Loss: 0.0006796742673031986\n","Step 20, Training Loss: 0.00180140882730484\n","Step 20, Training Loss: 0.0003898358845617622\n","Step 30, Training Loss: 0.00046879492583684623\n","Step 30, Training Loss: 0.0007943250238895416\n","Step 40, Training Loss: 0.0006468730862252414\n","Step 40, Training Loss: 0.0004810185346286744\n","Step 50, Training Loss: 0.0005206304485909641\n","Step 50, Training Loss: 0.00035834769369103014\n","Step 60, Training Loss: 0.0006490840460173786\n","Step 60, Training Loss: 0.000987538485787809\n","Step 70, Training Loss: 0.000864272762555629\n","Step 70, Training Loss: 0.0018308950820937753\n","Step 80, Training Loss: 0.0010152405593544245\n","Step 80, Training Loss: 0.0008790179272182286\n","Step 90, Training Loss: 0.0006377475801855326\n","Step 90, Training Loss: 0.0008636150741949677\n","Step 100, Training Loss: 0.0004814283165615052\n","Step 100, Training Loss: 0.001871854648925364\n","Step 110, Training Loss: 0.0005654964479617774\n","Step 110, Training Loss: 0.000450863124569878\n","Step 120, Training Loss: 0.00048123844317160547\n","Step 120, Training Loss: 0.0009217340848408639\n","Step 130, Training Loss: 0.0004103874962311238\n","Step 130, Training Loss: 0.0007779066218063235\n","Step 140, Training Loss: 0.0007628878811374307\n","Step 140, Training Loss: 0.00037794208037666976\n","Step 150, Training Loss: 0.0006180014461278915\n","Step 150, Training Loss: 0.011757506988942623\n","Step 160, Training Loss: 0.0005660623428411782\n","Step 160, Training Loss: 0.0007194748031906784\n","Step 170, Training Loss: 0.0007751128869131207\n","Step 170, Training Loss: 0.0008890243479982018\n","Step 180, Training Loss: 0.0004627603047993034\n","Step 180, Training Loss: 0.0005353090236894786\n"]},{"output_type":"stream","name":"stderr","text":["***** Running Evaluation *****\n","  Num examples = 377\n","  Batch size = 8\n"]},{"output_type":"stream","name":"stdout","text":["Step 190, Training Loss: 0.0011915038339793682\n","Step 190, Training Loss: 0.03244945406913757\n","Step 200, Training Loss: 0.0006035274127498269\n","Step 200, Training Loss: 0.0006930797244422138\n","Step 210, Training Loss: 0.000534654303919524\n","Step 210, Training Loss: 0.0007408387027680874\n","Step 220, Training Loss: 0.0006091782124713063\n","Step 220, Training Loss: 0.0009526239591650665\n","Step 230, Training Loss: 0.0012231067521497607\n","Step 230, Training Loss: 0.0005240571335889399\n","Step 240, Training Loss: 0.000599915802013129\n","Step 240, Training Loss: 0.0024585535284131765\n","Step 250, Training Loss: 0.0010115383192896843\n","Step 250, Training Loss: 0.0006595089216716588\n","Step 260, Training Loss: 0.00044748224900104105\n","Step 260, Training Loss: 0.0007267905748449266\n","Step 270, Training Loss: 0.0008660798775963485\n","Step 270, Training Loss: 0.0024899046402424574\n","Step 280, Training Loss: 0.0006290891324169934\n","Step 280, Training Loss: 0.0008516866946592927\n","Step 290, Training Loss: 0.0009771642507985234\n","Step 290, Training Loss: 0.00038750117528252304\n","Step 300, Training Loss: 0.0037617702037096024\n","Step 300, Training Loss: 0.001453705015592277\n","Step 310, Training Loss: 0.0005919677205383778\n","Step 310, Training Loss: 0.0008785970858298242\n","Step 320, Training Loss: 0.0006575810839422047\n","Step 320, Training Loss: 0.0003599317278712988\n","Step 330, Training Loss: 0.0007566764252260327\n","Step 330, Training Loss: 0.0008951874333433807\n","Step 340, Training Loss: 0.0004292142693884671\n","Step 340, Training Loss: 0.0011712390696629882\n","Step 350, Training Loss: 0.00045562232844531536\n","Step 350, Training Loss: 0.00120002799667418\n","Step 360, Training Loss: 0.0005901659023948014\n","Step 360, Training Loss: 0.0005907256854698062\n","Step 370, Training Loss: 0.00042036670492962003\n","Step 370, Training Loss: 0.0004699132405221462\n"]},{"output_type":"stream","name":"stderr","text":["***** Running Evaluation *****\n","  Num examples = 377\n","  Batch size = 8\n"]},{"output_type":"stream","name":"stdout","text":["Step 380, Training Loss: 0.0009896161500364542\n","Step 380, Training Loss: 0.003718199674040079\n","Step 390, Training Loss: 0.0003221432853024453\n","Step 390, Training Loss: 0.001422615023329854\n","Step 400, Training Loss: 0.000979867996647954\n","Step 400, Training Loss: 0.0006849809433333576\n","Step 410, Training Loss: 0.0006231052684597671\n","Step 410, Training Loss: 0.0007387601071968675\n","Step 420, Training Loss: 0.2315359115600586\n","Step 420, Training Loss: 0.000778040150180459\n","Step 430, Training Loss: 0.000363172817742452\n","Step 430, Training Loss: 0.00040686922147870064\n","Step 440, Training Loss: 0.00035202212166041136\n","Step 440, Training Loss: 0.0007335552363656461\n","Step 450, Training Loss: 0.0008961035055108368\n","Step 450, Training Loss: 0.011910867877304554\n","Step 460, Training Loss: 0.0009195355814881623\n","Step 460, Training Loss: 0.0005946889868937433\n","Step 470, Training Loss: 0.0010781994787976146\n","Step 470, Training Loss: 0.0005246262880973518\n","Step 480, Training Loss: 0.0009384882869198918\n","Step 480, Training Loss: 0.000495492247864604\n","Step 490, Training Loss: 0.0007635850342921913\n","Step 490, Training Loss: 0.0005000512464903295\n"]},{"output_type":"stream","name":"stderr","text":["Saving model checkpoint to ./results/checkpoint-500\n","Configuration saved in ./results/checkpoint-500/config.json\n","Model weights saved in ./results/checkpoint-500/pytorch_model.bin\n"]},{"output_type":"stream","name":"stdout","text":["Step 500, Training Loss: 0.0007822690531611443\n","Step 500, Training Loss: 0.000943429593462497\n","Step 510, Training Loss: 0.002432154957205057\n","Step 510, Training Loss: 0.00061945611378178\n","Step 520, Training Loss: 0.0006815422675572336\n","Step 520, Training Loss: 0.00123083614744246\n","Step 530, Training Loss: 0.0013643730198964477\n","Step 530, Training Loss: 0.0009537581354379654\n","Step 540, Training Loss: 0.0008101718849502504\n","Step 540, Training Loss: 0.0009755173814482987\n","Step 550, Training Loss: 0.0011400593211874366\n","Step 550, Training Loss: 0.0006544661009684205\n","Step 560, Training Loss: 0.0011761569185182452\n","Step 560, Training Loss: 0.0006961410981602967\n"]},{"output_type":"stream","name":"stderr","text":["***** Running Evaluation *****\n","  Num examples = 377\n","  Batch size = 8\n"]},{"output_type":"stream","name":"stdout","text":["Step 570, Training Loss: 0.000794896564912051\n","Step 570, Training Loss: 0.0007280438439920545\n","Step 580, Training Loss: 0.000453348271548748\n","Step 580, Training Loss: 0.000625968852546066\n","Step 590, Training Loss: 0.0004944385145790875\n","Step 590, Training Loss: 0.001630669110454619\n","Step 600, Training Loss: 0.00044372581760399044\n","Step 600, Training Loss: 0.0009323912090621889\n","Step 610, Training Loss: 0.0005163670866750181\n","Step 610, Training Loss: 0.0005346825928427279\n","Step 620, Training Loss: 0.00044734292896464467\n","Step 620, Training Loss: 0.0008175256079994142\n","Step 630, Training Loss: 0.0012887403136119246\n","Step 630, Training Loss: 0.000996523187495768\n","Step 640, Training Loss: 0.0011346553219482303\n","Step 640, Training Loss: 0.0004528334829956293\n","Step 650, Training Loss: 0.0008698822348378599\n","Step 650, Training Loss: 0.0007006394444033504\n","Step 660, Training Loss: 0.00038990468601696193\n","Step 660, Training Loss: 0.0011936428491026163\n","Step 670, Training Loss: 0.0007208501338027418\n","Step 670, Training Loss: 0.00041995319770649076\n","Step 680, Training Loss: 0.0007315977127291262\n","Step 680, Training Loss: 0.0005199958686716855\n","Step 690, Training Loss: 0.0006617696490138769\n","Step 690, Training Loss: 0.001090534497052431\n","Step 700, Training Loss: 0.0006677584024146199\n","Step 700, Training Loss: 0.00034104660153388977\n","Step 710, Training Loss: 0.000499363464768976\n","Step 710, Training Loss: 0.000781276379711926\n","Step 720, Training Loss: 0.0005692150443792343\n","Step 720, Training Loss: 0.002678488614037633\n","Step 730, Training Loss: 0.00046933101839385927\n","Step 730, Training Loss: 0.00040280306711792946\n","Step 740, Training Loss: 0.0015553366392850876\n","Step 740, Training Loss: 0.0005519679398275912\n","Step 750, Training Loss: 0.0016087687108665705\n","Step 750, Training Loss: 0.0006832079379819334\n"]},{"output_type":"stream","name":"stderr","text":["***** Running Evaluation *****\n","  Num examples = 377\n","  Batch size = 8\n"]},{"output_type":"stream","name":"stdout","text":["Step 760, Training Loss: 0.000526736315805465\n","Step 760, Training Loss: 0.007487689144909382\n","Step 770, Training Loss: 0.000542864843737334\n","Step 770, Training Loss: 0.0008120117709040642\n","Step 780, Training Loss: 0.0003664387040771544\n","Step 780, Training Loss: 0.00047754435217939317\n","Step 790, Training Loss: 0.00043882569298148155\n","Step 790, Training Loss: 0.0017423465615138412\n","Step 800, Training Loss: 0.0006268170545808971\n","Step 800, Training Loss: 0.00039935653330758214\n","Step 810, Training Loss: 0.0004015909507870674\n","Step 810, Training Loss: 0.0005431421450339258\n","Step 820, Training Loss: 0.0005988755146972835\n","Step 820, Training Loss: 0.000596150232013315\n","Step 830, Training Loss: 0.0011214334517717361\n","Step 830, Training Loss: 0.0011140205897390842\n","Step 840, Training Loss: 0.0009201606153510511\n","Step 840, Training Loss: 0.0005637130816467106\n","Step 850, Training Loss: 0.0005977050750516355\n","Step 850, Training Loss: 0.00035996694350615144\n","Step 860, Training Loss: 0.0004103758546989411\n","Step 860, Training Loss: 0.0004319119325373322\n","Step 870, Training Loss: 0.0019903776701539755\n","Step 870, Training Loss: 0.0006985847721807659\n","Step 880, Training Loss: 0.0006616621976718307\n","Step 880, Training Loss: 0.0006540770991705358\n","Step 890, Training Loss: 0.0005004373379051685\n","Step 890, Training Loss: 0.0005418274668045342\n","Step 900, Training Loss: 0.0016741190338507295\n","Step 900, Training Loss: 0.0003958866000175476\n","Step 910, Training Loss: 0.0014618048444390297\n","Step 910, Training Loss: 0.001085416879504919\n","Step 920, Training Loss: 0.0005447856965474784\n","Step 920, Training Loss: 0.0004678113909903914\n","Step 930, Training Loss: 0.0009101233445107937\n","Step 930, Training Loss: 0.00046160558122210205\n"]},{"output_type":"stream","name":"stderr","text":["***** Running Evaluation *****\n","  Num examples = 377\n","  Batch size = 8\n"]},{"output_type":"stream","name":"stdout","text":["Step 940, Training Loss: 0.0004413572314660996\n","Step 940, Training Loss: 0.8029905557632446\n","Step 940, Training Loss: 1.0225871801376343\n","Step 940, Training Loss: 0.8006955981254578\n","Step 940, Training Loss: 1.995270013809204\n","Step 940, Training Loss: 1.0809192657470703\n","Step 940, Training Loss: 0.9451606869697571\n","Step 940, Training Loss: 1.6230659484863281\n","Step 940, Training Loss: 0.6339321136474609\n","Step 940, Training Loss: 0.1277032494544983\n","Step 940, Training Loss: 1.5058079957962036\n","Step 940, Training Loss: 1.481846809387207\n","Step 940, Training Loss: 0.7086431384086609\n","Step 940, Training Loss: 0.6857826113700867\n","Step 940, Training Loss: 0.0005457925726659596\n","Step 940, Training Loss: 0.24171653389930725\n","Step 940, Training Loss: 0.7972408533096313\n","Step 940, Training Loss: 1.0456956624984741\n","Step 940, Training Loss: 0.5396822094917297\n","Step 940, Training Loss: 0.40562471747398376\n","Step 940, Training Loss: 0.04894212260842323\n","Step 940, Training Loss: 0.5017483234405518\n","Step 940, Training Loss: 0.7290148138999939\n","Step 940, Training Loss: 0.4868747293949127\n","Step 940, Training Loss: 0.19670520722866058\n","Step 940, Training Loss: 1.2439020872116089\n","Step 940, Training Loss: 0.6383955478668213\n","Step 940, Training Loss: 0.6692506670951843\n","Step 940, Training Loss: 0.9740467071533203\n","Step 940, Training Loss: 1.1028861999511719\n","Step 940, Training Loss: 1.0944257974624634\n","Step 940, Training Loss: 1.0358726978302002\n","Step 940, Training Loss: 1.4310356378555298\n","Step 940, Training Loss: 1.9899787902832031\n","Step 940, Training Loss: 0.36938297748565674\n","Step 940, Training Loss: 0.46445685625076294\n","Step 940, Training Loss: 0.9134705662727356\n","Step 940, Training Loss: 0.4142908751964569\n","Step 940, Training Loss: 0.22525636851787567\n","Step 940, Training Loss: 1.4865095615386963\n","Step 940, Training Loss: 0.16490410268306732\n","Step 940, Training Loss: 0.9058767557144165\n","Step 940, Training Loss: 0.5919389724731445\n","Step 940, Training Loss: 0.7415466904640198\n","Step 940, Training Loss: 0.4365754723548889\n","Step 940, Training Loss: 0.17775194346904755\n","Step 940, Training Loss: 0.28656336665153503\n","Step 940, Training Loss: 0.8448395133018494\n","Step 940, Training Loss: 0.0003238431818317622\n","Step 940, Training Loss: 0.001022474025376141\n","Step 940, Training Loss: 0.0009969546226784587\n","Step 950, Training Loss: 0.0005408158176578581\n","Step 950, Training Loss: 0.001454116660170257\n","Step 960, Training Loss: 0.0010645160218700767\n","Step 960, Training Loss: 0.0008347240509465337\n","Step 970, Training Loss: 0.0010860578622668982\n","Step 970, Training Loss: 0.0011054252972826362\n","Step 980, Training Loss: 0.0007296765106730163\n","Step 980, Training Loss: 0.0008587436750531197\n","Step 990, Training Loss: 0.00033599240123294294\n","Step 990, Training Loss: 0.0005058423848822713\n"]},{"output_type":"stream","name":"stderr","text":["Saving model checkpoint to ./results/checkpoint-1000\n","Configuration saved in ./results/checkpoint-1000/config.json\n","Model weights saved in ./results/checkpoint-1000/pytorch_model.bin\n"]},{"output_type":"stream","name":"stdout","text":["Step 1000, Training Loss: 0.001004698220640421\n","Step 1000, Training Loss: 0.0008254646672867239\n","Step 1010, Training Loss: 0.0004634493088815361\n","Step 1010, Training Loss: 0.0003212420269846916\n","Step 1020, Training Loss: 0.0007741045556031168\n","Step 1020, Training Loss: 0.001171817071735859\n","Step 1030, Training Loss: 0.0005903096753172576\n","Step 1030, Training Loss: 0.0005316189490258694\n","Step 1040, Training Loss: 0.0005022503319196403\n","Step 1040, Training Loss: 0.0007020501652732491\n","Step 1050, Training Loss: 0.0005708294920623302\n","Step 1050, Training Loss: 0.002191184088587761\n","Step 1060, Training Loss: 0.0004151889297645539\n","Step 1060, Training Loss: 0.000843039364553988\n","Step 1070, Training Loss: 0.0003362966235727072\n","Step 1070, Training Loss: 0.0005885364953428507\n","Step 1080, Training Loss: 0.0013028717366978526\n","Step 1080, Training Loss: 0.0007129833684302866\n","Step 1090, Training Loss: 0.00046119533362798393\n","Step 1090, Training Loss: 0.000689369800966233\n","Step 1100, Training Loss: 0.00038385085645131767\n","Step 1100, Training Loss: 0.0004831385740544647\n","Step 1110, Training Loss: 0.0004099619691260159\n","Step 1110, Training Loss: 0.0009705703123472631\n","Step 1120, Training Loss: 0.0006154102156870067\n","Step 1120, Training Loss: 0.0008934198995120823\n"]},{"output_type":"stream","name":"stderr","text":["***** Running Evaluation *****\n","  Num examples = 377\n","  Batch size = 8\n"]},{"output_type":"stream","name":"stdout","text":["Step 1130, Training Loss: 0.0016753353411331773\n","Step 1130, Training Loss: 0.0004525266122072935\n","Step 1140, Training Loss: 0.0007398303132504225\n","Step 1140, Training Loss: 0.0006752065382897854\n","Step 1150, Training Loss: 0.0005083965370431542\n","Step 1150, Training Loss: 0.0006578313186764717\n","Step 1160, Training Loss: 0.0010310712968930602\n","Step 1160, Training Loss: 0.00045580739970318973\n","Step 1170, Training Loss: 0.0005770119023509324\n","Step 1170, Training Loss: 0.0007275505340658128\n","Step 1180, Training Loss: 0.0008414915064349771\n","Step 1180, Training Loss: 0.0004961768281646073\n","Step 1190, Training Loss: 0.0005239483434706926\n","Step 1190, Training Loss: 0.0006717319483868778\n","Step 1200, Training Loss: 0.0008625217014923692\n","Step 1200, Training Loss: 0.000478098081657663\n","Step 1210, Training Loss: 0.0006315823993645608\n","Step 1210, Training Loss: 0.0003865384205710143\n","Step 1220, Training Loss: 0.0004392792470753193\n","Step 1220, Training Loss: 0.0005342098302207887\n","Step 1230, Training Loss: 0.0005539158592000604\n","Step 1230, Training Loss: 0.0012365947477519512\n","Step 1240, Training Loss: 0.0006773685454390943\n","Step 1240, Training Loss: 0.00040827016346156597\n","Step 1250, Training Loss: 0.0005478374660015106\n","Step 1250, Training Loss: 0.0007294723181985319\n","Step 1260, Training Loss: 0.009184466674923897\n","Step 1260, Training Loss: 0.0010471332352608442\n","Step 1270, Training Loss: 0.0005931825144216418\n","Step 1270, Training Loss: 0.0012354821665212512\n","Step 1280, Training Loss: 0.0004891527350991964\n","Step 1280, Training Loss: 0.0008058274397626519\n","Step 1290, Training Loss: 0.0006276198546402156\n","Step 1290, Training Loss: 0.0020987701136618853\n","Step 1300, Training Loss: 0.0005569499917328358\n","Step 1300, Training Loss: 0.00044192513450980186\n","Step 1310, Training Loss: 0.00044204480946063995\n","Step 1310, Training Loss: 0.0808112770318985\n"]},{"output_type":"stream","name":"stderr","text":["***** Running Evaluation *****\n","  Num examples = 377\n","  Batch size = 8\n","\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","***** Running Evaluation *****\n","  Num examples = 377\n","  Batch size = 8\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='48' max='48' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [48/48 00:00]\n","    </div>\n","    "]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["[I 2024-12-01 16:19:11,892] Trial 16 finished with value: 0.7022354308344535 and parameters: {'learning_rate': 3.5211814258203642e-06, 'batch_size': 4, 'num_train_epochs': 7, 'weight_decay': 0.011312415617228452}. Best is trial 14 with value: 0.7065333724623482.\n","<ipython-input-10-747f1883d862>:2: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n","  learning_rate = trial.suggest_loguniform(\"learning_rate\", 1e-6, 5e-5)\n","<ipython-input-10-747f1883d862>:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n","  weight_decay = trial.suggest_loguniform(\"weight_decay\", 1e-4, 0.1)\n","PyTorch: setting up devices\n","/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n","***** Running training *****\n","  Num examples = 1507\n","  Num Epochs = 7\n","  Instantaneous batch size per device = 4\n","  Total train batch size (w. parallel, distributed & accumulation) = 8\n","  Gradient Accumulation steps = 2\n","  Total optimization steps = 1316\n"]},{"output_type":"stream","name":"stdout","text":["Starting training...\n","Step 0, Training Loss: 0.0012899202993139625\n","Step 0, Training Loss: 0.0004512810555752367\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='1316' max='1316' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [1316/1316 01:09, Epoch 6/7]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","      <th>F1</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>0</td>\n","      <td>0.000700</td>\n","      <td>0.812849</td>\n","      <td>0.615385</td>\n","      <td>0.696497</td>\n","      <td>0.682266</td>\n","      <td>0.682511</td>\n","    </tr>\n","    <tr>\n","      <td>1</td>\n","      <td>0.000700</td>\n","      <td>0.802353</td>\n","      <td>0.623342</td>\n","      <td>0.706779</td>\n","      <td>0.704433</td>\n","      <td>0.697777</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>0.000700</td>\n","      <td>0.807047</td>\n","      <td>0.623342</td>\n","      <td>0.704187</td>\n","      <td>0.706897</td>\n","      <td>0.697182</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>0.000600</td>\n","      <td>0.791801</td>\n","      <td>0.623342</td>\n","      <td>0.703319</td>\n","      <td>0.701970</td>\n","      <td>0.696404</td>\n","    </tr>\n","    <tr>\n","      <td>4</td>\n","      <td>0.000600</td>\n","      <td>0.799847</td>\n","      <td>0.628647</td>\n","      <td>0.701699</td>\n","      <td>0.699507</td>\n","      <td>0.695660</td>\n","    </tr>\n","    <tr>\n","      <td>5</td>\n","      <td>0.000800</td>\n","      <td>0.789960</td>\n","      <td>0.625995</td>\n","      <td>0.700859</td>\n","      <td>0.701970</td>\n","      <td>0.695730</td>\n","    </tr>\n","    <tr>\n","      <td>6</td>\n","      <td>0.000800</td>\n","      <td>0.789874</td>\n","      <td>0.623342</td>\n","      <td>0.703445</td>\n","      <td>0.699507</td>\n","      <td>0.695380</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Step 10, Training Loss: 0.0005839088698849082\n","Step 10, Training Loss: 0.0006181422504596412\n","Step 20, Training Loss: 0.0015135223511606455\n","Step 20, Training Loss: 0.0003461962041910738\n","Step 30, Training Loss: 0.00041638663969933987\n","Step 30, Training Loss: 0.0007005395600572228\n","Step 40, Training Loss: 0.0005822702078148723\n","Step 40, Training Loss: 0.00043302244739606977\n","Step 50, Training Loss: 0.0004712182271759957\n","Step 50, Training Loss: 0.0003175022720824927\n","Step 60, Training Loss: 0.0005833522300235927\n","Step 60, Training Loss: 0.0009831705829128623\n","Step 70, Training Loss: 0.0006627819966524839\n","Step 70, Training Loss: 0.001161471358500421\n","Step 80, Training Loss: 0.0008552134386263788\n","Step 80, Training Loss: 0.0007986471173353493\n","Step 90, Training Loss: 0.0005684063653461635\n","Step 90, Training Loss: 0.0007374805863946676\n","Step 100, Training Loss: 0.000419508753111586\n","Step 100, Training Loss: 0.0013783375034108758\n","Step 110, Training Loss: 0.0005091968923807144\n","Step 110, Training Loss: 0.00040640318184159696\n","Step 120, Training Loss: 0.0004326240159571171\n","Step 120, Training Loss: 0.0008279504254460335\n","Step 130, Training Loss: 0.0003658620116766542\n","Step 130, Training Loss: 0.0006825929740443826\n","Step 140, Training Loss: 0.0006966497749090195\n","Step 140, Training Loss: 0.00034146473626606166\n","Step 150, Training Loss: 0.0005434236372821033\n","Step 150, Training Loss: 0.0028499742038547993\n","Step 160, Training Loss: 0.0005090352497063577\n","Step 160, Training Loss: 0.0006346134468913078\n","Step 170, Training Loss: 0.0006660131039097905\n","Step 170, Training Loss: 0.0007949117571115494\n","Step 180, Training Loss: 0.00041763653280213475\n","Step 180, Training Loss: 0.0004684179148171097\n"]},{"output_type":"stream","name":"stderr","text":["***** Running Evaluation *****\n","  Num examples = 377\n","  Batch size = 8\n"]},{"output_type":"stream","name":"stdout","text":["Step 190, Training Loss: 0.0012204231461510062\n","Step 190, Training Loss: 0.010716954246163368\n","Step 200, Training Loss: 0.0005364309181459248\n","Step 200, Training Loss: 0.0006020564469508827\n","Step 210, Training Loss: 0.0004730171640403569\n","Step 210, Training Loss: 0.0006572557031176984\n","Step 220, Training Loss: 0.0005290155531838536\n","Step 220, Training Loss: 0.0008570179343223572\n","Step 230, Training Loss: 0.0010141466045752168\n","Step 230, Training Loss: 0.0004508787824306637\n","Step 240, Training Loss: 0.0005252652917988598\n","Step 240, Training Loss: 0.0014478404773399234\n","Step 250, Training Loss: 0.0008947873720899224\n","Step 250, Training Loss: 0.0005696208099834621\n","Step 260, Training Loss: 0.00039651241968385875\n","Step 260, Training Loss: 0.0006334322388283908\n","Step 270, Training Loss: 0.0007872473797760904\n","Step 270, Training Loss: 0.0012556089786812663\n","Step 280, Training Loss: 0.0005593086825683713\n","Step 280, Training Loss: 0.000727335864212364\n","Step 290, Training Loss: 0.0008658386650495231\n","Step 290, Training Loss: 0.00033826573053374887\n","Step 300, Training Loss: 0.001419185078702867\n","Step 300, Training Loss: 0.0013428162783384323\n","Step 310, Training Loss: 0.0005235254648141563\n","Step 310, Training Loss: 0.0007855132571421564\n","Step 320, Training Loss: 0.0005940356641076505\n","Step 320, Training Loss: 0.00032166336313821375\n","Step 330, Training Loss: 0.0006519536837004125\n","Step 330, Training Loss: 0.000795690983068198\n","Step 340, Training Loss: 0.00038476893678307533\n","Step 340, Training Loss: 0.0008783377124927938\n","Step 350, Training Loss: 0.0004036336613353342\n","Step 350, Training Loss: 0.0010597746586427093\n","Step 360, Training Loss: 0.00046624126844108105\n","Step 360, Training Loss: 0.0005132360383868217\n","Step 370, Training Loss: 0.0003720162494573742\n","Step 370, Training Loss: 0.00041424130904488266\n"]},{"output_type":"stream","name":"stderr","text":["***** Running Evaluation *****\n","  Num examples = 377\n","  Batch size = 8\n"]},{"output_type":"stream","name":"stdout","text":["Step 380, Training Loss: 0.0008296307059936225\n","Step 380, Training Loss: 0.0017794404411688447\n","Step 390, Training Loss: 0.0002853207406587899\n","Step 390, Training Loss: 0.0010539882350713015\n","Step 400, Training Loss: 0.0008160012075677514\n","Step 400, Training Loss: 0.0005983374430797994\n","Step 410, Training Loss: 0.0005526645109057426\n","Step 410, Training Loss: 0.0006390957860276103\n","Step 420, Training Loss: 0.23826225101947784\n","Step 420, Training Loss: 0.0007419269531965256\n","Step 430, Training Loss: 0.0003151631390210241\n","Step 430, Training Loss: 0.0003676451160572469\n","Step 440, Training Loss: 0.00031320450943894684\n","Step 440, Training Loss: 0.0005635192501358688\n","Step 450, Training Loss: 0.000749822414945811\n","Step 450, Training Loss: 0.004406339954584837\n","Step 460, Training Loss: 0.0008171664667315781\n","Step 460, Training Loss: 0.0005034692585468292\n","Step 470, Training Loss: 0.0009383697179146111\n","Step 470, Training Loss: 0.0004528391291387379\n","Step 480, Training Loss: 0.0008042753906920552\n","Step 480, Training Loss: 0.0004383939958643168\n","Step 490, Training Loss: 0.0006862360751256347\n","Step 490, Training Loss: 0.0004424670187290758\n"]},{"output_type":"stream","name":"stderr","text":["Saving model checkpoint to ./results/checkpoint-500\n","Configuration saved in ./results/checkpoint-500/config.json\n","Model weights saved in ./results/checkpoint-500/pytorch_model.bin\n"]},{"output_type":"stream","name":"stdout","text":["Step 500, Training Loss: 0.000673058966640383\n","Step 500, Training Loss: 0.0008393790922127664\n","Step 510, Training Loss: 0.0018188493559136987\n","Step 510, Training Loss: 0.0005488424212671816\n","Step 520, Training Loss: 0.0004967536078765988\n","Step 520, Training Loss: 0.0010713317897170782\n","Step 530, Training Loss: 0.001233189832419157\n","Step 530, Training Loss: 0.0007449152763001621\n","Step 540, Training Loss: 0.0006788509199395776\n","Step 540, Training Loss: 0.0008351628785021603\n","Step 550, Training Loss: 0.0009660655632615089\n","Step 550, Training Loss: 0.0005754839512519538\n","Step 560, Training Loss: 0.0009050251101143658\n","Step 560, Training Loss: 0.0006084562628529966\n"]},{"output_type":"stream","name":"stderr","text":["***** Running Evaluation *****\n","  Num examples = 377\n","  Batch size = 8\n"]},{"output_type":"stream","name":"stdout","text":["Step 570, Training Loss: 0.0007052322034724057\n","Step 570, Training Loss: 0.0006418933044187725\n","Step 580, Training Loss: 0.00040274785715155303\n","Step 580, Training Loss: 0.000536297622602433\n","Step 590, Training Loss: 0.00041653402149677277\n","Step 590, Training Loss: 0.0012250846484676003\n","Step 600, Training Loss: 0.0003881577285937965\n","Step 600, Training Loss: 0.0008285412914119661\n","Step 610, Training Loss: 0.00045355907059274614\n","Step 610, Training Loss: 0.00046779346303083\n","Step 620, Training Loss: 0.000390712171792984\n","Step 620, Training Loss: 0.0007169530144892633\n","Step 630, Training Loss: 0.0009328488376922905\n","Step 630, Training Loss: 0.0009358230163343251\n","Step 640, Training Loss: 0.000880068342667073\n","Step 640, Training Loss: 0.00039174509583972394\n","Step 650, Training Loss: 0.0007208570023067296\n","Step 650, Training Loss: 0.0006134515861049294\n","Step 660, Training Loss: 0.0003481661551631987\n","Step 660, Training Loss: 0.0011630583321675658\n","Step 670, Training Loss: 0.0006364917499013245\n","Step 670, Training Loss: 0.0003606050449889153\n","Step 680, Training Loss: 0.0005930252955295146\n","Step 680, Training Loss: 0.0004575279599521309\n","Step 690, Training Loss: 0.0005812205490656197\n","Step 690, Training Loss: 0.0007789115188643336\n","Step 700, Training Loss: 0.0005899019888602197\n","Step 700, Training Loss: 0.00029976386576890945\n","Step 710, Training Loss: 0.00043594659655354917\n","Step 710, Training Loss: 0.0006933081313036382\n","Step 720, Training Loss: 0.0005006577703170478\n","Step 720, Training Loss: 0.002274357480928302\n","Step 730, Training Loss: 0.00041840109042823315\n","Step 730, Training Loss: 0.00035390755510888994\n","Step 740, Training Loss: 0.001332462066784501\n","Step 740, Training Loss: 0.00048329634591937065\n","Step 750, Training Loss: 0.0014797393232584\n","Step 750, Training Loss: 0.0004998694057576358\n"]},{"output_type":"stream","name":"stderr","text":["***** Running Evaluation *****\n","  Num examples = 377\n","  Batch size = 8\n"]},{"output_type":"stream","name":"stdout","text":["Step 760, Training Loss: 0.0004705085593741387\n","Step 760, Training Loss: 0.006919983774423599\n","Step 770, Training Loss: 0.0004812092229258269\n","Step 770, Training Loss: 0.000719449482858181\n","Step 780, Training Loss: 0.0003237749624531716\n","Step 780, Training Loss: 0.00041439911001361907\n","Step 790, Training Loss: 0.0003949717211071402\n","Step 790, Training Loss: 0.001889631967060268\n","Step 800, Training Loss: 0.0005582062876783311\n","Step 800, Training Loss: 0.0003553630376700312\n","Step 810, Training Loss: 0.00034994521411135793\n","Step 810, Training Loss: 0.00048291549319401383\n","Step 820, Training Loss: 0.0005365567049011588\n","Step 820, Training Loss: 0.0005307196406647563\n","Step 830, Training Loss: 0.000989651889540255\n","Step 830, Training Loss: 0.0009999278699979186\n","Step 840, Training Loss: 0.0007913528243079782\n","Step 840, Training Loss: 0.0005173165700398386\n","Step 850, Training Loss: 0.0005297023453749716\n","Step 850, Training Loss: 0.00031546884565614164\n","Step 860, Training Loss: 0.0003603831573855132\n","Step 860, Training Loss: 0.00038308152579702437\n","Step 870, Training Loss: 0.001600357936695218\n","Step 870, Training Loss: 0.0006248815916478634\n","Step 880, Training Loss: 0.0005936130764894187\n","Step 880, Training Loss: 0.0005717393360100687\n","Step 890, Training Loss: 0.0004314621037337929\n","Step 890, Training Loss: 0.00048281747149303555\n","Step 900, Training Loss: 0.0014519330579787493\n","Step 900, Training Loss: 0.00034505254006944597\n","Step 910, Training Loss: 0.0013084009988233447\n","Step 910, Training Loss: 0.0008799126371741295\n","Step 920, Training Loss: 0.0004805251082871109\n","Step 920, Training Loss: 0.0004122048558201641\n","Step 930, Training Loss: 0.000786776130553335\n","Step 930, Training Loss: 0.00040391815127804875\n"]},{"output_type":"stream","name":"stderr","text":["***** Running Evaluation *****\n","  Num examples = 377\n","  Batch size = 8\n"]},{"output_type":"stream","name":"stdout","text":["Step 940, Training Loss: 0.00038294997648335993\n","Step 940, Training Loss: 0.878862202167511\n","Step 940, Training Loss: 1.092942237854004\n","Step 940, Training Loss: 0.7816722393035889\n","Step 940, Training Loss: 2.0199503898620605\n","Step 940, Training Loss: 1.2118864059448242\n","Step 940, Training Loss: 0.9454770088195801\n","Step 940, Training Loss: 1.629920244216919\n","Step 940, Training Loss: 0.6398969292640686\n","Step 940, Training Loss: 0.13717792928218842\n","Step 940, Training Loss: 1.5375747680664062\n","Step 940, Training Loss: 1.55524742603302\n","Step 940, Training Loss: 0.7233849763870239\n","Step 940, Training Loss: 0.7016527652740479\n","Step 940, Training Loss: 0.00047628377797082067\n","Step 940, Training Loss: 0.2554297149181366\n","Step 940, Training Loss: 0.7845548987388611\n","Step 940, Training Loss: 1.0206798315048218\n","Step 940, Training Loss: 0.5255681872367859\n","Step 940, Training Loss: 0.35854372382164\n","Step 940, Training Loss: 0.0832669660449028\n","Step 940, Training Loss: 0.5088339447975159\n","Step 940, Training Loss: 0.7240197658538818\n","Step 940, Training Loss: 0.5130345225334167\n","Step 940, Training Loss: 0.35592350363731384\n","Step 940, Training Loss: 1.2437801361083984\n","Step 940, Training Loss: 0.6399149298667908\n","Step 940, Training Loss: 0.6869039535522461\n","Step 940, Training Loss: 1.0288013219833374\n","Step 940, Training Loss: 1.1490415334701538\n","Step 940, Training Loss: 1.1089575290679932\n","Step 940, Training Loss: 1.1475530862808228\n","Step 940, Training Loss: 1.4307655096054077\n","Step 940, Training Loss: 2.0904695987701416\n","Step 940, Training Loss: 0.37609753012657166\n","Step 940, Training Loss: 0.44108399748802185\n","Step 940, Training Loss: 0.8848279118537903\n","Step 940, Training Loss: 0.42171287536621094\n","Step 940, Training Loss: 0.18623541295528412\n","Step 940, Training Loss: 1.5180810689926147\n","Step 940, Training Loss: 0.16893862187862396\n","Step 940, Training Loss: 0.9386222958564758\n","Step 940, Training Loss: 0.6663066744804382\n","Step 940, Training Loss: 0.7679330706596375\n","Step 940, Training Loss: 0.4844159781932831\n","Step 940, Training Loss: 0.19640105962753296\n","Step 940, Training Loss: 0.2866758108139038\n","Step 940, Training Loss: 0.8432483673095703\n","Step 940, Training Loss: 0.00028017486329190433\n","Step 940, Training Loss: 0.0008693753043189645\n","Step 940, Training Loss: 0.0008416627533733845\n","Step 950, Training Loss: 0.00047210176126100123\n","Step 950, Training Loss: 0.0012002660660073161\n","Step 960, Training Loss: 0.0009301899117417634\n","Step 960, Training Loss: 0.000743615091778338\n","Step 970, Training Loss: 0.0009114245185628533\n","Step 970, Training Loss: 0.0010170125169679523\n","Step 980, Training Loss: 0.0006310059106908739\n","Step 980, Training Loss: 0.0007881298661231995\n","Step 990, Training Loss: 0.0002979622804559767\n","Step 990, Training Loss: 0.00044591506593860686\n"]},{"output_type":"stream","name":"stderr","text":["Saving model checkpoint to ./results/checkpoint-1000\n","Configuration saved in ./results/checkpoint-1000/config.json\n","Model weights saved in ./results/checkpoint-1000/pytorch_model.bin\n"]},{"output_type":"stream","name":"stdout","text":["Step 1000, Training Loss: 0.000890154973603785\n","Step 1000, Training Loss: 0.0007326340419240296\n","Step 1010, Training Loss: 0.000393980008084327\n","Step 1010, Training Loss: 0.0002822445530910045\n","Step 1020, Training Loss: 0.0006905299378558993\n","Step 1020, Training Loss: 0.0010524524841457605\n","Step 1030, Training Loss: 0.0005133274244144559\n","Step 1030, Training Loss: 0.00047060768702067435\n","Step 1040, Training Loss: 0.0004425829683896154\n","Step 1040, Training Loss: 0.0006150620174594223\n","Step 1050, Training Loss: 0.0005025797872804105\n","Step 1050, Training Loss: 0.0018929893849417567\n","Step 1060, Training Loss: 0.0003565297229215503\n","Step 1060, Training Loss: 0.0007908932748250663\n","Step 1070, Training Loss: 0.0002963700389955193\n","Step 1070, Training Loss: 0.0005244712228886783\n","Step 1080, Training Loss: 0.0010700974380597472\n","Step 1080, Training Loss: 0.0006026613409630954\n","Step 1090, Training Loss: 0.00040579933556728065\n","Step 1090, Training Loss: 0.0006330694304779172\n","Step 1100, Training Loss: 0.00033702322980389\n","Step 1100, Training Loss: 0.000423444522311911\n","Step 1110, Training Loss: 0.000359345693141222\n","Step 1110, Training Loss: 0.0008509103208780289\n","Step 1120, Training Loss: 0.0005350519786588848\n","Step 1120, Training Loss: 0.0007971323211677372\n"]},{"output_type":"stream","name":"stderr","text":["***** Running Evaluation *****\n","  Num examples = 377\n","  Batch size = 8\n"]},{"output_type":"stream","name":"stdout","text":["Step 1130, Training Loss: 0.0012853824300691485\n","Step 1130, Training Loss: 0.00040432155947200954\n","Step 1140, Training Loss: 0.0006383212166838348\n","Step 1140, Training Loss: 0.0005793892196379602\n","Step 1150, Training Loss: 0.0004450235574040562\n","Step 1150, Training Loss: 0.0005714159342460334\n","Step 1160, Training Loss: 0.0008842285606078804\n","Step 1160, Training Loss: 0.0003854316601064056\n","Step 1170, Training Loss: 0.0005223990301601589\n","Step 1170, Training Loss: 0.000635719217825681\n","Step 1180, Training Loss: 0.0007310977671295404\n","Step 1180, Training Loss: 0.0004269022902008146\n","Step 1190, Training Loss: 0.0004314979596529156\n","Step 1190, Training Loss: 0.0005691677797585726\n","Step 1200, Training Loss: 0.0007732493686489761\n","Step 1200, Training Loss: 0.00041658469126559794\n","Step 1210, Training Loss: 0.0005605553742498159\n","Step 1210, Training Loss: 0.0003385793825145811\n","Step 1220, Training Loss: 0.0003875309193972498\n","Step 1220, Training Loss: 0.00044991043978370726\n","Step 1230, Training Loss: 0.00048472429625689983\n","Step 1230, Training Loss: 0.0012495219707489014\n","Step 1240, Training Loss: 0.0005925979348830879\n","Step 1240, Training Loss: 0.00036255473969504237\n","Step 1250, Training Loss: 0.00048261499614454806\n","Step 1250, Training Loss: 0.0006222908268682659\n","Step 1260, Training Loss: 0.0032184855081140995\n","Step 1260, Training Loss: 0.0009140758193098009\n","Step 1270, Training Loss: 0.0005369149730540812\n","Step 1270, Training Loss: 0.0008017679792828858\n","Step 1280, Training Loss: 0.0004388991801533848\n","Step 1280, Training Loss: 0.0008028233423829079\n","Step 1290, Training Loss: 0.0005678083980455995\n","Step 1290, Training Loss: 0.001682677655480802\n","Step 1300, Training Loss: 0.0004895943566225469\n","Step 1300, Training Loss: 0.00039094677777029574\n","Step 1310, Training Loss: 0.0003884944599121809\n","Step 1310, Training Loss: 0.06464140862226486\n"]},{"output_type":"stream","name":"stderr","text":["***** Running Evaluation *****\n","  Num examples = 377\n","  Batch size = 8\n","\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","***** Running Evaluation *****\n","  Num examples = 377\n","  Batch size = 8\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='48' max='48' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [48/48 00:00]\n","    </div>\n","    "]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["[I 2024-12-01 16:20:22,302] Trial 17 finished with value: 0.6953799672565374 and parameters: {'learning_rate': 3.6651787702792726e-06, 'batch_size': 4, 'num_train_epochs': 7, 'weight_decay': 0.010790435245971444}. Best is trial 14 with value: 0.7065333724623482.\n","<ipython-input-10-747f1883d862>:2: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n","  learning_rate = trial.suggest_loguniform(\"learning_rate\", 1e-6, 5e-5)\n","<ipython-input-10-747f1883d862>:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n","  weight_decay = trial.suggest_loguniform(\"weight_decay\", 1e-4, 0.1)\n","PyTorch: setting up devices\n","/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n","***** Running training *****\n","  Num examples = 1507\n","  Num Epochs = 15\n","  Instantaneous batch size per device = 8\n","  Total train batch size (w. parallel, distributed & accumulation) = 16\n","  Gradient Accumulation steps = 2\n","  Total optimization steps = 1410\n"]},{"output_type":"stream","name":"stdout","text":["Starting training...\n","Step 0, Training Loss: 0.00143318937625736\n","Step 0, Training Loss: 0.0007455951999872923\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='1410' max='1410' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [1410/1410 02:02, Epoch 14/15]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","      <th>F1</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>0</td>\n","      <td>0.000700</td>\n","      <td>0.800677</td>\n","      <td>0.628647</td>\n","      <td>0.717760</td>\n","      <td>0.701970</td>\n","      <td>0.701763</td>\n","    </tr>\n","    <tr>\n","      <td>1</td>\n","      <td>0.028600</td>\n","      <td>0.824015</td>\n","      <td>0.628647</td>\n","      <td>0.706000</td>\n","      <td>0.699507</td>\n","      <td>0.692675</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>0.004200</td>\n","      <td>0.809460</td>\n","      <td>0.623342</td>\n","      <td>0.707324</td>\n","      <td>0.697044</td>\n","      <td>0.693020</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>0.000800</td>\n","      <td>0.823852</td>\n","      <td>0.625995</td>\n","      <td>0.709255</td>\n","      <td>0.706897</td>\n","      <td>0.697911</td>\n","    </tr>\n","    <tr>\n","      <td>4</td>\n","      <td>0.001800</td>\n","      <td>0.788771</td>\n","      <td>0.631300</td>\n","      <td>0.697386</td>\n","      <td>0.701970</td>\n","      <td>0.695627</td>\n","    </tr>\n","    <tr>\n","      <td>5</td>\n","      <td>0.000700</td>\n","      <td>0.824708</td>\n","      <td>0.615385</td>\n","      <td>0.703094</td>\n","      <td>0.699507</td>\n","      <td>0.691989</td>\n","    </tr>\n","    <tr>\n","      <td>6</td>\n","      <td>0.004300</td>\n","      <td>0.799922</td>\n","      <td>0.633952</td>\n","      <td>0.697375</td>\n","      <td>0.711823</td>\n","      <td>0.698479</td>\n","    </tr>\n","    <tr>\n","      <td>7</td>\n","      <td>0.002200</td>\n","      <td>0.808673</td>\n","      <td>0.625995</td>\n","      <td>0.705109</td>\n","      <td>0.701970</td>\n","      <td>0.698235</td>\n","    </tr>\n","    <tr>\n","      <td>8</td>\n","      <td>0.000900</td>\n","      <td>0.808236</td>\n","      <td>0.631300</td>\n","      <td>0.705714</td>\n","      <td>0.706897</td>\n","      <td>0.699793</td>\n","    </tr>\n","    <tr>\n","      <td>9</td>\n","      <td>0.014800</td>\n","      <td>0.818976</td>\n","      <td>0.620690</td>\n","      <td>0.703288</td>\n","      <td>0.699507</td>\n","      <td>0.694541</td>\n","    </tr>\n","    <tr>\n","      <td>10</td>\n","      <td>0.004100</td>\n","      <td>0.812962</td>\n","      <td>0.628647</td>\n","      <td>0.697377</td>\n","      <td>0.706897</td>\n","      <td>0.695805</td>\n","    </tr>\n","    <tr>\n","      <td>11</td>\n","      <td>0.000800</td>\n","      <td>0.812500</td>\n","      <td>0.625995</td>\n","      <td>0.707361</td>\n","      <td>0.699507</td>\n","      <td>0.697303</td>\n","    </tr>\n","    <tr>\n","      <td>12</td>\n","      <td>0.022100</td>\n","      <td>0.808429</td>\n","      <td>0.631300</td>\n","      <td>0.705449</td>\n","      <td>0.704433</td>\n","      <td>0.699418</td>\n","    </tr>\n","    <tr>\n","      <td>13</td>\n","      <td>0.000600</td>\n","      <td>0.806042</td>\n","      <td>0.633952</td>\n","      <td>0.706009</td>\n","      <td>0.706897</td>\n","      <td>0.701143</td>\n","    </tr>\n","    <tr>\n","      <td>14</td>\n","      <td>0.002300</td>\n","      <td>0.802818</td>\n","      <td>0.633952</td>\n","      <td>0.707559</td>\n","      <td>0.706897</td>\n","      <td>0.702033</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Step 10, Training Loss: 0.0013116345508024096\n","Step 10, Training Loss: 0.0017892204923555255\n","Step 20, Training Loss: 0.00045241182669997215\n","Step 20, Training Loss: 0.00045675854198634624\n","Step 30, Training Loss: 0.0010514972964301705\n","Step 30, Training Loss: 0.0003828388580586761\n","Step 40, Training Loss: 0.09591351449489594\n","Step 40, Training Loss: 0.0004538636712823063\n","Step 50, Training Loss: 0.0008443500846624374\n","Step 50, Training Loss: 0.0005870654131285846\n","Step 60, Training Loss: 0.0005876810173504055\n","Step 60, Training Loss: 0.0005522874416783452\n","Step 70, Training Loss: 0.0005140770226716995\n","Step 70, Training Loss: 0.0005921003175899386\n","Step 80, Training Loss: 0.0005465896101668477\n","Step 80, Training Loss: 0.0006326731527224183\n","Step 90, Training Loss: 0.0004641762934625149\n","Step 90, Training Loss: 0.0006306151044555008\n"]},{"output_type":"stream","name":"stderr","text":["***** Running Evaluation *****\n","  Num examples = 377\n","  Batch size = 16\n"]},{"output_type":"stream","name":"stdout","text":["Step 100, Training Loss: 0.0004974891780875623\n","Step 100, Training Loss: 0.00046227042912505567\n","Step 110, Training Loss: 0.0007063173106871545\n","Step 110, Training Loss: 0.0005788395064882934\n","Step 120, Training Loss: 0.0008376838522963226\n","Step 120, Training Loss: 0.00040005799382925034\n","Step 130, Training Loss: 0.0004717020783573389\n","Step 130, Training Loss: 0.001100472523830831\n","Step 140, Training Loss: 0.0006206069374457002\n","Step 140, Training Loss: 0.0039327386766672134\n","Step 150, Training Loss: 0.021068478003144264\n","Step 150, Training Loss: 0.0006314243073575199\n","Step 160, Training Loss: 0.0004722958547063172\n","Step 160, Training Loss: 0.006525011267513037\n","Step 170, Training Loss: 0.0004809501697309315\n","Step 170, Training Loss: 0.0003851671644952148\n","Step 180, Training Loss: 0.0004455047601368278\n","Step 180, Training Loss: 0.00032922683749347925\n"]},{"output_type":"stream","name":"stderr","text":["***** Running Evaluation *****\n","  Num examples = 377\n","  Batch size = 16\n"]},{"output_type":"stream","name":"stdout","text":["Step 190, Training Loss: 0.0010830080136656761\n","Step 190, Training Loss: 0.003049700055271387\n","Step 200, Training Loss: 0.0006280078669078648\n","Step 200, Training Loss: 0.00037959773908369243\n","Step 210, Training Loss: 0.0013654128415510058\n","Step 210, Training Loss: 0.0004978138022124767\n","Step 220, Training Loss: 0.0004917808691971004\n","Step 220, Training Loss: 0.0006054837140254676\n","Step 230, Training Loss: 0.0006802777643315494\n","Step 230, Training Loss: 0.0004770518862642348\n","Step 240, Training Loss: 0.0005386052653193474\n","Step 240, Training Loss: 0.0004428677202668041\n","Step 250, Training Loss: 0.0012569775572046638\n","Step 250, Training Loss: 0.00042477791430428624\n","Step 260, Training Loss: 0.0006593823782168329\n","Step 260, Training Loss: 0.0005464404239319265\n","Step 270, Training Loss: 0.0007669861661270261\n","Step 270, Training Loss: 0.000561639026273042\n","Step 280, Training Loss: 0.0009293243638239801\n","Step 280, Training Loss: 0.0010979730868712068\n"]},{"output_type":"stream","name":"stderr","text":["***** Running Evaluation *****\n","  Num examples = 377\n","  Batch size = 16\n"]},{"output_type":"stream","name":"stdout","text":["Step 290, Training Loss: 0.0010792253306135535\n","Step 290, Training Loss: 0.0013097962364554405\n","Step 300, Training Loss: 0.0008037209627218544\n","Step 300, Training Loss: 0.0005979553679935634\n","Step 310, Training Loss: 0.0004898515762761235\n","Step 310, Training Loss: 0.0004033762961626053\n","Step 320, Training Loss: 0.0006219266215339303\n","Step 320, Training Loss: 0.0024543553590774536\n","Step 330, Training Loss: 0.0011817021295428276\n","Step 330, Training Loss: 0.0005569449858739972\n","Step 340, Training Loss: 0.0005841728998348117\n","Step 340, Training Loss: 0.0008036191575229168\n","Step 350, Training Loss: 0.0003441742737777531\n","Step 350, Training Loss: 0.0004517287597991526\n","Step 360, Training Loss: 0.001118339248932898\n","Step 360, Training Loss: 0.0008727327222004533\n","Step 370, Training Loss: 0.0009445102186873555\n","Step 370, Training Loss: 0.0006322431727312505\n"]},{"output_type":"stream","name":"stderr","text":["***** Running Evaluation *****\n","  Num examples = 377\n","  Batch size = 16\n"]},{"output_type":"stream","name":"stdout","text":["Step 380, Training Loss: 0.001212810748256743\n","Step 380, Training Loss: 0.0004993273760192096\n","Step 390, Training Loss: 0.0003710721794050187\n","Step 390, Training Loss: 0.0010241549462080002\n","Step 400, Training Loss: 0.0003816714743152261\n","Step 400, Training Loss: 0.0015818551182746887\n","Step 410, Training Loss: 0.00045178766595199704\n","Step 410, Training Loss: 0.00036334537435323\n","Step 420, Training Loss: 0.0007530737202614546\n","Step 420, Training Loss: 0.0007270136848092079\n","Step 430, Training Loss: 0.0004080180951859802\n","Step 430, Training Loss: 0.001636435859836638\n","Step 440, Training Loss: 0.0010486138053238392\n","Step 440, Training Loss: 0.000487008219351992\n","Step 450, Training Loss: 0.0010169026209041476\n","Step 450, Training Loss: 0.0003691212914418429\n","Step 460, Training Loss: 0.0004653257492464036\n","Step 460, Training Loss: 0.00037018689909018576\n"]},{"output_type":"stream","name":"stderr","text":["***** Running Evaluation *****\n","  Num examples = 377\n","  Batch size = 16\n"]},{"output_type":"stream","name":"stdout","text":["Step 470, Training Loss: 0.0003928878577426076\n","Step 470, Training Loss: 0.9584582448005676\n","Step 470, Training Loss: 1.4430011510849\n","Step 470, Training Loss: 1.0839401483535767\n","Step 470, Training Loss: 1.1772083044052124\n","Step 470, Training Loss: 0.872927188873291\n","Step 470, Training Loss: 1.1393282413482666\n","Step 470, Training Loss: 0.331385999917984\n","Step 470, Training Loss: 0.5238199234008789\n","Step 470, Training Loss: 0.7715926766395569\n","Step 470, Training Loss: 0.1849571019411087\n","Step 470, Training Loss: 0.5647397041320801\n","Step 470, Training Loss: 0.3955864906311035\n","Step 470, Training Loss: 0.9590865969657898\n","Step 470, Training Loss: 0.8761882781982422\n","Step 470, Training Loss: 1.1385186910629272\n","Step 470, Training Loss: 1.1528570652008057\n","Step 470, Training Loss: 1.2267740964889526\n","Step 470, Training Loss: 0.7036615610122681\n","Step 470, Training Loss: 0.28784075379371643\n","Step 470, Training Loss: 0.8295363783836365\n","Step 470, Training Loss: 0.7532530426979065\n","Step 470, Training Loss: 0.5797995924949646\n","Step 470, Training Loss: 0.22450284659862518\n","Step 470, Training Loss: 0.7225873470306396\n","Step 470, Training Loss: 0.00080880057066679\n","Step 470, Training Loss: 0.0004756784182973206\n","Step 480, Training Loss: 0.001015492482110858\n","Step 480, Training Loss: 0.0006929468363523483\n","Step 490, Training Loss: 0.0005404574330896139\n","Step 490, Training Loss: 0.0009891841327771544\n"]},{"output_type":"stream","name":"stderr","text":["Saving model checkpoint to ./results/checkpoint-500\n","Configuration saved in ./results/checkpoint-500/config.json\n","Model weights saved in ./results/checkpoint-500/pytorch_model.bin\n"]},{"output_type":"stream","name":"stdout","text":["Step 500, Training Loss: 0.0007060366915538907\n","Step 500, Training Loss: 0.0004243192670401186\n","Step 510, Training Loss: 0.0036635471042245626\n","Step 510, Training Loss: 0.0003384923911653459\n","Step 520, Training Loss: 0.0005162443267181516\n","Step 520, Training Loss: 0.0008729115943424404\n","Step 530, Training Loss: 0.0006213872111402452\n","Step 530, Training Loss: 0.0005282334168441594\n","Step 540, Training Loss: 0.0010489135747775435\n","Step 540, Training Loss: 0.0010998060461133718\n","Step 550, Training Loss: 0.0004112570604775101\n","Step 550, Training Loss: 0.0009551529656164348\n","Step 560, Training Loss: 0.0008266842341981828\n","Step 560, Training Loss: 0.00044961986714042723\n"]},{"output_type":"stream","name":"stderr","text":["***** Running Evaluation *****\n","  Num examples = 377\n","  Batch size = 16\n"]},{"output_type":"stream","name":"stdout","text":["Step 570, Training Loss: 0.0006244652322493494\n","Step 570, Training Loss: 0.0006858290289528668\n","Step 580, Training Loss: 0.0005350431310944259\n","Step 580, Training Loss: 0.0005186586640775204\n","Step 590, Training Loss: 0.0006440231809392571\n","Step 590, Training Loss: 0.002783915726467967\n","Step 600, Training Loss: 0.0006017517298460007\n","Step 600, Training Loss: 0.0005814938922412694\n","Step 610, Training Loss: 0.0004469045379664749\n","Step 610, Training Loss: 0.0008181024459190667\n","Step 620, Training Loss: 0.0003412151418160647\n","Step 620, Training Loss: 0.029658565297722816\n","Step 630, Training Loss: 0.01203009020537138\n","Step 630, Training Loss: 0.00037592259468510747\n","Step 640, Training Loss: 0.0005085264565423131\n","Step 640, Training Loss: 0.0006625709938816726\n","Step 650, Training Loss: 0.000398333853809163\n","Step 650, Training Loss: 0.0004844749055337161\n"]},{"output_type":"stream","name":"stderr","text":["***** Running Evaluation *****\n","  Num examples = 377\n","  Batch size = 16\n"]},{"output_type":"stream","name":"stdout","text":["Step 660, Training Loss: 0.0006714857299812138\n","Step 660, Training Loss: 0.0011335030430927873\n","Step 670, Training Loss: 0.0003682613605633378\n","Step 670, Training Loss: 0.0004983115359209478\n","Step 680, Training Loss: 0.0017243660986423492\n","Step 680, Training Loss: 0.0007242739666253328\n","Step 690, Training Loss: 0.0006407073815353215\n","Step 690, Training Loss: 0.002643123734742403\n","Step 700, Training Loss: 0.000428275263402611\n","Step 700, Training Loss: 0.0006206191028468311\n","Step 710, Training Loss: 0.0008012704784050584\n","Step 710, Training Loss: 0.0007572493050247431\n","Step 720, Training Loss: 0.0007598126539960504\n","Step 720, Training Loss: 0.00043552377610467374\n","Step 730, Training Loss: 0.0014135775854811072\n","Step 730, Training Loss: 0.0006796260131523013\n","Step 740, Training Loss: 0.0006342225824482739\n","Step 740, Training Loss: 0.0008818014757707715\n","Step 750, Training Loss: 0.0005128765478730202\n","Step 750, Training Loss: 0.0003908360959030688\n"]},{"output_type":"stream","name":"stderr","text":["***** Running Evaluation *****\n","  Num examples = 377\n","  Batch size = 16\n"]},{"output_type":"stream","name":"stdout","text":["Step 760, Training Loss: 0.0008980087004601955\n","Step 760, Training Loss: 0.0005762643995694816\n","Step 770, Training Loss: 0.0006486960337497294\n","Step 770, Training Loss: 0.0009387838072143495\n","Step 780, Training Loss: 0.00042060762643814087\n","Step 780, Training Loss: 0.0002990599605254829\n","Step 790, Training Loss: 0.0005345330573618412\n","Step 790, Training Loss: 0.0007664731820113957\n","Step 800, Training Loss: 0.0009048402425833046\n","Step 800, Training Loss: 0.0004711573419626802\n","Step 810, Training Loss: 0.0010402142070233822\n","Step 810, Training Loss: 0.0011085544247180223\n","Step 820, Training Loss: 0.0034121815115213394\n","Step 820, Training Loss: 0.0005670577520504594\n","Step 830, Training Loss: 0.0008236687863245606\n","Step 830, Training Loss: 0.0006601342465728521\n","Step 840, Training Loss: 0.0005585114122368395\n","Step 840, Training Loss: 0.0004106749838683754\n"]},{"output_type":"stream","name":"stderr","text":["***** Running Evaluation *****\n","  Num examples = 377\n","  Batch size = 16\n"]},{"output_type":"stream","name":"stdout","text":["Step 850, Training Loss: 0.0013339131837710738\n","Step 850, Training Loss: 0.0003695784544106573\n","Step 860, Training Loss: 0.0004496797046158463\n","Step 860, Training Loss: 0.0004447684623301029\n","Step 870, Training Loss: 0.0004824109491892159\n","Step 870, Training Loss: 0.00039214928983710706\n","Step 880, Training Loss: 0.00041419267654418945\n","Step 880, Training Loss: 0.000814225641079247\n","Step 890, Training Loss: 0.0005139458226040006\n","Step 890, Training Loss: 0.0005867445725016296\n","Step 900, Training Loss: 0.0005352557054720819\n","Step 900, Training Loss: 0.0005276640295051038\n","Step 910, Training Loss: 0.0004775998822879046\n","Step 910, Training Loss: 0.0004880847118329257\n","Step 920, Training Loss: 0.0003217835910618305\n","Step 920, Training Loss: 0.0006941435276530683\n","Step 930, Training Loss: 0.0005687584052793682\n","Step 930, Training Loss: 0.2815230190753937\n"]},{"output_type":"stream","name":"stderr","text":["***** Running Evaluation *****\n","  Num examples = 377\n","  Batch size = 16\n"]},{"output_type":"stream","name":"stdout","text":["Step 940, Training Loss: 0.00036545543116517365\n","Step 940, Training Loss: 1.086336374282837\n","Step 940, Training Loss: 1.4798184633255005\n","Step 940, Training Loss: 1.1425986289978027\n","Step 940, Training Loss: 1.165953278541565\n","Step 940, Training Loss: 0.8477792739868164\n","Step 940, Training Loss: 1.1570223569869995\n","Step 940, Training Loss: 0.3489260673522949\n","Step 940, Training Loss: 0.5133746266365051\n","Step 940, Training Loss: 0.7764845490455627\n","Step 940, Training Loss: 0.2083752602338791\n","Step 940, Training Loss: 0.6418890357017517\n","Step 940, Training Loss: 0.40763139724731445\n","Step 940, Training Loss: 0.9417640566825867\n","Step 940, Training Loss: 0.8641638159751892\n","Step 940, Training Loss: 1.163014531135559\n","Step 940, Training Loss: 1.2926231622695923\n","Step 940, Training Loss: 1.2698687314987183\n","Step 940, Training Loss: 0.6642723679542542\n","Step 940, Training Loss: 0.31417471170425415\n","Step 940, Training Loss: 0.8464630246162415\n","Step 940, Training Loss: 0.8181099891662598\n","Step 940, Training Loss: 0.7169344425201416\n","Step 940, Training Loss: 0.24599921703338623\n","Step 940, Training Loss: 0.6818503141403198\n","Step 940, Training Loss: 0.0012083073379471898\n","Step 940, Training Loss: 0.0009718917426653206\n","Step 950, Training Loss: 0.0005981533322483301\n","Step 950, Training Loss: 0.000632653885986656\n","Step 960, Training Loss: 0.0006804694421589375\n","Step 960, Training Loss: 0.0008245454519055784\n","Step 970, Training Loss: 0.022197233512997627\n","Step 970, Training Loss: 0.00035013986052945256\n","Step 980, Training Loss: 0.0008548243204131722\n","Step 980, Training Loss: 0.00025134734460152686\n","Step 990, Training Loss: 0.000338092097081244\n","Step 990, Training Loss: 0.00039279862539842725\n"]},{"output_type":"stream","name":"stderr","text":["Saving model checkpoint to ./results/checkpoint-1000\n","Configuration saved in ./results/checkpoint-1000/config.json\n","Model weights saved in ./results/checkpoint-1000/pytorch_model.bin\n"]},{"output_type":"stream","name":"stdout","text":["Step 1000, Training Loss: 0.00040840861038304865\n","Step 1000, Training Loss: 0.0005325382808223367\n","Step 1010, Training Loss: 0.000611204479355365\n","Step 1010, Training Loss: 0.0010857083834707737\n","Step 1020, Training Loss: 0.01643487624824047\n","Step 1020, Training Loss: 0.0007665149751119316\n","Step 1030, Training Loss: 0.008671781979501247\n","Step 1030, Training Loss: 0.0011328719556331635\n"]},{"output_type":"stream","name":"stderr","text":["***** Running Evaluation *****\n","  Num examples = 377\n","  Batch size = 16\n"]},{"output_type":"stream","name":"stdout","text":["Step 1040, Training Loss: 0.0003949661913793534\n","Step 1040, Training Loss: 0.00047051478759385645\n","Step 1050, Training Loss: 0.04116501659154892\n","Step 1050, Training Loss: 0.00325757940299809\n","Step 1060, Training Loss: 0.0005911528714932501\n","Step 1060, Training Loss: 0.0009942407486960292\n","Step 1070, Training Loss: 0.0005357078625820577\n","Step 1070, Training Loss: 0.0004149063606746495\n","Step 1080, Training Loss: 0.0003523125487845391\n","Step 1080, Training Loss: 0.00047972946777008474\n","Step 1090, Training Loss: 0.0013147162972018123\n","Step 1090, Training Loss: 0.0003953358100261539\n","Step 1100, Training Loss: 0.00040992311551235616\n","Step 1100, Training Loss: 0.0004571392200887203\n","Step 1110, Training Loss: 0.000650190282613039\n","Step 1110, Training Loss: 0.0008507800521329045\n","Step 1120, Training Loss: 0.006206126883625984\n","Step 1120, Training Loss: 0.0005375741166062653\n"]},{"output_type":"stream","name":"stderr","text":["***** Running Evaluation *****\n","  Num examples = 377\n","  Batch size = 16\n"]},{"output_type":"stream","name":"stdout","text":["Step 1130, Training Loss: 0.0016481186030432582\n","Step 1130, Training Loss: 0.0006876298575662076\n","Step 1140, Training Loss: 0.000641321879811585\n","Step 1140, Training Loss: 0.0003461165470071137\n","Step 1150, Training Loss: 0.0011177059495821595\n","Step 1150, Training Loss: 0.0007177838124334812\n","Step 1160, Training Loss: 0.0008777654729783535\n","Step 1160, Training Loss: 0.0073657832108438015\n","Step 1170, Training Loss: 0.0004330408410169184\n","Step 1170, Training Loss: 0.00034820465953089297\n","Step 1180, Training Loss: 0.0012937140418216586\n","Step 1180, Training Loss: 0.0003730144817382097\n","Step 1190, Training Loss: 0.0005948656471446157\n","Step 1190, Training Loss: 0.001039690338075161\n","Step 1200, Training Loss: 0.07783036679029465\n","Step 1200, Training Loss: 0.0008562421426177025\n","Step 1210, Training Loss: 0.00044703250750899315\n","Step 1210, Training Loss: 0.000760052353143692\n","Step 1220, Training Loss: 0.0006445907638408244\n","Step 1220, Training Loss: 0.0008328661206178367\n"]},{"output_type":"stream","name":"stderr","text":["***** Running Evaluation *****\n","  Num examples = 377\n","  Batch size = 16\n"]},{"output_type":"stream","name":"stdout","text":["Step 1230, Training Loss: 0.002472171327099204\n","Step 1230, Training Loss: 0.0007119979127310216\n","Step 1240, Training Loss: 0.00033222275669686496\n","Step 1240, Training Loss: 0.000389266264392063\n","Step 1250, Training Loss: 0.000760322087444365\n","Step 1250, Training Loss: 0.0005585043109022081\n","Step 1260, Training Loss: 0.0006638686172664165\n","Step 1260, Training Loss: 0.0009221974760293961\n","Step 1270, Training Loss: 0.0006568368407897651\n","Step 1270, Training Loss: 0.0006566101801581681\n","Step 1280, Training Loss: 0.0003993174177594483\n","Step 1280, Training Loss: 0.000536927895154804\n","Step 1290, Training Loss: 0.0021949191577732563\n","Step 1290, Training Loss: 0.0004215440421830863\n","Step 1300, Training Loss: 0.0006263790419325233\n","Step 1300, Training Loss: 0.0007639280520379543\n","Step 1310, Training Loss: 0.0007553219911642373\n","Step 1310, Training Loss: 0.00033917688415385783\n"]},{"output_type":"stream","name":"stderr","text":["***** Running Evaluation *****\n","  Num examples = 377\n","  Batch size = 16\n"]},{"output_type":"stream","name":"stdout","text":["Step 1320, Training Loss: 0.0007344182813540101\n","Step 1320, Training Loss: 0.0004741040465887636\n","Step 1330, Training Loss: 0.0005599892465397716\n","Step 1330, Training Loss: 0.0003491422103252262\n","Step 1340, Training Loss: 0.0004268481570761651\n","Step 1340, Training Loss: 0.0019460341427475214\n","Step 1350, Training Loss: 0.0006563194328919053\n","Step 1350, Training Loss: 0.0004929841379635036\n","Step 1360, Training Loss: 0.00046800493146292865\n","Step 1360, Training Loss: 0.0053083100356161594\n","Step 1370, Training Loss: 0.0004138350486755371\n","Step 1370, Training Loss: 0.0006319498643279076\n","Step 1380, Training Loss: 0.0003392197540961206\n","Step 1380, Training Loss: 0.0008653683471493423\n","Step 1390, Training Loss: 0.0004139998054597527\n","Step 1390, Training Loss: 0.0005174693069420755\n","Step 1400, Training Loss: 0.0006698802462778986\n","Step 1400, Training Loss: 0.024695390835404396\n"]},{"output_type":"stream","name":"stderr","text":["***** Running Evaluation *****\n","  Num examples = 377\n","  Batch size = 16\n"]},{"output_type":"stream","name":"stdout","text":["Step 1410, Training Loss: 1.0040321350097656\n","Step 1410, Training Loss: 1.472594141960144\n","Step 1410, Training Loss: 1.0910718441009521\n","Step 1410, Training Loss: 1.181198000907898\n","Step 1410, Training Loss: 0.8459623456001282\n","Step 1410, Training Loss: 1.1237443685531616\n","Step 1410, Training Loss: 0.34348559379577637\n","Step 1410, Training Loss: 0.5174481272697449\n","Step 1410, Training Loss: 0.7704172730445862\n","Step 1410, Training Loss: 0.1854945421218872\n","Step 1410, Training Loss: 0.6284902691841125\n","Step 1410, Training Loss: 0.38425958156585693\n","Step 1410, Training Loss: 0.9516134262084961\n","Step 1410, Training Loss: 0.8746145367622375\n","Step 1410, Training Loss: 1.1448354721069336\n","Step 1410, Training Loss: 1.2843908071517944\n","Step 1410, Training Loss: 1.2614024877548218\n","Step 1410, Training Loss: 0.6792675852775574\n","Step 1410, Training Loss: 0.3087587058544159\n","Step 1410, Training Loss: 0.8334819078445435\n","Step 1410, Training Loss: 0.7517253756523132\n","Step 1410, Training Loss: 0.6541156768798828\n","Step 1410, Training Loss: 0.21729464828968048\n","Step 1410, Training Loss: 0.7230042815208435\n"]},{"output_type":"stream","name":"stderr","text":["\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","***** Running Evaluation *****\n","  Num examples = 377\n","  Batch size = 16\n"]},{"output_type":"stream","name":"stdout","text":["Step 1410, Training Loss: 1.0040321350097656\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='24' max='24' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [24/24 00:00]\n","    </div>\n","    "]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Step 1410, Training Loss: 1.472594141960144\n","Step 1410, Training Loss: 1.0910718441009521\n","Step 1410, Training Loss: 1.181198000907898\n","Step 1410, Training Loss: 0.8459623456001282\n","Step 1410, Training Loss: 1.1237443685531616\n","Step 1410, Training Loss: 0.34348559379577637\n","Step 1410, Training Loss: 0.5174481272697449\n","Step 1410, Training Loss: 0.7704172730445862\n","Step 1410, Training Loss: 0.1854945421218872\n","Step 1410, Training Loss: 0.6284902691841125\n","Step 1410, Training Loss: 0.38425958156585693\n","Step 1410, Training Loss: 0.9516134262084961\n","Step 1410, Training Loss: 0.8746145367622375\n","Step 1410, Training Loss: 1.1448354721069336\n","Step 1410, Training Loss: 1.2843908071517944\n","Step 1410, Training Loss: 1.2614024877548218\n","Step 1410, Training Loss: 0.6792675852775574\n","Step 1410, Training Loss: 0.3087587058544159\n","Step 1410, Training Loss: 0.8334819078445435\n","Step 1410, Training Loss: 0.7517253756523132\n","Step 1410, Training Loss: 0.6541156768798828\n","Step 1410, Training Loss: 0.21729464828968048\n","Step 1410, Training Loss: 0.7230042815208435\n"]},{"output_type":"stream","name":"stderr","text":["[I 2024-12-01 16:22:25,881] Trial 18 finished with value: 0.7020325112587961 and parameters: {'learning_rate': 3.771249927617681e-06, 'batch_size': 8, 'num_train_epochs': 15, 'weight_decay': 0.0005956909932240638}. Best is trial 14 with value: 0.7065333724623482.\n","<ipython-input-10-747f1883d862>:2: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n","  learning_rate = trial.suggest_loguniform(\"learning_rate\", 1e-6, 5e-5)\n","<ipython-input-10-747f1883d862>:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n","  weight_decay = trial.suggest_loguniform(\"weight_decay\", 1e-4, 0.1)\n","PyTorch: setting up devices\n"]},{"output_type":"stream","name":"stdout","text":["Starting training..."]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n","***** Running training *****\n","  Num examples = 1507\n","  Num Epochs = 8\n","  Instantaneous batch size per device = 16\n","  Total train batch size (w. parallel, distributed & accumulation) = 32\n","  Gradient Accumulation steps = 2\n","  Total optimization steps = 376\n"]},{"output_type":"stream","name":"stdout","text":["\n","Step 0, Training Loss: 0.0007649659528397024\n","Step 0, Training Loss: 0.0006319668027572334\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='376' max='376' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [376/376 00:59, Epoch 7/8]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","      <th>F1</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>0</td>\n","      <td>0.001500</td>\n","      <td>0.788083</td>\n","      <td>0.631300</td>\n","      <td>0.708057</td>\n","      <td>0.706897</td>\n","      <td>0.702477</td>\n","    </tr>\n","    <tr>\n","      <td>1</td>\n","      <td>0.000800</td>\n","      <td>0.794435</td>\n","      <td>0.625995</td>\n","      <td>0.700391</td>\n","      <td>0.694581</td>\n","      <td>0.693097</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>0.001300</td>\n","      <td>0.795437</td>\n","      <td>0.633952</td>\n","      <td>0.706046</td>\n","      <td>0.709360</td>\n","      <td>0.702443</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>0.002100</td>\n","      <td>0.802270</td>\n","      <td>0.633952</td>\n","      <td>0.706828</td>\n","      <td>0.701970</td>\n","      <td>0.697530</td>\n","    </tr>\n","    <tr>\n","      <td>4</td>\n","      <td>0.009100</td>\n","      <td>0.802125</td>\n","      <td>0.633952</td>\n","      <td>0.709728</td>\n","      <td>0.711823</td>\n","      <td>0.704168</td>\n","    </tr>\n","    <tr>\n","      <td>5</td>\n","      <td>0.000700</td>\n","      <td>0.801294</td>\n","      <td>0.633952</td>\n","      <td>0.707754</td>\n","      <td>0.711823</td>\n","      <td>0.703273</td>\n","    </tr>\n","    <tr>\n","      <td>6</td>\n","      <td>0.001900</td>\n","      <td>0.799649</td>\n","      <td>0.636605</td>\n","      <td>0.711919</td>\n","      <td>0.706897</td>\n","      <td>0.703277</td>\n","    </tr>\n","    <tr>\n","      <td>7</td>\n","      <td>0.000800</td>\n","      <td>0.798723</td>\n","      <td>0.636605</td>\n","      <td>0.712545</td>\n","      <td>0.706897</td>\n","      <td>0.703598</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Step 10, Training Loss: 0.0003897285205312073\n","Step 10, Training Loss: 0.00045049103209748864\n","Step 20, Training Loss: 0.0005069861654192209\n","Step 20, Training Loss: 0.0004923863452859223\n","Step 30, Training Loss: 0.0005074506043456495\n","Step 30, Training Loss: 0.0005149138160049915\n","Step 40, Training Loss: 0.000548683456145227\n","Step 40, Training Loss: 0.0004000238550361246\n"]},{"output_type":"stream","name":"stderr","text":["***** Running Evaluation *****\n","  Num examples = 377\n","  Batch size = 32\n"]},{"output_type":"stream","name":"stdout","text":["Step 50, Training Loss: 0.0005612632958218455\n","Step 50, Training Loss: 0.0003759245155379176\n","Step 60, Training Loss: 0.0005561296711675823\n","Step 60, Training Loss: 0.0009011133806779981\n","Step 70, Training Loss: 0.0060983216390013695\n","Step 70, Training Loss: 0.000499270623549819\n","Step 80, Training Loss: 0.0009770601755008101\n","Step 80, Training Loss: 0.000746053468901664\n","Step 90, Training Loss: 0.00032659913995303214\n","Step 90, Training Loss: 0.000576684542465955\n"]},{"output_type":"stream","name":"stderr","text":["***** Running Evaluation *****\n","  Num examples = 377\n","  Batch size = 32\n"]},{"output_type":"stream","name":"stdout","text":["Step 100, Training Loss: 0.0007494608871638775\n","Step 100, Training Loss: 0.0009042326710186899\n","Step 110, Training Loss: 0.0004060023056808859\n","Step 110, Training Loss: 0.0006141532212495804\n","Step 120, Training Loss: 0.0004405580984894186\n","Step 120, Training Loss: 0.0008512345375493169\n","Step 130, Training Loss: 0.0005591502413153648\n","Step 130, Training Loss: 0.0006901759770698845\n","Step 140, Training Loss: 0.0007332709501497447\n","Step 140, Training Loss: 0.0003866933984681964\n"]},{"output_type":"stream","name":"stderr","text":["***** Running Evaluation *****\n","  Num examples = 377\n","  Batch size = 32\n"]},{"output_type":"stream","name":"stdout","text":["Step 150, Training Loss: 0.0010119950165972114\n","Step 150, Training Loss: 0.0003348055761307478\n","Step 160, Training Loss: 0.0006867427728138864\n","Step 160, Training Loss: 0.00048691625124774873\n","Step 170, Training Loss: 0.02364218421280384\n","Step 170, Training Loss: 0.0010736971162259579\n","Step 180, Training Loss: 0.0011938285315409303\n","Step 180, Training Loss: 0.0005148187628947198\n"]},{"output_type":"stream","name":"stderr","text":["***** Running Evaluation *****\n","  Num examples = 377\n","  Batch size = 32\n"]},{"output_type":"stream","name":"stdout","text":["Step 190, Training Loss: 0.0019277142127975821\n","Step 190, Training Loss: 0.0008465704740956426\n","Step 200, Training Loss: 0.0006446935003623366\n","Step 200, Training Loss: 0.00046288204612210393\n","Step 210, Training Loss: 0.0006347898743115366\n","Step 210, Training Loss: 0.0005754853482358158\n","Step 220, Training Loss: 0.0004910325515083969\n","Step 220, Training Loss: 0.0007185558788478374\n","Step 230, Training Loss: 0.0004914934979751706\n","Step 230, Training Loss: 0.001405651681125164\n"]},{"output_type":"stream","name":"stderr","text":["***** Running Evaluation *****\n","  Num examples = 377\n","  Batch size = 32\n"]},{"output_type":"stream","name":"stdout","text":["Step 240, Training Loss: 0.0008004985866136849\n","Step 240, Training Loss: 0.0005728969117626548\n","Step 250, Training Loss: 0.0005967000615783036\n","Step 250, Training Loss: 0.0028646260034292936\n","Step 260, Training Loss: 0.0019130426226183772\n","Step 260, Training Loss: 0.0005011269240640104\n","Step 270, Training Loss: 0.0010009927209466696\n","Step 270, Training Loss: 0.000779139285441488\n","Step 280, Training Loss: 0.00048210471868515015\n","Step 280, Training Loss: 0.0012161144986748695\n"]},{"output_type":"stream","name":"stderr","text":["***** Running Evaluation *****\n","  Num examples = 377\n","  Batch size = 32\n"]},{"output_type":"stream","name":"stdout","text":["Step 290, Training Loss: 0.000537540705408901\n","Step 290, Training Loss: 0.0008255500579252839\n","Step 300, Training Loss: 0.00044780425378121436\n","Step 300, Training Loss: 0.000524011324159801\n","Step 310, Training Loss: 0.0014809303684160113\n","Step 310, Training Loss: 0.0006496601272374392\n","Step 320, Training Loss: 0.0006187377730384469\n","Step 320, Training Loss: 0.0005657924921251833\n"]},{"output_type":"stream","name":"stderr","text":["***** Running Evaluation *****\n","  Num examples = 377\n","  Batch size = 32\n"]},{"output_type":"stream","name":"stdout","text":["Step 330, Training Loss: 0.0008603844908066094\n","Step 330, Training Loss: 0.0005630910163745284\n","Step 340, Training Loss: 0.0006920182495377958\n","Step 340, Training Loss: 0.00040861821616999805\n","Step 350, Training Loss: 0.0006494270637631416\n","Step 350, Training Loss: 0.0012426788453012705\n","Step 360, Training Loss: 0.0005317928153090179\n","Step 360, Training Loss: 0.00042957710684277117\n","Step 370, Training Loss: 0.004443936515599489\n","Step 370, Training Loss: 0.0004019782063551247\n"]},{"output_type":"stream","name":"stderr","text":["***** Running Evaluation *****\n","  Num examples = 377\n","  Batch size = 32\n","\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","***** Running Evaluation *****\n","  Num examples = 377\n","  Batch size = 32\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='12' max='12' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [12/12 00:00]\n","    </div>\n","    "]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["[I 2024-12-01 16:23:26,701] Trial 19 finished with value: 0.703597522054273 and parameters: {'learning_rate': 2.176329965042299e-06, 'batch_size': 16, 'num_train_epochs': 8, 'weight_decay': 0.015273142582206321}. Best is trial 14 with value: 0.7065333724623482.\n"]},{"output_type":"stream","name":"stdout","text":["Best trial:\n","FrozenTrial(number=14, state=TrialState.COMPLETE, values=[0.7065333724623482], datetime_start=datetime.datetime(2024, 12, 1, 16, 15, 37, 997274), datetime_complete=datetime.datetime(2024, 12, 1, 16, 16, 49, 853177), params={'learning_rate': 4.961806220526654e-05, 'batch_size': 4, 'num_train_epochs': 7, 'weight_decay': 0.09875385997446333}, user_attrs={}, system_attrs={}, intermediate_values={}, distributions={'learning_rate': FloatDistribution(high=5e-05, log=True, low=1e-06, step=None), 'batch_size': CategoricalDistribution(choices=(4, 8, 16)), 'num_train_epochs': IntDistribution(high=20, log=False, low=3, step=1), 'weight_decay': FloatDistribution(high=0.1, log=True, low=0.0001, step=None)}, trial_id=14, value=None)\n"]}],"source":["study = optuna.create_study(direction=\"maximize\")\n","study.optimize(objective, n_trials=20)\n","\n","print(\"Best trial:\")\n","print(study.best_trial)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YJceIAKW02z7","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1733070789508,"user_tz":480,"elapsed":69329,"user":{"displayName":"i200981 Wajid Ali","userId":"09984756062104537001"}},"outputId":"66da6ea7-8f8c-4fd3-881b-80939c3b939a"},"outputs":[{"output_type":"stream","name":"stderr","text":["PyTorch: setting up devices\n","/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n","***** Running training *****\n","  Num examples = 1507\n","  Num Epochs = 7\n","  Instantaneous batch size per device = 4\n","  Total train batch size (w. parallel, distributed & accumulation) = 8\n","  Gradient Accumulation steps = 2\n","  Total optimization steps = 1316\n"]},{"output_type":"stream","name":"stdout","text":["Starting training...\n","Step 0, Training Loss: 0.0005677736480720341\n","Step 0, Training Loss: 0.00019565853290259838\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='1316' max='1316' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [1316/1316 01:09, Epoch 6/7]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","      <th>F1</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>0</td>\n","      <td>No log</td>\n","      <td>0.919494</td>\n","      <td>0.620690</td>\n","      <td>0.694112</td>\n","      <td>0.694581</td>\n","      <td>0.686467</td>\n","    </tr>\n","    <tr>\n","      <td>1</td>\n","      <td>No log</td>\n","      <td>0.906385</td>\n","      <td>0.623342</td>\n","      <td>0.707778</td>\n","      <td>0.682266</td>\n","      <td>0.682748</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>0.010500</td>\n","      <td>0.987548</td>\n","      <td>0.618037</td>\n","      <td>0.697830</td>\n","      <td>0.662562</td>\n","      <td>0.668768</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>0.010500</td>\n","      <td>0.963170</td>\n","      <td>0.641910</td>\n","      <td>0.713637</td>\n","      <td>0.689655</td>\n","      <td>0.694927</td>\n","    </tr>\n","    <tr>\n","      <td>4</td>\n","      <td>0.010500</td>\n","      <td>0.995234</td>\n","      <td>0.625995</td>\n","      <td>0.705441</td>\n","      <td>0.677340</td>\n","      <td>0.686664</td>\n","    </tr>\n","    <tr>\n","      <td>5</td>\n","      <td>0.006100</td>\n","      <td>0.975010</td>\n","      <td>0.636605</td>\n","      <td>0.702263</td>\n","      <td>0.687192</td>\n","      <td>0.690891</td>\n","    </tr>\n","    <tr>\n","      <td>6</td>\n","      <td>0.006100</td>\n","      <td>0.959731</td>\n","      <td>0.628647</td>\n","      <td>0.707609</td>\n","      <td>0.682266</td>\n","      <td>0.689437</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["***** Running Evaluation *****\n","  Num examples = 377\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 377\n","  Batch size = 8\n","Saving model checkpoint to ./best_results/checkpoint-500\n","Configuration saved in ./best_results/checkpoint-500/config.json\n","Model weights saved in ./best_results/checkpoint-500/pytorch_model.bin\n"]},{"output_type":"stream","name":"stdout","text":["Step 500, Training Loss: 0.0004373354895506054\n","Step 500, Training Loss: 0.00030765365227125585\n"]},{"output_type":"stream","name":"stderr","text":["***** Running Evaluation *****\n","  Num examples = 377\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 377\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 377\n","  Batch size = 8\n","Saving model checkpoint to ./best_results/checkpoint-1000\n","Configuration saved in ./best_results/checkpoint-1000/config.json\n","Model weights saved in ./best_results/checkpoint-1000/pytorch_model.bin\n"]},{"output_type":"stream","name":"stdout","text":["Step 1000, Training Loss: 0.0003291229368187487\n","Step 1000, Training Loss: 0.00022600183729082346\n"]},{"output_type":"stream","name":"stderr","text":["***** Running Evaluation *****\n","  Num examples = 377\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 377\n","  Batch size = 8\n","\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n"]},{"output_type":"stream","name":"stdout","text":["Training completed in 69.15 seconds.\n"]}],"source":["best_hyperparams = study.best_trial.params\n","\n","training_args = TrainingArguments(\n","    output_dir=\"./best_results\",\n","    num_train_epochs=best_hyperparams[\"num_train_epochs\"],\n","    per_device_train_batch_size=best_hyperparams[\"batch_size\"],\n","    per_device_eval_batch_size=best_hyperparams[\"batch_size\"] * 2,\n","    learning_rate=best_hyperparams[\"learning_rate\"],\n","    weight_decay=best_hyperparams[\"weight_decay\"],\n","    warmup_steps=10,\n","    evaluation_strategy=\"epoch\",\n","    gradient_accumulation_steps=2,\n","    report_to=\"none\",\n",")\n","\n","trainer = OneHotTrainer(\n","    model=model,\n","    args=training_args,\n","    train_dataset=train_dataset,\n","    eval_dataset=eval_dataset,\n","    compute_metrics=compute_metrics,\n",")\n","\n","# Measure training time\n","print(\"Starting training...\")\n","start_time = time.time()  # Start the timer\n","\n","trainer.train()\n","\n","end_time = time.time()  # End the timer\n","training_duration = end_time - start_time  # Calculate the duration\n","\n","# Print the training duration\n","print(f\"Training completed in {training_duration:.2f} seconds.\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gmbA8nKW1IAq","colab":{"base_uri":"https://localhost:8080/","height":89},"executionInfo":{"status":"ok","timestamp":1733070790480,"user_tz":480,"elapsed":986,"user":{"displayName":"i200981 Wajid Ali","userId":"09984756062104537001"}},"outputId":"54dd2882-9303-4fb5-f881-0368941d0b0b"},"outputs":[{"output_type":"stream","name":"stderr","text":["***** Running Evaluation *****\n","  Num examples = 377\n","  Batch size = 8\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='48' max='48' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [48/48 00:00]\n","    </div>\n","    "]},"metadata":{}}],"source":["eval_results = trainer.evaluate()"]},{"cell_type":"markdown","metadata":{"id":"nlNRlUlh-7y0"},"source":["### Model Saving"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"maZXl_QZ-Psa","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1733070791014,"user_tz":480,"elapsed":543,"user":{"displayName":"i200981 Wajid Ali","userId":"09984756062104537001"}},"outputId":"49014276-0141-4708-d7db-e32348f63284"},"outputs":[{"output_type":"stream","name":"stdout","text":["mkdir: cannot create directory ‘best_model_huawei-noah/TinyBERT_General_4L_312D_python’: File exists\n","mkdir: cannot create directory ‘best_model_huawei-noah/TinyBERT_General_4L_312D_python_tokenizer’: File exists\n"]},{"output_type":"stream","name":"stderr","text":["Saving model checkpoint to ./best_model_huawei-noah/TinyBERT_General_4L_312D_python\n","Configuration saved in ./best_model_huawei-noah/TinyBERT_General_4L_312D_python/config.json\n","Model weights saved in ./best_model_huawei-noah/TinyBERT_General_4L_312D_python/pytorch_model.bin\n","tokenizer config file saved in ./best_model_huawei-noah/TinyBERT_General_4L_312D_python_tokenizer/tokenizer_config.json\n","Special tokens file saved in ./best_model_huawei-noah/TinyBERT_General_4L_312D_python_tokenizer/special_tokens_map.json\n","loading configuration file ./best_model_huawei-noah/TinyBERT_General_4L_312D_python/config.json\n","Model config BertConfig {\n","  \"_name_or_path\": \"./best_model_huawei-noah/TinyBERT_General_4L_312D_python\",\n","  \"architectures\": [\n","    \"BertForSequenceClassification\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"cell\": {},\n","  \"classifier_dropout\": null,\n","  \"emb_size\": 312,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 312,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\",\n","    \"1\": \"LABEL_1\",\n","    \"2\": \"LABEL_2\",\n","    \"3\": \"LABEL_3\",\n","    \"4\": \"LABEL_4\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 1200,\n","  \"label2id\": {\n","    \"LABEL_0\": 0,\n","    \"LABEL_1\": 1,\n","    \"LABEL_2\": 2,\n","    \"LABEL_3\": 3,\n","    \"LABEL_4\": 4\n","  },\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 4,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"pre_trained\": \"\",\n","  \"structure\": [],\n","  \"torch_dtype\": \"float32\",\n","  \"transformers_version\": \"4.17.0\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n","loading weights file ./best_model_huawei-noah/TinyBERT_General_4L_312D_python/pytorch_model.bin\n","/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py:1439: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  state_dict = torch.load(resolved_archive_file, map_location=\"cpu\")\n","All model checkpoint weights were used when initializing BertForSequenceClassification.\n","\n","All the weights of BertForSequenceClassification were initialized from the model checkpoint at ./best_model_huawei-noah/TinyBERT_General_4L_312D_python.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForSequenceClassification for predictions without further training.\n","Didn't find file ./best_model_huawei-noah/TinyBERT_General_4L_312D_python_tokenizer/added_tokens.json. We won't load it.\n","loading file ./best_model_huawei-noah/TinyBERT_General_4L_312D_python_tokenizer/vocab.txt\n","loading file ./best_model_huawei-noah/TinyBERT_General_4L_312D_python_tokenizer/tokenizer.json\n","loading file None\n","loading file ./best_model_huawei-noah/TinyBERT_General_4L_312D_python_tokenizer/special_tokens_map.json\n","loading file ./best_model_huawei-noah/TinyBERT_General_4L_312D_python_tokenizer/tokenizer_config.json\n"]}],"source":["!mkdir 'best_model_huawei-noah_TinyBERT_General_4L_312D_python'\n","!mkdir 'best_model_huawei-noah_TinyBERT_General_4L_312D_python_tokenizer'\n","\n","# Save model and tokenizer\n","trainer.save_model('./best_model_huawei-noah_TinyBERT_General_4L_312D_python')\n","tokenizer.save_pretrained('./best_model_huawei-noah_TinyBERT_General_4L_312D_python_tokenizer')\n","\n","# Load model and tokenizer\n","from transformers import AutoTokenizer, AutoModelForSequenceClassification\n","\n","model = AutoModelForSequenceClassification.from_pretrained('./best_model_huawei-noah_TinyBERT_General_4L_312D_python')\n","tokenizer = AutoTokenizer.from_pretrained('./best_model_huawei-noah_TinyBERT_General_4L_312D_python_tokenizer')"]},{"cell_type":"markdown","metadata":{"id":"ejkhNznm77uR"},"source":["### Model Testing"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EMvNuBr37lzY"},"outputs":[],"source":["X_test = list(test_df['combo'])\n","y_test = list(test_df['labels'])"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"Tut5zEUc790m","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1733070791016,"user_tz":480,"elapsed":24,"user":{"displayName":"i200981 Wajid Ali","userId":"09984756062104537001"}},"outputId":"8f18fa25-a1c7-422c-adaf-86af5d2fc8a4"},"outputs":[{"output_type":"stream","name":"stdout","text":["[array([0, 0, 0, 0, 1]) array([0, 0, 0, 0, 1]) array([0, 0, 0, 0, 1])\n"," array([0, 1, 0, 0, 0]) array([0, 0, 1, 0, 1]) array([0, 0, 0, 0, 1])\n"," array([0, 0, 0, 0, 1]) array([1, 0, 0, 1, 0]) array([1, 0, 0, 0, 0])\n"," array([1, 0, 0, 0, 0]) array([1, 0, 0, 0, 0]) array([1, 0, 0, 0, 0])\n"," array([1, 0, 0, 0, 0]) array([0, 0, 0, 0, 1]) array([0, 0, 0, 1, 0])\n"," array([0, 0, 0, 1, 0]) array([0, 0, 0, 1, 0]) array([0, 0, 0, 1, 0])\n"," array([0, 0, 0, 0, 1]) array([0, 0, 0, 1, 0]) array([1, 0, 0, 0, 0])\n"," array([1, 0, 0, 0, 0]) array([0, 0, 1, 0, 0]) array([0, 0, 0, 0, 1])\n"," array([1, 0, 0, 0, 0]) array([1, 0, 0, 0, 0]) array([0, 1, 0, 0, 0])\n"," array([0, 1, 0, 0, 0]) array([0, 1, 0, 0, 0]) array([0, 1, 0, 0, 0])\n"," array([0, 1, 0, 0, 0]) array([0, 1, 0, 0, 0]) array([0, 0, 0, 1, 0])\n"," array([0, 0, 0, 1, 0]) array([0, 0, 0, 1, 0]) array([0, 0, 0, 1, 0])\n"," array([0, 0, 1, 0, 0]) array([0, 0, 0, 0, 1]) array([0, 0, 0, 0, 1])\n"," array([0, 0, 0, 0, 1]) array([0, 0, 0, 0, 1]) array([0, 0, 1, 0, 0])\n"," array([0, 0, 0, 0, 1]) array([1, 0, 0, 0, 0]) array([0, 0, 0, 1, 0])\n"," array([0, 1, 1, 0, 0]) array([1, 0, 0, 0, 0]) array([1, 0, 0, 0, 0])\n"," array([1, 0, 0, 0, 0]) array([1, 0, 0, 0, 0]) array([0, 1, 0, 0, 0])\n"," array([0, 0, 0, 0, 1]) array([0, 0, 0, 1, 0]) array([0, 0, 0, 1, 0])\n"," array([0, 0, 0, 1, 0]) array([0, 0, 0, 1, 0]) array([1, 0, 0, 0, 0])\n"," array([1, 0, 0, 0, 0]) array([0, 0, 0, 0, 1]) array([0, 0, 0, 0, 1])\n"," array([0, 0, 0, 0, 1]) array([0, 0, 0, 0, 1]) array([0, 0, 0, 0, 1])\n"," array([0, 0, 0, 0, 1]) array([1, 0, 0, 0, 0]) array([1, 0, 0, 0, 0])\n"," array([0, 0, 1, 0, 1]) array([0, 1, 0, 0, 1]) array([0, 0, 0, 1, 0])\n"," array([0, 0, 0, 0, 1]) array([0, 0, 0, 1, 0]) array([0, 1, 0, 0, 0])\n"," array([0, 1, 0, 0, 0]) array([0, 1, 0, 0, 0]) array([0, 1, 0, 0, 0])\n"," array([0, 1, 0, 0, 0]) array([0, 1, 0, 0, 0]) array([0, 1, 0, 0, 0])\n"," array([0, 1, 0, 0, 0]) array([0, 1, 0, 0, 0]) array([1, 0, 0, 0, 0])\n"," array([1, 0, 0, 0, 0]) array([1, 0, 0, 0, 0]) array([1, 0, 0, 0, 0])\n"," array([1, 0, 0, 0, 0]) array([1, 0, 0, 0, 0]) array([1, 0, 0, 0, 0])\n"," array([0, 0, 0, 0, 1]) array([0, 0, 1, 0, 0]) array([0, 0, 1, 0, 0])\n"," array([0, 0, 0, 1, 0]) array([0, 0, 0, 0, 1]) array([0, 0, 0, 0, 1])\n"," array([1, 0, 0, 0, 0]) array([0, 0, 0, 1, 0]) array([0, 0, 0, 0, 1])\n"," array([0, 0, 0, 0, 1]) array([1, 0, 0, 0, 0]) array([0, 0, 0, 1, 0])\n"," array([0, 0, 0, 1, 0]) array([1, 0, 0, 0, 0]) array([1, 0, 0, 0, 0])\n"," array([1, 0, 0, 0, 0]) array([1, 0, 0, 0, 0]) array([1, 0, 0, 0, 0])\n"," array([1, 0, 0, 0, 0]) array([1, 0, 0, 0, 0]) array([1, 0, 0, 0, 0])\n"," array([1, 0, 0, 0, 0]) array([1, 0, 0, 0, 0]) array([1, 0, 0, 0, 0])\n"," array([1, 0, 0, 0, 0]) array([1, 0, 0, 0, 0]) array([1, 0, 0, 0, 0])\n"," array([1, 0, 0, 0, 0]) array([1, 0, 0, 0, 0]) array([1, 0, 0, 0, 0])\n"," array([1, 0, 0, 0, 0]) array([1, 0, 0, 0, 0]) array([1, 0, 0, 0, 0])\n"," array([0, 0, 1, 0, 0]) array([0, 0, 1, 0, 0]) array([0, 0, 0, 1, 0])\n"," array([0, 0, 0, 1, 0]) array([0, 0, 0, 0, 1]) array([0, 0, 0, 0, 1])\n"," array([0, 0, 0, 0, 1]) array([0, 0, 0, 0, 1]) array([0, 0, 0, 0, 1])\n"," array([0, 0, 0, 0, 1]) array([0, 0, 0, 0, 1]) array([0, 0, 0, 0, 1])\n"," array([0, 0, 0, 0, 1]) array([0, 0, 0, 0, 1]) array([0, 0, 0, 0, 1])\n"," array([0, 0, 0, 1, 0]) array([0, 0, 0, 1, 0]) array([0, 0, 0, 1, 0])\n"," array([1, 0, 0, 0, 0]) array([0, 1, 0, 0, 0]) array([0, 1, 0, 0, 0])\n"," array([1, 0, 0, 1, 0]) array([1, 0, 0, 0, 0]) array([0, 1, 0, 0, 0])\n"," array([0, 1, 0, 1, 0]) array([0, 1, 0, 0, 0]) array([1, 0, 0, 0, 0])\n"," array([1, 0, 0, 0, 0]) array([1, 0, 0, 0, 0]) array([0, 1, 0, 0, 0])\n"," array([0, 1, 0, 0, 0]) array([0, 1, 0, 0, 0]) array([0, 1, 0, 0, 0])\n"," array([0, 1, 0, 0, 0]) array([0, 0, 0, 0, 1]) array([0, 0, 1, 1, 0])\n"," array([0, 0, 1, 0, 0]) array([0, 1, 0, 0, 0]) array([0, 1, 0, 0, 0])\n"," array([0, 1, 0, 0, 0]) array([0, 1, 0, 0, 0]) array([0, 1, 0, 0, 0])\n"," array([0, 1, 0, 0, 0]) array([0, 1, 0, 0, 0]) array([0, 1, 0, 0, 0])\n"," array([0, 1, 0, 0, 0]) array([0, 1, 0, 0, 0]) array([0, 1, 0, 0, 0])\n"," array([0, 1, 0, 0, 0]) array([0, 1, 0, 0, 0]) array([0, 1, 0, 0, 0])\n"," array([0, 1, 0, 0, 0]) array([0, 0, 0, 0, 1]) array([1, 0, 0, 0, 0])\n"," array([0, 0, 1, 0, 0]) array([0, 0, 1, 0, 0]) array([1, 0, 0, 0, 0])\n"," array([0, 0, 0, 0, 1]) array([0, 0, 0, 1, 0]) array([0, 0, 0, 0, 1])\n"," array([0, 0, 1, 0, 0]) array([0, 0, 0, 0, 1]) array([0, 0, 0, 0, 1])\n"," array([0, 0, 0, 0, 1]) array([0, 0, 1, 1, 0]) array([0, 0, 1, 1, 0])\n"," array([0, 0, 0, 0, 1]) array([0, 0, 0, 1, 0]) array([0, 1, 0, 1, 1])\n"," array([0, 1, 0, 0, 0]) array([0, 1, 0, 0, 0]) array([0, 1, 0, 0, 0])\n"," array([0, 1, 0, 0, 0]) array([0, 1, 0, 0, 0]) array([0, 1, 0, 0, 0])\n"," array([0, 1, 0, 0, 0]) array([0, 1, 0, 0, 0]) array([0, 1, 0, 0, 0])\n"," array([0, 1, 0, 0, 0]) array([0, 1, 0, 0, 0]) array([0, 1, 0, 0, 0])\n"," array([0, 1, 0, 0, 0]) array([0, 1, 0, 0, 0]) array([0, 1, 0, 0, 0])\n"," array([0, 1, 0, 0, 0]) array([0, 1, 0, 0, 0]) array([0, 1, 0, 0, 0])\n"," array([0, 1, 0, 0, 0]) array([0, 1, 0, 0, 0]) array([0, 1, 0, 0, 0])\n"," array([0, 1, 0, 0, 0]) array([0, 1, 0, 0, 0]) array([0, 1, 0, 0, 0])\n"," array([0, 1, 0, 0, 0]) array([0, 1, 0, 0, 0]) array([0, 1, 0, 0, 0])\n"," array([0, 1, 0, 0, 0]) array([0, 1, 0, 0, 0]) array([0, 0, 0, 0, 1])\n"," array([0, 0, 0, 0, 1]) array([0, 0, 0, 0, 1]) array([0, 0, 0, 0, 1])\n"," array([0, 0, 0, 0, 1]) array([0, 0, 0, 0, 1]) array([0, 0, 1, 0, 0])\n"," array([0, 0, 1, 0, 0]) array([0, 1, 0, 1, 0]) array([1, 1, 0, 1, 0])\n"," array([1, 0, 1, 0, 0]) array([1, 0, 1, 0, 0]) array([1, 0, 1, 0, 0])\n"," array([0, 0, 0, 0, 1]) array([1, 0, 0, 0, 0]) array([1, 0, 0, 0, 0])\n"," array([1, 0, 0, 0, 0]) array([1, 0, 0, 0, 0]) array([1, 0, 0, 0, 0])\n"," array([1, 0, 0, 0, 0]) array([1, 0, 0, 0, 0]) array([1, 0, 0, 0, 0])\n"," array([1, 0, 0, 0, 0]) array([1, 0, 0, 0, 0]) array([1, 0, 0, 0, 0])\n"," array([1, 0, 0, 0, 0]) array([1, 0, 0, 0, 0]) array([1, 0, 0, 0, 0])\n"," array([1, 0, 0, 0, 0]) array([1, 0, 0, 0, 0]) array([1, 0, 0, 0, 0])\n"," array([1, 0, 0, 0, 0]) array([1, 0, 0, 0, 0]) array([1, 0, 0, 0, 0])\n"," array([1, 0, 0, 0, 0]) array([1, 0, 0, 0, 0]) array([1, 0, 0, 0, 0])\n"," array([0, 0, 0, 1, 0]) array([0, 0, 0, 1, 0]) array([0, 0, 0, 1, 0])\n"," array([0, 0, 1, 0, 0]) array([0, 1, 0, 0, 1]) array([0, 1, 1, 0, 0])\n"," array([0, 1, 1, 0, 0]) array([0, 0, 1, 0, 0]) array([0, 1, 0, 0, 0])\n"," array([0, 1, 0, 0, 0]) array([0, 1, 0, 0, 0]) array([0, 1, 0, 0, 0])\n"," array([0, 1, 0, 0, 0]) array([0, 1, 0, 0, 0]) array([0, 1, 0, 0, 0])\n"," array([0, 1, 0, 0, 0]) array([0, 1, 0, 0, 0]) array([0, 1, 0, 0, 0])\n"," array([0, 0, 0, 0, 1]) array([0, 0, 0, 0, 1]) array([0, 1, 0, 0, 0])\n"," array([0, 1, 0, 0, 0]) array([0, 1, 0, 0, 0]) array([0, 1, 0, 0, 0])\n"," array([0, 1, 0, 0, 0]) array([0, 1, 0, 0, 0]) array([0, 1, 0, 0, 0])\n"," array([1, 0, 0, 0, 0]) array([1, 0, 0, 0, 0]) array([1, 0, 0, 0, 0])\n"," array([1, 0, 0, 0, 0]) array([0, 0, 1, 0, 0]) array([1, 0, 0, 0, 0])\n"," array([1, 0, 0, 0, 0]) array([1, 0, 0, 0, 0]) array([1, 0, 0, 0, 0])\n"," array([0, 0, 0, 0, 1]) array([0, 0, 0, 0, 1]) array([0, 0, 0, 1, 0])\n"," array([0, 0, 0, 0, 1]) array([0, 0, 0, 0, 1]) array([0, 0, 0, 0, 1])\n"," array([1, 0, 0, 0, 0]) array([0, 0, 0, 0, 1]) array([1, 0, 1, 0, 0])\n"," array([0, 0, 1, 0, 0]) array([0, 0, 1, 0, 0]) array([0, 0, 1, 0, 0])\n"," array([1, 0, 1, 0, 0]) array([0, 1, 0, 0, 0]) array([0, 1, 0, 0, 0])\n"," array([0, 1, 0, 0, 0]) array([0, 0, 0, 0, 1]) array([0, 0, 0, 1, 0])\n"," array([0, 0, 0, 1, 0]) array([0, 0, 0, 1, 0]) array([0, 0, 0, 1, 0])\n"," array([0, 0, 1, 0, 0]) array([0, 0, 1, 0, 0]) array([0, 0, 1, 1, 0])\n"," array([0, 0, 1, 0, 0]) array([0, 0, 1, 1, 0]) array([0, 0, 1, 0, 0])\n"," array([0, 0, 1, 0, 0]) array([0, 0, 0, 1, 0]) array([0, 0, 0, 1, 0])\n"," array([0, 0, 0, 1, 0]) array([0, 0, 0, 1, 0]) array([0, 0, 0, 1, 0])\n"," array([0, 0, 0, 0, 1]) array([1, 0, 0, 0, 0]) array([1, 0, 0, 0, 0])\n"," array([1, 0, 0, 0, 0]) array([0, 1, 0, 0, 0]) array([0, 0, 0, 1, 0])\n"," array([0, 0, 0, 1, 0]) array([0, 0, 0, 1, 0]) array([0, 1, 0, 0, 0])\n"," array([0, 1, 0, 0, 0]) array([0, 1, 0, 0, 0]) array([0, 1, 0, 0, 0])\n"," array([1, 0, 0, 0, 0]) array([1, 0, 0, 0, 0]) array([0, 0, 0, 1, 0])\n"," array([0, 0, 0, 1, 0]) array([0, 1, 0, 0, 0]) array([0, 1, 0, 0, 0])\n"," array([0, 1, 0, 0, 0]) array([0, 1, 0, 0, 0]) array([0, 1, 0, 0, 0])\n"," array([0, 0, 0, 1, 0]) array([0, 0, 0, 0, 1]) array([0, 0, 0, 0, 1])\n"," array([0, 0, 0, 0, 1]) array([0, 0, 0, 0, 1]) array([1, 0, 0, 0, 0])\n"," array([1, 0, 0, 0, 0]) array([1, 0, 0, 0, 0]) array([1, 0, 0, 0, 0])\n"," array([1, 0, 0, 0, 0]) array([0, 1, 0, 0, 0]) array([0, 1, 0, 0, 0])\n"," array([0, 1, 0, 0, 0]) array([0, 1, 0, 0, 0]) array([0, 1, 0, 0, 0])\n"," array([0, 1, 0, 0, 0]) array([0, 1, 0, 0, 0]) array([0, 0, 0, 0, 1])\n"," array([1, 0, 0, 0, 0]) array([1, 0, 0, 0, 0]) array([0, 0, 0, 0, 1])\n"," array([0, 0, 0, 1, 0]) array([0, 0, 1, 0, 0]) array([0, 0, 1, 0, 0])\n"," array([0, 0, 0, 0, 1]) array([0, 0, 0, 0, 1]) array([0, 0, 0, 0, 1])\n"," array([0, 0, 0, 0, 1]) array([0, 0, 0, 0, 1]) array([0, 1, 0, 0, 0])\n"," array([0, 1, 0, 0, 0]) array([0, 1, 0, 0, 0]) array([1, 0, 0, 0, 0])\n"," array([0, 0, 0, 0, 1]) array([1, 0, 0, 1, 0]) array([1, 0, 0, 1, 0])\n"," array([1, 0, 0, 1, 0]) array([1, 0, 0, 1, 0]) array([0, 0, 0, 1, 0])\n"," array([1, 0, 0, 1, 0]) array([0, 1, 0, 0, 0]) array([0, 1, 0, 0, 0])\n"," array([0, 1, 0, 0, 0]) array([0, 1, 0, 0, 0]) array([0, 1, 0, 0, 0])\n"," array([0, 1, 0, 0, 0]) array([0, 1, 0, 0, 0]) array([0, 1, 0, 0, 0])\n"," array([0, 1, 0, 0, 0]) array([1, 0, 0, 0, 0]) array([1, 0, 0, 0, 0])\n"," array([1, 0, 0, 0, 0]) array([1, 0, 0, 0, 0]) array([1, 0, 0, 0, 0])\n"," array([1, 0, 0, 0, 0]) array([1, 0, 0, 0, 0]) array([1, 0, 0, 0, 0])\n"," array([0, 0, 0, 0, 1]) array([0, 0, 0, 0, 1]) array([1, 0, 0, 0, 0])\n"," array([0, 0, 0, 0, 1])]\n"]}],"source":["df_test = pd.DataFrame({\"combo\":X_test,\"labels\":y_test})\n","test_text = df_test.combo.values\n","test_label = df_test.labels.values\n","\n","print(test_label)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"v43dDI8E8AFv","colab":{"base_uri":"https://localhost:8080/","height":124},"executionInfo":{"status":"ok","timestamp":1733070792282,"user_tz":480,"elapsed":1285,"user":{"displayName":"i200981 Wajid Ali","userId":"09984756062104537001"}},"outputId":"1edf3e2f-6497-4a79-cfcb-acecf3c54768"},"outputs":[{"output_type":"stream","name":"stderr","text":["***** Running Prediction *****\n","  Num examples = 406\n","  Batch size = 8\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='99' max='48' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [48/48 00:02]\n","    </div>\n","    "]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Evaluation Metrics: {'accuracy': 0.5566502463054187, 'precision': 0.6591005454171888, 'recall': 0.6100917431192661, 'f1': 0.6288028934457701}\n","Inference Time: 0.95 seconds\n"]}],"source":["test_dataset = TextClassificationDataset(test_text, test_label, tokenizer)\n","\n","# Measure inference time\n","start_time = time.time()\n","\n","# Predict on the test dataset\n","predictions = trainer.predict(test_dataset)\n","\n","# Calculate elapsed time\n","end_time = time.time()\n","inference_time = end_time - start_time\n","\n","# Compute metrics using the `compute_metrics` function\n","metrics = compute_metrics(predictions)\n","\n","# Display metrics and inference time\n","print(\"Evaluation Metrics:\", metrics)\n","print(f\"Inference Time: {inference_time:.2f} seconds\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hdd0lVlvALIk","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1733070796266,"user_tz":480,"elapsed":3989,"user":{"displayName":"i200981 Wajid Ali","userId":"09984756062104537001"}},"outputId":"f57db4f4-fa16-4fd6-9925-2ac710ba4cdd"},"outputs":[{"output_type":"stream","name":"stdout","text":["updating: best_model_huawei-noah/TinyBERT_General_4L_312D_python/ (stored 0%)\n","updating: best_model_huawei-noah/TinyBERT_General_4L_312D_python/pytorch_model.bin (deflated 7%)\n","updating: best_model_huawei-noah/TinyBERT_General_4L_312D_python/training_args.bin (deflated 51%)\n","updating: best_model_huawei-noah/TinyBERT_General_4L_312D_python/config.json (deflated 52%)\n","updating: best_model_huawei-noah/TinyBERT_General_4L_312D_python_tokenizer/ (stored 0%)\n","updating: best_model_huawei-noah/TinyBERT_General_4L_312D_python_tokenizer/tokenizer_config.json (deflated 42%)\n","updating: best_model_huawei-noah/TinyBERT_General_4L_312D_python_tokenizer/special_tokens_map.json (deflated 40%)\n","updating: best_model_huawei-noah/TinyBERT_General_4L_312D_python_tokenizer/tokenizer.json (deflated 71%)\n","updating: best_model_huawei-noah/TinyBERT_General_4L_312D_python_tokenizer/vocab.txt (deflated 53%)\n"]}],"source":["!zip -r best_model_huawei-noah_TinyBERT_General_4L_312D_python.zip './best_model_huawei-noah_TinyBERT_General_4L_312D_python'\n","!zip -r best_model_huawei-noah_TinyBERT_General_4L_312D_python_tokenizer.zip './best_model_huawei-noah_TinyBERT_General_4L_312D_python_tokenizer'"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WI4xntMmASPi","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1733070798719,"user_tz":480,"elapsed":2457,"user":{"displayName":"i200981 Wajid Ali","userId":"09984756062104537001"}},"outputId":"8295aefc-4872-4fea-d4d0-7ea1f14f9d82"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","cp: cannot create regular file '/content/drive/MyDrive/FYP/Revised/Models/huawei-noah/TinyBERT_General_4L_312D_Python/': No such file or directory\n","cp: cannot create regular file '/content/drive/MyDrive/FYP/Revised/Models/huawei-noah/TinyBERT_General_4L_312D_Python_Tokenizer/': No such file or directory\n"]}],"source":["# Transferring the model to Google Drive\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","!cp best_model_huawei-noah_TinyBERT_General_4L_312D_python.zip \"/content/drive/MyDrive/FYP/Revised/Models/huawei-noah_TinyBERT_General_4L_312D_Python/\"\n","!cp best_model_huawei-noah_TinyBERT_General_4L_312D_python_tokenizer.zip \"/content/drive/MyDrive/FYP/Revised/Models/huawei-noah_TinyBERT_General_4L_312D_Python_Tokenizer/\""]},{"cell_type":"markdown","metadata":{"id":"Le2HUk3TBuT1"},"source":["### Load and Test Model"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"ei49o7unAVBx","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1733074470548,"user_tz":480,"elapsed":27387,"user":{"displayName":"i200981 Wajid Ali","userId":"09984756062104537001"}},"outputId":"45cc01a9-42f7-4a26-e1b4-ed0dc2afa563"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"6Dskeu5MBz_j","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1733074476052,"user_tz":480,"elapsed":1805,"user":{"displayName":"i200981 Wajid Ali","userId":"09984756062104537001"}},"outputId":"96911138-2f7c-4918-c1e6-0fb0dbf9845b"},"outputs":[{"output_type":"stream","name":"stdout","text":["unzip:  cannot find or open /content/drive/MyDrive/FYP/Revised/Models/huawei-noah_TinyBERT_General_4L_312D_Python/best_model_huawei-noah_TinyBERT_General_4L_312D_python.zip, /content/drive/MyDrive/FYP/Revised/Models/huawei-noah_TinyBERT_General_4L_312D_Python/best_model_huawei-noah_TinyBERT_General_4L_312D_python.zip.zip or /content/drive/MyDrive/FYP/Revised/Models/huawei-noah_TinyBERT_General_4L_312D_Python/best_model_huawei-noah_TinyBERT_General_4L_312D_python.zip.ZIP.\n","unzip:  cannot find or open /content/drive/MyDrive/FYP/Revised/Models/huawei-noah_TinyBERT_General_4L_312D_Python_Tokenizer/best_model_huawei-noah_TinyBERT_General_4L_312D_python_tokenizer.zip, /content/drive/MyDrive/FYP/Revised/Models/huawei-noah_TinyBERT_General_4L_312D_Python_Tokenizer/best_model_huawei-noah_TinyBERT_General_4L_312D_python_tokenizer.zip.zip or /content/drive/MyDrive/FYP/Revised/Models/huawei-noah_TinyBERT_General_4L_312D_Python_Tokenizer/best_model_huawei-noah_TinyBERT_General_4L_312D_python_tokenizer.zip.ZIP.\n"]}],"source":["from transformers import AutoTokenizer, AutoModelForSequenceClassification\n","\n","# Replace 'path/to/checkpoint-folder' with the actual path to your checkpoint folder.\n","model_folder = '/content/drive/MyDrive/FYP/Revised/Models/huawei-noah_TinyBERT_General_4L_312D_Python/'\n","tokenizer_folder = '/content/drive/MyDrive/FYP/Revised/Models/huawei-noah_TinyBERT_General_4L_312D_Python_Tokenizer/'\n","\n","!unzip '/content/drive/MyDrive/FYP/Revised/Models/huawei-noah_TinyBERT_General_4L_312D_Python/best_model_huawei-noah_TinyBERT_General_4L_312D_python.zip' -d './'\n","!unzip '/content/drive/MyDrive/FYP/Revised/Models/huawei-noah_TinyBERT_General_4L_312D_Python_Tokenizer/best_model_huawei-noah_TinyBERT_General_4L_312D_python_tokenizer.zip' -d './'"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"SzPbirKSCa8B","executionInfo":{"status":"ok","timestamp":1733074480363,"user_tz":480,"elapsed":435,"user":{"displayName":"i200981 Wajid Ali","userId":"09984756062104537001"}}},"outputs":[],"source":["best_model_huawei_noah_TinyBERT_General_4L_312D_python = 'best_model_huawei-noah_TinyBERT_General_4L_312D_python'\n","best_model_huawei_noah_TinyBERT_General_4L_312D_python_tokenizer = 'best_model_huawei-noah_TinyBERT_General_4L_312D_python_tokenizer'"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"TVduIUYeCbc1","colab":{"base_uri":"https://localhost:8080/","height":793},"executionInfo":{"status":"error","timestamp":1733074483872,"user_tz":480,"elapsed":1912,"user":{"displayName":"i200981 Wajid Ali","userId":"09984756062104537001"}},"outputId":"c74fa5f9-19f0-4453-cede-37538d2d7c2c"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n","The secret `HF_TOKEN` does not exist in your Colab secrets.\n","To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n","You will be able to reuse this secret in all of your notebooks.\n","Please note that authentication is recommended but still optional to access public models or datasets.\n","  warnings.warn(\n"]},{"output_type":"error","ename":"OSError","evalue":"best_model_huawei-noah_TinyBERT_General_4L_312D_python is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'\nIf this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_http.py\u001b[0m in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    405\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 406\u001b[0;31m         \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_for_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    407\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mHTTPError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/requests/models.py\u001b[0m in \u001b[0;36mraise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1023\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhttp_error_msg\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1024\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mHTTPError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhttp_error_msg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mHTTPError\u001b[0m: 401 Client Error: Unauthorized for url: https://huggingface.co/best_model_huawei-noah_TinyBERT_General_4L_312D_python/resolve/main/config.json","\nThe above exception was the direct cause of the following exception:\n","\u001b[0;31mRepositoryNotFoundError\u001b[0m                   Traceback (most recent call last)","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/utils/hub.py\u001b[0m in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    402\u001b[0m         \u001b[0;31m# Load from URL or cache if already cached\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 403\u001b[0;31m         resolved_file = hf_hub_download(\n\u001b[0m\u001b[1;32m    404\u001b[0m             \u001b[0mpath_or_repo_id\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_validators.py\u001b[0m in \u001b[0;36m_inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36mhf_hub_download\u001b[0;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, user_agent, force_download, proxies, etag_timeout, token, local_files_only, headers, endpoint, resume_download, force_filename, local_dir_use_symlinks)\u001b[0m\n\u001b[1;32m    861\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 862\u001b[0;31m         return _hf_hub_download_to_cache_dir(\n\u001b[0m\u001b[1;32m    863\u001b[0m             \u001b[0;31m# Destination\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36m_hf_hub_download_to_cache_dir\u001b[0;34m(cache_dir, repo_id, filename, repo_type, revision, endpoint, etag_timeout, headers, proxies, token, local_files_only, force_download)\u001b[0m\n\u001b[1;32m    968\u001b[0m         \u001b[0;31m# Otherwise, raise appropriate error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 969\u001b[0;31m         \u001b[0m_raise_on_head_call_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhead_call_error\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_download\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocal_files_only\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    970\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36m_raise_on_head_call_error\u001b[0;34m(head_call_error, force_download, local_files_only)\u001b[0m\n\u001b[1;32m   1483\u001b[0m         \u001b[0;31m# Repo not found or gated => let's raise the actual error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1484\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mhead_call_error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1485\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36m_get_metadata_or_catch_error\u001b[0;34m(repo_id, filename, repo_type, revision, endpoint, proxies, etag_timeout, headers, token, local_files_only, relative_filename, storage_folder)\u001b[0m\n\u001b[1;32m   1375\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1376\u001b[0;31m                 metadata = get_hf_file_metadata(\n\u001b[0m\u001b[1;32m   1377\u001b[0m                     \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproxies\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mproxies\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0metag_timeout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_validators.py\u001b[0m in \u001b[0;36m_inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36mget_hf_file_metadata\u001b[0;34m(url, token, proxies, timeout, library_name, library_version, user_agent, headers)\u001b[0m\n\u001b[1;32m   1295\u001b[0m     \u001b[0;31m# Retrieve metadata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1296\u001b[0;31m     r = _request_wrapper(\n\u001b[0m\u001b[1;32m   1297\u001b[0m         \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"HEAD\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36m_request_wrapper\u001b[0;34m(method, url, follow_relative_redirects, **params)\u001b[0m\n\u001b[1;32m    276\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfollow_relative_redirects\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 277\u001b[0;31m         response = _request_wrapper(\n\u001b[0m\u001b[1;32m    278\u001b[0m             \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36m_request_wrapper\u001b[0;34m(method, url, follow_relative_redirects, **params)\u001b[0m\n\u001b[1;32m    300\u001b[0m     \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 301\u001b[0;31m     \u001b[0mhf_raise_for_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    302\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_http.py\u001b[0m in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    453\u001b[0m             )\n\u001b[0;32m--> 454\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0m_format\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mRepositoryNotFoundError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRepositoryNotFoundError\u001b[0m: 401 Client Error. (Request ID: Root=1-674c9e33-626f455b69156f2b546cd53c;64e84324-e82f-4933-a556-6f1e0788e94e)\n\nRepository Not Found for url: https://huggingface.co/best_model_huawei-noah_TinyBERT_General_4L_312D_python/resolve/main/config.json.\nPlease make sure you specified the correct `repo_id` and `repo_type`.\nIf you are trying to access a private or gated repo, make sure you are authenticated.\nInvalid username or password.","\nThe above exception was the direct cause of the following exception:\n","\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)","\u001b[0;32m<ipython-input-7-789712737e6f>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Load the model and tokenizer from the checkpoint\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoModelForSequenceClassification\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbest_model_huawei_noah_TinyBERT_General_4L_312D_python\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbest_model_huawei_noah_TinyBERT_General_4L_312D_python_tokenizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/auto/auto_factory.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    485\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPretrainedConfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    486\u001b[0m                 \u001b[0;31m# We make a call to the config file first (which may be absent) to get the commit hash as soon as possible\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 487\u001b[0;31m                 resolved_config_file = cached_file(\n\u001b[0m\u001b[1;32m    488\u001b[0m                     \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    489\u001b[0m                     \u001b[0mCONFIG_NAME\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/utils/hub.py\u001b[0m in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    424\u001b[0m         ) from e\n\u001b[1;32m    425\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mRepositoryNotFoundError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 426\u001b[0;31m         raise EnvironmentError(\n\u001b[0m\u001b[1;32m    427\u001b[0m             \u001b[0;34mf\"{path_or_repo_id} is not a local folder and is not a valid model identifier \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    428\u001b[0m             \u001b[0;34m\"listed on 'https://huggingface.co/models'\\nIf this is a private repository, make sure to pass a token \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mOSError\u001b[0m: best_model_huawei-noah_TinyBERT_General_4L_312D_python is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'\nIf this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`"]}],"source":["# Load the model and tokenizer from the checkpoint\n","model = AutoModelForSequenceClassification.from_pretrained(best_model_huawei_noah_TinyBERT_General_4L_312D_python)\n","tokenizer = AutoTokenizer.from_pretrained(best_model_huawei_noah_TinyBERT_General_4L_312D_python_tokenizer)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mraiRFgv8MG6","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1733070801983,"user_tz":480,"elapsed":10,"user":{"displayName":"i200981 Wajid Ali","userId":"09984756062104537001"}},"outputId":"36763079-18fe-496b-d15b-077d58560be3"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["SequenceClassifierOutput(loss=None, logits=tensor([[-7.7154, -8.3863,  8.6639, -7.5060, -8.8254]],\n","       grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)"]},"metadata":{},"execution_count":43}],"source":["text = \"for detecting automatically generated fields.\"\n","encoded_input = tokenizer(text, return_tensors='pt')\n","output = model(**encoded_input)\n","output"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WNGL38Kk9Vua","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1733070801983,"user_tz":480,"elapsed":5,"user":{"displayName":"i200981 Wajid Ali","userId":"09984756062104537001"}},"outputId":"a6808015-c513-4ea6-f047-48372c18e852"},"outputs":[{"output_type":"stream","name":"stdout","text":["Predicted class: 3\n"]}],"source":["logits = output.logits\n","\n","# Apply sigmoid to convert logits to probabilities\n","probabilities = torch.sigmoid(logits)\n","\n","# Define a threshold to determine if a class is positive\n","threshold = 0.5\n","predicted_indices = (probabilities > threshold).nonzero(as_tuple=True)[1]\n","\n","# Convert indices to a list for output\n","predicted_classes = (predicted_indices + 1).tolist()\n","\n","# Format the output to display classes\n","if len(predicted_classes) > 0:\n","    predicted_classes_str = \", \".join(map(str, predicted_classes))\n","    print(f\"Predicted class/es: {predicted_classes_str}\")\n","else:\n","    print(\"No positive classes predicted.\")"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[],"authorship_tag":"ABX9TyN+CwgKaxOD7yuD+dT1C6sI"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"efb7b34e5fcd407ab2f81e45b8197319":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_334c2d58d36c4989803bb315cb5a7d0f","IPY_MODEL_26d7d8620d584aa78f38eb5ed9d86dcf","IPY_MODEL_30705faac2a34642b50c13af042b242e"],"layout":"IPY_MODEL_e59a83763ea1411fb55c8c909ab1c713"}},"334c2d58d36c4989803bb315cb5a7d0f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_23032f1fdba54fdb8ea4ee2f561c16a3","placeholder":"​","style":"IPY_MODEL_066b9ff6d12d40d9928a2d8c1c97eadf","value":"Downloading: 100%"}},"26d7d8620d584aa78f38eb5ed9d86dcf":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_e9ee6e85b36241949816fe50d76fbeab","max":409,"min":0,"orientation":"horizontal","style":"IPY_MODEL_b1be8b2321854cfe8848a871931544cd","value":409}},"30705faac2a34642b50c13af042b242e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e379dc26b73f46bd850932cbe89e38ed","placeholder":"​","style":"IPY_MODEL_ae8e8fda121c48249707680efc5e3282","value":" 409/409 [00:00&lt;00:00, 4.88kB/s]"}},"e59a83763ea1411fb55c8c909ab1c713":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"23032f1fdba54fdb8ea4ee2f561c16a3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"066b9ff6d12d40d9928a2d8c1c97eadf":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e9ee6e85b36241949816fe50d76fbeab":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b1be8b2321854cfe8848a871931544cd":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"e379dc26b73f46bd850932cbe89e38ed":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ae8e8fda121c48249707680efc5e3282":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"bdffab7bfd73431191b0982603de044f":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_de7a06340b93489fb7a5cd743b96eb22","IPY_MODEL_74cfdb87914b478892d20d36e6761ecf","IPY_MODEL_b6b6682a1f3e469d84779c5c0b0a6a56"],"layout":"IPY_MODEL_1247a74ac7344f9b8cfe98af8c59ce7a"}},"de7a06340b93489fb7a5cd743b96eb22":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5c8df202f00a411c9b1fe2a21fceddd4","placeholder":"​","style":"IPY_MODEL_fb57b3c8e21c4b20a8fb63aa92a9e583","value":"Downloading: 100%"}},"74cfdb87914b478892d20d36e6761ecf":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_464d31353c884215b65b1088406cd07e","max":231508,"min":0,"orientation":"horizontal","style":"IPY_MODEL_623ce299f5f84eb2944e89e5472492ac","value":231508}},"b6b6682a1f3e469d84779c5c0b0a6a56":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_efd8e939ec6e4fecb2cd222f6fa3ad75","placeholder":"​","style":"IPY_MODEL_1f4f42bb1993482d946a655ab1ed8f9b","value":" 226k/226k [00:00&lt;00:00, 1.51MB/s]"}},"1247a74ac7344f9b8cfe98af8c59ce7a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5c8df202f00a411c9b1fe2a21fceddd4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"fb57b3c8e21c4b20a8fb63aa92a9e583":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"464d31353c884215b65b1088406cd07e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"623ce299f5f84eb2944e89e5472492ac":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"efd8e939ec6e4fecb2cd222f6fa3ad75":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1f4f42bb1993482d946a655ab1ed8f9b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"7ffe5037639a4d259145135ddb29f74a":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_11a88252c70f4e668645a9598d1d2c5c","IPY_MODEL_ad9bd499bace4be5842929eda2e20e86","IPY_MODEL_e34077a3c7a142fcbb85eb9fed83fd6f"],"layout":"IPY_MODEL_1cb9b80524d64fb8b2b35849d270feeb"}},"11a88252c70f4e668645a9598d1d2c5c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_876bcacfc9694512ad24604300dc2c36","placeholder":"​","style":"IPY_MODEL_9ccd3d9d9e7d4d86b38e58c68d145d40","value":"Downloading: 100%"}},"ad9bd499bace4be5842929eda2e20e86":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_bcf02cdf5fc64ebab523c6df7d969ec0","max":62747391,"min":0,"orientation":"horizontal","style":"IPY_MODEL_cf63badb6d2740079ab81d42e8eac42f","value":62747391}},"e34077a3c7a142fcbb85eb9fed83fd6f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_8da47d52503242dc9c98fcab497acf07","placeholder":"​","style":"IPY_MODEL_368cb507daca42efacf52eb727d816ed","value":" 59.8M/59.8M [00:03&lt;00:00, 21.3MB/s]"}},"1cb9b80524d64fb8b2b35849d270feeb":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"876bcacfc9694512ad24604300dc2c36":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9ccd3d9d9e7d4d86b38e58c68d145d40":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"bcf02cdf5fc64ebab523c6df7d969ec0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"cf63badb6d2740079ab81d42e8eac42f":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"8da47d52503242dc9c98fcab497acf07":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"368cb507daca42efacf52eb727d816ed":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":0}