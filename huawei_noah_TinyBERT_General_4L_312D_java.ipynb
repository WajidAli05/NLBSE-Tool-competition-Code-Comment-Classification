{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sHWJIUgTy7qd",
        "outputId": "63225e69-6bb8-4eed-985e-cd6bfefab154"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.9/67.9 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m18.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m897.5/897.5 kB\u001b[0m \u001b[31m20.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "sentence-transformers 3.2.1 requires transformers<5.0.0,>=4.41.0, but you have transformers 4.17.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m364.4/364.4 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m233.5/233.5 kB\u001b[0m \u001b[31m15.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.6/78.6 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting datasets\n",
            "  Downloading datasets-3.1.0-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.16.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.26.4)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (17.0.0)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.6)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess<0.70.17 (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n",
            "Collecting fsspec<=2024.9.0,>=2023.1.0 (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets)\n",
            "  Downloading fsspec-2024.9.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.11.9)\n",
            "Requirement already satisfied: huggingface-hub>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.26.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.18.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.23.0->datasets) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2024.8.30)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
            "Downloading datasets-3.1.0-py3-none-any.whl (480 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m480.6/480.6 kB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fsspec-2024.9.0-py3-none-any.whl (179 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m179.3/179.3 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m17.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xxhash, fsspec, dill, multiprocess, datasets\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2024.10.0\n",
            "    Uninstalling fsspec-2024.10.0:\n",
            "      Successfully uninstalled fsspec-2024.10.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2024.9.0 which is incompatible.\n",
            "sentence-transformers 3.2.1 requires transformers<5.0.0,>=4.41.0, but you have transformers 4.17.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed datasets-3.1.0 dill-0.3.8 fsspec-2024.9.0 multiprocess-0.70.16 xxhash-3.5.0\n"
          ]
        }
      ],
      "source": [
        "!pip -q install transformers==4.17\n",
        "!pip -q install optuna\n",
        "!pip install datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "P2E2HOAZzM-8"
      },
      "outputs": [],
      "source": [
        "from transformers import Trainer, TrainingArguments, AutoModel, AutoTokenizer, AutoModelForSequenceClassification\n",
        "from datasets import load_dataset\n",
        "import torch\n",
        "import pandas as pd\n",
        "import nltk\n",
        "import re\n",
        "import ast\n",
        "import numpy as np\n",
        "from torch.utils.data import Dataset\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, precision_recall_fscore_support, accuracy_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "import torch.nn as nn\n",
        "import optuna\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 413
        },
        "id": "_b1pJXwqzbkx",
        "outputId": "ec73cdc8-bb60-48d3-a380-f5f857a214bb"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "   index                                   class  \\\n",
              "0      0                              Abfss.java   \n",
              "1      1                              Abfss.java   \n",
              "2      2  AbstractContractGetFileStatusTest.java   \n",
              "3     11  AbstractContractGetFileStatusTest.java   \n",
              "4     15  AbstractContractGetFileStatusTest.java   \n",
              "\n",
              "                                    comment_sentence  partition  \\\n",
              "0  azure blob file system implementation of abstr...          0   \n",
              "1          this impl delegates to the old filesystem          0   \n",
              "2  test getfilestatus and related listing operati...          0   \n",
              "3  path filter which only expects paths whose fin...          0   \n",
              "4  a filesystem filter which exposes the protecte...          0   \n",
              "\n",
              "                                               combo                 labels  \n",
              "0  azure blob file system implementation of abstr...  [1, 0, 0, 0, 0, 0, 0]  \n",
              "1  this impl delegates to the old filesystem | Ab...  [0, 0, 1, 0, 0, 0, 0]  \n",
              "2  test getfilestatus and related listing operati...  [1, 0, 0, 0, 0, 0, 0]  \n",
              "3  path filter which only expects paths whose fin...  [0, 0, 1, 0, 0, 0, 0]  \n",
              "4  a filesystem filter which exposes the protecte...  [0, 0, 1, 0, 0, 0, 0]  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-c33e3a27-a64b-48e1-ab73-ad5d89138268\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>index</th>\n",
              "      <th>class</th>\n",
              "      <th>comment_sentence</th>\n",
              "      <th>partition</th>\n",
              "      <th>combo</th>\n",
              "      <th>labels</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>Abfss.java</td>\n",
              "      <td>azure blob file system implementation of abstr...</td>\n",
              "      <td>0</td>\n",
              "      <td>azure blob file system implementation of abstr...</td>\n",
              "      <td>[1, 0, 0, 0, 0, 0, 0]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>Abfss.java</td>\n",
              "      <td>this impl delegates to the old filesystem</td>\n",
              "      <td>0</td>\n",
              "      <td>this impl delegates to the old filesystem | Ab...</td>\n",
              "      <td>[0, 0, 1, 0, 0, 0, 0]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>AbstractContractGetFileStatusTest.java</td>\n",
              "      <td>test getfilestatus and related listing operati...</td>\n",
              "      <td>0</td>\n",
              "      <td>test getfilestatus and related listing operati...</td>\n",
              "      <td>[1, 0, 0, 0, 0, 0, 0]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>11</td>\n",
              "      <td>AbstractContractGetFileStatusTest.java</td>\n",
              "      <td>path filter which only expects paths whose fin...</td>\n",
              "      <td>0</td>\n",
              "      <td>path filter which only expects paths whose fin...</td>\n",
              "      <td>[0, 0, 1, 0, 0, 0, 0]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>15</td>\n",
              "      <td>AbstractContractGetFileStatusTest.java</td>\n",
              "      <td>a filesystem filter which exposes the protecte...</td>\n",
              "      <td>0</td>\n",
              "      <td>a filesystem filter which exposes the protecte...</td>\n",
              "      <td>[0, 0, 1, 0, 0, 0, 0]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c33e3a27-a64b-48e1-ab73-ad5d89138268')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-c33e3a27-a64b-48e1-ab73-ad5d89138268 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-c33e3a27-a64b-48e1-ab73-ad5d89138268');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-d64f7b6b-991c-45d6-b383-9c7d77663327\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-d64f7b6b-991c-45d6-b383-9c7d77663327')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-d64f7b6b-991c-45d6-b383-9c7d77663327 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"print(train_df\",\n  \"rows\": 5,\n  \"fields\": [\n    {\n      \"column\": \"index\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 6,\n        \"min\": 0,\n        \"max\": 15,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          1,\n          15,\n          2\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"class\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"AbstractContractGetFileStatusTest.java\",\n          \"Abfss.java\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"comment_sentence\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"this impl delegates to the old filesystem\",\n          \"a filesystem filter which exposes the protected method\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"partition\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 0,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"combo\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"this impl delegates to the old filesystem | Abfss.java\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"labels\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "   index                                   class  \\\n",
              "0      5  AbstractContractGetFileStatusTest.java   \n",
              "1      8  AbstractContractGetFileStatusTest.java   \n",
              "2     12  AbstractContractGetFileStatusTest.java   \n",
              "3     17        AbstractS3ACommitterFactory.java   \n",
              "4     19               ApplicationConstants.java   \n",
              "\n",
              "                                    comment_sentence  partition  \\\n",
              "0                                 accept everything.          1   \n",
              "1                                    accept nothing.          1   \n",
              "2                      equals the @code match field.          1   \n",
              "3  dynamically create the output committer based ...          1   \n",
              "4                      environment for applications.          1   \n",
              "\n",
              "                                               combo                 labels  \n",
              "0  accept everything. | AbstractContractGetFileSt...  [0, 0, 1, 0, 0, 0, 0]  \n",
              "1  accept nothing. | AbstractContractGetFileStatu...  [0, 0, 1, 0, 0, 0, 0]  \n",
              "2  equals the @code match field. | AbstractContra...  [0, 0, 1, 0, 0, 0, 0]  \n",
              "3  dynamically create the output committer based ...  [0, 0, 1, 0, 0, 0, 0]  \n",
              "4  environment for applications. | ApplicationCon...  [0, 0, 1, 0, 0, 0, 0]  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-bc0abed8-91df-4edd-9b25-c3e1f7784984\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>index</th>\n",
              "      <th>class</th>\n",
              "      <th>comment_sentence</th>\n",
              "      <th>partition</th>\n",
              "      <th>combo</th>\n",
              "      <th>labels</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>5</td>\n",
              "      <td>AbstractContractGetFileStatusTest.java</td>\n",
              "      <td>accept everything.</td>\n",
              "      <td>1</td>\n",
              "      <td>accept everything. | AbstractContractGetFileSt...</td>\n",
              "      <td>[0, 0, 1, 0, 0, 0, 0]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>8</td>\n",
              "      <td>AbstractContractGetFileStatusTest.java</td>\n",
              "      <td>accept nothing.</td>\n",
              "      <td>1</td>\n",
              "      <td>accept nothing. | AbstractContractGetFileStatu...</td>\n",
              "      <td>[0, 0, 1, 0, 0, 0, 0]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>12</td>\n",
              "      <td>AbstractContractGetFileStatusTest.java</td>\n",
              "      <td>equals the @code match field.</td>\n",
              "      <td>1</td>\n",
              "      <td>equals the @code match field. | AbstractContra...</td>\n",
              "      <td>[0, 0, 1, 0, 0, 0, 0]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>17</td>\n",
              "      <td>AbstractS3ACommitterFactory.java</td>\n",
              "      <td>dynamically create the output committer based ...</td>\n",
              "      <td>1</td>\n",
              "      <td>dynamically create the output committer based ...</td>\n",
              "      <td>[0, 0, 1, 0, 0, 0, 0]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>19</td>\n",
              "      <td>ApplicationConstants.java</td>\n",
              "      <td>environment for applications.</td>\n",
              "      <td>1</td>\n",
              "      <td>environment for applications. | ApplicationCon...</td>\n",
              "      <td>[0, 0, 1, 0, 0, 0, 0]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-bc0abed8-91df-4edd-9b25-c3e1f7784984')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-bc0abed8-91df-4edd-9b25-c3e1f7784984 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-bc0abed8-91df-4edd-9b25-c3e1f7784984');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-5bb3c587-98a4-40eb-8442-bbb14092f7a0\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-5bb3c587-98a4-40eb-8442-bbb14092f7a0')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-5bb3c587-98a4-40eb-8442-bbb14092f7a0 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"print(train_df\",\n  \"rows\": 5,\n  \"fields\": [\n    {\n      \"column\": \"index\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 5,\n        \"min\": 5,\n        \"max\": 19,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          8,\n          19,\n          12\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"class\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"AbstractContractGetFileStatusTest.java\",\n          \"AbstractS3ACommitterFactory.java\",\n          \"ApplicationConstants.java\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"comment_sentence\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"accept nothing.\",\n          \"environment for applications.\",\n          \"equals the @code match field.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"partition\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 1,\n        \"max\": 1,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"combo\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"accept nothing. | AbstractContractGetFileStatusTest.java\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"labels\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(7614, 6)\n"
          ]
        }
      ],
      "source": [
        "splits = {'java_train': 'data/java_train-00000-of-00001.parquet', 'java_test': 'data/java_test-00000-of-00001.parquet'}\n",
        "train_df = pd.read_parquet(\"hf://datasets/NLBSE/nlbse25-code-comment-classification/\" + splits[\"java_train\"])\n",
        "test_df = pd.read_parquet(\"hf://datasets/NLBSE/nlbse25-code-comment-classification/\" + splits[\"java_test\"])\n",
        "\n",
        "display(train_df.head())\n",
        "display(test_df.head())\n",
        "print(train_df.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 465
        },
        "id": "9x93EZXwzo-f",
        "outputId": "f3956c5f-9cd6-480c-83a0-f4760c6810b8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-6-df4392ae2e99>:9: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
            "  train_str_df = train_df[[\"class\", \"comment_sentence\", \"combo\"]].applymap(\n",
            "<ipython-input-6-df4392ae2e99>:12: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
            "  test_str_df = test_df[[\"class\", \"comment_sentence\", \"combo\"]].applymap(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "   index                                   class  \\\n",
              "0      0                              abfss.java   \n",
              "1      1                              abfss.java   \n",
              "2      2  abstractcontractgetfilestatustest.java   \n",
              "3     11  abstractcontractgetfilestatustest.java   \n",
              "4     15  abstractcontractgetfilestatustest.java   \n",
              "\n",
              "                                    comment_sentence  partition  \\\n",
              "0  azure blob file system implementation of abstr...          0   \n",
              "1          this impl delegates to the old filesystem          0   \n",
              "2  test getfilestatus and related listing operati...          0   \n",
              "3  path filter which only expects paths whose fin...          0   \n",
              "4  a filesystem filter which exposes the protecte...          0   \n",
              "\n",
              "                                               combo                 labels  \n",
              "0  azure blob file system implementation of abstr...  [1, 0, 0, 0, 0, 0, 0]  \n",
              "1  this impl delegates to the old filesystem | ab...  [0, 0, 1, 0, 0, 0, 0]  \n",
              "2  test getfilestatus and related listing operati...  [1, 0, 0, 0, 0, 0, 0]  \n",
              "3  path filter which only expects paths whose fin...  [0, 0, 1, 0, 0, 0, 0]  \n",
              "4  a filesystem filter which exposes the protecte...  [0, 0, 1, 0, 0, 0, 0]  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-3f5e28d0-ee72-46d7-9656-5ad132c68b7a\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>index</th>\n",
              "      <th>class</th>\n",
              "      <th>comment_sentence</th>\n",
              "      <th>partition</th>\n",
              "      <th>combo</th>\n",
              "      <th>labels</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>abfss.java</td>\n",
              "      <td>azure blob file system implementation of abstr...</td>\n",
              "      <td>0</td>\n",
              "      <td>azure blob file system implementation of abstr...</td>\n",
              "      <td>[1, 0, 0, 0, 0, 0, 0]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>abfss.java</td>\n",
              "      <td>this impl delegates to the old filesystem</td>\n",
              "      <td>0</td>\n",
              "      <td>this impl delegates to the old filesystem | ab...</td>\n",
              "      <td>[0, 0, 1, 0, 0, 0, 0]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>abstractcontractgetfilestatustest.java</td>\n",
              "      <td>test getfilestatus and related listing operati...</td>\n",
              "      <td>0</td>\n",
              "      <td>test getfilestatus and related listing operati...</td>\n",
              "      <td>[1, 0, 0, 0, 0, 0, 0]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>11</td>\n",
              "      <td>abstractcontractgetfilestatustest.java</td>\n",
              "      <td>path filter which only expects paths whose fin...</td>\n",
              "      <td>0</td>\n",
              "      <td>path filter which only expects paths whose fin...</td>\n",
              "      <td>[0, 0, 1, 0, 0, 0, 0]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>15</td>\n",
              "      <td>abstractcontractgetfilestatustest.java</td>\n",
              "      <td>a filesystem filter which exposes the protecte...</td>\n",
              "      <td>0</td>\n",
              "      <td>a filesystem filter which exposes the protecte...</td>\n",
              "      <td>[0, 0, 1, 0, 0, 0, 0]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-3f5e28d0-ee72-46d7-9656-5ad132c68b7a')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-3f5e28d0-ee72-46d7-9656-5ad132c68b7a button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-3f5e28d0-ee72-46d7-9656-5ad132c68b7a');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-6c7c1fba-a776-4352-9b84-b6da88a682c3\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-6c7c1fba-a776-4352-9b84-b6da88a682c3')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-6c7c1fba-a776-4352-9b84-b6da88a682c3 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"display(test_df\",\n  \"rows\": 5,\n  \"fields\": [\n    {\n      \"column\": \"index\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 6,\n        \"min\": 0,\n        \"max\": 15,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          1,\n          15,\n          2\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"class\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"abstractcontractgetfilestatustest.java\",\n          \"abfss.java\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"comment_sentence\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"this impl delegates to the old filesystem\",\n          \"a filesystem filter which exposes the protected method\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"partition\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 0,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"combo\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"this impl delegates to the old filesystem | abfssjava\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"labels\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "   index                                   class  \\\n",
              "0      5  abstractcontractgetfilestatustest.java   \n",
              "1      8  abstractcontractgetfilestatustest.java   \n",
              "2     12  abstractcontractgetfilestatustest.java   \n",
              "3     17        abstracts3acommitterfactory.java   \n",
              "4     19               applicationconstants.java   \n",
              "\n",
              "                                    comment_sentence  partition  \\\n",
              "0                                 accept everything.          1   \n",
              "1                                    accept nothing.          1   \n",
              "2                      equals the @code match field.          1   \n",
              "3  dynamically create the output committer based ...          1   \n",
              "4                      environment for applications.          1   \n",
              "\n",
              "                                               combo                 labels  \n",
              "0  accept everything | abstractcontractgetfilesta...  [0, 0, 1, 0, 0, 0, 0]  \n",
              "1  accept nothing | abstractcontractgetfilestatus...  [0, 0, 1, 0, 0, 0, 0]  \n",
              "2  equals the code match field | abstractcontract...  [0, 0, 1, 0, 0, 0, 0]  \n",
              "3  dynamically create the output committer based ...  [0, 0, 1, 0, 0, 0, 0]  \n",
              "4  environment for applications | applicationcons...  [0, 0, 1, 0, 0, 0, 0]  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-1447597e-5e22-4990-a837-f30828662d52\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>index</th>\n",
              "      <th>class</th>\n",
              "      <th>comment_sentence</th>\n",
              "      <th>partition</th>\n",
              "      <th>combo</th>\n",
              "      <th>labels</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>5</td>\n",
              "      <td>abstractcontractgetfilestatustest.java</td>\n",
              "      <td>accept everything.</td>\n",
              "      <td>1</td>\n",
              "      <td>accept everything | abstractcontractgetfilesta...</td>\n",
              "      <td>[0, 0, 1, 0, 0, 0, 0]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>8</td>\n",
              "      <td>abstractcontractgetfilestatustest.java</td>\n",
              "      <td>accept nothing.</td>\n",
              "      <td>1</td>\n",
              "      <td>accept nothing | abstractcontractgetfilestatus...</td>\n",
              "      <td>[0, 0, 1, 0, 0, 0, 0]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>12</td>\n",
              "      <td>abstractcontractgetfilestatustest.java</td>\n",
              "      <td>equals the @code match field.</td>\n",
              "      <td>1</td>\n",
              "      <td>equals the code match field | abstractcontract...</td>\n",
              "      <td>[0, 0, 1, 0, 0, 0, 0]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>17</td>\n",
              "      <td>abstracts3acommitterfactory.java</td>\n",
              "      <td>dynamically create the output committer based ...</td>\n",
              "      <td>1</td>\n",
              "      <td>dynamically create the output committer based ...</td>\n",
              "      <td>[0, 0, 1, 0, 0, 0, 0]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>19</td>\n",
              "      <td>applicationconstants.java</td>\n",
              "      <td>environment for applications.</td>\n",
              "      <td>1</td>\n",
              "      <td>environment for applications | applicationcons...</td>\n",
              "      <td>[0, 0, 1, 0, 0, 0, 0]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-1447597e-5e22-4990-a837-f30828662d52')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-1447597e-5e22-4990-a837-f30828662d52 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-1447597e-5e22-4990-a837-f30828662d52');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-db1ba7f5-ddb4-4244-9b96-6c244f9c1deb\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-db1ba7f5-ddb4-4244-9b96-6c244f9c1deb')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-db1ba7f5-ddb4-4244-9b96-6c244f9c1deb button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"display(test_df\",\n  \"rows\": 5,\n  \"fields\": [\n    {\n      \"column\": \"index\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 5,\n        \"min\": 5,\n        \"max\": 19,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          8,\n          19,\n          12\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"class\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"abstractcontractgetfilestatustest.java\",\n          \"abstracts3acommitterfactory.java\",\n          \"applicationconstants.java\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"comment_sentence\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"accept nothing.\",\n          \"environment for applications.\",\n          \"equals the @code match field.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"partition\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 1,\n        \"max\": 1,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"combo\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"accept nothing | abstractcontractgetfilestatustestjava\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"labels\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "def remove_punctuation_except_pipe(text):\n",
        "    return re.sub(r\"[^\\w\\s\\|]\", \"\", text)\n",
        "\n",
        "# Apply the function to the 'combo' column\n",
        "train_df['combo'] = train_df['combo'].apply(remove_punctuation_except_pipe)\n",
        "test_df['combo'] = test_df['combo'].apply(remove_punctuation_except_pipe)\n",
        "\n",
        "# Convert data to lowercase\n",
        "train_str_df = train_df[[\"class\", \"comment_sentence\", \"combo\"]].applymap(\n",
        "    lambda x: x.lower() if isinstance(x, str) else str(x)\n",
        ")\n",
        "test_str_df = test_df[[\"class\", \"comment_sentence\", \"combo\"]].applymap(\n",
        "    lambda x: x.lower() if isinstance(x, str) else str(x)\n",
        ")\n",
        "\n",
        "train_df[[\"class\", \"comment_sentence\", \"combo\"]] = train_str_df[[\"class\", \"comment_sentence\", \"combo\"]]\n",
        "test_df[[\"class\", \"comment_sentence\", \"combo\"]] = test_str_df[[\"class\", \"comment_sentence\", \"combo\"]]\n",
        "\n",
        "display(train_df.head())\n",
        "display(test_df.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "20n8nIz2zr-2"
      },
      "outputs": [],
      "source": [
        "X = list(train_df['combo'])\n",
        "y = list(train_df['labels'])\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272,
          "referenced_widgets": [
            "f305934bad7346c39e2138434499c165",
            "5e030adc0dd34eb4abc9494603bcc33f",
            "755d27a89f6b43938cef965498724f67",
            "3261f69aa9134467a6c7be0e54c69ec0",
            "d52c30006e51406bb49456455dc8f6df",
            "e4534745d1244405b68878b0f6f7cc61",
            "fe589315eff74fb7a46d52c9629832c2",
            "9370a04a7d5a42eba5a0b569722fb619",
            "81266e3c51434d1398db5a46b58add1c",
            "0c2d690fcf4f4fbb857c008bae7f1b12",
            "94b0b7204fdc41109c19cd27ee66a039",
            "342df5dbd71841aebbfbe42791eafbfb",
            "5b17febaf1974bda8628583799b61045",
            "f51835395eb24fcba89a3eed46218123",
            "97825d75548e40a1881fb258c2e7beeb",
            "1bd46438f777449db23f9a815334a70f",
            "5a09384224e94a688d48f754b2d38551",
            "32fb0d854fa04e598aaacc92c95ac2c3",
            "b8083aa880be4c4fa925b3e0ad0d5878",
            "8a1e30fc96744f1c9e219f2b7bab44e4",
            "ec137894b50846739d4d2aa569ee0190",
            "efa25a580b8643368a2b1773e4296c0d",
            "7e015dca2f194f74886615592a729c21",
            "c5772d7780e044b798969f3bc660fa5c",
            "9373e22e12a4467ca233c8894754e73e",
            "6eee226a54384f12a80fb25a2629b875",
            "154203c10176432786aefa00a5ca651b",
            "e7f1a49d201c41c8a59da116b34caa04",
            "e99df93b272545ff900e6fbeb9b3dd77",
            "221e8e1847c44dfbb4ec61bbde7ca28e",
            "203b69099d9c4775b944d9d066a07ac5",
            "f24c949b6a4744b18460151d93d6fa41",
            "5b7404e4775445279facc4ba1e0b2b45"
          ]
        },
        "id": "4lJ7iGgPzwDz",
        "outputId": "bb32f98e-8b50-4d0b-af35-7ca4dd289041"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading:   0%|          | 0.00/409 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f305934bad7346c39e2138434499c165"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading:   0%|          | 0.00/226k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "342df5dbd71841aebbfbe42791eafbfb"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading:   0%|          | 0.00/59.8M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7e015dca2f194f74886615592a729c21"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py:1439: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  state_dict = torch.load(resolved_archive_file, map_location=\"cpu\")\n",
            "Some weights of the model checkpoint at huawei-noah/TinyBERT_General_4L_312D were not used when initializing BertForSequenceClassification: ['fit_denses.0.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'fit_denses.0.weight', 'cls.predictions.transform.LayerNorm.bias', 'fit_denses.2.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'fit_denses.4.bias', 'fit_denses.1.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'fit_denses.3.weight', 'fit_denses.2.bias', 'fit_denses.4.weight', 'fit_denses.3.bias', 'fit_denses.1.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at huawei-noah/TinyBERT_General_4L_312D and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained('huawei-noah/TinyBERT_General_4L_312D')\n",
        "model = AutoModelForSequenceClassification.from_pretrained('huawei-noah/TinyBERT_General_4L_312D', num_labels=7)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "BLFFkwfl0j2W"
      },
      "outputs": [],
      "source": [
        "class TextClassificationDataset(Dataset):\n",
        "    def __init__(self, texts, labels, tokenizer, device=\"cpu\"):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "        self.tokenizer = tokenizer\n",
        "        self.device = device\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        text = self.texts[index]\n",
        "        label = self.labels[index]\n",
        "\n",
        "        encoded_text = self.tokenizer.encode_plus(\n",
        "            text,\n",
        "            add_special_tokens=True,\n",
        "            max_length=256,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            return_token_type_ids=False,\n",
        "            return_attention_mask=True,\n",
        "            return_tensors='pt',\n",
        "        )\n",
        "\n",
        "        input_ids = encoded_text['input_ids'].squeeze().to(self.device)\n",
        "        attention_mask = encoded_text['attention_mask'].squeeze().to(self.device)\n",
        "        label = torch.tensor(label, dtype=torch.float).to(self.device)\n",
        "\n",
        "        return {\n",
        "            'input_ids': input_ids,\n",
        "            'attention_mask': attention_mask,\n",
        "            'labels': label\n",
        "        }\n",
        "\n",
        "train_dataset = TextClassificationDataset(X_train, y_train, tokenizer)\n",
        "eval_dataset = TextClassificationDataset(X_val, y_val, tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "aO311Hhf0rKO"
      },
      "outputs": [],
      "source": [
        "class OneHotTrainer(Trainer):\n",
        "    def compute_loss(self, model, inputs, return_outputs=False):\n",
        "        labels = inputs.get(\"labels\")\n",
        "        model_inputs = {k: v for k, v in inputs.items() if k != \"labels\"}\n",
        "        outputs = model(**model_inputs)\n",
        "        logits = outputs.get(\"logits\")\n",
        "\n",
        "        if logits is None:\n",
        "            raise ValueError(\"Logits are missing from the model output.\")\n",
        "\n",
        "        loss_fct = nn.BCEWithLogitsLoss()\n",
        "        loss = loss_fct(logits, labels.float())\n",
        "\n",
        "        # Log training loss\n",
        "        if self.state.global_step % self.args.logging_steps == 0:\n",
        "            print(f\"Step {self.state.global_step}, Training Loss: {loss.item()}\")\n",
        "\n",
        "        return (loss, outputs) if return_outputs else loss\n",
        "\n",
        "\n",
        "# class OneHotTrainer(Trainer):\n",
        "#     def compute_loss(self, model, inputs, return_outputs=False):\n",
        "#         labels = inputs.get(\"labels\")\n",
        "#         # Remove 'labels' from inputs before passing to model\n",
        "#         model_inputs = {k: v for k, v in inputs.items() if k != \"labels\"}\n",
        "#         outputs = model(**model_inputs)  # Pass only the required inputs to the model\n",
        "#         logits = outputs.get(\"logits\")\n",
        "\n",
        "#         if logits is None:\n",
        "#             # Handle the case where logits are missing, e.g., raise an exception or return a default loss\n",
        "#             raise ValueError(\"Logits are missing from the model output.\")  # Or return a default loss\n",
        "\n",
        "#         # Use BCEWithLogitsLoss for multi-label classification\n",
        "#         loss_fct = nn.BCEWithLogitsLoss()\n",
        "#         loss = loss_fct(logits, labels.float())\n",
        "\n",
        "#         return (loss, outputs) if return_outputs else loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "-pVPFv7O0ut3"
      },
      "outputs": [],
      "source": [
        "def compute_metrics(pred):\n",
        "    logits = pred.predictions\n",
        "    preds = (logits > 0).astype(int)\n",
        "    labels = pred.label_ids\n",
        "\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='weighted', zero_division=1)\n",
        "    accuracy = accuracy_score(labels, preds)\n",
        "\n",
        "    return {\n",
        "        'accuracy': accuracy,\n",
        "        'precision': precision,\n",
        "        'recall': recall,\n",
        "        'f1': f1\n",
        "    }\n",
        "\n",
        "\n",
        "# def compute_metrics(pred):\n",
        "#     logits = pred.predictions\n",
        "#     preds = (logits > 0).astype(int)\n",
        "\n",
        "#     labels = pred.label_ids\n",
        "\n",
        "#     precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='weighted')\n",
        "#     accuracy = (preds == labels).mean()\n",
        "\n",
        "#     return {\n",
        "#         'accuracy': accuracy,\n",
        "#         'precision': precision,\n",
        "#         'recall': recall,\n",
        "#         'f1': f1\n",
        "#     }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "nOK9HTwq9wuM"
      },
      "outputs": [],
      "source": [
        "def objective(trial):\n",
        "    learning_rate = trial.suggest_loguniform(\"learning_rate\", 1e-6, 5e-5)\n",
        "    batch_size = trial.suggest_categorical(\"batch_size\", [4, 8, 16])\n",
        "    num_train_epochs = trial.suggest_int(\"num_train_epochs\", 3, 10)\n",
        "    weight_decay = trial.suggest_loguniform(\"weight_decay\", 1e-4, 0.1)\n",
        "\n",
        "    # training_args = TrainingArguments(\n",
        "    #     output_dir=\"./results\",\n",
        "    #     num_train_epochs=num_train_epochs,\n",
        "    #     per_device_train_batch_size=batch_size,\n",
        "    #     per_device_eval_batch_size=batch_size * 2,\n",
        "    #     warmup_steps=10,\n",
        "    #     weight_decay=weight_decay,\n",
        "    #     evaluation_strategy=\"epoch\",\n",
        "    #     learning_rate=learning_rate,\n",
        "    #     gradient_accumulation_steps=2,\n",
        "    #     report_to=\"none\",\n",
        "    # )\n",
        "\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=\"./results\",\n",
        "        num_train_epochs=num_train_epochs,\n",
        "        per_device_train_batch_size=batch_size,\n",
        "        per_device_eval_batch_size=batch_size * 2,\n",
        "        warmup_steps=10,\n",
        "        weight_decay=weight_decay,\n",
        "        evaluation_strategy=\"epoch\",\n",
        "        logging_dir=\"./logs\",  # Directory for logging\n",
        "        logging_steps=10,  # Log every 10 steps\n",
        "        save_steps=500,\n",
        "        learning_rate=learning_rate,\n",
        "        gradient_accumulation_steps=2,\n",
        "        report_to=\"none\",  # Avoid reporting to third-party loggers like WandB\n",
        "        logging_first_step=True,  # Log the first step\n",
        "    )\n",
        "\n",
        "    trainer = OneHotTrainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=train_dataset,\n",
        "        eval_dataset=eval_dataset,\n",
        "        compute_metrics=compute_metrics,\n",
        "    )\n",
        "    print(\"Starting training...\")\n",
        "    start_time = time.time()\n",
        "    trainer.train()\n",
        "    end_time = time.time()\n",
        "    training_duration = end_time - start_time\n",
        "    print(f\"Training completed in {training_duration:.2f} seconds.\")\n",
        "\n",
        "    eval_results = trainer.evaluate()\n",
        "\n",
        "    return eval_results[\"eval_f1\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "-12Jc6Vq-FNz",
        "outputId": "6a993132-d9aa-4150-dc69-3f91cfe2e91e"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2024-12-09 18:49:34,941] A new study created in memory with name: no-name-62a26867-41d7-4419-95be-4b93d57f058a\n",
            "<ipython-input-12-42465065092b>:2: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  learning_rate = trial.suggest_loguniform(\"learning_rate\", 1e-6, 5e-5)\n",
            "<ipython-input-12-42465065092b>:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  weight_decay = trial.suggest_loguniform(\"weight_decay\", 1e-4, 0.1)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting training...\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n",
            "***** Running training *****\n",
            "  Num examples = 6091\n",
            "  Num Epochs = 6\n",
            "  Instantaneous batch size per device = 8\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
            "  Gradient Accumulation steps = 2\n",
            "  Total optimization steps = 2286\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step 0, Training Loss: 0.6944229006767273\n",
            "Step 0, Training Loss: 0.6944547891616821\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='489' max='2286' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [ 489/2286 00:41 < 02:34, 11.62 it/s, Epoch 1.28/6]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "      <th>F1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.337000</td>\n",
              "      <td>0.333940</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step 10, Training Loss: 0.6870340704917908\n",
            "Step 10, Training Loss: 0.6871289014816284\n",
            "Step 20, Training Loss: 0.6664006114006042\n",
            "Step 20, Training Loss: 0.6650136113166809\n",
            "Step 30, Training Loss: 0.6398572325706482\n",
            "Step 30, Training Loss: 0.6362842321395874\n",
            "Step 40, Training Loss: 0.6145318150520325\n",
            "Step 40, Training Loss: 0.6168367266654968\n",
            "Step 50, Training Loss: 0.58025723695755\n",
            "Step 50, Training Loss: 0.5870285034179688\n",
            "Step 60, Training Loss: 0.5608417987823486\n",
            "Step 60, Training Loss: 0.5549347400665283\n",
            "Step 70, Training Loss: 0.5770939588546753\n",
            "Step 70, Training Loss: 0.5426504015922546\n",
            "Step 80, Training Loss: 0.5192143321037292\n",
            "Step 80, Training Loss: 0.5093728303909302\n",
            "Step 90, Training Loss: 0.5377962589263916\n",
            "Step 90, Training Loss: 0.5311123132705688\n",
            "Step 100, Training Loss: 0.4806445837020874\n",
            "Step 100, Training Loss: 0.48036330938339233\n",
            "Step 110, Training Loss: 0.46954190731048584\n",
            "Step 110, Training Loss: 0.4944554269313812\n",
            "Step 120, Training Loss: 0.4413367807865143\n",
            "Step 120, Training Loss: 0.4307389557361603\n",
            "Step 130, Training Loss: 0.43518319725990295\n",
            "Step 130, Training Loss: 0.4632112383842468\n",
            "Step 140, Training Loss: 0.41900330781936646\n",
            "Step 140, Training Loss: 0.44756844639778137\n",
            "Step 150, Training Loss: 0.43852561712265015\n",
            "Step 150, Training Loss: 0.3796481788158417\n",
            "Step 160, Training Loss: 0.37269026041030884\n",
            "Step 160, Training Loss: 0.3894900381565094\n",
            "Step 170, Training Loss: 0.3872995674610138\n",
            "Step 170, Training Loss: 0.3727831244468689\n",
            "Step 180, Training Loss: 0.40592852234840393\n",
            "Step 180, Training Loss: 0.3716281056404114\n",
            "Step 190, Training Loss: 0.3341010510921478\n",
            "Step 190, Training Loss: 0.39498406648635864\n",
            "Step 200, Training Loss: 0.34145215153694153\n",
            "Step 200, Training Loss: 0.37038689851760864\n",
            "Step 210, Training Loss: 0.3704851269721985\n",
            "Step 210, Training Loss: 0.4174896776676178\n",
            "Step 220, Training Loss: 0.3789823353290558\n",
            "Step 220, Training Loss: 0.38299697637557983\n",
            "Step 230, Training Loss: 0.3505669832229614\n",
            "Step 230, Training Loss: 0.3578185439109802\n",
            "Step 240, Training Loss: 0.4037224054336548\n",
            "Step 240, Training Loss: 0.3286297917366028\n",
            "Step 250, Training Loss: 0.38518279790878296\n",
            "Step 250, Training Loss: 0.37419894337654114\n",
            "Step 260, Training Loss: 0.40699678659439087\n",
            "Step 260, Training Loss: 0.3696579337120056\n",
            "Step 270, Training Loss: 0.3059033155441284\n",
            "Step 270, Training Loss: 0.26421651244163513\n",
            "Step 280, Training Loss: 0.37860700488090515\n",
            "Step 280, Training Loss: 0.32317104935646057\n",
            "Step 290, Training Loss: 0.3610534071922302\n",
            "Step 290, Training Loss: 0.3034213185310364\n",
            "Step 300, Training Loss: 0.3794538080692291\n",
            "Step 300, Training Loss: 0.3495999574661255\n",
            "Step 310, Training Loss: 0.3787877857685089\n",
            "Step 310, Training Loss: 0.35121116042137146\n",
            "Step 320, Training Loss: 0.29833945631980896\n",
            "Step 320, Training Loss: 0.3389873802661896\n",
            "Step 330, Training Loss: 0.35360077023506165\n",
            "Step 330, Training Loss: 0.3098776042461395\n",
            "Step 340, Training Loss: 0.32127588987350464\n",
            "Step 340, Training Loss: 0.34023353457450867\n",
            "Step 350, Training Loss: 0.36709269881248474\n",
            "Step 350, Training Loss: 0.347295880317688\n",
            "Step 360, Training Loss: 0.3894229829311371\n",
            "Step 360, Training Loss: 0.3741944432258606\n",
            "Step 370, Training Loss: 0.30245503783226013\n",
            "Step 370, Training Loss: 0.3829554319381714\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 1523\n",
            "  Batch size = 16\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step 380, Training Loss: 0.2576654255390167\n",
            "Step 380, Training Loss: 0.5419287085533142\n",
            "Step 390, Training Loss: 0.38813844323158264\n",
            "Step 390, Training Loss: 0.26610898971557617\n",
            "Step 400, Training Loss: 0.3518052399158478\n",
            "Step 400, Training Loss: 0.3357836902141571\n",
            "Step 410, Training Loss: 0.2657945156097412\n",
            "Step 410, Training Loss: 0.34090766310691833\n",
            "Step 420, Training Loss: 0.40807196497917175\n",
            "Step 420, Training Loss: 0.3457743227481842\n",
            "Step 430, Training Loss: 0.23442649841308594\n",
            "Step 430, Training Loss: 0.2959577143192291\n",
            "Step 440, Training Loss: 0.49928033351898193\n",
            "Step 440, Training Loss: 0.31642499566078186\n",
            "Step 450, Training Loss: 0.320622980594635\n",
            "Step 450, Training Loss: 0.3446042835712433\n",
            "Step 460, Training Loss: 0.3869488537311554\n",
            "Step 460, Training Loss: 0.3906506597995758\n",
            "Step 470, Training Loss: 0.40644755959510803\n",
            "Step 470, Training Loss: 0.3040095865726471\n",
            "Step 480, Training Loss: 0.25632911920547485\n",
            "Step 480, Training Loss: 0.3467935621738434\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='2286' max='2286' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [2286/2286 03:18, Epoch 6/6]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "      <th>F1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.337000</td>\n",
              "      <td>0.333940</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.270000</td>\n",
              "      <td>0.271964</td>\n",
              "      <td>0.458963</td>\n",
              "      <td>0.894877</td>\n",
              "      <td>0.454429</td>\n",
              "      <td>0.405839</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.240700</td>\n",
              "      <td>0.232138</td>\n",
              "      <td>0.692055</td>\n",
              "      <td>0.877608</td>\n",
              "      <td>0.684852</td>\n",
              "      <td>0.672646</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.211200</td>\n",
              "      <td>0.213048</td>\n",
              "      <td>0.766907</td>\n",
              "      <td>0.872820</td>\n",
              "      <td>0.760591</td>\n",
              "      <td>0.731143</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>0.199100</td>\n",
              "      <td>0.203210</td>\n",
              "      <td>0.772160</td>\n",
              "      <td>0.875179</td>\n",
              "      <td>0.765725</td>\n",
              "      <td>0.735587</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>0.216700</td>\n",
              "      <td>0.201077</td>\n",
              "      <td>0.772160</td>\n",
              "      <td>0.874815</td>\n",
              "      <td>0.766367</td>\n",
              "      <td>0.735691</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 490, Training Loss: 0.3676314055919647\n",
            "Step 490, Training Loss: 0.32156485319137573\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Saving model checkpoint to ./results/checkpoint-500\n",
            "Configuration saved in ./results/checkpoint-500/config.json\n",
            "Model weights saved in ./results/checkpoint-500/pytorch_model.bin\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 500, Training Loss: 0.357412725687027\n",
            "Step 500, Training Loss: 0.29738444089889526\n",
            "Step 510, Training Loss: 0.434436559677124\n",
            "Step 510, Training Loss: 0.2860284447669983\n",
            "Step 520, Training Loss: 0.30925485491752625\n",
            "Step 520, Training Loss: 0.27411696314811707\n",
            "Step 530, Training Loss: 0.2270229011774063\n",
            "Step 530, Training Loss: 0.253682941198349\n",
            "Step 540, Training Loss: 0.3739299178123474\n",
            "Step 540, Training Loss: 0.3477252125740051\n",
            "Step 550, Training Loss: 0.40974029898643494\n",
            "Step 550, Training Loss: 0.2759014964103699\n",
            "Step 560, Training Loss: 0.3233150243759155\n",
            "Step 560, Training Loss: 0.341105580329895\n",
            "Step 570, Training Loss: 0.3021380603313446\n",
            "Step 570, Training Loss: 0.23271134495735168\n",
            "Step 580, Training Loss: 0.2941843569278717\n",
            "Step 580, Training Loss: 0.24322228133678436\n",
            "Step 590, Training Loss: 0.40670859813690186\n",
            "Step 590, Training Loss: 0.37545663118362427\n",
            "Step 600, Training Loss: 0.3082030415534973\n",
            "Step 600, Training Loss: 0.30058521032333374\n",
            "Step 610, Training Loss: 0.30332446098327637\n",
            "Step 610, Training Loss: 0.39425748586654663\n",
            "Step 620, Training Loss: 0.2901131808757782\n",
            "Step 620, Training Loss: 0.3998449444770813\n",
            "Step 630, Training Loss: 0.30453914403915405\n",
            "Step 630, Training Loss: 0.27754247188568115\n",
            "Step 640, Training Loss: 0.22316105663776398\n",
            "Step 640, Training Loss: 0.3065703511238098\n",
            "Step 650, Training Loss: 0.3262979984283447\n",
            "Step 650, Training Loss: 0.2658647894859314\n",
            "Step 660, Training Loss: 0.36335453391075134\n",
            "Step 660, Training Loss: 0.2954455316066742\n",
            "Step 670, Training Loss: 0.3418709337711334\n",
            "Step 670, Training Loss: 0.19048751890659332\n",
            "Step 680, Training Loss: 0.33099135756492615\n",
            "Step 680, Training Loss: 0.397480845451355\n",
            "Step 690, Training Loss: 0.1950911432504654\n",
            "Step 690, Training Loss: 0.4328862726688385\n",
            "Step 700, Training Loss: 0.26266661286354065\n",
            "Step 700, Training Loss: 0.2969426214694977\n",
            "Step 710, Training Loss: 0.25045016407966614\n",
            "Step 710, Training Loss: 0.2314532995223999\n",
            "Step 720, Training Loss: 0.26986926794052124\n",
            "Step 720, Training Loss: 0.27453044056892395\n",
            "Step 730, Training Loss: 0.24762150645256042\n",
            "Step 730, Training Loss: 0.20210756361484528\n",
            "Step 740, Training Loss: 0.2931637167930603\n",
            "Step 740, Training Loss: 0.2556626796722412\n",
            "Step 750, Training Loss: 0.1842726320028305\n",
            "Step 750, Training Loss: 0.3319896161556244\n",
            "Step 760, Training Loss: 0.2029867321252823\n",
            "Step 760, Training Loss: 0.2979528605937958\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 1523\n",
            "  Batch size = 16\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 770, Training Loss: 0.30156561732292175\n",
            "Step 770, Training Loss: 0.15510651469230652\n",
            "Step 780, Training Loss: 0.2915593683719635\n",
            "Step 780, Training Loss: 0.289424866437912\n",
            "Step 790, Training Loss: 0.22514137625694275\n",
            "Step 790, Training Loss: 0.3328663408756256\n",
            "Step 800, Training Loss: 0.24640615284442902\n",
            "Step 800, Training Loss: 0.2504641115665436\n",
            "Step 810, Training Loss: 0.2928217053413391\n",
            "Step 810, Training Loss: 0.3070789575576782\n",
            "Step 820, Training Loss: 0.193418487906456\n",
            "Step 820, Training Loss: 0.2625616192817688\n",
            "Step 830, Training Loss: 0.31591731309890747\n",
            "Step 830, Training Loss: 0.3472908139228821\n",
            "Step 840, Training Loss: 0.27834072709083557\n",
            "Step 840, Training Loss: 0.3152371048927307\n",
            "Step 850, Training Loss: 0.1766374707221985\n",
            "Step 850, Training Loss: 0.3614867925643921\n",
            "Step 860, Training Loss: 0.2514597773551941\n",
            "Step 860, Training Loss: 0.17164598405361176\n",
            "Step 870, Training Loss: 0.24806824326515198\n",
            "Step 870, Training Loss: 0.21072903275489807\n",
            "Step 880, Training Loss: 0.3360694348812103\n",
            "Step 880, Training Loss: 0.22233422100543976\n",
            "Step 890, Training Loss: 0.24387142062187195\n",
            "Step 890, Training Loss: 0.19091565907001495\n",
            "Step 900, Training Loss: 0.21166758239269257\n",
            "Step 900, Training Loss: 0.29206323623657227\n",
            "Step 910, Training Loss: 0.3251270651817322\n",
            "Step 910, Training Loss: 0.35739219188690186\n",
            "Step 920, Training Loss: 0.24313382804393768\n",
            "Step 920, Training Loss: 0.18107697367668152\n",
            "Step 930, Training Loss: 0.2727939486503601\n",
            "Step 930, Training Loss: 0.2307833731174469\n",
            "Step 940, Training Loss: 0.32176485657691956\n",
            "Step 940, Training Loss: 0.1272890567779541\n",
            "Step 950, Training Loss: 0.16820655763149261\n",
            "Step 950, Training Loss: 0.3371141850948334\n",
            "Step 960, Training Loss: 0.27672475576400757\n",
            "Step 960, Training Loss: 0.21135467290878296\n",
            "Step 970, Training Loss: 0.20935267210006714\n",
            "Step 970, Training Loss: 0.25826048851013184\n",
            "Step 980, Training Loss: 0.259880393743515\n",
            "Step 980, Training Loss: 0.21150757372379303\n",
            "Step 990, Training Loss: 0.1540416181087494\n",
            "Step 990, Training Loss: 0.36987367272377014\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Saving model checkpoint to ./results/checkpoint-1000\n",
            "Configuration saved in ./results/checkpoint-1000/config.json\n",
            "Model weights saved in ./results/checkpoint-1000/pytorch_model.bin\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 1000, Training Loss: 0.3276289999485016\n",
            "Step 1000, Training Loss: 0.19739440083503723\n",
            "Step 1010, Training Loss: 0.16403740644454956\n",
            "Step 1010, Training Loss: 0.1897042840719223\n",
            "Step 1020, Training Loss: 0.2607617676258087\n",
            "Step 1020, Training Loss: 0.26438355445861816\n",
            "Step 1030, Training Loss: 0.28476423025131226\n",
            "Step 1030, Training Loss: 0.3077844977378845\n",
            "Step 1040, Training Loss: 0.2790820300579071\n",
            "Step 1040, Training Loss: 0.280045747756958\n",
            "Step 1050, Training Loss: 0.16367487609386444\n",
            "Step 1050, Training Loss: 0.25447413325309753\n",
            "Step 1060, Training Loss: 0.2419123351573944\n",
            "Step 1060, Training Loss: 0.3114205300807953\n",
            "Step 1070, Training Loss: 0.1909448206424713\n",
            "Step 1070, Training Loss: 0.21891148388385773\n",
            "Step 1080, Training Loss: 0.2987141013145447\n",
            "Step 1080, Training Loss: 0.24547550082206726\n",
            "Step 1090, Training Loss: 0.33334383368492126\n",
            "Step 1090, Training Loss: 0.24132034182548523\n",
            "Step 1100, Training Loss: 0.22359777987003326\n",
            "Step 1100, Training Loss: 0.2568705081939697\n",
            "Step 1110, Training Loss: 0.17932839691638947\n",
            "Step 1110, Training Loss: 0.17572836577892303\n",
            "Step 1120, Training Loss: 0.3126825988292694\n",
            "Step 1120, Training Loss: 0.20594654977321625\n",
            "Step 1130, Training Loss: 0.3283033072948456\n",
            "Step 1130, Training Loss: 0.22852647304534912\n",
            "Step 1140, Training Loss: 0.2871084213256836\n",
            "Step 1140, Training Loss: 0.2203032374382019\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 1523\n",
            "  Batch size = 16\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 1150, Training Loss: 0.359002023935318\n",
            "Step 1150, Training Loss: 0.25723037123680115\n",
            "Step 1160, Training Loss: 0.1403246968984604\n",
            "Step 1160, Training Loss: 0.1825752854347229\n",
            "Step 1170, Training Loss: 0.21830102801322937\n",
            "Step 1170, Training Loss: 0.22307929396629333\n",
            "Step 1180, Training Loss: 0.31243783235549927\n",
            "Step 1180, Training Loss: 0.2078840285539627\n",
            "Step 1190, Training Loss: 0.19021083414554596\n",
            "Step 1190, Training Loss: 0.2575231194496155\n",
            "Step 1200, Training Loss: 0.302498459815979\n",
            "Step 1200, Training Loss: 0.2896731495857239\n",
            "Step 1210, Training Loss: 0.19429998099803925\n",
            "Step 1210, Training Loss: 0.3142681121826172\n",
            "Step 1220, Training Loss: 0.3417590260505676\n",
            "Step 1220, Training Loss: 0.46540477871894836\n",
            "Step 1230, Training Loss: 0.20873622596263885\n",
            "Step 1230, Training Loss: 0.1846817135810852\n",
            "Step 1240, Training Loss: 0.23756879568099976\n",
            "Step 1240, Training Loss: 0.235459104180336\n",
            "Step 1250, Training Loss: 0.12732329964637756\n",
            "Step 1250, Training Loss: 0.26635971665382385\n",
            "Step 1260, Training Loss: 0.20952171087265015\n",
            "Step 1260, Training Loss: 0.2685486674308777\n",
            "Step 1270, Training Loss: 0.20877651870250702\n",
            "Step 1270, Training Loss: 0.2321627140045166\n",
            "Step 1280, Training Loss: 0.17610546946525574\n",
            "Step 1280, Training Loss: 0.2678978145122528\n",
            "Step 1290, Training Loss: 0.20116476714611053\n",
            "Step 1290, Training Loss: 0.2284851223230362\n",
            "Step 1300, Training Loss: 0.16892866790294647\n",
            "Step 1300, Training Loss: 0.19230955839157104\n",
            "Step 1310, Training Loss: 0.2444847822189331\n",
            "Step 1310, Training Loss: 0.4127683937549591\n",
            "Step 1320, Training Loss: 0.33859288692474365\n",
            "Step 1320, Training Loss: 0.19255058467388153\n",
            "Step 1330, Training Loss: 0.16669513285160065\n",
            "Step 1330, Training Loss: 0.3572901785373688\n",
            "Step 1340, Training Loss: 0.2431684285402298\n",
            "Step 1340, Training Loss: 0.28467047214508057\n",
            "Step 1350, Training Loss: 0.22912821173667908\n",
            "Step 1350, Training Loss: 0.24426037073135376\n",
            "Step 1360, Training Loss: 0.162293940782547\n",
            "Step 1360, Training Loss: 0.24137350916862488\n",
            "Step 1370, Training Loss: 0.20895515382289886\n",
            "Step 1370, Training Loss: 0.2373441755771637\n",
            "Step 1380, Training Loss: 0.2142905592918396\n",
            "Step 1380, Training Loss: 0.14405925571918488\n",
            "Step 1390, Training Loss: 0.1785830557346344\n",
            "Step 1390, Training Loss: 0.26326701045036316\n",
            "Step 1400, Training Loss: 0.2199498862028122\n",
            "Step 1400, Training Loss: 0.2049141377210617\n",
            "Step 1410, Training Loss: 0.18996506929397583\n",
            "Step 1410, Training Loss: 0.15991878509521484\n",
            "Step 1420, Training Loss: 0.27600380778312683\n",
            "Step 1420, Training Loss: 0.3643684685230255\n",
            "Step 1430, Training Loss: 0.1665363907814026\n",
            "Step 1430, Training Loss: 0.22901329398155212\n",
            "Step 1440, Training Loss: 0.353185772895813\n",
            "Step 1440, Training Loss: 0.20912130177021027\n",
            "Step 1450, Training Loss: 0.27756792306900024\n",
            "Step 1450, Training Loss: 0.19312886893749237\n",
            "Step 1460, Training Loss: 0.22982247173786163\n",
            "Step 1460, Training Loss: 0.17524443566799164\n",
            "Step 1470, Training Loss: 0.3068455457687378\n",
            "Step 1470, Training Loss: 0.1528674066066742\n",
            "Step 1480, Training Loss: 0.12094984948635101\n",
            "Step 1480, Training Loss: 0.2231394499540329\n",
            "Step 1490, Training Loss: 0.21329936385154724\n",
            "Step 1490, Training Loss: 0.1828792244195938\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Saving model checkpoint to ./results/checkpoint-1500\n",
            "Configuration saved in ./results/checkpoint-1500/config.json\n",
            "Model weights saved in ./results/checkpoint-1500/pytorch_model.bin\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 1500, Training Loss: 0.1757267564535141\n",
            "Step 1500, Training Loss: 0.17719343304634094\n",
            "Step 1510, Training Loss: 0.16712920367717743\n",
            "Step 1510, Training Loss: 0.2006775438785553\n",
            "Step 1520, Training Loss: 0.3253864049911499\n",
            "Step 1520, Training Loss: 0.18577198684215546\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 1523\n",
            "  Batch size = 16\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 1530, Training Loss: 0.20004750788211823\n",
            "Step 1530, Training Loss: 0.15144005417823792\n",
            "Step 1540, Training Loss: 0.1896202266216278\n",
            "Step 1540, Training Loss: 0.1329672932624817\n",
            "Step 1550, Training Loss: 0.20550626516342163\n",
            "Step 1550, Training Loss: 0.27121466398239136\n",
            "Step 1560, Training Loss: 0.2046629786491394\n",
            "Step 1560, Training Loss: 0.24305269122123718\n",
            "Step 1570, Training Loss: 0.13791435956954956\n",
            "Step 1570, Training Loss: 0.21279463171958923\n",
            "Step 1580, Training Loss: 0.09146963059902191\n",
            "Step 1580, Training Loss: 0.2556469142436981\n",
            "Step 1590, Training Loss: 0.1708860546350479\n",
            "Step 1590, Training Loss: 0.20083533227443695\n",
            "Step 1600, Training Loss: 0.25941944122314453\n",
            "Step 1600, Training Loss: 0.1848371922969818\n",
            "Step 1610, Training Loss: 0.13185842335224152\n",
            "Step 1610, Training Loss: 0.3054519593715668\n",
            "Step 1620, Training Loss: 0.310472309589386\n",
            "Step 1620, Training Loss: 0.20664869248867035\n",
            "Step 1630, Training Loss: 0.2534862160682678\n",
            "Step 1630, Training Loss: 0.3104797601699829\n",
            "Step 1640, Training Loss: 0.17988906800746918\n",
            "Step 1640, Training Loss: 0.2096046358346939\n",
            "Step 1650, Training Loss: 0.10531585663557053\n",
            "Step 1650, Training Loss: 0.12365534901618958\n",
            "Step 1660, Training Loss: 0.1688242405653\n",
            "Step 1660, Training Loss: 0.1489463895559311\n",
            "Step 1670, Training Loss: 0.23226623237133026\n",
            "Step 1670, Training Loss: 0.11911781132221222\n",
            "Step 1680, Training Loss: 0.18844622373580933\n",
            "Step 1680, Training Loss: 0.1485801488161087\n",
            "Step 1690, Training Loss: 0.1844474971294403\n",
            "Step 1690, Training Loss: 0.16811276972293854\n",
            "Step 1700, Training Loss: 0.0869651511311531\n",
            "Step 1700, Training Loss: 0.23515263199806213\n",
            "Step 1710, Training Loss: 0.12140719592571259\n",
            "Step 1710, Training Loss: 0.20496581494808197\n",
            "Step 1720, Training Loss: 0.33388763666152954\n",
            "Step 1720, Training Loss: 0.3147491216659546\n",
            "Step 1730, Training Loss: 0.17843791842460632\n",
            "Step 1730, Training Loss: 0.19687218964099884\n",
            "Step 1740, Training Loss: 0.16961711645126343\n",
            "Step 1740, Training Loss: 0.12635616958141327\n",
            "Step 1750, Training Loss: 0.16472476720809937\n",
            "Step 1750, Training Loss: 0.15749263763427734\n",
            "Step 1760, Training Loss: 0.12806661427021027\n",
            "Step 1760, Training Loss: 0.1973380297422409\n",
            "Step 1770, Training Loss: 0.4745860695838928\n",
            "Step 1770, Training Loss: 0.1508338451385498\n",
            "Step 1780, Training Loss: 0.17492608726024628\n",
            "Step 1780, Training Loss: 0.1468559205532074\n",
            "Step 1790, Training Loss: 0.15418392419815063\n",
            "Step 1790, Training Loss: 0.21164043247699738\n",
            "Step 1800, Training Loss: 0.18937966227531433\n",
            "Step 1800, Training Loss: 0.2040184885263443\n",
            "Step 1810, Training Loss: 0.20981059968471527\n",
            "Step 1810, Training Loss: 0.27453356981277466\n",
            "Step 1820, Training Loss: 0.2159373164176941\n",
            "Step 1820, Training Loss: 0.14107337594032288\n",
            "Step 1830, Training Loss: 0.1260896623134613\n",
            "Step 1830, Training Loss: 0.16149944067001343\n",
            "Step 1840, Training Loss: 0.1105678528547287\n",
            "Step 1840, Training Loss: 0.1712886393070221\n",
            "Step 1850, Training Loss: 0.19296084344387054\n",
            "Step 1850, Training Loss: 0.15489768981933594\n",
            "Step 1860, Training Loss: 0.288578599691391\n",
            "Step 1860, Training Loss: 0.15866321325302124\n",
            "Step 1870, Training Loss: 0.18016333878040314\n",
            "Step 1870, Training Loss: 0.18501867353916168\n",
            "Step 1880, Training Loss: 0.16032655537128448\n",
            "Step 1880, Training Loss: 0.29538631439208984\n",
            "Step 1890, Training Loss: 0.3969670534133911\n",
            "Step 1890, Training Loss: 0.15019038319587708\n",
            "Step 1900, Training Loss: 0.19060355424880981\n",
            "Step 1900, Training Loss: 0.2152521014213562\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 1523\n",
            "  Batch size = 16\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 1910, Training Loss: 0.15638737380504608\n",
            "Step 1910, Training Loss: 0.24936723709106445\n",
            "Step 1920, Training Loss: 0.07920542359352112\n",
            "Step 1920, Training Loss: 0.30088934302330017\n",
            "Step 1930, Training Loss: 0.11787249147891998\n",
            "Step 1930, Training Loss: 0.09023907035589218\n",
            "Step 1940, Training Loss: 0.25786304473876953\n",
            "Step 1940, Training Loss: 0.2292027622461319\n",
            "Step 1950, Training Loss: 0.2638795077800751\n",
            "Step 1950, Training Loss: 0.33420613408088684\n",
            "Step 1960, Training Loss: 0.25286436080932617\n",
            "Step 1960, Training Loss: 0.27855533361434937\n",
            "Step 1970, Training Loss: 0.19950276613235474\n",
            "Step 1970, Training Loss: 0.14530204236507416\n",
            "Step 1980, Training Loss: 0.20714740455150604\n",
            "Step 1980, Training Loss: 0.14088428020477295\n",
            "Step 1990, Training Loss: 0.25326451659202576\n",
            "Step 1990, Training Loss: 0.3065458834171295\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Saving model checkpoint to ./results/checkpoint-2000\n",
            "Configuration saved in ./results/checkpoint-2000/config.json\n",
            "Model weights saved in ./results/checkpoint-2000/pytorch_model.bin\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 2000, Training Loss: 0.21437636017799377\n",
            "Step 2000, Training Loss: 0.1146240234375\n",
            "Step 2010, Training Loss: 0.1418096125125885\n",
            "Step 2010, Training Loss: 0.2917560040950775\n",
            "Step 2020, Training Loss: 0.1454983353614807\n",
            "Step 2020, Training Loss: 0.16268640756607056\n",
            "Step 2030, Training Loss: 0.3308833837509155\n",
            "Step 2030, Training Loss: 0.09346107393503189\n",
            "Step 2040, Training Loss: 0.14602512121200562\n",
            "Step 2040, Training Loss: 0.15213806927204132\n",
            "Step 2050, Training Loss: 0.11496569961309433\n",
            "Step 2050, Training Loss: 0.2431124895811081\n",
            "Step 2060, Training Loss: 0.10070762038230896\n",
            "Step 2060, Training Loss: 0.12214555591344833\n",
            "Step 2070, Training Loss: 0.14761300384998322\n",
            "Step 2070, Training Loss: 0.14569346606731415\n",
            "Step 2080, Training Loss: 0.14501450955867767\n",
            "Step 2080, Training Loss: 0.16383454203605652\n",
            "Step 2090, Training Loss: 0.16015741229057312\n",
            "Step 2090, Training Loss: 0.12579140067100525\n",
            "Step 2100, Training Loss: 0.23372866213321686\n",
            "Step 2100, Training Loss: 0.2905563712120056\n",
            "Step 2110, Training Loss: 0.14867430925369263\n",
            "Step 2110, Training Loss: 0.33790943026542664\n",
            "Step 2120, Training Loss: 0.3071509301662445\n",
            "Step 2120, Training Loss: 0.13745766878128052\n",
            "Step 2130, Training Loss: 0.21896322071552277\n",
            "Step 2130, Training Loss: 0.24352432787418365\n",
            "Step 2140, Training Loss: 0.29437118768692017\n",
            "Step 2140, Training Loss: 0.1396629512310028\n",
            "Step 2150, Training Loss: 0.09533368051052094\n",
            "Step 2150, Training Loss: 0.19348040223121643\n",
            "Step 2160, Training Loss: 0.21305671334266663\n",
            "Step 2160, Training Loss: 0.17092706263065338\n",
            "Step 2170, Training Loss: 0.1829311102628708\n",
            "Step 2170, Training Loss: 0.1813681423664093\n",
            "Step 2180, Training Loss: 0.21038289368152618\n",
            "Step 2180, Training Loss: 0.2257671058177948\n",
            "Step 2190, Training Loss: 0.1299833059310913\n",
            "Step 2190, Training Loss: 0.16772888600826263\n",
            "Step 2200, Training Loss: 0.2462397962808609\n",
            "Step 2200, Training Loss: 0.15212500095367432\n",
            "Step 2210, Training Loss: 0.2380632609128952\n",
            "Step 2210, Training Loss: 0.09184692054986954\n",
            "Step 2220, Training Loss: 0.21349328756332397\n",
            "Step 2220, Training Loss: 0.14388054609298706\n",
            "Step 2230, Training Loss: 0.10641907155513763\n",
            "Step 2230, Training Loss: 0.3701823651790619\n",
            "Step 2240, Training Loss: 0.10772392898797989\n",
            "Step 2240, Training Loss: 0.21761295199394226\n",
            "Step 2250, Training Loss: 0.07779952883720398\n",
            "Step 2250, Training Loss: 0.17656320333480835\n",
            "Step 2260, Training Loss: 0.28248316049575806\n",
            "Step 2260, Training Loss: 0.19714750349521637\n",
            "Step 2270, Training Loss: 0.24352708458900452\n",
            "Step 2270, Training Loss: 0.17601116001605988\n",
            "Step 2280, Training Loss: 0.24734999239444733\n",
            "Step 2280, Training Loss: 0.32171693444252014\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 1523\n",
            "  Batch size = 16\n",
            "\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 1523\n",
            "  Batch size = 16\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training completed in 202.08 seconds.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='96' max='96' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [96/96 00:02]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-12-09 18:53:00,191] Trial 0 finished with value: 0.7356905524424617 and parameters: {'learning_rate': 1.7906948615477383e-05, 'batch_size': 8, 'num_train_epochs': 6, 'weight_decay': 0.0010375690924254424}. Best is trial 0 with value: 0.7356905524424617.\n",
            "<ipython-input-12-42465065092b>:2: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  learning_rate = trial.suggest_loguniform(\"learning_rate\", 1e-6, 5e-5)\n",
            "<ipython-input-12-42465065092b>:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  weight_decay = trial.suggest_loguniform(\"weight_decay\", 1e-4, 0.1)\n",
            "PyTorch: setting up devices\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n",
            "***** Running training *****\n",
            "  Num examples = 6091\n",
            "  Num Epochs = 6\n",
            "  Instantaneous batch size per device = 4\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
            "  Gradient Accumulation steps = 2\n",
            "  Total optimization steps = 4566\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting training...\n",
            "Step 0, Training Loss: 0.14326421916484833\n",
            "Step 0, Training Loss: 0.10562647134065628\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='4566' max='4566' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [4566/4566 03:57, Epoch 5/6]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "      <th>F1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>0.146900</td>\n",
              "      <td>0.166629</td>\n",
              "      <td>0.808930</td>\n",
              "      <td>0.886467</td>\n",
              "      <td>0.801669</td>\n",
              "      <td>0.776997</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.093800</td>\n",
              "      <td>0.157157</td>\n",
              "      <td>0.805647</td>\n",
              "      <td>0.895261</td>\n",
              "      <td>0.799743</td>\n",
              "      <td>0.780610</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.107000</td>\n",
              "      <td>0.142038</td>\n",
              "      <td>0.799737</td>\n",
              "      <td>0.910379</td>\n",
              "      <td>0.794608</td>\n",
              "      <td>0.788058</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.096300</td>\n",
              "      <td>0.142649</td>\n",
              "      <td>0.813526</td>\n",
              "      <td>0.902760</td>\n",
              "      <td>0.810013</td>\n",
              "      <td>0.801975</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.068700</td>\n",
              "      <td>0.150417</td>\n",
              "      <td>0.809586</td>\n",
              "      <td>0.882747</td>\n",
              "      <td>0.804236</td>\n",
              "      <td>0.797268</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>0.111200</td>\n",
              "      <td>0.148312</td>\n",
              "      <td>0.806303</td>\n",
              "      <td>0.894582</td>\n",
              "      <td>0.802311</td>\n",
              "      <td>0.800997</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 10, Training Loss: 0.07734455168247223\n",
            "Step 10, Training Loss: 0.12946775555610657\n",
            "Step 20, Training Loss: 0.32544204592704773\n",
            "Step 20, Training Loss: 0.16765724122524261\n",
            "Step 30, Training Loss: 0.13969428837299347\n",
            "Step 30, Training Loss: 0.33575668931007385\n",
            "Step 40, Training Loss: 0.14032435417175293\n",
            "Step 40, Training Loss: 0.13576386868953705\n",
            "Step 50, Training Loss: 0.14088231325149536\n",
            "Step 50, Training Loss: 0.18378564715385437\n",
            "Step 60, Training Loss: 0.18987324833869934\n",
            "Step 60, Training Loss: 0.10261957347393036\n",
            "Step 70, Training Loss: 0.24197931587696075\n",
            "Step 70, Training Loss: 0.24342183768749237\n",
            "Step 80, Training Loss: 0.18386417627334595\n",
            "Step 80, Training Loss: 0.17697365581989288\n",
            "Step 90, Training Loss: 0.09956318885087967\n",
            "Step 90, Training Loss: 0.1894368976354599\n",
            "Step 100, Training Loss: 0.17437605559825897\n",
            "Step 100, Training Loss: 0.09037236124277115\n",
            "Step 110, Training Loss: 0.0966283455491066\n",
            "Step 110, Training Loss: 0.22383913397789001\n",
            "Step 120, Training Loss: 0.06722467392683029\n",
            "Step 120, Training Loss: 0.2366625815629959\n",
            "Step 130, Training Loss: 0.26583194732666016\n",
            "Step 130, Training Loss: 0.1019989475607872\n",
            "Step 140, Training Loss: 0.2806266248226166\n",
            "Step 140, Training Loss: 0.31850114464759827\n",
            "Step 150, Training Loss: 0.16586655378341675\n",
            "Step 150, Training Loss: 0.233644038438797\n",
            "Step 160, Training Loss: 0.08465960621833801\n",
            "Step 160, Training Loss: 0.1502191722393036\n",
            "Step 170, Training Loss: 0.08700810372829437\n",
            "Step 170, Training Loss: 0.0752190575003624\n",
            "Step 180, Training Loss: 0.31981271505355835\n",
            "Step 180, Training Loss: 0.1656334102153778\n",
            "Step 190, Training Loss: 0.5520563125610352\n",
            "Step 190, Training Loss: 0.3365950882434845\n",
            "Step 200, Training Loss: 0.08144231885671616\n",
            "Step 200, Training Loss: 0.20446331799030304\n",
            "Step 210, Training Loss: 0.3277639150619507\n",
            "Step 210, Training Loss: 0.3222423493862152\n",
            "Step 220, Training Loss: 0.2236470878124237\n",
            "Step 220, Training Loss: 0.4374955892562866\n",
            "Step 230, Training Loss: 0.10014084726572037\n",
            "Step 230, Training Loss: 0.09356458485126495\n",
            "Step 240, Training Loss: 0.25353503227233887\n",
            "Step 240, Training Loss: 0.1996435821056366\n",
            "Step 250, Training Loss: 0.23407599329948425\n",
            "Step 250, Training Loss: 0.2504423260688782\n",
            "Step 260, Training Loss: 0.06135769188404083\n",
            "Step 260, Training Loss: 0.12105211615562439\n",
            "Step 270, Training Loss: 0.24773111939430237\n",
            "Step 270, Training Loss: 0.05959639698266983\n",
            "Step 280, Training Loss: 0.24297058582305908\n",
            "Step 280, Training Loss: 0.15381009876728058\n",
            "Step 290, Training Loss: 0.10266365855932236\n",
            "Step 290, Training Loss: 0.09522471576929092\n",
            "Step 300, Training Loss: 0.2674255967140198\n",
            "Step 300, Training Loss: 0.09281957894563675\n",
            "Step 310, Training Loss: 0.05898667871952057\n",
            "Step 310, Training Loss: 0.12715159356594086\n",
            "Step 320, Training Loss: 0.05203596502542496\n",
            "Step 320, Training Loss: 0.1430324912071228\n",
            "Step 330, Training Loss: 0.07104536890983582\n",
            "Step 330, Training Loss: 0.04574427753686905\n",
            "Step 340, Training Loss: 0.16325896978378296\n",
            "Step 340, Training Loss: 0.21889956295490265\n",
            "Step 350, Training Loss: 0.08181212097406387\n",
            "Step 350, Training Loss: 0.2868610918521881\n",
            "Step 360, Training Loss: 0.14600485563278198\n",
            "Step 360, Training Loss: 0.11139193177223206\n",
            "Step 370, Training Loss: 0.054490942507982254\n",
            "Step 370, Training Loss: 0.2491072416305542\n",
            "Step 380, Training Loss: 0.04946934059262276\n",
            "Step 380, Training Loss: 0.1307763010263443\n",
            "Step 390, Training Loss: 0.2164352387189865\n",
            "Step 390, Training Loss: 0.09676442295312881\n",
            "Step 400, Training Loss: 0.05862591415643692\n",
            "Step 400, Training Loss: 0.08380956202745438\n",
            "Step 410, Training Loss: 0.04896264523267746\n",
            "Step 410, Training Loss: 0.5414091348648071\n",
            "Step 420, Training Loss: 0.0738525241613388\n",
            "Step 420, Training Loss: 0.17133943736553192\n",
            "Step 430, Training Loss: 0.3434141278266907\n",
            "Step 430, Training Loss: 0.2886008024215698\n",
            "Step 440, Training Loss: 0.07396195083856583\n",
            "Step 440, Training Loss: 0.4321110248565674\n",
            "Step 450, Training Loss: 0.20110277831554413\n",
            "Step 450, Training Loss: 0.37444183230400085\n",
            "Step 460, Training Loss: 0.05768388509750366\n",
            "Step 460, Training Loss: 0.41449397802352905\n",
            "Step 470, Training Loss: 0.28638553619384766\n",
            "Step 470, Training Loss: 0.08877895772457123\n",
            "Step 480, Training Loss: 0.13693083822727203\n",
            "Step 480, Training Loss: 0.380367636680603\n",
            "Step 490, Training Loss: 0.18043234944343567\n",
            "Step 490, Training Loss: 0.0861639529466629\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Saving model checkpoint to ./results/checkpoint-500\n",
            "Configuration saved in ./results/checkpoint-500/config.json\n",
            "Model weights saved in ./results/checkpoint-500/pytorch_model.bin\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 500, Training Loss: 0.2303774654865265\n",
            "Step 500, Training Loss: 0.054769352078437805\n",
            "Step 510, Training Loss: 0.2906760275363922\n",
            "Step 510, Training Loss: 0.37256065011024475\n",
            "Step 520, Training Loss: 0.2666206955909729\n",
            "Step 520, Training Loss: 0.08545084297657013\n",
            "Step 530, Training Loss: 0.2625182867050171\n",
            "Step 530, Training Loss: 0.0494101457297802\n",
            "Step 540, Training Loss: 0.06332085281610489\n",
            "Step 540, Training Loss: 0.059090755879879\n",
            "Step 550, Training Loss: 0.3141208291053772\n",
            "Step 550, Training Loss: 0.16522829234600067\n",
            "Step 560, Training Loss: 0.3225232660770416\n",
            "Step 560, Training Loss: 0.3861478567123413\n",
            "Step 570, Training Loss: 0.0867391899228096\n",
            "Step 570, Training Loss: 0.132599875330925\n",
            "Step 580, Training Loss: 0.18527719378471375\n",
            "Step 580, Training Loss: 0.06111444905400276\n",
            "Step 590, Training Loss: 0.047861889004707336\n",
            "Step 590, Training Loss: 0.12201108783483505\n",
            "Step 600, Training Loss: 0.16382846236228943\n",
            "Step 600, Training Loss: 0.22600999474525452\n",
            "Step 610, Training Loss: 0.17021077871322632\n",
            "Step 610, Training Loss: 0.26850977540016174\n",
            "Step 620, Training Loss: 0.24428512156009674\n",
            "Step 620, Training Loss: 0.1030903160572052\n",
            "Step 630, Training Loss: 0.04041217267513275\n",
            "Step 630, Training Loss: 0.44892001152038574\n",
            "Step 640, Training Loss: 0.11353486776351929\n",
            "Step 640, Training Loss: 0.21029111742973328\n",
            "Step 650, Training Loss: 0.1787956804037094\n",
            "Step 650, Training Loss: 0.0875495970249176\n",
            "Step 660, Training Loss: 0.1692679077386856\n",
            "Step 660, Training Loss: 0.06238873302936554\n",
            "Step 670, Training Loss: 0.06235888972878456\n",
            "Step 670, Training Loss: 0.17371712625026703\n",
            "Step 680, Training Loss: 0.19303490221500397\n",
            "Step 680, Training Loss: 0.24843063950538635\n",
            "Step 690, Training Loss: 0.23286139965057373\n",
            "Step 690, Training Loss: 0.14433208107948303\n",
            "Step 700, Training Loss: 0.10955611616373062\n",
            "Step 700, Training Loss: 0.1794455349445343\n",
            "Step 710, Training Loss: 0.1575070172548294\n",
            "Step 710, Training Loss: 0.23358899354934692\n",
            "Step 720, Training Loss: 0.3391108810901642\n",
            "Step 720, Training Loss: 0.27064576745033264\n",
            "Step 730, Training Loss: 0.11511079967021942\n",
            "Step 730, Training Loss: 0.04483114928007126\n",
            "Step 740, Training Loss: 0.04446490481495857\n",
            "Step 740, Training Loss: 0.07416781038045883\n",
            "Step 750, Training Loss: 0.07440835237503052\n",
            "Step 750, Training Loss: 0.4532749056816101\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 1523\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 760, Training Loss: 0.05176761373877525\n",
            "Step 760, Training Loss: 0.1161973774433136\n",
            "Step 770, Training Loss: 0.04607198387384415\n",
            "Step 770, Training Loss: 0.26071056723594666\n",
            "Step 780, Training Loss: 0.05176098272204399\n",
            "Step 780, Training Loss: 0.04106460511684418\n",
            "Step 790, Training Loss: 0.03391767293214798\n",
            "Step 790, Training Loss: 0.04643239453434944\n",
            "Step 800, Training Loss: 0.16799400746822357\n",
            "Step 800, Training Loss: 0.28954753279685974\n",
            "Step 810, Training Loss: 0.12242302298545837\n",
            "Step 810, Training Loss: 0.27240997552871704\n",
            "Step 820, Training Loss: 0.09037602692842484\n",
            "Step 820, Training Loss: 0.052247460931539536\n",
            "Step 830, Training Loss: 0.05241863429546356\n",
            "Step 830, Training Loss: 0.19013334810733795\n",
            "Step 840, Training Loss: 0.1561206579208374\n",
            "Step 840, Training Loss: 0.05680171400308609\n",
            "Step 850, Training Loss: 0.057710204273462296\n",
            "Step 850, Training Loss: 0.16247409582138062\n",
            "Step 860, Training Loss: 0.07655034214258194\n",
            "Step 860, Training Loss: 0.12747211754322052\n",
            "Step 870, Training Loss: 0.13178355991840363\n",
            "Step 870, Training Loss: 0.4338001608848572\n",
            "Step 880, Training Loss: 0.052166957408189774\n",
            "Step 880, Training Loss: 0.13277655839920044\n",
            "Step 890, Training Loss: 0.14700181782245636\n",
            "Step 890, Training Loss: 0.3303162157535553\n",
            "Step 900, Training Loss: 0.24423852562904358\n",
            "Step 900, Training Loss: 0.03820560872554779\n",
            "Step 910, Training Loss: 0.3009488582611084\n",
            "Step 910, Training Loss: 0.21535295248031616\n",
            "Step 920, Training Loss: 0.38178834319114685\n",
            "Step 920, Training Loss: 0.2733631730079651\n",
            "Step 930, Training Loss: 0.04162243753671646\n",
            "Step 930, Training Loss: 0.07644283771514893\n",
            "Step 940, Training Loss: 0.03487497195601463\n",
            "Step 940, Training Loss: 0.12932637333869934\n",
            "Step 950, Training Loss: 0.19831492006778717\n",
            "Step 950, Training Loss: 0.2361571490764618\n",
            "Step 960, Training Loss: 0.2738925814628601\n",
            "Step 960, Training Loss: 0.047272939234972\n",
            "Step 970, Training Loss: 0.09446994215250015\n",
            "Step 970, Training Loss: 0.06621305644512177\n",
            "Step 980, Training Loss: 0.21749961376190186\n",
            "Step 980, Training Loss: 0.08125446736812592\n",
            "Step 990, Training Loss: 0.3451465666294098\n",
            "Step 990, Training Loss: 0.3021431565284729\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Saving model checkpoint to ./results/checkpoint-1000\n",
            "Configuration saved in ./results/checkpoint-1000/config.json\n",
            "Model weights saved in ./results/checkpoint-1000/pytorch_model.bin\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 1000, Training Loss: 0.0404057502746582\n",
            "Step 1000, Training Loss: 0.05256085470318794\n",
            "Step 1010, Training Loss: 0.22115488350391388\n",
            "Step 1010, Training Loss: 0.03288203850388527\n",
            "Step 1020, Training Loss: 0.11586186289787292\n",
            "Step 1020, Training Loss: 0.04235496744513512\n",
            "Step 1030, Training Loss: 0.24956810474395752\n",
            "Step 1030, Training Loss: 0.2008669227361679\n",
            "Step 1040, Training Loss: 0.029141638427972794\n",
            "Step 1040, Training Loss: 0.3228474259376526\n",
            "Step 1050, Training Loss: 0.12046375870704651\n",
            "Step 1050, Training Loss: 0.2761790454387665\n",
            "Step 1060, Training Loss: 0.10905276238918304\n",
            "Step 1060, Training Loss: 0.02720189467072487\n",
            "Step 1070, Training Loss: 0.04351320490241051\n",
            "Step 1070, Training Loss: 0.13520579040050507\n",
            "Step 1080, Training Loss: 0.04314406216144562\n",
            "Step 1080, Training Loss: 0.11873515695333481\n",
            "Step 1090, Training Loss: 0.044166747480630875\n",
            "Step 1090, Training Loss: 0.027005767449736595\n",
            "Step 1100, Training Loss: 0.0409831777215004\n",
            "Step 1100, Training Loss: 0.04124048352241516\n",
            "Step 1110, Training Loss: 0.09385700523853302\n",
            "Step 1110, Training Loss: 0.04171384871006012\n",
            "Step 1120, Training Loss: 0.050997544080019\n",
            "Step 1120, Training Loss: 0.10475867241621017\n",
            "Step 1130, Training Loss: 0.05059249699115753\n",
            "Step 1130, Training Loss: 0.07954297959804535\n",
            "Step 1140, Training Loss: 0.030714645981788635\n",
            "Step 1140, Training Loss: 0.03623066842556\n",
            "Step 1150, Training Loss: 0.07323522865772247\n",
            "Step 1150, Training Loss: 0.25656890869140625\n",
            "Step 1160, Training Loss: 0.2257353961467743\n",
            "Step 1160, Training Loss: 0.10626427084207535\n",
            "Step 1170, Training Loss: 0.42095765471458435\n",
            "Step 1170, Training Loss: 0.22823460400104523\n",
            "Step 1180, Training Loss: 0.059738654643297195\n",
            "Step 1180, Training Loss: 0.13679179549217224\n",
            "Step 1190, Training Loss: 0.17108331620693207\n",
            "Step 1190, Training Loss: 0.18632151186466217\n",
            "Step 1200, Training Loss: 0.15085278451442719\n",
            "Step 1200, Training Loss: 0.08040079474449158\n",
            "Step 1210, Training Loss: 0.14058427512645721\n",
            "Step 1210, Training Loss: 0.07557902485132217\n",
            "Step 1220, Training Loss: 0.17867104709148407\n",
            "Step 1220, Training Loss: 0.36583954095840454\n",
            "Step 1230, Training Loss: 0.3554443120956421\n",
            "Step 1230, Training Loss: 0.10074259340763092\n",
            "Step 1240, Training Loss: 0.4183658957481384\n",
            "Step 1240, Training Loss: 0.24311761558055878\n",
            "Step 1250, Training Loss: 0.02963077649474144\n",
            "Step 1250, Training Loss: 0.45205438137054443\n",
            "Step 1260, Training Loss: 0.06603723019361496\n",
            "Step 1260, Training Loss: 0.045782797038555145\n",
            "Step 1270, Training Loss: 0.3383350670337677\n",
            "Step 1270, Training Loss: 0.028892774134874344\n",
            "Step 1280, Training Loss: 0.2658059597015381\n",
            "Step 1280, Training Loss: 0.06362410634756088\n",
            "Step 1290, Training Loss: 0.057809002697467804\n",
            "Step 1290, Training Loss: 0.13828222453594208\n",
            "Step 1300, Training Loss: 0.04257063567638397\n",
            "Step 1300, Training Loss: 0.03696135804057121\n",
            "Step 1310, Training Loss: 0.45966637134552\n",
            "Step 1310, Training Loss: 0.3309183716773987\n",
            "Step 1320, Training Loss: 0.045692674815654755\n",
            "Step 1320, Training Loss: 0.08489814400672913\n",
            "Step 1330, Training Loss: 0.19469814002513885\n",
            "Step 1330, Training Loss: 0.10419604182243347\n",
            "Step 1340, Training Loss: 0.0333968922495842\n",
            "Step 1340, Training Loss: 0.023688960820436478\n",
            "Step 1350, Training Loss: 0.10765949636697769\n",
            "Step 1350, Training Loss: 0.10531004518270493\n",
            "Step 1360, Training Loss: 0.4371258020401001\n",
            "Step 1360, Training Loss: 0.22853749990463257\n",
            "Step 1370, Training Loss: 0.1772741973400116\n",
            "Step 1370, Training Loss: 0.23197686672210693\n",
            "Step 1380, Training Loss: 0.26064103841781616\n",
            "Step 1380, Training Loss: 0.25277021527290344\n",
            "Step 1390, Training Loss: 0.47882023453712463\n",
            "Step 1390, Training Loss: 0.12019336968660355\n",
            "Step 1400, Training Loss: 0.24615895748138428\n",
            "Step 1400, Training Loss: 0.10020416229963303\n",
            "Step 1410, Training Loss: 0.03164175897836685\n",
            "Step 1410, Training Loss: 0.038007814437150955\n",
            "Step 1420, Training Loss: 0.031582653522491455\n",
            "Step 1420, Training Loss: 0.032242171466350555\n",
            "Step 1430, Training Loss: 0.30056771636009216\n",
            "Step 1430, Training Loss: 0.1796697974205017\n",
            "Step 1440, Training Loss: 0.07534095644950867\n",
            "Step 1440, Training Loss: 0.0269149336963892\n",
            "Step 1450, Training Loss: 0.03339320048689842\n",
            "Step 1450, Training Loss: 0.029345309361815453\n",
            "Step 1460, Training Loss: 0.037202056497335434\n",
            "Step 1460, Training Loss: 0.027436228469014168\n",
            "Step 1470, Training Loss: 0.037596940994262695\n",
            "Step 1470, Training Loss: 0.2546094059944153\n",
            "Step 1480, Training Loss: 0.030562158674001694\n",
            "Step 1480, Training Loss: 0.16570067405700684\n",
            "Step 1490, Training Loss: 0.10808710008859634\n",
            "Step 1490, Training Loss: 0.28702688217163086\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Saving model checkpoint to ./results/checkpoint-1500\n",
            "Configuration saved in ./results/checkpoint-1500/config.json\n",
            "Model weights saved in ./results/checkpoint-1500/pytorch_model.bin\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 1500, Training Loss: 0.1625397503376007\n",
            "Step 1500, Training Loss: 0.12236348539590836\n",
            "Step 1510, Training Loss: 0.03526878356933594\n",
            "Step 1510, Training Loss: 0.0325862355530262\n",
            "Step 1520, Training Loss: 0.11412367969751358\n",
            "Step 1520, Training Loss: 0.0415964275598526\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 1523\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 1530, Training Loss: 0.02980783022940159\n",
            "Step 1530, Training Loss: 0.14545956254005432\n",
            "Step 1540, Training Loss: 0.03721897304058075\n",
            "Step 1540, Training Loss: 0.05788280442357063\n",
            "Step 1550, Training Loss: 0.03369784355163574\n",
            "Step 1550, Training Loss: 0.1843004673719406\n",
            "Step 1560, Training Loss: 0.04034079611301422\n",
            "Step 1560, Training Loss: 0.23641876876354218\n",
            "Step 1570, Training Loss: 0.02644430473446846\n",
            "Step 1570, Training Loss: 0.02547709085047245\n",
            "Step 1580, Training Loss: 0.2704838216304779\n",
            "Step 1580, Training Loss: 0.037539444863796234\n",
            "Step 1590, Training Loss: 0.05935530364513397\n",
            "Step 1590, Training Loss: 0.02719780057668686\n",
            "Step 1600, Training Loss: 0.06516548246145248\n",
            "Step 1600, Training Loss: 0.09019380062818527\n",
            "Step 1610, Training Loss: 0.16152524948120117\n",
            "Step 1610, Training Loss: 0.29044604301452637\n",
            "Step 1620, Training Loss: 0.10764484107494354\n",
            "Step 1620, Training Loss: 0.10344813764095306\n",
            "Step 1630, Training Loss: 0.04748905077576637\n",
            "Step 1630, Training Loss: 0.5407842993736267\n",
            "Step 1640, Training Loss: 0.024529797956347466\n",
            "Step 1640, Training Loss: 0.15804734826087952\n",
            "Step 1650, Training Loss: 0.08691433072090149\n",
            "Step 1650, Training Loss: 0.2692320942878723\n",
            "Step 1660, Training Loss: 0.03639019653201103\n",
            "Step 1660, Training Loss: 0.028837375342845917\n",
            "Step 1670, Training Loss: 0.06256910413503647\n",
            "Step 1670, Training Loss: 0.18614523112773895\n",
            "Step 1680, Training Loss: 0.030034851282835007\n",
            "Step 1680, Training Loss: 0.02753576636314392\n",
            "Step 1690, Training Loss: 0.02670244686305523\n",
            "Step 1690, Training Loss: 0.15589599311351776\n",
            "Step 1700, Training Loss: 0.029661143198609352\n",
            "Step 1700, Training Loss: 0.03225993365049362\n",
            "Step 1710, Training Loss: 0.02363910898566246\n",
            "Step 1710, Training Loss: 0.027411187067627907\n",
            "Step 1720, Training Loss: 0.05922720581293106\n",
            "Step 1720, Training Loss: 0.36863672733306885\n",
            "Step 1730, Training Loss: 0.10939586907625198\n",
            "Step 1730, Training Loss: 0.21245060861110687\n",
            "Step 1740, Training Loss: 0.04356415942311287\n",
            "Step 1740, Training Loss: 0.028615297749638557\n",
            "Step 1750, Training Loss: 0.2519457936286926\n",
            "Step 1750, Training Loss: 0.4008558988571167\n",
            "Step 1760, Training Loss: 0.13121846318244934\n",
            "Step 1760, Training Loss: 0.47884446382522583\n",
            "Step 1770, Training Loss: 0.0900936871767044\n",
            "Step 1770, Training Loss: 0.2984940707683563\n",
            "Step 1780, Training Loss: 0.12497350573539734\n",
            "Step 1780, Training Loss: 0.02471688948571682\n",
            "Step 1790, Training Loss: 0.05445463955402374\n",
            "Step 1790, Training Loss: 0.22377897799015045\n",
            "Step 1800, Training Loss: 0.25892192125320435\n",
            "Step 1800, Training Loss: 0.08339090645313263\n",
            "Step 1810, Training Loss: 0.2581651210784912\n",
            "Step 1810, Training Loss: 0.20158244669437408\n",
            "Step 1820, Training Loss: 0.08285114169120789\n",
            "Step 1820, Training Loss: 0.043259184807538986\n",
            "Step 1830, Training Loss: 0.3923954367637634\n",
            "Step 1830, Training Loss: 0.03956308960914612\n",
            "Step 1840, Training Loss: 0.12553055584430695\n",
            "Step 1840, Training Loss: 0.11540057510137558\n",
            "Step 1850, Training Loss: 0.03142709285020828\n",
            "Step 1850, Training Loss: 0.023706385865807533\n",
            "Step 1860, Training Loss: 0.23385751247406006\n",
            "Step 1860, Training Loss: 0.31975066661834717\n",
            "Step 1870, Training Loss: 0.14808136224746704\n",
            "Step 1870, Training Loss: 0.10931502282619476\n",
            "Step 1880, Training Loss: 0.028654690831899643\n",
            "Step 1880, Training Loss: 0.08293995261192322\n",
            "Step 1890, Training Loss: 0.2224387228488922\n",
            "Step 1890, Training Loss: 0.024511612951755524\n",
            "Step 1900, Training Loss: 0.23041746020317078\n",
            "Step 1900, Training Loss: 0.13939841091632843\n",
            "Step 1910, Training Loss: 0.03418700397014618\n",
            "Step 1910, Training Loss: 0.14454679191112518\n",
            "Step 1920, Training Loss: 0.025035608559846878\n",
            "Step 1920, Training Loss: 0.04426952078938484\n",
            "Step 1930, Training Loss: 0.020915020257234573\n",
            "Step 1930, Training Loss: 0.02587079629302025\n",
            "Step 1940, Training Loss: 0.02282143570482731\n",
            "Step 1940, Training Loss: 0.02876240946352482\n",
            "Step 1950, Training Loss: 0.5194546580314636\n",
            "Step 1950, Training Loss: 0.06136278808116913\n",
            "Step 1960, Training Loss: 0.02311721257865429\n",
            "Step 1960, Training Loss: 0.1360156536102295\n",
            "Step 1970, Training Loss: 0.16990788280963898\n",
            "Step 1970, Training Loss: 0.055726274847984314\n",
            "Step 1980, Training Loss: 0.02513226494193077\n",
            "Step 1980, Training Loss: 0.15602222084999084\n",
            "Step 1990, Training Loss: 0.02536151185631752\n",
            "Step 1990, Training Loss: 0.14464811980724335\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Saving model checkpoint to ./results/checkpoint-2000\n",
            "Configuration saved in ./results/checkpoint-2000/config.json\n",
            "Model weights saved in ./results/checkpoint-2000/pytorch_model.bin\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 2000, Training Loss: 0.22970421612262726\n",
            "Step 2000, Training Loss: 0.05999518185853958\n",
            "Step 2010, Training Loss: 0.050169024616479874\n",
            "Step 2010, Training Loss: 0.1767527461051941\n",
            "Step 2020, Training Loss: 0.10904356837272644\n",
            "Step 2020, Training Loss: 0.023531993851065636\n",
            "Step 2030, Training Loss: 0.055663127452135086\n",
            "Step 2030, Training Loss: 0.40049678087234497\n",
            "Step 2040, Training Loss: 0.028820475563406944\n",
            "Step 2040, Training Loss: 0.08850599825382233\n",
            "Step 2050, Training Loss: 0.02813650853931904\n",
            "Step 2050, Training Loss: 0.020637551322579384\n",
            "Step 2060, Training Loss: 0.12023541331291199\n",
            "Step 2060, Training Loss: 0.27581340074539185\n",
            "Step 2070, Training Loss: 0.027202466502785683\n",
            "Step 2070, Training Loss: 0.22400860488414764\n",
            "Step 2080, Training Loss: 0.1582065224647522\n",
            "Step 2080, Training Loss: 0.33394965529441833\n",
            "Step 2090, Training Loss: 0.3621886968612671\n",
            "Step 2090, Training Loss: 0.03345772251486778\n",
            "Step 2100, Training Loss: 0.10505519807338715\n",
            "Step 2100, Training Loss: 0.050339505076408386\n",
            "Step 2110, Training Loss: 0.04799823835492134\n",
            "Step 2110, Training Loss: 0.24951758980751038\n",
            "Step 2120, Training Loss: 0.02753535471856594\n",
            "Step 2120, Training Loss: 0.028431225568056107\n",
            "Step 2130, Training Loss: 0.24592694640159607\n",
            "Step 2130, Training Loss: 0.03045051358640194\n",
            "Step 2140, Training Loss: 0.1482478678226471\n",
            "Step 2140, Training Loss: 0.025815671309828758\n",
            "Step 2150, Training Loss: 0.027335485443472862\n",
            "Step 2150, Training Loss: 0.024852536618709564\n",
            "Step 2160, Training Loss: 0.12873929738998413\n",
            "Step 2160, Training Loss: 0.38133540749549866\n",
            "Step 2170, Training Loss: 0.34795668721199036\n",
            "Step 2170, Training Loss: 0.03403646871447563\n",
            "Step 2180, Training Loss: 0.25135838985443115\n",
            "Step 2180, Training Loss: 0.030773993581533432\n",
            "Step 2190, Training Loss: 0.026693392544984818\n",
            "Step 2190, Training Loss: 0.024873536080121994\n",
            "Step 2200, Training Loss: 0.037513285875320435\n",
            "Step 2200, Training Loss: 0.04613615944981575\n",
            "Step 2210, Training Loss: 0.24727031588554382\n",
            "Step 2210, Training Loss: 0.04961200803518295\n",
            "Step 2220, Training Loss: 0.436884343624115\n",
            "Step 2220, Training Loss: 0.11160025745630264\n",
            "Step 2230, Training Loss: 0.02646712027490139\n",
            "Step 2230, Training Loss: 0.030707867816090584\n",
            "Step 2240, Training Loss: 0.029851848259568214\n",
            "Step 2240, Training Loss: 0.15694577991962433\n",
            "Step 2250, Training Loss: 0.09252656996250153\n",
            "Step 2250, Training Loss: 0.13743281364440918\n",
            "Step 2260, Training Loss: 0.024534903466701508\n",
            "Step 2260, Training Loss: 0.22896893322467804\n",
            "Step 2270, Training Loss: 0.17330698668956757\n",
            "Step 2270, Training Loss: 0.030747875571250916\n",
            "Step 2280, Training Loss: 0.12389199435710907\n",
            "Step 2280, Training Loss: 0.10122843086719513\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 1523\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 2290, Training Loss: 0.10107298195362091\n",
            "Step 2290, Training Loss: 0.16172932088375092\n",
            "Step 2300, Training Loss: 0.020117977634072304\n",
            "Step 2300, Training Loss: 0.026680873706936836\n",
            "Step 2310, Training Loss: 0.15700402855873108\n",
            "Step 2310, Training Loss: 0.025708723813295364\n",
            "Step 2320, Training Loss: 0.031633567065000534\n",
            "Step 2320, Training Loss: 0.09252514690160751\n",
            "Step 2330, Training Loss: 0.06833038479089737\n",
            "Step 2330, Training Loss: 0.04714027792215347\n",
            "Step 2340, Training Loss: 0.07651342451572418\n",
            "Step 2340, Training Loss: 0.09243650734424591\n",
            "Step 2350, Training Loss: 0.0337657555937767\n",
            "Step 2350, Training Loss: 0.021843116730451584\n",
            "Step 2360, Training Loss: 0.022784266620874405\n",
            "Step 2360, Training Loss: 0.0253144558519125\n",
            "Step 2370, Training Loss: 0.08169397711753845\n",
            "Step 2370, Training Loss: 0.3780265152454376\n",
            "Step 2380, Training Loss: 0.2822771668434143\n",
            "Step 2380, Training Loss: 0.02177789993584156\n",
            "Step 2390, Training Loss: 0.2507387101650238\n",
            "Step 2390, Training Loss: 0.24889065325260162\n",
            "Step 2400, Training Loss: 0.10406412184238434\n",
            "Step 2400, Training Loss: 0.020176341757178307\n",
            "Step 2410, Training Loss: 0.24107426404953003\n",
            "Step 2410, Training Loss: 0.12668156623840332\n",
            "Step 2420, Training Loss: 0.046021685004234314\n",
            "Step 2420, Training Loss: 0.0514870323240757\n",
            "Step 2430, Training Loss: 0.17032687366008759\n",
            "Step 2430, Training Loss: 0.02720719762146473\n",
            "Step 2440, Training Loss: 0.05318308621644974\n",
            "Step 2440, Training Loss: 0.02230636216700077\n",
            "Step 2450, Training Loss: 0.025748157873749733\n",
            "Step 2450, Training Loss: 0.03672400861978531\n",
            "Step 2460, Training Loss: 0.08725278824567795\n",
            "Step 2460, Training Loss: 0.11005523800849915\n",
            "Step 2470, Training Loss: 0.46707281470298767\n",
            "Step 2470, Training Loss: 0.024852795526385307\n",
            "Step 2480, Training Loss: 0.04407356679439545\n",
            "Step 2480, Training Loss: 0.028263414278626442\n",
            "Step 2490, Training Loss: 0.2631319761276245\n",
            "Step 2490, Training Loss: 0.02362539805471897\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Saving model checkpoint to ./results/checkpoint-2500\n",
            "Configuration saved in ./results/checkpoint-2500/config.json\n",
            "Model weights saved in ./results/checkpoint-2500/pytorch_model.bin\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 2500, Training Loss: 0.21816202998161316\n",
            "Step 2500, Training Loss: 0.028655515983700752\n",
            "Step 2510, Training Loss: 0.023479772731661797\n",
            "Step 2510, Training Loss: 0.026320183649659157\n",
            "Step 2520, Training Loss: 0.03134576976299286\n",
            "Step 2520, Training Loss: 0.25965210795402527\n",
            "Step 2530, Training Loss: 0.15374204516410828\n",
            "Step 2530, Training Loss: 0.02456958219408989\n",
            "Step 2540, Training Loss: 0.04581190273165703\n",
            "Step 2540, Training Loss: 0.020652486011385918\n",
            "Step 2550, Training Loss: 0.35178035497665405\n",
            "Step 2550, Training Loss: 0.10255884379148483\n",
            "Step 2560, Training Loss: 0.1585402935743332\n",
            "Step 2560, Training Loss: 0.04674503952264786\n",
            "Step 2570, Training Loss: 0.22301943600177765\n",
            "Step 2570, Training Loss: 0.047529954463243484\n",
            "Step 2580, Training Loss: 0.13701286911964417\n",
            "Step 2580, Training Loss: 0.09060736745595932\n",
            "Step 2590, Training Loss: 0.04625089466571808\n",
            "Step 2590, Training Loss: 0.13686318695545197\n",
            "Step 2600, Training Loss: 0.21549221873283386\n",
            "Step 2600, Training Loss: 0.2728196680545807\n",
            "Step 2610, Training Loss: 0.02048501931130886\n",
            "Step 2610, Training Loss: 0.1727394163608551\n",
            "Step 2620, Training Loss: 0.12331975251436234\n",
            "Step 2620, Training Loss: 0.4011844992637634\n",
            "Step 2630, Training Loss: 0.07457898557186127\n",
            "Step 2630, Training Loss: 0.028015168383717537\n",
            "Step 2640, Training Loss: 0.1960594356060028\n",
            "Step 2640, Training Loss: 0.10096164047718048\n",
            "Step 2650, Training Loss: 0.1907835751771927\n",
            "Step 2650, Training Loss: 0.06259331107139587\n",
            "Step 2660, Training Loss: 0.02943936549127102\n",
            "Step 2660, Training Loss: 0.01973537914454937\n",
            "Step 2670, Training Loss: 0.02670561894774437\n",
            "Step 2670, Training Loss: 0.01608654111623764\n",
            "Step 2680, Training Loss: 0.02835141494870186\n",
            "Step 2680, Training Loss: 0.024607470259070396\n",
            "Step 2690, Training Loss: 0.017796548083424568\n",
            "Step 2690, Training Loss: 0.21507108211517334\n",
            "Step 2700, Training Loss: 0.027346696704626083\n",
            "Step 2700, Training Loss: 0.01729481667280197\n",
            "Step 2710, Training Loss: 0.32148152589797974\n",
            "Step 2710, Training Loss: 0.5713726282119751\n",
            "Step 2720, Training Loss: 0.018593672662973404\n",
            "Step 2720, Training Loss: 0.05319003760814667\n",
            "Step 2730, Training Loss: 0.02881636470556259\n",
            "Step 2730, Training Loss: 0.022028107196092606\n",
            "Step 2740, Training Loss: 0.4854201376438141\n",
            "Step 2740, Training Loss: 0.019247330725193024\n",
            "Step 2750, Training Loss: 0.15859730541706085\n",
            "Step 2750, Training Loss: 0.49743109941482544\n",
            "Step 2760, Training Loss: 0.035633739084005356\n",
            "Step 2760, Training Loss: 0.024408912286162376\n",
            "Step 2770, Training Loss: 0.027130737900733948\n",
            "Step 2770, Training Loss: 0.026687638834118843\n",
            "Step 2780, Training Loss: 0.09249570220708847\n",
            "Step 2780, Training Loss: 0.04149074852466583\n",
            "Step 2790, Training Loss: 0.04425964504480362\n",
            "Step 2790, Training Loss: 0.24070602655410767\n",
            "Step 2800, Training Loss: 0.02400228939950466\n",
            "Step 2800, Training Loss: 0.14680922031402588\n",
            "Step 2810, Training Loss: 0.4306715130805969\n",
            "Step 2810, Training Loss: 0.026864461600780487\n",
            "Step 2820, Training Loss: 0.08729968219995499\n",
            "Step 2820, Training Loss: 0.016132786870002747\n",
            "Step 2830, Training Loss: 0.14160999655723572\n",
            "Step 2830, Training Loss: 0.01803498901426792\n",
            "Step 2840, Training Loss: 0.023116974160075188\n",
            "Step 2840, Training Loss: 0.2659253180027008\n",
            "Step 2850, Training Loss: 0.02350955456495285\n",
            "Step 2850, Training Loss: 0.05924577638506889\n",
            "Step 2860, Training Loss: 0.023402906954288483\n",
            "Step 2860, Training Loss: 0.02360696531832218\n",
            "Step 2870, Training Loss: 0.022241530939936638\n",
            "Step 2870, Training Loss: 0.03704135864973068\n",
            "Step 2880, Training Loss: 0.06447352468967438\n",
            "Step 2880, Training Loss: 0.02522163838148117\n",
            "Step 2890, Training Loss: 0.2625983655452728\n",
            "Step 2890, Training Loss: 0.10574087500572205\n",
            "Step 2900, Training Loss: 0.018697449937462807\n",
            "Step 2900, Training Loss: 0.022503705695271492\n",
            "Step 2910, Training Loss: 0.020213712006807327\n",
            "Step 2910, Training Loss: 0.22141189873218536\n",
            "Step 2920, Training Loss: 0.022132840007543564\n",
            "Step 2920, Training Loss: 0.04312850162386894\n",
            "Step 2930, Training Loss: 0.04085211828351021\n",
            "Step 2930, Training Loss: 0.20139454305171967\n",
            "Step 2940, Training Loss: 0.047300998121500015\n",
            "Step 2940, Training Loss: 0.3929930031299591\n",
            "Step 2950, Training Loss: 0.21413739025592804\n",
            "Step 2950, Training Loss: 0.2089737504720688\n",
            "Step 2960, Training Loss: 0.19842897355556488\n",
            "Step 2960, Training Loss: 0.15844349563121796\n",
            "Step 2970, Training Loss: 0.02370051108300686\n",
            "Step 2970, Training Loss: 0.09348924458026886\n",
            "Step 2980, Training Loss: 0.32386133074760437\n",
            "Step 2980, Training Loss: 0.03978816419839859\n",
            "Step 2990, Training Loss: 0.12887071073055267\n",
            "Step 2990, Training Loss: 0.1685979813337326\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Saving model checkpoint to ./results/checkpoint-3000\n",
            "Configuration saved in ./results/checkpoint-3000/config.json\n",
            "Model weights saved in ./results/checkpoint-3000/pytorch_model.bin\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 3000, Training Loss: 0.09188710153102875\n",
            "Step 3000, Training Loss: 0.2300737500190735\n",
            "Step 3010, Training Loss: 0.2839421033859253\n",
            "Step 3010, Training Loss: 0.14996978640556335\n",
            "Step 3020, Training Loss: 0.2562379837036133\n",
            "Step 3020, Training Loss: 0.09270841628313065\n",
            "Step 3030, Training Loss: 0.03846243768930435\n",
            "Step 3030, Training Loss: 0.02490697056055069\n",
            "Step 3040, Training Loss: 0.020536329597234726\n",
            "Step 3040, Training Loss: 0.15109951794147491\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 1523\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 3050, Training Loss: 0.031126495450735092\n",
            "Step 3050, Training Loss: 0.021520378068089485\n",
            "Step 3060, Training Loss: 0.041328199207782745\n",
            "Step 3060, Training Loss: 0.023746496066451073\n",
            "Step 3070, Training Loss: 0.01896974816918373\n",
            "Step 3070, Training Loss: 0.31587016582489014\n",
            "Step 3080, Training Loss: 0.4179461598396301\n",
            "Step 3080, Training Loss: 0.13848312199115753\n",
            "Step 3090, Training Loss: 0.043648526072502136\n",
            "Step 3090, Training Loss: 0.03922390565276146\n",
            "Step 3100, Training Loss: 0.07152024656534195\n",
            "Step 3100, Training Loss: 0.39558976888656616\n",
            "Step 3110, Training Loss: 0.09502307325601578\n",
            "Step 3110, Training Loss: 0.027711035683751106\n",
            "Step 3120, Training Loss: 0.016303829848766327\n",
            "Step 3120, Training Loss: 0.09645219147205353\n",
            "Step 3130, Training Loss: 0.02991831861436367\n",
            "Step 3130, Training Loss: 0.12020983546972275\n",
            "Step 3140, Training Loss: 0.021627746522426605\n",
            "Step 3140, Training Loss: 0.25238877534866333\n",
            "Step 3150, Training Loss: 0.01648951880633831\n",
            "Step 3150, Training Loss: 0.13528060913085938\n",
            "Step 3160, Training Loss: 0.07844561338424683\n",
            "Step 3160, Training Loss: 0.026171015575528145\n",
            "Step 3170, Training Loss: 0.07505479454994202\n",
            "Step 3170, Training Loss: 0.17230869829654694\n",
            "Step 3180, Training Loss: 0.01899595372378826\n",
            "Step 3180, Training Loss: 0.01543930359184742\n",
            "Step 3190, Training Loss: 0.12543560564517975\n",
            "Step 3190, Training Loss: 0.016686229035258293\n",
            "Step 3200, Training Loss: 0.2788749635219574\n",
            "Step 3200, Training Loss: 0.15690356492996216\n",
            "Step 3210, Training Loss: 0.09913673251867294\n",
            "Step 3210, Training Loss: 0.28745579719543457\n",
            "Step 3220, Training Loss: 0.03766696900129318\n",
            "Step 3220, Training Loss: 0.2627474367618561\n",
            "Step 3230, Training Loss: 0.021234950050711632\n",
            "Step 3230, Training Loss: 0.02009720169007778\n",
            "Step 3240, Training Loss: 0.44375693798065186\n",
            "Step 3240, Training Loss: 0.02329677902162075\n",
            "Step 3250, Training Loss: 0.1165972501039505\n",
            "Step 3250, Training Loss: 0.4077557325363159\n",
            "Step 3260, Training Loss: 0.021740499883890152\n",
            "Step 3260, Training Loss: 0.09556160867214203\n",
            "Step 3270, Training Loss: 0.36907199025154114\n",
            "Step 3270, Training Loss: 0.1770697683095932\n",
            "Step 3280, Training Loss: 0.016642993316054344\n",
            "Step 3280, Training Loss: 0.09715209901332855\n",
            "Step 3290, Training Loss: 0.2561318874359131\n",
            "Step 3290, Training Loss: 0.03476043418049812\n",
            "Step 3300, Training Loss: 0.04033824801445007\n",
            "Step 3300, Training Loss: 0.20992620289325714\n",
            "Step 3310, Training Loss: 0.1447478085756302\n",
            "Step 3310, Training Loss: 0.01711740717291832\n",
            "Step 3320, Training Loss: 0.05088845267891884\n",
            "Step 3320, Training Loss: 0.025899963453412056\n",
            "Step 3330, Training Loss: 0.28054845333099365\n",
            "Step 3330, Training Loss: 0.018034769222140312\n",
            "Step 3340, Training Loss: 0.20486418902873993\n",
            "Step 3340, Training Loss: 0.019064946100115776\n",
            "Step 3350, Training Loss: 0.03767312690615654\n",
            "Step 3350, Training Loss: 0.3803442716598511\n",
            "Step 3360, Training Loss: 0.019581299275159836\n",
            "Step 3360, Training Loss: 0.17614494264125824\n",
            "Step 3370, Training Loss: 0.3536347448825836\n",
            "Step 3370, Training Loss: 0.13659539818763733\n",
            "Step 3380, Training Loss: 0.044958122074604034\n",
            "Step 3380, Training Loss: 0.043514031916856766\n",
            "Step 3390, Training Loss: 0.2712741196155548\n",
            "Step 3390, Training Loss: 0.022499728947877884\n",
            "Step 3400, Training Loss: 0.18889454007148743\n",
            "Step 3400, Training Loss: 0.27785226702690125\n",
            "Step 3410, Training Loss: 0.022145485505461693\n",
            "Step 3410, Training Loss: 0.036941733211278915\n",
            "Step 3420, Training Loss: 0.029232561588287354\n",
            "Step 3420, Training Loss: 0.016786063089966774\n",
            "Step 3430, Training Loss: 0.14580266177654266\n",
            "Step 3430, Training Loss: 0.020756134763360023\n",
            "Step 3440, Training Loss: 0.020608484745025635\n",
            "Step 3440, Training Loss: 0.15339922904968262\n",
            "Step 3450, Training Loss: 0.1512959748506546\n",
            "Step 3450, Training Loss: 0.16748017072677612\n",
            "Step 3460, Training Loss: 0.09351230412721634\n",
            "Step 3460, Training Loss: 0.01787891983985901\n",
            "Step 3470, Training Loss: 0.02258601225912571\n",
            "Step 3470, Training Loss: 0.08000998198986053\n",
            "Step 3480, Training Loss: 0.04540109261870384\n",
            "Step 3480, Training Loss: 0.15066926181316376\n",
            "Step 3490, Training Loss: 0.295739084482193\n",
            "Step 3490, Training Loss: 0.016407106071710587\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Saving model checkpoint to ./results/checkpoint-3500\n",
            "Configuration saved in ./results/checkpoint-3500/config.json\n",
            "Model weights saved in ./results/checkpoint-3500/pytorch_model.bin\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 3500, Training Loss: 0.021204562857747078\n",
            "Step 3500, Training Loss: 0.0886477455496788\n",
            "Step 3510, Training Loss: 0.12161575257778168\n",
            "Step 3510, Training Loss: 0.4187696576118469\n",
            "Step 3520, Training Loss: 0.1802183836698532\n",
            "Step 3520, Training Loss: 0.01756316050887108\n",
            "Step 3530, Training Loss: 0.2689586877822876\n",
            "Step 3530, Training Loss: 0.020370377227663994\n",
            "Step 3540, Training Loss: 0.09706901013851166\n",
            "Step 3540, Training Loss: 0.02029060199856758\n",
            "Step 3550, Training Loss: 0.02235429733991623\n",
            "Step 3550, Training Loss: 0.09673650562763214\n",
            "Step 3560, Training Loss: 0.07262937724590302\n",
            "Step 3560, Training Loss: 0.01876003108918667\n",
            "Step 3570, Training Loss: 0.02010824717581272\n",
            "Step 3570, Training Loss: 0.022872542962431908\n",
            "Step 3580, Training Loss: 0.017137326300144196\n",
            "Step 3580, Training Loss: 0.21088840067386627\n",
            "Step 3590, Training Loss: 0.020526057109236717\n",
            "Step 3590, Training Loss: 0.0850483626127243\n",
            "Step 3600, Training Loss: 0.016528025269508362\n",
            "Step 3600, Training Loss: 0.026100365445017815\n",
            "Step 3610, Training Loss: 0.08542980998754501\n",
            "Step 3610, Training Loss: 0.2014818638563156\n",
            "Step 3620, Training Loss: 0.2324707955121994\n",
            "Step 3620, Training Loss: 0.034130796790122986\n",
            "Step 3630, Training Loss: 0.10020942986011505\n",
            "Step 3630, Training Loss: 0.428979754447937\n",
            "Step 3640, Training Loss: 0.25176846981048584\n",
            "Step 3640, Training Loss: 0.14003759622573853\n",
            "Step 3650, Training Loss: 0.15537887811660767\n",
            "Step 3650, Training Loss: 0.09424268454313278\n",
            "Step 3660, Training Loss: 0.019656186923384666\n",
            "Step 3660, Training Loss: 0.04929809272289276\n",
            "Step 3670, Training Loss: 0.019827647134661674\n",
            "Step 3670, Training Loss: 0.2762190103530884\n",
            "Step 3680, Training Loss: 0.01912611350417137\n",
            "Step 3680, Training Loss: 0.4657018482685089\n",
            "Step 3690, Training Loss: 0.04093889892101288\n",
            "Step 3690, Training Loss: 0.38743725419044495\n",
            "Step 3700, Training Loss: 0.14392003417015076\n",
            "Step 3700, Training Loss: 0.42190369963645935\n",
            "Step 3710, Training Loss: 0.1861242949962616\n",
            "Step 3710, Training Loss: 0.01654658280313015\n",
            "Step 3720, Training Loss: 0.016631821170449257\n",
            "Step 3720, Training Loss: 0.05994922295212746\n",
            "Step 3730, Training Loss: 0.01655491441488266\n",
            "Step 3730, Training Loss: 0.14464259147644043\n",
            "Step 3740, Training Loss: 0.19021707773208618\n",
            "Step 3740, Training Loss: 0.02728739008307457\n",
            "Step 3750, Training Loss: 0.020989691838622093\n",
            "Step 3750, Training Loss: 0.0166531503200531\n",
            "Step 3760, Training Loss: 0.02084636129438877\n",
            "Step 3760, Training Loss: 0.020827582105994225\n",
            "Step 3770, Training Loss: 0.04243402183055878\n",
            "Step 3770, Training Loss: 0.017514808103442192\n",
            "Step 3780, Training Loss: 0.015414172783493996\n",
            "Step 3780, Training Loss: 0.034551043063402176\n",
            "Step 3790, Training Loss: 0.026581455022096634\n",
            "Step 3790, Training Loss: 0.020337969064712524\n",
            "Step 3800, Training Loss: 0.02833603322505951\n",
            "Step 3800, Training Loss: 0.09172693639993668\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 1523\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 3810, Training Loss: 0.22897189855575562\n",
            "Step 3810, Training Loss: 0.01625944674015045\n",
            "Step 3820, Training Loss: 0.01894856058061123\n",
            "Step 3820, Training Loss: 0.02488127537071705\n",
            "Step 3830, Training Loss: 0.23230651021003723\n",
            "Step 3830, Training Loss: 0.09602772444486618\n",
            "Step 3840, Training Loss: 0.11797009408473969\n",
            "Step 3840, Training Loss: 0.01672222837805748\n",
            "Step 3850, Training Loss: 0.08406062424182892\n",
            "Step 3850, Training Loss: 0.1495652049779892\n",
            "Step 3860, Training Loss: 0.12225939333438873\n",
            "Step 3860, Training Loss: 0.01799696311354637\n",
            "Step 3870, Training Loss: 0.02171400375664234\n",
            "Step 3870, Training Loss: 0.03347141295671463\n",
            "Step 3880, Training Loss: 0.4259977638721466\n",
            "Step 3880, Training Loss: 0.018601790070533752\n",
            "Step 3890, Training Loss: 0.07345345616340637\n",
            "Step 3890, Training Loss: 0.02394925430417061\n",
            "Step 3900, Training Loss: 0.28858035802841187\n",
            "Step 3900, Training Loss: 0.03455276042222977\n",
            "Step 3910, Training Loss: 0.027193039655685425\n",
            "Step 3910, Training Loss: 0.1475507766008377\n",
            "Step 3920, Training Loss: 0.01588086597621441\n",
            "Step 3920, Training Loss: 0.03301294147968292\n",
            "Step 3930, Training Loss: 0.01866276003420353\n",
            "Step 3930, Training Loss: 0.10103155672550201\n",
            "Step 3940, Training Loss: 0.13820026814937592\n",
            "Step 3940, Training Loss: 0.037216585129499435\n",
            "Step 3950, Training Loss: 0.22107920050621033\n",
            "Step 3950, Training Loss: 0.020416731014847755\n",
            "Step 3960, Training Loss: 0.019018370658159256\n",
            "Step 3960, Training Loss: 0.028601959347724915\n",
            "Step 3970, Training Loss: 0.18023888766765594\n",
            "Step 3970, Training Loss: 0.3018510937690735\n",
            "Step 3980, Training Loss: 0.01854640059173107\n",
            "Step 3980, Training Loss: 0.2553185522556305\n",
            "Step 3990, Training Loss: 0.15608571469783783\n",
            "Step 3990, Training Loss: 0.01877010613679886\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Saving model checkpoint to ./results/checkpoint-4000\n",
            "Configuration saved in ./results/checkpoint-4000/config.json\n",
            "Model weights saved in ./results/checkpoint-4000/pytorch_model.bin\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 4000, Training Loss: 0.02105138637125492\n",
            "Step 4000, Training Loss: 0.017128068953752518\n",
            "Step 4010, Training Loss: 0.020594056695699692\n",
            "Step 4010, Training Loss: 0.022071247920393944\n",
            "Step 4020, Training Loss: 0.018344614654779434\n",
            "Step 4020, Training Loss: 0.12894418835639954\n",
            "Step 4030, Training Loss: 0.18319375813007355\n",
            "Step 4030, Training Loss: 0.01656225137412548\n",
            "Step 4040, Training Loss: 0.29197749495506287\n",
            "Step 4040, Training Loss: 0.01499891560524702\n",
            "Step 4050, Training Loss: 0.0742744654417038\n",
            "Step 4050, Training Loss: 0.017073629423975945\n",
            "Step 4060, Training Loss: 0.017544599249958992\n",
            "Step 4060, Training Loss: 0.11977994441986084\n",
            "Step 4070, Training Loss: 0.5711628198623657\n",
            "Step 4070, Training Loss: 0.11581823229789734\n",
            "Step 4080, Training Loss: 0.36041247844696045\n",
            "Step 4080, Training Loss: 0.1540812849998474\n",
            "Step 4090, Training Loss: 0.14109963178634644\n",
            "Step 4090, Training Loss: 0.01989384926855564\n",
            "Step 4100, Training Loss: 0.03701988607645035\n",
            "Step 4100, Training Loss: 0.016618875786662102\n",
            "Step 4110, Training Loss: 0.020015371963381767\n",
            "Step 4110, Training Loss: 0.07328973710536957\n",
            "Step 4120, Training Loss: 0.17640680074691772\n",
            "Step 4120, Training Loss: 0.08555075526237488\n",
            "Step 4130, Training Loss: 0.4145444631576538\n",
            "Step 4130, Training Loss: 0.2638666331768036\n",
            "Step 4140, Training Loss: 0.30266115069389343\n",
            "Step 4140, Training Loss: 0.022312385961413383\n",
            "Step 4150, Training Loss: 0.11167538911104202\n",
            "Step 4150, Training Loss: 0.22105395793914795\n",
            "Step 4160, Training Loss: 0.10336407274007797\n",
            "Step 4160, Training Loss: 0.025422897189855576\n",
            "Step 4170, Training Loss: 0.02708842046558857\n",
            "Step 4170, Training Loss: 0.2037004828453064\n",
            "Step 4180, Training Loss: 0.0706663429737091\n",
            "Step 4180, Training Loss: 0.042953427881002426\n",
            "Step 4190, Training Loss: 0.07479297369718552\n",
            "Step 4190, Training Loss: 0.10467299818992615\n",
            "Step 4200, Training Loss: 0.030083369463682175\n",
            "Step 4200, Training Loss: 0.17469975352287292\n",
            "Step 4210, Training Loss: 0.43236592411994934\n",
            "Step 4210, Training Loss: 0.2760983407497406\n",
            "Step 4220, Training Loss: 0.4054054915904999\n",
            "Step 4220, Training Loss: 0.014334502629935741\n",
            "Step 4230, Training Loss: 0.26129114627838135\n",
            "Step 4230, Training Loss: 0.17234615981578827\n",
            "Step 4240, Training Loss: 0.04492240399122238\n",
            "Step 4240, Training Loss: 0.21595780551433563\n",
            "Step 4250, Training Loss: 0.0825788825750351\n",
            "Step 4250, Training Loss: 0.18181471526622772\n",
            "Step 4260, Training Loss: 0.09011448174715042\n",
            "Step 4260, Training Loss: 0.02114642597734928\n",
            "Step 4270, Training Loss: 0.01497387420386076\n",
            "Step 4270, Training Loss: 0.013787703588604927\n",
            "Step 4280, Training Loss: 0.01762860082089901\n",
            "Step 4280, Training Loss: 0.17551830410957336\n",
            "Step 4290, Training Loss: 0.2671371400356293\n",
            "Step 4290, Training Loss: 0.07978310436010361\n",
            "Step 4300, Training Loss: 0.08965806663036346\n",
            "Step 4300, Training Loss: 0.1437813639640808\n",
            "Step 4310, Training Loss: 0.1313140094280243\n",
            "Step 4310, Training Loss: 0.03933517634868622\n",
            "Step 4320, Training Loss: 0.09314069151878357\n",
            "Step 4320, Training Loss: 0.014026375487446785\n",
            "Step 4330, Training Loss: 0.0437440387904644\n",
            "Step 4330, Training Loss: 0.2715981602668762\n",
            "Step 4340, Training Loss: 0.2815549373626709\n",
            "Step 4340, Training Loss: 0.2908676266670227\n",
            "Step 4350, Training Loss: 0.024064477533102036\n",
            "Step 4350, Training Loss: 0.10931015759706497\n",
            "Step 4360, Training Loss: 0.10675819218158722\n",
            "Step 4360, Training Loss: 0.17416290938854218\n",
            "Step 4370, Training Loss: 0.1765473186969757\n",
            "Step 4370, Training Loss: 0.23269487917423248\n",
            "Step 4380, Training Loss: 0.013097206130623817\n",
            "Step 4380, Training Loss: 0.08725852519273758\n",
            "Step 4390, Training Loss: 0.014465090818703175\n",
            "Step 4390, Training Loss: 0.037147801369428635\n",
            "Step 4400, Training Loss: 0.015161301009356976\n",
            "Step 4400, Training Loss: 0.14241349697113037\n",
            "Step 4410, Training Loss: 0.0161590538918972\n",
            "Step 4410, Training Loss: 0.01670229062438011\n",
            "Step 4420, Training Loss: 0.07446740567684174\n",
            "Step 4420, Training Loss: 0.07882291078567505\n",
            "Step 4430, Training Loss: 0.07761666178703308\n",
            "Step 4430, Training Loss: 0.12437734752893448\n",
            "Step 4440, Training Loss: 0.07726295292377472\n",
            "Step 4440, Training Loss: 0.1986803263425827\n",
            "Step 4450, Training Loss: 0.11788326501846313\n",
            "Step 4450, Training Loss: 0.2621469497680664\n",
            "Step 4460, Training Loss: 0.018759343773126602\n",
            "Step 4460, Training Loss: 0.017985310405492783\n",
            "Step 4480, Training Loss: 0.12462419271469116\n",
            "Step 4480, Training Loss: 0.03740277886390686\n",
            "Step 4490, Training Loss: 0.3068777918815613\n",
            "Step 4490, Training Loss: 0.027764977887272835\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Saving model checkpoint to ./results/checkpoint-4500\n",
            "Configuration saved in ./results/checkpoint-4500/config.json\n",
            "Model weights saved in ./results/checkpoint-4500/pytorch_model.bin\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 4500, Training Loss: 0.033254608511924744\n",
            "Step 4500, Training Loss: 0.2766255736351013\n",
            "Step 4510, Training Loss: 0.5289905667304993\n",
            "Step 4510, Training Loss: 0.01375378668308258\n",
            "Step 4520, Training Loss: 0.24517735838890076\n",
            "Step 4520, Training Loss: 0.21637023985385895\n",
            "Step 4530, Training Loss: 0.20198099315166473\n",
            "Step 4530, Training Loss: 0.016197992488741875\n",
            "Step 4540, Training Loss: 0.08607311546802521\n",
            "Step 4540, Training Loss: 0.01641130819916725\n",
            "Step 4550, Training Loss: 0.10452810674905777\n",
            "Step 4550, Training Loss: 0.023258240893483162\n",
            "Step 4560, Training Loss: 0.015391839668154716\n",
            "Step 4560, Training Loss: 0.03582320362329483\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 1523\n",
            "  Batch size = 8\n",
            "\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 1523\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training completed in 238.01 seconds.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='191' max='191' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [191/191 00:02]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-12-09 18:57:01,232] Trial 1 finished with value: 0.8009969785322928 and parameters: {'learning_rate': 2.1322017137616586e-05, 'batch_size': 4, 'num_train_epochs': 6, 'weight_decay': 0.016321114759497466}. Best is trial 1 with value: 0.8009969785322928.\n",
            "<ipython-input-12-42465065092b>:2: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  learning_rate = trial.suggest_loguniform(\"learning_rate\", 1e-6, 5e-5)\n",
            "<ipython-input-12-42465065092b>:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  weight_decay = trial.suggest_loguniform(\"weight_decay\", 1e-4, 0.1)\n",
            "PyTorch: setting up devices\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n",
            "***** Running training *****\n",
            "  Num examples = 6091\n",
            "  Num Epochs = 9\n",
            "  Instantaneous batch size per device = 8\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
            "  Gradient Accumulation steps = 2\n",
            "  Total optimization steps = 3429\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting training...\n",
            "Step 0, Training Loss: 0.02142171747982502\n",
            "Step 0, Training Loss: 0.0679028183221817\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='3429' max='3429' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [3429/3429 04:59, Epoch 9/9]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "      <th>F1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.073100</td>\n",
              "      <td>0.157571</td>\n",
              "      <td>0.814183</td>\n",
              "      <td>0.855679</td>\n",
              "      <td>0.809371</td>\n",
              "      <td>0.795052</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.063700</td>\n",
              "      <td>0.144727</td>\n",
              "      <td>0.824688</td>\n",
              "      <td>0.877931</td>\n",
              "      <td>0.819641</td>\n",
              "      <td>0.813806</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.094300</td>\n",
              "      <td>0.132817</td>\n",
              "      <td>0.821405</td>\n",
              "      <td>0.892198</td>\n",
              "      <td>0.820924</td>\n",
              "      <td>0.825800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.074800</td>\n",
              "      <td>0.136544</td>\n",
              "      <td>0.836507</td>\n",
              "      <td>0.874983</td>\n",
              "      <td>0.834403</td>\n",
              "      <td>0.837280</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>0.064000</td>\n",
              "      <td>0.142014</td>\n",
              "      <td>0.833880</td>\n",
              "      <td>0.861946</td>\n",
              "      <td>0.835045</td>\n",
              "      <td>0.838954</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>0.058000</td>\n",
              "      <td>0.143648</td>\n",
              "      <td>0.835194</td>\n",
              "      <td>0.868792</td>\n",
              "      <td>0.835687</td>\n",
              "      <td>0.837461</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>0.048400</td>\n",
              "      <td>0.141674</td>\n",
              "      <td>0.838477</td>\n",
              "      <td>0.860112</td>\n",
              "      <td>0.841463</td>\n",
              "      <td>0.844856</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>0.076300</td>\n",
              "      <td>0.142046</td>\n",
              "      <td>0.840446</td>\n",
              "      <td>0.858978</td>\n",
              "      <td>0.849166</td>\n",
              "      <td>0.850188</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>0.043500</td>\n",
              "      <td>0.143230</td>\n",
              "      <td>0.843729</td>\n",
              "      <td>0.865391</td>\n",
              "      <td>0.845315</td>\n",
              "      <td>0.845213</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 10, Training Loss: 0.245442196726799\n",
            "Step 10, Training Loss: 0.06332959979772568\n",
            "Step 20, Training Loss: 0.09288682788610458\n",
            "Step 20, Training Loss: 0.01870398037135601\n",
            "Step 30, Training Loss: 0.023124760016798973\n",
            "Step 30, Training Loss: 0.1437031775712967\n",
            "Step 40, Training Loss: 0.0706169381737709\n",
            "Step 40, Training Loss: 0.05800265818834305\n",
            "Step 50, Training Loss: 0.016548551619052887\n",
            "Step 50, Training Loss: 0.13392853736877441\n",
            "Step 60, Training Loss: 0.1701238602399826\n",
            "Step 60, Training Loss: 0.0523635596036911\n",
            "Step 70, Training Loss: 0.18836838006973267\n",
            "Step 70, Training Loss: 0.030735168606042862\n",
            "Step 80, Training Loss: 0.023288138210773468\n",
            "Step 80, Training Loss: 0.12029960751533508\n",
            "Step 90, Training Loss: 0.19945403933525085\n",
            "Step 90, Training Loss: 0.2761858105659485\n",
            "Step 100, Training Loss: 0.04339663311839104\n",
            "Step 100, Training Loss: 0.02410796843469143\n",
            "Step 110, Training Loss: 0.2650224566459656\n",
            "Step 110, Training Loss: 0.2064235359430313\n",
            "Step 120, Training Loss: 0.13723790645599365\n",
            "Step 120, Training Loss: 0.03471411019563675\n",
            "Step 130, Training Loss: 0.02769806608557701\n",
            "Step 130, Training Loss: 0.20510227978229523\n",
            "Step 140, Training Loss: 0.12933169305324554\n",
            "Step 140, Training Loss: 0.09481518715620041\n",
            "Step 150, Training Loss: 0.05019411817193031\n",
            "Step 150, Training Loss: 0.07766421884298325\n",
            "Step 160, Training Loss: 0.04574747756123543\n",
            "Step 160, Training Loss: 0.19519923627376556\n",
            "Step 170, Training Loss: 0.04511568695306778\n",
            "Step 170, Training Loss: 0.05332215875387192\n",
            "Step 180, Training Loss: 0.05804594233632088\n",
            "Step 180, Training Loss: 0.06087172403931618\n",
            "Step 190, Training Loss: 0.04130379855632782\n",
            "Step 190, Training Loss: 0.16732248663902283\n",
            "Step 200, Training Loss: 0.015399252064526081\n",
            "Step 200, Training Loss: 0.1867918074131012\n",
            "Step 210, Training Loss: 0.07256653904914856\n",
            "Step 210, Training Loss: 0.12345252931118011\n",
            "Step 220, Training Loss: 0.09056282788515091\n",
            "Step 220, Training Loss: 0.15490126609802246\n",
            "Step 230, Training Loss: 0.11650670319795609\n",
            "Step 230, Training Loss: 0.058423370122909546\n",
            "Step 240, Training Loss: 0.22756290435791016\n",
            "Step 240, Training Loss: 0.10369325429201126\n",
            "Step 250, Training Loss: 0.1461043357849121\n",
            "Step 250, Training Loss: 0.07870104163885117\n",
            "Step 260, Training Loss: 0.08325944095849991\n",
            "Step 260, Training Loss: 0.06622806191444397\n",
            "Step 270, Training Loss: 0.015166538767516613\n",
            "Step 270, Training Loss: 0.21017521619796753\n",
            "Step 280, Training Loss: 0.19246430695056915\n",
            "Step 280, Training Loss: 0.030714649707078934\n",
            "Step 290, Training Loss: 0.09842037409543991\n",
            "Step 290, Training Loss: 0.1688946783542633\n",
            "Step 300, Training Loss: 0.15973038971424103\n",
            "Step 300, Training Loss: 0.07570343464612961\n",
            "Step 310, Training Loss: 0.09672743827104568\n",
            "Step 310, Training Loss: 0.15457791090011597\n",
            "Step 320, Training Loss: 0.06261129677295685\n",
            "Step 320, Training Loss: 0.24182888865470886\n",
            "Step 330, Training Loss: 0.09462638944387436\n",
            "Step 330, Training Loss: 0.014441505074501038\n",
            "Step 340, Training Loss: 0.23625445365905762\n",
            "Step 340, Training Loss: 0.13005411624908447\n",
            "Step 350, Training Loss: 0.15586155652999878\n",
            "Step 350, Training Loss: 0.1613750457763672\n",
            "Step 360, Training Loss: 0.16583415865898132\n",
            "Step 360, Training Loss: 0.17425745725631714\n",
            "Step 370, Training Loss: 0.19108547270298004\n",
            "Step 370, Training Loss: 0.05051640793681145\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 1523\n",
            "  Batch size = 16\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 380, Training Loss: 0.013473652303218842\n",
            "Step 380, Training Loss: 0.2949722707271576\n",
            "Step 390, Training Loss: 0.45028695464134216\n",
            "Step 390, Training Loss: 0.030138647183775902\n",
            "Step 400, Training Loss: 0.021254757419228554\n",
            "Step 400, Training Loss: 0.18238995969295502\n",
            "Step 410, Training Loss: 0.05959797278046608\n",
            "Step 410, Training Loss: 0.029521312564611435\n",
            "Step 420, Training Loss: 0.1433442234992981\n",
            "Step 420, Training Loss: 0.04469219222664833\n",
            "Step 430, Training Loss: 0.015917455777525902\n",
            "Step 430, Training Loss: 0.05364422872662544\n",
            "Step 440, Training Loss: 0.19223572313785553\n",
            "Step 440, Training Loss: 0.077447809278965\n",
            "Step 450, Training Loss: 0.28405922651290894\n",
            "Step 450, Training Loss: 0.07124056667089462\n",
            "Step 460, Training Loss: 0.07421133667230606\n",
            "Step 460, Training Loss: 0.12859676778316498\n",
            "Step 470, Training Loss: 0.10231401771306992\n",
            "Step 470, Training Loss: 0.04929761961102486\n",
            "Step 480, Training Loss: 0.01222794409841299\n",
            "Step 480, Training Loss: 0.07902290672063828\n",
            "Step 490, Training Loss: 0.08002196997404099\n",
            "Step 490, Training Loss: 0.14298957586288452\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Saving model checkpoint to ./results/checkpoint-500\n",
            "Configuration saved in ./results/checkpoint-500/config.json\n",
            "Model weights saved in ./results/checkpoint-500/pytorch_model.bin\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 500, Training Loss: 0.04402751475572586\n",
            "Step 500, Training Loss: 0.014746145345270634\n",
            "Step 510, Training Loss: 0.22304634749889374\n",
            "Step 510, Training Loss: 0.047333974391222\n",
            "Step 520, Training Loss: 0.12552443146705627\n",
            "Step 520, Training Loss: 0.09182925522327423\n",
            "Step 530, Training Loss: 0.16521494090557098\n",
            "Step 530, Training Loss: 0.03702199459075928\n",
            "Step 540, Training Loss: 0.05715618282556534\n",
            "Step 540, Training Loss: 0.053659796714782715\n",
            "Step 550, Training Loss: 0.12063872814178467\n",
            "Step 550, Training Loss: 0.019707083702087402\n",
            "Step 560, Training Loss: 0.0621071383357048\n",
            "Step 560, Training Loss: 0.0424128882586956\n",
            "Step 570, Training Loss: 0.11388511210680008\n",
            "Step 570, Training Loss: 0.01290037389844656\n",
            "Step 580, Training Loss: 0.04198450595140457\n",
            "Step 580, Training Loss: 0.09424243867397308\n",
            "Step 590, Training Loss: 0.14906805753707886\n",
            "Step 590, Training Loss: 0.06124509498476982\n",
            "Step 600, Training Loss: 0.04724077507853508\n",
            "Step 600, Training Loss: 0.1014016643166542\n",
            "Step 610, Training Loss: 0.1094452440738678\n",
            "Step 610, Training Loss: 0.2868834435939789\n",
            "Step 620, Training Loss: 0.044215939939022064\n",
            "Step 620, Training Loss: 0.23903745412826538\n",
            "Step 630, Training Loss: 0.01793537102639675\n",
            "Step 630, Training Loss: 0.01766868308186531\n",
            "Step 640, Training Loss: 0.010442020371556282\n",
            "Step 640, Training Loss: 0.1678278148174286\n",
            "Step 650, Training Loss: 0.15492354333400726\n",
            "Step 650, Training Loss: 0.014922992326319218\n",
            "Step 660, Training Loss: 0.09278042614459991\n",
            "Step 660, Training Loss: 0.02233286388218403\n",
            "Step 670, Training Loss: 0.24963772296905518\n",
            "Step 670, Training Loss: 0.011668589897453785\n",
            "Step 680, Training Loss: 0.0594344325363636\n",
            "Step 680, Training Loss: 0.2317257523536682\n",
            "Step 690, Training Loss: 0.010786759667098522\n",
            "Step 690, Training Loss: 0.1289391964673996\n",
            "Step 700, Training Loss: 0.038169171661138535\n",
            "Step 700, Training Loss: 0.12145863473415375\n",
            "Step 710, Training Loss: 0.019874900579452515\n",
            "Step 710, Training Loss: 0.04508334770798683\n",
            "Step 720, Training Loss: 0.0410262793302536\n",
            "Step 720, Training Loss: 0.018343616276979446\n",
            "Step 730, Training Loss: 0.057708803564310074\n",
            "Step 730, Training Loss: 0.012603751383721828\n",
            "Step 740, Training Loss: 0.09757209569215775\n",
            "Step 740, Training Loss: 0.09229008853435516\n",
            "Step 750, Training Loss: 0.011575711891055107\n",
            "Step 750, Training Loss: 0.11545861512422562\n",
            "Step 760, Training Loss: 0.018695248290896416\n",
            "Step 760, Training Loss: 0.06109817326068878\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 1523\n",
            "  Batch size = 16\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 770, Training Loss: 0.17284730076789856\n",
            "Step 770, Training Loss: 0.015708647668361664\n",
            "Step 780, Training Loss: 0.03829401358962059\n",
            "Step 780, Training Loss: 0.01794980838894844\n",
            "Step 790, Training Loss: 0.013525329530239105\n",
            "Step 790, Training Loss: 0.13006550073623657\n",
            "Step 800, Training Loss: 0.04996717348694801\n",
            "Step 800, Training Loss: 0.07495062798261642\n",
            "Step 810, Training Loss: 0.1978774219751358\n",
            "Step 810, Training Loss: 0.2306201159954071\n",
            "Step 820, Training Loss: 0.012576174922287464\n",
            "Step 820, Training Loss: 0.10767406225204468\n",
            "Step 830, Training Loss: 0.0922987312078476\n",
            "Step 830, Training Loss: 0.10484437644481659\n",
            "Step 840, Training Loss: 0.12003584206104279\n",
            "Step 840, Training Loss: 0.10185722261667252\n",
            "Step 850, Training Loss: 0.011004195548593998\n",
            "Step 850, Training Loss: 0.3041279911994934\n",
            "Step 860, Training Loss: 0.04670850932598114\n",
            "Step 860, Training Loss: 0.012997889891266823\n",
            "Step 870, Training Loss: 0.10278914123773575\n",
            "Step 870, Training Loss: 0.016031259670853615\n",
            "Step 880, Training Loss: 0.11875257641077042\n",
            "Step 880, Training Loss: 0.06738954037427902\n",
            "Step 890, Training Loss: 0.0338037870824337\n",
            "Step 890, Training Loss: 0.029676051810383797\n",
            "Step 900, Training Loss: 0.08066824823617935\n",
            "Step 900, Training Loss: 0.08490172773599625\n",
            "Step 910, Training Loss: 0.05045170336961746\n",
            "Step 910, Training Loss: 0.2252914309501648\n",
            "Step 920, Training Loss: 0.08121493458747864\n",
            "Step 920, Training Loss: 0.0390150249004364\n",
            "Step 930, Training Loss: 0.04448514059185982\n",
            "Step 930, Training Loss: 0.16089294850826263\n",
            "Step 940, Training Loss: 0.0786990150809288\n",
            "Step 940, Training Loss: 0.009623166173696518\n",
            "Step 950, Training Loss: 0.014943791553378105\n",
            "Step 950, Training Loss: 0.28805309534072876\n",
            "Step 960, Training Loss: 0.1264263242483139\n",
            "Step 960, Training Loss: 0.09251954406499863\n",
            "Step 970, Training Loss: 0.1351524442434311\n",
            "Step 970, Training Loss: 0.08190055191516876\n",
            "Step 980, Training Loss: 0.1388009488582611\n",
            "Step 980, Training Loss: 0.04336540400981903\n",
            "Step 990, Training Loss: 0.013417527079582214\n",
            "Step 990, Training Loss: 0.22922483086585999\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Saving model checkpoint to ./results/checkpoint-1000\n",
            "Configuration saved in ./results/checkpoint-1000/config.json\n",
            "Model weights saved in ./results/checkpoint-1000/pytorch_model.bin\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 1000, Training Loss: 0.21780502796173096\n",
            "Step 1000, Training Loss: 0.0359879732131958\n",
            "Step 1010, Training Loss: 0.01733197644352913\n",
            "Step 1010, Training Loss: 0.01028929091989994\n",
            "Step 1020, Training Loss: 0.1057591363787651\n",
            "Step 1020, Training Loss: 0.21804162859916687\n",
            "Step 1030, Training Loss: 0.13363400101661682\n",
            "Step 1030, Training Loss: 0.23001930117607117\n",
            "Step 1040, Training Loss: 0.08882637321949005\n",
            "Step 1040, Training Loss: 0.09559457004070282\n",
            "Step 1050, Training Loss: 0.013849908486008644\n",
            "Step 1050, Training Loss: 0.12187443673610687\n",
            "Step 1060, Training Loss: 0.1171320453286171\n",
            "Step 1060, Training Loss: 0.3013659119606018\n",
            "Step 1070, Training Loss: 0.011931369081139565\n",
            "Step 1070, Training Loss: 0.12215787172317505\n",
            "Step 1080, Training Loss: 0.2137003242969513\n",
            "Step 1080, Training Loss: 0.04009004309773445\n",
            "Step 1090, Training Loss: 0.13162364065647125\n",
            "Step 1090, Training Loss: 0.043432753533124924\n",
            "Step 1100, Training Loss: 0.16778837144374847\n",
            "Step 1100, Training Loss: 0.05917591229081154\n",
            "Step 1110, Training Loss: 0.016360238194465637\n",
            "Step 1110, Training Loss: 0.048868585377931595\n",
            "Step 1120, Training Loss: 0.07354686409235\n",
            "Step 1120, Training Loss: 0.01612771302461624\n",
            "Step 1130, Training Loss: 0.1694153994321823\n",
            "Step 1130, Training Loss: 0.042252179235219955\n",
            "Step 1140, Training Loss: 0.0711231529712677\n",
            "Step 1140, Training Loss: 0.03424586355686188\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 1523\n",
            "  Batch size = 16\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 1150, Training Loss: 0.29346442222595215\n",
            "Step 1150, Training Loss: 0.07120643556118011\n",
            "Step 1160, Training Loss: 0.01010954286903143\n",
            "Step 1160, Training Loss: 0.015070311725139618\n",
            "Step 1170, Training Loss: 0.017802942544221878\n",
            "Step 1170, Training Loss: 0.0439416840672493\n",
            "Step 1180, Training Loss: 0.20618373155593872\n",
            "Step 1180, Training Loss: 0.04788872227072716\n",
            "Step 1190, Training Loss: 0.02501567266881466\n",
            "Step 1190, Training Loss: 0.05339479818940163\n",
            "Step 1200, Training Loss: 0.16427521407604218\n",
            "Step 1200, Training Loss: 0.22127290070056915\n",
            "Step 1210, Training Loss: 0.04438876360654831\n",
            "Step 1210, Training Loss: 0.20308628678321838\n",
            "Step 1220, Training Loss: 0.2748776376247406\n",
            "Step 1220, Training Loss: 0.3264369070529938\n",
            "Step 1230, Training Loss: 0.021510232239961624\n",
            "Step 1230, Training Loss: 0.04173729941248894\n",
            "Step 1240, Training Loss: 0.055876582860946655\n",
            "Step 1240, Training Loss: 0.11613301932811737\n",
            "Step 1250, Training Loss: 0.010570376180112362\n",
            "Step 1250, Training Loss: 0.05872352048754692\n",
            "Step 1260, Training Loss: 0.035219162702560425\n",
            "Step 1260, Training Loss: 0.04159381613135338\n",
            "Step 1270, Training Loss: 0.05821198970079422\n",
            "Step 1270, Training Loss: 0.045448653399944305\n",
            "Step 1280, Training Loss: 0.015787510201334953\n",
            "Step 1280, Training Loss: 0.12458357214927673\n",
            "Step 1290, Training Loss: 0.016810666769742966\n",
            "Step 1290, Training Loss: 0.049408432096242905\n",
            "Step 1300, Training Loss: 0.0900016501545906\n",
            "Step 1300, Training Loss: 0.011457865126430988\n",
            "Step 1310, Training Loss: 0.050246771425008774\n",
            "Step 1310, Training Loss: 0.35458797216415405\n",
            "Step 1320, Training Loss: 0.08055093139410019\n",
            "Step 1320, Training Loss: 0.012331870384514332\n",
            "Step 1330, Training Loss: 0.07934226840734482\n",
            "Step 1330, Training Loss: 0.20583564043045044\n",
            "Step 1340, Training Loss: 0.04119217023253441\n",
            "Step 1340, Training Loss: 0.20308606326580048\n",
            "Step 1350, Training Loss: 0.04805435240268707\n",
            "Step 1350, Training Loss: 0.08668852597475052\n",
            "Step 1360, Training Loss: 0.03982151672244072\n",
            "Step 1360, Training Loss: 0.02862127497792244\n",
            "Step 1370, Training Loss: 0.016624465584754944\n",
            "Step 1370, Training Loss: 0.02063717320561409\n",
            "Step 1380, Training Loss: 0.03144535422325134\n",
            "Step 1380, Training Loss: 0.012058882974088192\n",
            "Step 1390, Training Loss: 0.043026991188526154\n",
            "Step 1390, Training Loss: 0.20999464392662048\n",
            "Step 1400, Training Loss: 0.037279821932315826\n",
            "Step 1400, Training Loss: 0.06261692941188812\n",
            "Step 1410, Training Loss: 0.04836172237992287\n",
            "Step 1410, Training Loss: 0.009339678101241589\n",
            "Step 1420, Training Loss: 0.06685445457696915\n",
            "Step 1420, Training Loss: 0.04312179982662201\n",
            "Step 1430, Training Loss: 0.013974601402878761\n",
            "Step 1430, Training Loss: 0.07765665650367737\n",
            "Step 1440, Training Loss: 0.3086104094982147\n",
            "Step 1440, Training Loss: 0.10974588990211487\n",
            "Step 1450, Training Loss: 0.20481474697589874\n",
            "Step 1450, Training Loss: 0.07161206007003784\n",
            "Step 1460, Training Loss: 0.10477393120527267\n",
            "Step 1460, Training Loss: 0.011738253757357597\n",
            "Step 1470, Training Loss: 0.2231484055519104\n",
            "Step 1470, Training Loss: 0.009674694389104843\n",
            "Step 1480, Training Loss: 0.009776708669960499\n",
            "Step 1480, Training Loss: 0.12439585477113724\n",
            "Step 1490, Training Loss: 0.039184391498565674\n",
            "Step 1490, Training Loss: 0.03214030712842941\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Saving model checkpoint to ./results/checkpoint-1500\n",
            "Configuration saved in ./results/checkpoint-1500/config.json\n",
            "Model weights saved in ./results/checkpoint-1500/pytorch_model.bin\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 1500, Training Loss: 0.012888903729617596\n",
            "Step 1500, Training Loss: 0.010638708248734474\n",
            "Step 1510, Training Loss: 0.013084636069834232\n",
            "Step 1510, Training Loss: 0.08723776787519455\n",
            "Step 1520, Training Loss: 0.1172029972076416\n",
            "Step 1520, Training Loss: 0.04480146989226341\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 1523\n",
            "  Batch size = 16\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 1530, Training Loss: 0.07591467350721359\n",
            "Step 1530, Training Loss: 0.015886757522821426\n",
            "Step 1540, Training Loss: 0.12694762647151947\n",
            "Step 1540, Training Loss: 0.010476349852979183\n",
            "Step 1550, Training Loss: 0.03794887289404869\n",
            "Step 1550, Training Loss: 0.07681664824485779\n",
            "Step 1560, Training Loss: 0.16070285439491272\n",
            "Step 1560, Training Loss: 0.042821988463401794\n",
            "Step 1570, Training Loss: 0.01002970989793539\n",
            "Step 1570, Training Loss: 0.05489606410264969\n",
            "Step 1580, Training Loss: 0.006721546873450279\n",
            "Step 1580, Training Loss: 0.17099127173423767\n",
            "Step 1590, Training Loss: 0.015128345228731632\n",
            "Step 1590, Training Loss: 0.0996546596288681\n",
            "Step 1600, Training Loss: 0.10217613726854324\n",
            "Step 1600, Training Loss: 0.014900412410497665\n",
            "Step 1610, Training Loss: 0.009191860444843769\n",
            "Step 1610, Training Loss: 0.1370798945426941\n",
            "Step 1620, Training Loss: 0.23631161451339722\n",
            "Step 1620, Training Loss: 0.08376798033714294\n",
            "Step 1630, Training Loss: 0.09843869507312775\n",
            "Step 1630, Training Loss: 0.08229082822799683\n",
            "Step 1640, Training Loss: 0.040889251977205276\n",
            "Step 1640, Training Loss: 0.028598235920071602\n",
            "Step 1650, Training Loss: 0.007443383801728487\n",
            "Step 1650, Training Loss: 0.010246437974274158\n",
            "Step 1660, Training Loss: 0.04270687326788902\n",
            "Step 1660, Training Loss: 0.03205118700861931\n",
            "Step 1670, Training Loss: 0.08153390139341354\n",
            "Step 1670, Training Loss: 0.03120720572769642\n",
            "Step 1680, Training Loss: 0.04786728695034981\n",
            "Step 1680, Training Loss: 0.03183072805404663\n",
            "Step 1690, Training Loss: 0.03864721208810806\n",
            "Step 1690, Training Loss: 0.015197801403701305\n",
            "Step 1700, Training Loss: 0.01287917885929346\n",
            "Step 1700, Training Loss: 0.0414288230240345\n",
            "Step 1710, Training Loss: 0.009993365965783596\n",
            "Step 1710, Training Loss: 0.04282555729150772\n",
            "Step 1720, Training Loss: 0.13950426876544952\n",
            "Step 1720, Training Loss: 0.1612047255039215\n",
            "Step 1730, Training Loss: 0.03283309563994408\n",
            "Step 1730, Training Loss: 0.17509368062019348\n",
            "Step 1740, Training Loss: 0.025080520659685135\n",
            "Step 1740, Training Loss: 0.01047822181135416\n",
            "Step 1750, Training Loss: 0.048256952315568924\n",
            "Step 1750, Training Loss: 0.008848564699292183\n",
            "Step 1760, Training Loss: 0.010620338842272758\n",
            "Step 1760, Training Loss: 0.04098707437515259\n",
            "Step 1770, Training Loss: 0.3775727450847626\n",
            "Step 1770, Training Loss: 0.011945987120270729\n",
            "Step 1780, Training Loss: 0.049411285668611526\n",
            "Step 1780, Training Loss: 0.010901344940066338\n",
            "Step 1790, Training Loss: 0.029275860637426376\n",
            "Step 1790, Training Loss: 0.1458682417869568\n",
            "Step 1800, Training Loss: 0.042216088622808456\n",
            "Step 1800, Training Loss: 0.0414087176322937\n",
            "Step 1810, Training Loss: 0.08459042757749557\n",
            "Step 1810, Training Loss: 0.0789337009191513\n",
            "Step 1820, Training Loss: 0.11242412030696869\n",
            "Step 1820, Training Loss: 0.008545324206352234\n",
            "Step 1830, Training Loss: 0.010624253191053867\n",
            "Step 1830, Training Loss: 0.011834920383989811\n",
            "Step 1840, Training Loss: 0.009772801771759987\n",
            "Step 1840, Training Loss: 0.05088706314563751\n",
            "Step 1850, Training Loss: 0.04985711723566055\n",
            "Step 1850, Training Loss: 0.040891170501708984\n",
            "Step 1860, Training Loss: 0.1745356023311615\n",
            "Step 1860, Training Loss: 0.04042188823223114\n",
            "Step 1870, Training Loss: 0.030114183202385902\n",
            "Step 1870, Training Loss: 0.03230825066566467\n",
            "Step 1880, Training Loss: 0.012763102538883686\n",
            "Step 1880, Training Loss: 0.08092009276151657\n",
            "Step 1890, Training Loss: 0.23334524035453796\n",
            "Step 1890, Training Loss: 0.13118135929107666\n",
            "Step 1900, Training Loss: 0.015511225908994675\n",
            "Step 1900, Training Loss: 0.04788120463490486\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 1523\n",
            "  Batch size = 16\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 1910, Training Loss: 0.08372524380683899\n",
            "Step 1910, Training Loss: 0.06297612190246582\n",
            "Step 1920, Training Loss: 0.006865783594548702\n",
            "Step 1920, Training Loss: 0.18241773545742035\n",
            "Step 1930, Training Loss: 0.009687861427664757\n",
            "Step 1930, Training Loss: 0.007131243124604225\n",
            "Step 1940, Training Loss: 0.07914070785045624\n",
            "Step 1940, Training Loss: 0.049156732857227325\n",
            "Step 1950, Training Loss: 0.09590546041727066\n",
            "Step 1950, Training Loss: 0.4220861494541168\n",
            "Step 1960, Training Loss: 0.07455730438232422\n",
            "Step 1960, Training Loss: 0.07059045135974884\n",
            "Step 1970, Training Loss: 0.08343736827373505\n",
            "Step 1970, Training Loss: 0.011405265890061855\n",
            "Step 1980, Training Loss: 0.16197215020656586\n",
            "Step 1980, Training Loss: 0.013493545353412628\n",
            "Step 1990, Training Loss: 0.039117712527513504\n",
            "Step 1990, Training Loss: 0.13800819218158722\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Saving model checkpoint to ./results/checkpoint-2000\n",
            "Configuration saved in ./results/checkpoint-2000/config.json\n",
            "Model weights saved in ./results/checkpoint-2000/pytorch_model.bin\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 2000, Training Loss: 0.07505570352077484\n",
            "Step 2000, Training Loss: 0.007786903530359268\n",
            "Step 2010, Training Loss: 0.013150145299732685\n",
            "Step 2010, Training Loss: 0.13951332867145538\n",
            "Step 2020, Training Loss: 0.0365491583943367\n",
            "Step 2020, Training Loss: 0.03379176929593086\n",
            "Step 2030, Training Loss: 0.13506144285202026\n",
            "Step 2030, Training Loss: 0.009927105158567429\n",
            "Step 2040, Training Loss: 0.03825749084353447\n",
            "Step 2040, Training Loss: 0.0286647267639637\n",
            "Step 2050, Training Loss: 0.008196435868740082\n",
            "Step 2050, Training Loss: 0.14338451623916626\n",
            "Step 2060, Training Loss: 0.007930638268589973\n",
            "Step 2060, Training Loss: 0.010356780141592026\n",
            "Step 2070, Training Loss: 0.028595903888344765\n",
            "Step 2070, Training Loss: 0.03815355524420738\n",
            "Step 2080, Training Loss: 0.03480711206793785\n",
            "Step 2080, Training Loss: 0.00711409654468298\n",
            "Step 2090, Training Loss: 0.0334760919213295\n",
            "Step 2090, Training Loss: 0.008753237314522266\n",
            "Step 2100, Training Loss: 0.052738603204488754\n",
            "Step 2100, Training Loss: 0.10186561942100525\n",
            "Step 2110, Training Loss: 0.027468347921967506\n",
            "Step 2110, Training Loss: 0.11640788614749908\n",
            "Step 2120, Training Loss: 0.09328682720661163\n",
            "Step 2120, Training Loss: 0.009248492307960987\n",
            "Step 2130, Training Loss: 0.022087503224611282\n",
            "Step 2130, Training Loss: 0.21596091985702515\n",
            "Step 2140, Training Loss: 0.0871371328830719\n",
            "Step 2140, Training Loss: 0.010857761837542057\n",
            "Step 2150, Training Loss: 0.007448989897966385\n",
            "Step 2150, Training Loss: 0.17397716641426086\n",
            "Step 2160, Training Loss: 0.06837175041437149\n",
            "Step 2160, Training Loss: 0.2385893613100052\n",
            "Step 2170, Training Loss: 0.09075536578893661\n",
            "Step 2170, Training Loss: 0.17294016480445862\n",
            "Step 2180, Training Loss: 0.051231227815151215\n",
            "Step 2180, Training Loss: 0.03197459876537323\n",
            "Step 2190, Training Loss: 0.009234908036887646\n",
            "Step 2190, Training Loss: 0.028619060292840004\n",
            "Step 2200, Training Loss: 0.06911490857601166\n",
            "Step 2200, Training Loss: 0.011471305973827839\n",
            "Step 2210, Training Loss: 0.18211178481578827\n",
            "Step 2210, Training Loss: 0.008255560882389545\n",
            "Step 2220, Training Loss: 0.03567235916852951\n",
            "Step 2220, Training Loss: 0.03314827382564545\n",
            "Step 2230, Training Loss: 0.007855445146560669\n",
            "Step 2230, Training Loss: 0.19244781136512756\n",
            "Step 2240, Training Loss: 0.007477447856217623\n",
            "Step 2240, Training Loss: 0.1397591382265091\n",
            "Step 2250, Training Loss: 0.006048595532774925\n",
            "Step 2250, Training Loss: 0.04773662984371185\n",
            "Step 2260, Training Loss: 0.15334492921829224\n",
            "Step 2260, Training Loss: 0.03691396862268448\n",
            "Step 2270, Training Loss: 0.1773456186056137\n",
            "Step 2270, Training Loss: 0.0993993803858757\n",
            "Step 2280, Training Loss: 0.10188856720924377\n",
            "Step 2280, Training Loss: 0.1604069322347641\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 1523\n",
            "  Batch size = 16\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 2290, Training Loss: 0.013128990307450294\n",
            "Step 2290, Training Loss: 0.007518056780099869\n",
            "Step 2300, Training Loss: 0.04908164590597153\n",
            "Step 2300, Training Loss: 0.06267930567264557\n",
            "Step 2310, Training Loss: 0.042308323085308075\n",
            "Step 2310, Training Loss: 0.04698022082448006\n",
            "Step 2320, Training Loss: 0.07439597696065903\n",
            "Step 2320, Training Loss: 0.007533686701208353\n",
            "Step 2330, Training Loss: 0.03340265899896622\n",
            "Step 2330, Training Loss: 0.0353965163230896\n",
            "Step 2340, Training Loss: 0.07220885902643204\n",
            "Step 2340, Training Loss: 0.05039204657077789\n",
            "Step 2350, Training Loss: 0.2651791572570801\n",
            "Step 2350, Training Loss: 0.0384325347840786\n",
            "Step 2360, Training Loss: 0.04207516089081764\n",
            "Step 2360, Training Loss: 0.010997741483151913\n",
            "Step 2370, Training Loss: 0.0102962302044034\n",
            "Step 2370, Training Loss: 0.13591936230659485\n",
            "Step 2380, Training Loss: 0.012910032644867897\n",
            "Step 2380, Training Loss: 0.049645859748125076\n",
            "Step 2390, Training Loss: 0.006603293586522341\n",
            "Step 2390, Training Loss: 0.15331020951271057\n",
            "Step 2400, Training Loss: 0.1925130933523178\n",
            "Step 2400, Training Loss: 0.10159550607204437\n",
            "Step 2410, Training Loss: 0.04603271558880806\n",
            "Step 2410, Training Loss: 0.039192840456962585\n",
            "Step 2420, Training Loss: 0.006194553337991238\n",
            "Step 2420, Training Loss: 0.15050911903381348\n",
            "Step 2430, Training Loss: 0.024122919887304306\n",
            "Step 2430, Training Loss: 0.006615774240344763\n",
            "Step 2440, Training Loss: 0.04731007665395737\n",
            "Step 2440, Training Loss: 0.01731027476489544\n",
            "Step 2450, Training Loss: 0.029467696323990822\n",
            "Step 2450, Training Loss: 0.008763665333390236\n",
            "Step 2460, Training Loss: 0.029575321823358536\n",
            "Step 2460, Training Loss: 0.17309007048606873\n",
            "Step 2470, Training Loss: 0.007361636962741613\n",
            "Step 2470, Training Loss: 0.20552100241184235\n",
            "Step 2480, Training Loss: 0.2559874951839447\n",
            "Step 2480, Training Loss: 0.03520650416612625\n",
            "Step 2490, Training Loss: 0.011743207462131977\n",
            "Step 2490, Training Loss: 0.01252188440412283\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Saving model checkpoint to ./results/checkpoint-2500\n",
            "Configuration saved in ./results/checkpoint-2500/config.json\n",
            "Model weights saved in ./results/checkpoint-2500/pytorch_model.bin\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 2500, Training Loss: 0.05252645164728165\n",
            "Step 2500, Training Loss: 0.1562390774488449\n",
            "Step 2510, Training Loss: 0.030498847365379333\n",
            "Step 2510, Training Loss: 0.0695112943649292\n",
            "Step 2520, Training Loss: 0.011594175361096859\n",
            "Step 2520, Training Loss: 0.1765681803226471\n",
            "Step 2530, Training Loss: 0.19853505492210388\n",
            "Step 2530, Training Loss: 0.006615216378122568\n",
            "Step 2540, Training Loss: 0.09046272188425064\n",
            "Step 2540, Training Loss: 0.03186216205358505\n",
            "Step 2550, Training Loss: 0.03877084702253342\n",
            "Step 2550, Training Loss: 0.0671829879283905\n",
            "Step 2560, Training Loss: 0.12403035163879395\n",
            "Step 2560, Training Loss: 0.14055494964122772\n",
            "Step 2570, Training Loss: 0.013073256239295006\n",
            "Step 2570, Training Loss: 0.09587469696998596\n",
            "Step 2580, Training Loss: 0.030391821637749672\n",
            "Step 2580, Training Loss: 0.013579333201050758\n",
            "Step 2590, Training Loss: 0.009235297329723835\n",
            "Step 2590, Training Loss: 0.15815500915050507\n",
            "Step 2600, Training Loss: 0.04394637793302536\n",
            "Step 2600, Training Loss: 0.01980935037136078\n",
            "Step 2610, Training Loss: 0.10384176671504974\n",
            "Step 2610, Training Loss: 0.10419894754886627\n",
            "Step 2620, Training Loss: 0.053551290184259415\n",
            "Step 2620, Training Loss: 0.05036763846874237\n",
            "Step 2630, Training Loss: 0.010510030202567577\n",
            "Step 2630, Training Loss: 0.24679365754127502\n",
            "Step 2640, Training Loss: 0.08564469963312149\n",
            "Step 2640, Training Loss: 0.007508715149015188\n",
            "Step 2650, Training Loss: 0.010385271161794662\n",
            "Step 2650, Training Loss: 0.052804600447416306\n",
            "Step 2660, Training Loss: 0.03043583780527115\n",
            "Step 2660, Training Loss: 0.03549062833189964\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 1523\n",
            "  Batch size = 16\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 2670, Training Loss: 0.010753125883638859\n",
            "Step 2670, Training Loss: 0.033132754266262054\n",
            "Step 2680, Training Loss: 0.00831648614257574\n",
            "Step 2680, Training Loss: 0.10033677518367767\n",
            "Step 2690, Training Loss: 0.04342911019921303\n",
            "Step 2690, Training Loss: 0.047382645308971405\n",
            "Step 2700, Training Loss: 0.006282958667725325\n",
            "Step 2700, Training Loss: 0.09818646311759949\n",
            "Step 2710, Training Loss: 0.07406017184257507\n",
            "Step 2710, Training Loss: 0.06409952789545059\n",
            "Step 2720, Training Loss: 0.04683404415845871\n",
            "Step 2720, Training Loss: 0.03549804911017418\n",
            "Step 2730, Training Loss: 0.05473331734538078\n",
            "Step 2730, Training Loss: 0.06727520376443863\n",
            "Step 2740, Training Loss: 0.1702578216791153\n",
            "Step 2740, Training Loss: 0.007918324321508408\n",
            "Step 2750, Training Loss: 0.008602526038885117\n",
            "Step 2750, Training Loss: 0.048539746552705765\n",
            "Step 2760, Training Loss: 0.04876881092786789\n",
            "Step 2760, Training Loss: 0.0069809770211577415\n",
            "Step 2770, Training Loss: 0.1899266541004181\n",
            "Step 2770, Training Loss: 0.019737163558602333\n",
            "Step 2780, Training Loss: 0.37617915868759155\n",
            "Step 2780, Training Loss: 0.02459592930972576\n",
            "Step 2790, Training Loss: 0.1023053303360939\n",
            "Step 2790, Training Loss: 0.06752952933311462\n",
            "Step 2800, Training Loss: 0.008969289250671864\n",
            "Step 2800, Training Loss: 0.04076516628265381\n",
            "Step 2810, Training Loss: 0.00679825060069561\n",
            "Step 2810, Training Loss: 0.048388924449682236\n",
            "Step 2820, Training Loss: 0.10910819470882416\n",
            "Step 2820, Training Loss: 0.07607928663492203\n",
            "Step 2830, Training Loss: 0.014543863944709301\n",
            "Step 2830, Training Loss: 0.04678346589207649\n",
            "Step 2840, Training Loss: 0.1140066459774971\n",
            "Step 2840, Training Loss: 0.1366473138332367\n",
            "Step 2850, Training Loss: 0.05282166972756386\n",
            "Step 2850, Training Loss: 0.035923104733228683\n",
            "Step 2860, Training Loss: 0.08988805115222931\n",
            "Step 2860, Training Loss: 0.06928695738315582\n",
            "Step 2870, Training Loss: 0.030101660639047623\n",
            "Step 2870, Training Loss: 0.024427207186818123\n",
            "Step 2880, Training Loss: 0.010302530601620674\n",
            "Step 2880, Training Loss: 0.04363106191158295\n",
            "Step 2890, Training Loss: 0.03883959352970123\n",
            "Step 2890, Training Loss: 0.010100239887833595\n",
            "Step 2900, Training Loss: 0.009877954609692097\n",
            "Step 2900, Training Loss: 0.005808754824101925\n",
            "Step 2910, Training Loss: 0.006286932621151209\n",
            "Step 2910, Training Loss: 0.03837795928120613\n",
            "Step 2920, Training Loss: 0.1654053032398224\n",
            "Step 2920, Training Loss: 0.11117897927761078\n",
            "Step 2930, Training Loss: 0.20497968792915344\n",
            "Step 2930, Training Loss: 0.005756048019975424\n",
            "Step 2940, Training Loss: 0.031603094190359116\n",
            "Step 2940, Training Loss: 0.05393386259675026\n",
            "Step 2950, Training Loss: 0.08918531984090805\n",
            "Step 2950, Training Loss: 0.06725016236305237\n",
            "Step 2960, Training Loss: 0.054464612156152725\n",
            "Step 2960, Training Loss: 0.21073777973651886\n",
            "Step 2970, Training Loss: 0.027110429480671883\n",
            "Step 2970, Training Loss: 0.07310009002685547\n",
            "Step 2980, Training Loss: 0.041070617735385895\n",
            "Step 2980, Training Loss: 0.007419171743094921\n",
            "Step 2990, Training Loss: 0.0409039631485939\n",
            "Step 2990, Training Loss: 0.058017000555992126\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Saving model checkpoint to ./results/checkpoint-3000\n",
            "Configuration saved in ./results/checkpoint-3000/config.json\n",
            "Model weights saved in ./results/checkpoint-3000/pytorch_model.bin\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 3000, Training Loss: 0.09264007210731506\n",
            "Step 3000, Training Loss: 0.009535244666039944\n",
            "Step 3010, Training Loss: 0.15132878720760345\n",
            "Step 3010, Training Loss: 0.010799453593790531\n",
            "Step 3020, Training Loss: 0.02557838149368763\n",
            "Step 3020, Training Loss: 0.025038573890924454\n",
            "Step 3030, Training Loss: 0.00566491624340415\n",
            "Step 3030, Training Loss: 0.06923865526914597\n",
            "Step 3040, Training Loss: 0.014002799987792969\n",
            "Step 3040, Training Loss: 0.09418817609548569\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 1523\n",
            "  Batch size = 16\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 3050, Training Loss: 0.037472616881132126\n",
            "Step 3050, Training Loss: 0.02764752134680748\n",
            "Step 3060, Training Loss: 0.03587426617741585\n",
            "Step 3060, Training Loss: 0.007033207453787327\n",
            "Step 3070, Training Loss: 0.06733975559473038\n",
            "Step 3070, Training Loss: 0.05468878522515297\n",
            "Step 3080, Training Loss: 0.09220775216817856\n",
            "Step 3080, Training Loss: 0.03576204180717468\n",
            "Step 3090, Training Loss: 0.09968442469835281\n",
            "Step 3090, Training Loss: 0.04845275357365608\n",
            "Step 3100, Training Loss: 0.031446073204278946\n",
            "Step 3100, Training Loss: 0.03689252585172653\n",
            "Step 3110, Training Loss: 0.008933214470744133\n",
            "Step 3110, Training Loss: 0.036889657378196716\n",
            "Step 3120, Training Loss: 0.056322693824768066\n",
            "Step 3120, Training Loss: 0.02385416068136692\n",
            "Step 3130, Training Loss: 0.006171756889671087\n",
            "Step 3130, Training Loss: 0.03121674619615078\n",
            "Step 3140, Training Loss: 0.1505386084318161\n",
            "Step 3140, Training Loss: 0.08373323827981949\n",
            "Step 3150, Training Loss: 0.07611822336912155\n",
            "Step 3150, Training Loss: 0.008910929784178734\n",
            "Step 3160, Training Loss: 0.05587615445256233\n",
            "Step 3160, Training Loss: 0.007109695114195347\n",
            "Step 3170, Training Loss: 0.006232674699276686\n",
            "Step 3170, Training Loss: 0.07396958768367767\n",
            "Step 3180, Training Loss: 0.006641071755439043\n",
            "Step 3180, Training Loss: 0.2015644758939743\n",
            "Step 3190, Training Loss: 0.03319687396287918\n",
            "Step 3190, Training Loss: 0.00915618147701025\n",
            "Step 3200, Training Loss: 0.006219063885509968\n",
            "Step 3200, Training Loss: 0.10697813332080841\n",
            "Step 3210, Training Loss: 0.006040027365088463\n",
            "Step 3210, Training Loss: 0.05071185156702995\n",
            "Step 3220, Training Loss: 0.05710877105593681\n",
            "Step 3220, Training Loss: 0.0681944489479065\n",
            "Step 3230, Training Loss: 0.10126200318336487\n",
            "Step 3230, Training Loss: 0.09708688408136368\n",
            "Step 3240, Training Loss: 0.04252905026078224\n",
            "Step 3240, Training Loss: 0.024148063734173775\n",
            "Step 3250, Training Loss: 0.2601684331893921\n",
            "Step 3250, Training Loss: 0.03168661147356033\n",
            "Step 3260, Training Loss: 0.09394577145576477\n",
            "Step 3260, Training Loss: 0.04722313582897186\n",
            "Step 3270, Training Loss: 0.035411085933446884\n",
            "Step 3270, Training Loss: 0.06623779982328415\n",
            "Step 3280, Training Loss: 0.15585453808307648\n",
            "Step 3280, Training Loss: 0.010558507405221462\n",
            "Step 3290, Training Loss: 0.05548577010631561\n",
            "Step 3290, Training Loss: 0.039465952664613724\n",
            "Step 3300, Training Loss: 0.030271941795945168\n",
            "Step 3300, Training Loss: 0.031634371727705\n",
            "Step 3310, Training Loss: 0.006505780387669802\n",
            "Step 3310, Training Loss: 0.03666147217154503\n",
            "Step 3320, Training Loss: 0.14557147026062012\n",
            "Step 3320, Training Loss: 0.032100044190883636\n",
            "Step 3330, Training Loss: 0.030459174886345863\n",
            "Step 3330, Training Loss: 0.03127995878458023\n",
            "Step 3340, Training Loss: 0.04581921547651291\n",
            "Step 3340, Training Loss: 0.0319426953792572\n",
            "Step 3350, Training Loss: 0.006269008386880159\n",
            "Step 3350, Training Loss: 0.19445370137691498\n",
            "Step 3360, Training Loss: 0.030230434611439705\n",
            "Step 3360, Training Loss: 0.03560943529009819\n",
            "Step 3370, Training Loss: 0.08651234209537506\n",
            "Step 3370, Training Loss: 0.12015946954488754\n",
            "Step 3380, Training Loss: 0.05786197632551193\n",
            "Step 3380, Training Loss: 0.03252148628234863\n",
            "Step 3390, Training Loss: 0.11474252492189407\n",
            "Step 3390, Training Loss: 0.030626224353909492\n",
            "Step 3400, Training Loss: 0.008763493970036507\n",
            "Step 3400, Training Loss: 0.030625630170106888\n",
            "Step 3410, Training Loss: 0.026191767305135727\n",
            "Step 3410, Training Loss: 0.09447643160820007\n",
            "Step 3420, Training Loss: 0.034322500228881836\n",
            "Step 3420, Training Loss: 0.01054768543690443\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 1523\n",
            "  Batch size = 16\n",
            "\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 1523\n",
            "  Batch size = 16\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training completed in 299.66 seconds.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='96' max='96' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [96/96 00:02]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-12-09 19:02:03,779] Trial 2 finished with value: 0.8452128992403838 and parameters: {'learning_rate': 2.422325045462102e-05, 'batch_size': 8, 'num_train_epochs': 9, 'weight_decay': 0.011844399570276054}. Best is trial 2 with value: 0.8452128992403838.\n",
            "<ipython-input-12-42465065092b>:2: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  learning_rate = trial.suggest_loguniform(\"learning_rate\", 1e-6, 5e-5)\n",
            "<ipython-input-12-42465065092b>:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  weight_decay = trial.suggest_loguniform(\"weight_decay\", 1e-4, 0.1)\n",
            "PyTorch: setting up devices\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n",
            "***** Running training *****\n",
            "  Num examples = 6091\n",
            "  Num Epochs = 10\n",
            "  Instantaneous batch size per device = 16\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
            "  Gradient Accumulation steps = 2\n",
            "  Total optimization steps = 1900\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting training...\n",
            "Step 0, Training Loss: 0.024000700563192368\n",
            "Step 0, Training Loss: 0.04743264615535736\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1900' max='1900' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1900/1900 05:07, Epoch 9/10]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "      <th>F1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>0.040700</td>\n",
              "      <td>0.144012</td>\n",
              "      <td>0.836507</td>\n",
              "      <td>0.864835</td>\n",
              "      <td>0.837612</td>\n",
              "      <td>0.841382</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.038600</td>\n",
              "      <td>0.141743</td>\n",
              "      <td>0.841103</td>\n",
              "      <td>0.873501</td>\n",
              "      <td>0.844031</td>\n",
              "      <td>0.848103</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.056200</td>\n",
              "      <td>0.143995</td>\n",
              "      <td>0.843073</td>\n",
              "      <td>0.867482</td>\n",
              "      <td>0.847240</td>\n",
              "      <td>0.849733</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.054600</td>\n",
              "      <td>0.144006</td>\n",
              "      <td>0.845699</td>\n",
              "      <td>0.864033</td>\n",
              "      <td>0.850449</td>\n",
              "      <td>0.850639</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.044800</td>\n",
              "      <td>0.143132</td>\n",
              "      <td>0.843729</td>\n",
              "      <td>0.866057</td>\n",
              "      <td>0.847240</td>\n",
              "      <td>0.850219</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>0.051300</td>\n",
              "      <td>0.145005</td>\n",
              "      <td>0.842416</td>\n",
              "      <td>0.863733</td>\n",
              "      <td>0.845315</td>\n",
              "      <td>0.846958</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>0.042800</td>\n",
              "      <td>0.142196</td>\n",
              "      <td>0.848982</td>\n",
              "      <td>0.868158</td>\n",
              "      <td>0.850449</td>\n",
              "      <td>0.852489</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>0.058700</td>\n",
              "      <td>0.143682</td>\n",
              "      <td>0.843729</td>\n",
              "      <td>0.862782</td>\n",
              "      <td>0.846598</td>\n",
              "      <td>0.848572</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>0.044100</td>\n",
              "      <td>0.143354</td>\n",
              "      <td>0.845043</td>\n",
              "      <td>0.862353</td>\n",
              "      <td>0.851091</td>\n",
              "      <td>0.851797</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>0.050400</td>\n",
              "      <td>0.143849</td>\n",
              "      <td>0.841760</td>\n",
              "      <td>0.861132</td>\n",
              "      <td>0.846598</td>\n",
              "      <td>0.848533</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 10, Training Loss: 0.008725306019186974\n",
            "Step 10, Training Loss: 0.0911753699183464\n",
            "Step 20, Training Loss: 0.03949439525604248\n",
            "Step 20, Training Loss: 0.01549556665122509\n",
            "Step 30, Training Loss: 0.04471024125814438\n",
            "Step 30, Training Loss: 0.0895194262266159\n",
            "Step 40, Training Loss: 0.011561202816665173\n",
            "Step 40, Training Loss: 0.0058341179974377155\n",
            "Step 50, Training Loss: 0.008217815309762955\n",
            "Step 50, Training Loss: 0.049030203372240067\n",
            "Step 60, Training Loss: 0.009197493083775043\n",
            "Step 60, Training Loss: 0.17687954008579254\n",
            "Step 70, Training Loss: 0.08036231994628906\n",
            "Step 70, Training Loss: 0.06865616887807846\n",
            "Step 80, Training Loss: 0.054912835359573364\n",
            "Step 80, Training Loss: 0.033088285475969315\n",
            "Step 90, Training Loss: 0.01847977377474308\n",
            "Step 90, Training Loss: 0.03983765095472336\n",
            "Step 100, Training Loss: 0.033987995237112045\n",
            "Step 100, Training Loss: 0.00940043292939663\n",
            "Step 110, Training Loss: 0.08206088840961456\n",
            "Step 110, Training Loss: 0.06806735694408417\n",
            "Step 120, Training Loss: 0.04717891663312912\n",
            "Step 120, Training Loss: 0.012973347678780556\n",
            "Step 130, Training Loss: 0.06728412210941315\n",
            "Step 130, Training Loss: 0.020082196220755577\n",
            "Step 140, Training Loss: 0.07856343686580658\n",
            "Step 140, Training Loss: 0.03285899758338928\n",
            "Step 150, Training Loss: 0.08079437166452408\n",
            "Step 150, Training Loss: 0.08523067831993103\n",
            "Step 160, Training Loss: 0.14896872639656067\n",
            "Step 160, Training Loss: 0.053843267261981964\n",
            "Step 170, Training Loss: 0.11491556465625763\n",
            "Step 170, Training Loss: 0.05159640684723854\n",
            "Step 180, Training Loss: 0.07720761746168137\n",
            "Step 180, Training Loss: 0.04670488089323044\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 1523\n",
            "  Batch size = 32\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 190, Training Loss: 0.04216034337878227\n",
            "Step 190, Training Loss: 0.2325299233198166\n",
            "Step 190, Training Loss: 0.24401816725730896\n",
            "Step 190, Training Loss: 0.08649842441082001\n",
            "Step 190, Training Loss: 0.17822474241256714\n",
            "Step 190, Training Loss: 0.14435875415802002\n",
            "Step 190, Training Loss: 0.2001018226146698\n",
            "Step 190, Training Loss: 0.0993945524096489\n",
            "Step 190, Training Loss: 0.2001628577709198\n",
            "Step 190, Training Loss: 0.08367487043142319\n",
            "Step 190, Training Loss: 0.1650930494070053\n",
            "Step 190, Training Loss: 0.06400076299905777\n",
            "Step 190, Training Loss: 0.1685567945241928\n",
            "Step 190, Training Loss: 0.12919966876506805\n",
            "Step 190, Training Loss: 0.04700843244791031\n",
            "Step 190, Training Loss: 0.09678150713443756\n",
            "Step 190, Training Loss: 0.10668493807315826\n",
            "Step 190, Training Loss: 0.1538398712873459\n",
            "Step 190, Training Loss: 0.04211262986063957\n",
            "Step 190, Training Loss: 0.2013956904411316\n",
            "Step 190, Training Loss: 0.10764679312705994\n",
            "Step 190, Training Loss: 0.21465684473514557\n",
            "Step 190, Training Loss: 0.21270617842674255\n",
            "Step 190, Training Loss: 0.10625595599412918\n",
            "Step 190, Training Loss: 0.13551494479179382\n",
            "Step 190, Training Loss: 0.2536121606826782\n",
            "Step 190, Training Loss: 0.2367258369922638\n",
            "Step 190, Training Loss: 0.10312162339687347\n",
            "Step 190, Training Loss: 0.16857923567295074\n",
            "Step 190, Training Loss: 0.18744637072086334\n",
            "Step 190, Training Loss: 0.04718229919672012\n",
            "Step 190, Training Loss: 0.22491487860679626\n",
            "Step 190, Training Loss: 0.14047832787036896\n",
            "Step 190, Training Loss: 0.17083124816417694\n",
            "Step 190, Training Loss: 0.11895464360713959\n",
            "Step 190, Training Loss: 0.020444132387638092\n",
            "Step 190, Training Loss: 0.24521209299564362\n",
            "Step 190, Training Loss: 0.13663789629936218\n",
            "Step 190, Training Loss: 0.14828269183635712\n",
            "Step 190, Training Loss: 0.16188178956508636\n",
            "Step 190, Training Loss: 0.09907045215368271\n",
            "Step 190, Training Loss: 0.19484911859035492\n",
            "Step 190, Training Loss: 0.1697748303413391\n",
            "Step 190, Training Loss: 0.05990344658493996\n",
            "Step 190, Training Loss: 0.08175869286060333\n",
            "Step 190, Training Loss: 0.17557908594608307\n",
            "Step 190, Training Loss: 0.07737817615270615\n",
            "Step 190, Training Loss: 0.15427428483963013\n",
            "Step 190, Training Loss: 0.09558521956205368\n",
            "Step 190, Training Loss: 0.05088113620877266\n",
            "Step 190, Training Loss: 0.05127561464905739\n",
            "Step 200, Training Loss: 0.010126208886504173\n",
            "Step 200, Training Loss: 0.033274002373218536\n",
            "Step 210, Training Loss: 0.07668482512235641\n",
            "Step 210, Training Loss: 0.031478408724069595\n",
            "Step 220, Training Loss: 0.01658639870584011\n",
            "Step 220, Training Loss: 0.12148386240005493\n",
            "Step 230, Training Loss: 0.009667468257248402\n",
            "Step 230, Training Loss: 0.021245544776320457\n",
            "Step 240, Training Loss: 0.03768245503306389\n",
            "Step 240, Training Loss: 0.08587294071912766\n",
            "Step 250, Training Loss: 0.009822617284953594\n",
            "Step 250, Training Loss: 0.03212420269846916\n",
            "Step 260, Training Loss: 0.028312498703598976\n",
            "Step 260, Training Loss: 0.11600395292043686\n",
            "Step 270, Training Loss: 0.022409580647945404\n",
            "Step 270, Training Loss: 0.0787113755941391\n",
            "Step 280, Training Loss: 0.019780734553933144\n",
            "Step 280, Training Loss: 0.06671898066997528\n",
            "Step 290, Training Loss: 0.07429920881986618\n",
            "Step 290, Training Loss: 0.07444097101688385\n",
            "Step 300, Training Loss: 0.09793781489133835\n",
            "Step 300, Training Loss: 0.03852250799536705\n",
            "Step 310, Training Loss: 0.0366784892976284\n",
            "Step 310, Training Loss: 0.034338392317295074\n",
            "Step 320, Training Loss: 0.007198790088295937\n",
            "Step 320, Training Loss: 0.036644406616687775\n",
            "Step 330, Training Loss: 0.03347775340080261\n",
            "Step 330, Training Loss: 0.0416906513273716\n",
            "Step 340, Training Loss: 0.007173608988523483\n",
            "Step 340, Training Loss: 0.03463366627693176\n",
            "Step 350, Training Loss: 0.04022656008601189\n",
            "Step 350, Training Loss: 0.08891800791025162\n",
            "Step 360, Training Loss: 0.06807556003332138\n",
            "Step 360, Training Loss: 0.04227480664849281\n",
            "Step 370, Training Loss: 0.009372152388095856\n",
            "Step 370, Training Loss: 0.038601577281951904\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 1523\n",
            "  Batch size = 32\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 380, Training Loss: 0.033601127564907074\n",
            "Step 380, Training Loss: 0.22662419080734253\n",
            "Step 380, Training Loss: 0.2518859803676605\n",
            "Step 380, Training Loss: 0.08511309325695038\n",
            "Step 380, Training Loss: 0.16720181703567505\n",
            "Step 380, Training Loss: 0.14552411437034607\n",
            "Step 380, Training Loss: 0.19860197603702545\n",
            "Step 380, Training Loss: 0.09889526665210724\n",
            "Step 380, Training Loss: 0.19428390264511108\n",
            "Step 380, Training Loss: 0.08220207691192627\n",
            "Step 380, Training Loss: 0.1645907163619995\n",
            "Step 380, Training Loss: 0.06501482427120209\n",
            "Step 380, Training Loss: 0.15554891526699066\n",
            "Step 380, Training Loss: 0.1133500412106514\n",
            "Step 380, Training Loss: 0.047969020903110504\n",
            "Step 380, Training Loss: 0.09331095963716507\n",
            "Step 380, Training Loss: 0.10191486775875092\n",
            "Step 380, Training Loss: 0.1612733006477356\n",
            "Step 380, Training Loss: 0.04164062440395355\n",
            "Step 380, Training Loss: 0.18913131952285767\n",
            "Step 380, Training Loss: 0.1131521686911583\n",
            "Step 380, Training Loss: 0.21590536832809448\n",
            "Step 380, Training Loss: 0.20932058990001678\n",
            "Step 380, Training Loss: 0.10472069680690765\n",
            "Step 380, Training Loss: 0.13489983975887299\n",
            "Step 380, Training Loss: 0.24625654518604279\n",
            "Step 380, Training Loss: 0.24007955193519592\n",
            "Step 380, Training Loss: 0.08365871757268906\n",
            "Step 380, Training Loss: 0.16630423069000244\n",
            "Step 380, Training Loss: 0.18411746621131897\n",
            "Step 380, Training Loss: 0.05081908777356148\n",
            "Step 380, Training Loss: 0.22406089305877686\n",
            "Step 380, Training Loss: 0.12732432782649994\n",
            "Step 380, Training Loss: 0.1652870923280716\n",
            "Step 380, Training Loss: 0.1233176663517952\n",
            "Step 380, Training Loss: 0.02030838094651699\n",
            "Step 380, Training Loss: 0.26269710063934326\n",
            "Step 380, Training Loss: 0.13686715066432953\n",
            "Step 380, Training Loss: 0.15018706023693085\n",
            "Step 380, Training Loss: 0.16085681319236755\n",
            "Step 380, Training Loss: 0.09648207575082779\n",
            "Step 380, Training Loss: 0.1970827281475067\n",
            "Step 380, Training Loss: 0.17374619841575623\n",
            "Step 380, Training Loss: 0.04426946863532066\n",
            "Step 380, Training Loss: 0.08061724156141281\n",
            "Step 380, Training Loss: 0.16036930680274963\n",
            "Step 380, Training Loss: 0.08027980476617813\n",
            "Step 380, Training Loss: 0.15147408843040466\n",
            "Step 380, Training Loss: 0.09695203602313995\n",
            "Step 380, Training Loss: 0.02344503439962864\n",
            "Step 380, Training Loss: 0.06641488522291183\n",
            "Step 390, Training Loss: 0.015400056727230549\n",
            "Step 390, Training Loss: 0.06000811606645584\n",
            "Step 400, Training Loss: 0.033346518874168396\n",
            "Step 400, Training Loss: 0.06949785351753235\n",
            "Step 410, Training Loss: 0.028604405000805855\n",
            "Step 410, Training Loss: 0.037983477115631104\n",
            "Step 420, Training Loss: 0.05063356086611748\n",
            "Step 420, Training Loss: 0.07691480219364166\n",
            "Step 430, Training Loss: 0.044174324721097946\n",
            "Step 430, Training Loss: 0.10795370489358902\n",
            "Step 440, Training Loss: 0.1242433562874794\n",
            "Step 440, Training Loss: 0.04072074964642525\n",
            "Step 450, Training Loss: 0.06632331758737564\n",
            "Step 450, Training Loss: 0.024131065234541893\n",
            "Step 460, Training Loss: 0.040190041065216064\n",
            "Step 460, Training Loss: 0.08956453949213028\n",
            "Step 470, Training Loss: 0.04966986924409866\n",
            "Step 470, Training Loss: 0.07266642153263092\n",
            "Step 480, Training Loss: 0.060044292360544205\n",
            "Step 480, Training Loss: 0.14186379313468933\n",
            "Step 490, Training Loss: 0.03044985979795456\n",
            "Step 490, Training Loss: 0.05540505796670914\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Saving model checkpoint to ./results/checkpoint-500\n",
            "Configuration saved in ./results/checkpoint-500/config.json\n",
            "Model weights saved in ./results/checkpoint-500/pytorch_model.bin\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 500, Training Loss: 0.08369715511798859\n",
            "Step 500, Training Loss: 0.09247162193059921\n",
            "Step 510, Training Loss: 0.01642516627907753\n",
            "Step 510, Training Loss: 0.038920026272535324\n",
            "Step 520, Training Loss: 0.03305889293551445\n",
            "Step 520, Training Loss: 0.0363009050488472\n",
            "Step 530, Training Loss: 0.029590612277388573\n",
            "Step 530, Training Loss: 0.09046341478824615\n",
            "Step 540, Training Loss: 0.10126695036888123\n",
            "Step 540, Training Loss: 0.07919075340032578\n",
            "Step 550, Training Loss: 0.039485856890678406\n",
            "Step 550, Training Loss: 0.06338198482990265\n",
            "Step 560, Training Loss: 0.04070603474974632\n",
            "Step 560, Training Loss: 0.08518389612436295\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 1523\n",
            "  Batch size = 32\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 570, Training Loss: 0.08336596935987473\n",
            "Step 570, Training Loss: 0.22662237286567688\n",
            "Step 570, Training Loss: 0.24985259771347046\n",
            "Step 570, Training Loss: 0.08559213578701019\n",
            "Step 570, Training Loss: 0.17317909002304077\n",
            "Step 570, Training Loss: 0.14258289337158203\n",
            "Step 570, Training Loss: 0.19925667345523834\n",
            "Step 570, Training Loss: 0.09941845387220383\n",
            "Step 570, Training Loss: 0.19366689026355743\n",
            "Step 570, Training Loss: 0.08504362404346466\n",
            "Step 570, Training Loss: 0.16328267753124237\n",
            "Step 570, Training Loss: 0.06443934142589569\n",
            "Step 570, Training Loss: 0.1633601188659668\n",
            "Step 570, Training Loss: 0.1335313618183136\n",
            "Step 570, Training Loss: 0.04628757759928703\n",
            "Step 570, Training Loss: 0.0920340046286583\n",
            "Step 570, Training Loss: 0.10348536819219589\n",
            "Step 570, Training Loss: 0.1555982530117035\n",
            "Step 570, Training Loss: 0.042693208903074265\n",
            "Step 570, Training Loss: 0.19012980163097382\n",
            "Step 570, Training Loss: 0.12170156836509705\n",
            "Step 570, Training Loss: 0.21782641112804413\n",
            "Step 570, Training Loss: 0.21443146467208862\n",
            "Step 570, Training Loss: 0.10618098825216293\n",
            "Step 570, Training Loss: 0.13961510360240936\n",
            "Step 570, Training Loss: 0.253918319940567\n",
            "Step 570, Training Loss: 0.23709356784820557\n",
            "Step 570, Training Loss: 0.09151025861501694\n",
            "Step 570, Training Loss: 0.1693667769432068\n",
            "Step 570, Training Loss: 0.19016723334789276\n",
            "Step 570, Training Loss: 0.049078915268182755\n",
            "Step 570, Training Loss: 0.2238883525133133\n",
            "Step 570, Training Loss: 0.1369258165359497\n",
            "Step 570, Training Loss: 0.173698291182518\n",
            "Step 570, Training Loss: 0.12196049094200134\n",
            "Step 570, Training Loss: 0.021537048742175102\n",
            "Step 570, Training Loss: 0.25157105922698975\n",
            "Step 570, Training Loss: 0.13503743708133698\n",
            "Step 570, Training Loss: 0.1498575657606125\n",
            "Step 570, Training Loss: 0.16017764806747437\n",
            "Step 570, Training Loss: 0.10292040556669235\n",
            "Step 570, Training Loss: 0.20459984242916107\n",
            "Step 570, Training Loss: 0.17690721154212952\n",
            "Step 570, Training Loss: 0.05115358531475067\n",
            "Step 570, Training Loss: 0.08095836639404297\n",
            "Step 570, Training Loss: 0.17144538462162018\n",
            "Step 570, Training Loss: 0.07837150245904922\n",
            "Step 570, Training Loss: 0.1536872237920761\n",
            "Step 570, Training Loss: 0.09704770147800446\n",
            "Step 570, Training Loss: 0.043925248086452484\n",
            "Step 570, Training Loss: 0.05980601906776428\n",
            "Step 580, Training Loss: 0.05873630940914154\n",
            "Step 580, Training Loss: 0.07750129699707031\n",
            "Step 590, Training Loss: 0.047128237783908844\n",
            "Step 590, Training Loss: 0.08228620886802673\n",
            "Step 600, Training Loss: 0.033908676356077194\n",
            "Step 600, Training Loss: 0.030585426837205887\n",
            "Step 610, Training Loss: 0.07393632084131241\n",
            "Step 610, Training Loss: 0.006876555271446705\n",
            "Step 620, Training Loss: 0.06139747425913811\n",
            "Step 620, Training Loss: 0.03839072212576866\n",
            "Step 630, Training Loss: 0.06812665611505508\n",
            "Step 630, Training Loss: 0.018615294247865677\n",
            "Step 640, Training Loss: 0.050383273512125015\n",
            "Step 640, Training Loss: 0.10649402439594269\n",
            "Step 650, Training Loss: 0.05110106244683266\n",
            "Step 650, Training Loss: 0.03835838660597801\n",
            "Step 660, Training Loss: 0.007907059043645859\n",
            "Step 660, Training Loss: 0.05015651136636734\n",
            "Step 670, Training Loss: 0.18231526017189026\n",
            "Step 670, Training Loss: 0.10557429492473602\n",
            "Step 680, Training Loss: 0.15785188972949982\n",
            "Step 680, Training Loss: 0.022169852629303932\n",
            "Step 690, Training Loss: 0.025503475219011307\n",
            "Step 690, Training Loss: 0.056444186717271805\n",
            "Step 700, Training Loss: 0.016602924093604088\n",
            "Step 700, Training Loss: 0.123079814016819\n",
            "Step 710, Training Loss: 0.05445706099271774\n",
            "Step 710, Training Loss: 0.02786524035036564\n",
            "Step 720, Training Loss: 0.02680610492825508\n",
            "Step 720, Training Loss: 0.040268223732709885\n",
            "Step 730, Training Loss: 0.03874220326542854\n",
            "Step 730, Training Loss: 0.06877616047859192\n",
            "Step 740, Training Loss: 0.06407196819782257\n",
            "Step 740, Training Loss: 0.018252605572342873\n",
            "Step 750, Training Loss: 0.07305474579334259\n",
            "Step 750, Training Loss: 0.1745980679988861\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 1523\n",
            "  Batch size = 32\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 760, Training Loss: 0.014304009266197681\n",
            "Step 760, Training Loss: 0.22913995385169983\n",
            "Step 760, Training Loss: 0.24972017109394073\n",
            "Step 760, Training Loss: 0.0854816809296608\n",
            "Step 760, Training Loss: 0.16898296773433685\n",
            "Step 760, Training Loss: 0.15338726341724396\n",
            "Step 760, Training Loss: 0.19878371059894562\n",
            "Step 760, Training Loss: 0.0996578186750412\n",
            "Step 760, Training Loss: 0.1955956518650055\n",
            "Step 760, Training Loss: 0.0856727734208107\n",
            "Step 760, Training Loss: 0.16231504082679749\n",
            "Step 760, Training Loss: 0.06457017362117767\n",
            "Step 760, Training Loss: 0.16366004943847656\n",
            "Step 760, Training Loss: 0.1345129907131195\n",
            "Step 760, Training Loss: 0.04713566228747368\n",
            "Step 760, Training Loss: 0.09093604981899261\n",
            "Step 760, Training Loss: 0.1071796789765358\n",
            "Step 760, Training Loss: 0.15145809948444366\n",
            "Step 760, Training Loss: 0.041461728513240814\n",
            "Step 760, Training Loss: 0.18895630538463593\n",
            "Step 760, Training Loss: 0.11536360532045364\n",
            "Step 760, Training Loss: 0.21604885160923004\n",
            "Step 760, Training Loss: 0.21477657556533813\n",
            "Step 760, Training Loss: 0.1069992408156395\n",
            "Step 760, Training Loss: 0.13656607270240784\n",
            "Step 760, Training Loss: 0.2472974956035614\n",
            "Step 760, Training Loss: 0.23635630309581757\n",
            "Step 760, Training Loss: 0.08778581768274307\n",
            "Step 760, Training Loss: 0.1690620481967926\n",
            "Step 760, Training Loss: 0.1909414827823639\n",
            "Step 760, Training Loss: 0.04776518791913986\n",
            "Step 760, Training Loss: 0.227986142039299\n",
            "Step 760, Training Loss: 0.13366742432117462\n",
            "Step 760, Training Loss: 0.18008418381214142\n",
            "Step 760, Training Loss: 0.12029386311769485\n",
            "Step 760, Training Loss: 0.019356094300746918\n",
            "Step 760, Training Loss: 0.2520352005958557\n",
            "Step 760, Training Loss: 0.14150556921958923\n",
            "Step 760, Training Loss: 0.14708848297595978\n",
            "Step 760, Training Loss: 0.1683589518070221\n",
            "Step 760, Training Loss: 0.10344737023115158\n",
            "Step 760, Training Loss: 0.20047713816165924\n",
            "Step 760, Training Loss: 0.17387598752975464\n",
            "Step 760, Training Loss: 0.059993162751197815\n",
            "Step 760, Training Loss: 0.07984793186187744\n",
            "Step 760, Training Loss: 0.1653268039226532\n",
            "Step 760, Training Loss: 0.07807984203100204\n",
            "Step 760, Training Loss: 0.15329933166503906\n",
            "Step 760, Training Loss: 0.10357385128736496\n",
            "Step 760, Training Loss: 0.021802755072712898\n",
            "Step 760, Training Loss: 0.011212519370019436\n",
            "Step 770, Training Loss: 0.009831777773797512\n",
            "Step 770, Training Loss: 0.02630353905260563\n",
            "Step 780, Training Loss: 0.007356191519647837\n",
            "Step 780, Training Loss: 0.05704052373766899\n",
            "Step 790, Training Loss: 0.01724311150610447\n",
            "Step 790, Training Loss: 0.03902244195342064\n",
            "Step 800, Training Loss: 0.03358534350991249\n",
            "Step 800, Training Loss: 0.045635025948286057\n",
            "Step 810, Training Loss: 0.07102030515670776\n",
            "Step 810, Training Loss: 0.07684244960546494\n",
            "Step 820, Training Loss: 0.06530460715293884\n",
            "Step 820, Training Loss: 0.037214428186416626\n",
            "Step 830, Training Loss: 0.041786469519138336\n",
            "Step 830, Training Loss: 0.029627375304698944\n",
            "Step 840, Training Loss: 0.08618876338005066\n",
            "Step 840, Training Loss: 0.02000410296022892\n",
            "Step 850, Training Loss: 0.07356315851211548\n",
            "Step 850, Training Loss: 0.017946474254131317\n",
            "Step 860, Training Loss: 0.00910062063485384\n",
            "Step 860, Training Loss: 0.03441769257187843\n",
            "Step 870, Training Loss: 0.04744352400302887\n",
            "Step 870, Training Loss: 0.07380536943674088\n",
            "Step 880, Training Loss: 0.03246277570724487\n",
            "Step 880, Training Loss: 0.03144028037786484\n",
            "Step 890, Training Loss: 0.07470330595970154\n",
            "Step 890, Training Loss: 0.021373538300395012\n",
            "Step 900, Training Loss: 0.14044667780399323\n",
            "Step 900, Training Loss: 0.03210161253809929\n",
            "Step 910, Training Loss: 0.006708943750709295\n",
            "Step 910, Training Loss: 0.032075002789497375\n",
            "Step 920, Training Loss: 0.015953753143548965\n",
            "Step 920, Training Loss: 0.09136806428432465\n",
            "Step 930, Training Loss: 0.06245390698313713\n",
            "Step 930, Training Loss: 0.02546832151710987\n",
            "Step 940, Training Loss: 0.08490738272666931\n",
            "Step 940, Training Loss: 0.05027105286717415\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 1523\n",
            "  Batch size = 32\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 950, Training Loss: 0.007978216744959354\n",
            "Step 950, Training Loss: 0.2265135496854782\n",
            "Step 950, Training Loss: 0.24758870899677277\n",
            "Step 950, Training Loss: 0.08608266711235046\n",
            "Step 950, Training Loss: 0.17200590670108795\n",
            "Step 950, Training Loss: 0.14751148223876953\n",
            "Step 950, Training Loss: 0.19825640320777893\n",
            "Step 950, Training Loss: 0.09889480471611023\n",
            "Step 950, Training Loss: 0.19106772541999817\n",
            "Step 950, Training Loss: 0.08708801120519638\n",
            "Step 950, Training Loss: 0.16755953431129456\n",
            "Step 950, Training Loss: 0.06262017786502838\n",
            "Step 950, Training Loss: 0.15957990288734436\n",
            "Step 950, Training Loss: 0.12228566408157349\n",
            "Step 950, Training Loss: 0.048484839498996735\n",
            "Step 950, Training Loss: 0.09557066857814789\n",
            "Step 950, Training Loss: 0.11059658229351044\n",
            "Step 950, Training Loss: 0.1519710123538971\n",
            "Step 950, Training Loss: 0.0418674610555172\n",
            "Step 950, Training Loss: 0.1847107857465744\n",
            "Step 950, Training Loss: 0.11918755620718002\n",
            "Step 950, Training Loss: 0.2210771143436432\n",
            "Step 950, Training Loss: 0.2122274488210678\n",
            "Step 950, Training Loss: 0.10614100843667984\n",
            "Step 950, Training Loss: 0.13847486674785614\n",
            "Step 950, Training Loss: 0.25486987829208374\n",
            "Step 950, Training Loss: 0.23614169657230377\n",
            "Step 950, Training Loss: 0.08862657845020294\n",
            "Step 950, Training Loss: 0.16968576610088348\n",
            "Step 950, Training Loss: 0.18847587704658508\n",
            "Step 950, Training Loss: 0.04883391410112381\n",
            "Step 950, Training Loss: 0.2228669375181198\n",
            "Step 950, Training Loss: 0.13167007267475128\n",
            "Step 950, Training Loss: 0.17121385037899017\n",
            "Step 950, Training Loss: 0.12446413934230804\n",
            "Step 950, Training Loss: 0.02304469794034958\n",
            "Step 950, Training Loss: 0.25321322679519653\n",
            "Step 950, Training Loss: 0.1330724060535431\n",
            "Step 950, Training Loss: 0.14760726690292358\n",
            "Step 950, Training Loss: 0.1613241285085678\n",
            "Step 950, Training Loss: 0.10140521079301834\n",
            "Step 950, Training Loss: 0.19842664897441864\n",
            "Step 950, Training Loss: 0.1770232766866684\n",
            "Step 950, Training Loss: 0.0505065955221653\n",
            "Step 950, Training Loss: 0.08072558790445328\n",
            "Step 950, Training Loss: 0.16991840302944183\n",
            "Step 950, Training Loss: 0.07586982101202011\n",
            "Step 950, Training Loss: 0.15066319704055786\n",
            "Step 950, Training Loss: 0.09293866157531738\n",
            "Step 950, Training Loss: 0.04414992034435272\n",
            "Step 950, Training Loss: 0.061860695481300354\n",
            "Step 960, Training Loss: 0.00851228553801775\n",
            "Step 960, Training Loss: 0.03559274971485138\n",
            "Step 970, Training Loss: 0.017614005133509636\n",
            "Step 970, Training Loss: 0.05280932039022446\n",
            "Step 980, Training Loss: 0.020191563293337822\n",
            "Step 980, Training Loss: 0.0540836825966835\n",
            "Step 990, Training Loss: 0.04837365821003914\n",
            "Step 990, Training Loss: 0.03591001778841019\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Saving model checkpoint to ./results/checkpoint-1000\n",
            "Configuration saved in ./results/checkpoint-1000/config.json\n",
            "Model weights saved in ./results/checkpoint-1000/pytorch_model.bin\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 1000, Training Loss: 0.026871535927057266\n",
            "Step 1000, Training Loss: 0.051975276321172714\n",
            "Step 1010, Training Loss: 0.03063884750008583\n",
            "Step 1010, Training Loss: 0.05762050673365593\n",
            "Step 1020, Training Loss: 0.03133314102888107\n",
            "Step 1020, Training Loss: 0.03899306058883667\n",
            "Step 1030, Training Loss: 0.046515483409166336\n",
            "Step 1030, Training Loss: 0.10676709562540054\n",
            "Step 1040, Training Loss: 0.022636601701378822\n",
            "Step 1040, Training Loss: 0.009490394964814186\n",
            "Step 1050, Training Loss: 0.028437402099370956\n",
            "Step 1050, Training Loss: 0.06540991365909576\n",
            "Step 1060, Training Loss: 0.0501345656812191\n",
            "Step 1060, Training Loss: 0.12965954840183258\n",
            "Step 1070, Training Loss: 0.11240983754396439\n",
            "Step 1070, Training Loss: 0.034629255533218384\n",
            "Step 1080, Training Loss: 0.00670692790299654\n",
            "Step 1080, Training Loss: 0.03480643779039383\n",
            "Step 1090, Training Loss: 0.021203508600592613\n",
            "Step 1090, Training Loss: 0.023139258846640587\n",
            "Step 1100, Training Loss: 0.059056300669908524\n",
            "Step 1100, Training Loss: 0.05474896356463432\n",
            "Step 1110, Training Loss: 0.04410303756594658\n",
            "Step 1110, Training Loss: 0.03684080392122269\n",
            "Step 1120, Training Loss: 0.052857767790555954\n",
            "Step 1120, Training Loss: 0.03198699653148651\n",
            "Step 1130, Training Loss: 0.10352010279893875\n",
            "Step 1130, Training Loss: 0.005866608116775751\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 1523\n",
            "  Batch size = 32\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 1140, Training Loss: 0.168514221906662\n",
            "Step 1140, Training Loss: 0.23014934360980988\n",
            "Step 1140, Training Loss: 0.24917283654212952\n",
            "Step 1140, Training Loss: 0.08685194700956345\n",
            "Step 1140, Training Loss: 0.17290247976779938\n",
            "Step 1140, Training Loss: 0.15390048921108246\n",
            "Step 1140, Training Loss: 0.19802336394786835\n",
            "Step 1140, Training Loss: 0.09915570169687271\n",
            "Step 1140, Training Loss: 0.1967148333787918\n",
            "Step 1140, Training Loss: 0.08702235668897629\n",
            "Step 1140, Training Loss: 0.16609100997447968\n",
            "Step 1140, Training Loss: 0.06560356914997101\n",
            "Step 1140, Training Loss: 0.17042550444602966\n",
            "Step 1140, Training Loss: 0.14073359966278076\n",
            "Step 1140, Training Loss: 0.045966070145368576\n",
            "Step 1140, Training Loss: 0.09257357567548752\n",
            "Step 1140, Training Loss: 0.10965389758348465\n",
            "Step 1140, Training Loss: 0.14923785626888275\n",
            "Step 1140, Training Loss: 0.04195844382047653\n",
            "Step 1140, Training Loss: 0.19263000786304474\n",
            "Step 1140, Training Loss: 0.1154223084449768\n",
            "Step 1140, Training Loss: 0.22068913280963898\n",
            "Step 1140, Training Loss: 0.21184667944908142\n",
            "Step 1140, Training Loss: 0.10766953229904175\n",
            "Step 1140, Training Loss: 0.13631747663021088\n",
            "Step 1140, Training Loss: 0.2561569809913635\n",
            "Step 1140, Training Loss: 0.2362542450428009\n",
            "Step 1140, Training Loss: 0.08890393376350403\n",
            "Step 1140, Training Loss: 0.17107726633548737\n",
            "Step 1140, Training Loss: 0.1916080266237259\n",
            "Step 1140, Training Loss: 0.04673529788851738\n",
            "Step 1140, Training Loss: 0.23071449995040894\n",
            "Step 1140, Training Loss: 0.14063780009746552\n",
            "Step 1140, Training Loss: 0.1779535412788391\n",
            "Step 1140, Training Loss: 0.12037990242242813\n",
            "Step 1140, Training Loss: 0.01761549897491932\n",
            "Step 1140, Training Loss: 0.245120570063591\n",
            "Step 1140, Training Loss: 0.1374807208776474\n",
            "Step 1140, Training Loss: 0.14758813381195068\n",
            "Step 1140, Training Loss: 0.1709682047367096\n",
            "Step 1140, Training Loss: 0.10424225777387619\n",
            "Step 1140, Training Loss: 0.20548143982887268\n",
            "Step 1140, Training Loss: 0.17174603044986725\n",
            "Step 1140, Training Loss: 0.06444649398326874\n",
            "Step 1140, Training Loss: 0.07957492768764496\n",
            "Step 1140, Training Loss: 0.16860823333263397\n",
            "Step 1140, Training Loss: 0.07629149407148361\n",
            "Step 1140, Training Loss: 0.1535339206457138\n",
            "Step 1140, Training Loss: 0.09685301780700684\n",
            "Step 1140, Training Loss: 0.007704558316618204\n",
            "Step 1140, Training Loss: 0.0453791543841362\n",
            "Step 1150, Training Loss: 0.06600798666477203\n",
            "Step 1150, Training Loss: 0.025780584663152695\n",
            "Step 1160, Training Loss: 0.1943579465150833\n",
            "Step 1160, Training Loss: 0.03276989609003067\n",
            "Step 1170, Training Loss: 0.1148042231798172\n",
            "Step 1170, Training Loss: 0.030742118135094643\n",
            "Step 1180, Training Loss: 0.021351773291826248\n",
            "Step 1180, Training Loss: 0.006857772823423147\n",
            "Step 1190, Training Loss: 0.047076210379600525\n",
            "Step 1190, Training Loss: 0.01654868945479393\n",
            "Step 1200, Training Loss: 0.031122073531150818\n",
            "Step 1200, Training Loss: 0.055955082178115845\n",
            "Step 1210, Training Loss: 0.04966401308774948\n",
            "Step 1210, Training Loss: 0.06471750140190125\n",
            "Step 1220, Training Loss: 0.07685785740613937\n",
            "Step 1220, Training Loss: 0.05972237512469292\n",
            "Step 1230, Training Loss: 0.030511261895298958\n",
            "Step 1230, Training Loss: 0.06469305604696274\n",
            "Step 1240, Training Loss: 0.05141090974211693\n",
            "Step 1240, Training Loss: 0.030851587653160095\n",
            "Step 1250, Training Loss: 0.05154695361852646\n",
            "Step 1250, Training Loss: 0.045117881149053574\n",
            "Step 1260, Training Loss: 0.017702704295516014\n",
            "Step 1260, Training Loss: 0.030656790360808372\n",
            "Step 1270, Training Loss: 0.0914214551448822\n",
            "Step 1270, Training Loss: 0.08357253670692444\n",
            "Step 1280, Training Loss: 0.09185279160737991\n",
            "Step 1280, Training Loss: 0.00807967595756054\n",
            "Step 1290, Training Loss: 0.053724341094493866\n",
            "Step 1290, Training Loss: 0.032168909907341\n",
            "Step 1300, Training Loss: 0.019631102681159973\n",
            "Step 1300, Training Loss: 0.01697704754769802\n",
            "Step 1310, Training Loss: 0.024808045476675034\n",
            "Step 1310, Training Loss: 0.028203001245856285\n",
            "Step 1320, Training Loss: 0.025483762845396996\n",
            "Step 1320, Training Loss: 0.060237813740968704\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 1523\n",
            "  Batch size = 32\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 1330, Training Loss: 0.08103784918785095\n",
            "Step 1330, Training Loss: 0.22679689526557922\n",
            "Step 1330, Training Loss: 0.24280321598052979\n",
            "Step 1330, Training Loss: 0.08528221398591995\n",
            "Step 1330, Training Loss: 0.16684332489967346\n",
            "Step 1330, Training Loss: 0.15225178003311157\n",
            "Step 1330, Training Loss: 0.19536931812763214\n",
            "Step 1330, Training Loss: 0.09819484502077103\n",
            "Step 1330, Training Loss: 0.19461600482463837\n",
            "Step 1330, Training Loss: 0.08368971198797226\n",
            "Step 1330, Training Loss: 0.165980726480484\n",
            "Step 1330, Training Loss: 0.0632670596241951\n",
            "Step 1330, Training Loss: 0.16179870069026947\n",
            "Step 1330, Training Loss: 0.12175919860601425\n",
            "Step 1330, Training Loss: 0.048789869993925095\n",
            "Step 1330, Training Loss: 0.09487227350473404\n",
            "Step 1330, Training Loss: 0.11201044917106628\n",
            "Step 1330, Training Loss: 0.15280866622924805\n",
            "Step 1330, Training Loss: 0.040950972586870193\n",
            "Step 1330, Training Loss: 0.1898249238729477\n",
            "Step 1330, Training Loss: 0.10789988189935684\n",
            "Step 1330, Training Loss: 0.21469391882419586\n",
            "Step 1330, Training Loss: 0.20908863842487335\n",
            "Step 1330, Training Loss: 0.10689486563205719\n",
            "Step 1330, Training Loss: 0.13756126165390015\n",
            "Step 1330, Training Loss: 0.2529081702232361\n",
            "Step 1330, Training Loss: 0.23571327328681946\n",
            "Step 1330, Training Loss: 0.08501850068569183\n",
            "Step 1330, Training Loss: 0.16881342232227325\n",
            "Step 1330, Training Loss: 0.18861563503742218\n",
            "Step 1330, Training Loss: 0.04727739468216896\n",
            "Step 1330, Training Loss: 0.2300487756729126\n",
            "Step 1330, Training Loss: 0.12922942638397217\n",
            "Step 1330, Training Loss: 0.17539626359939575\n",
            "Step 1330, Training Loss: 0.12147343903779984\n",
            "Step 1330, Training Loss: 0.017424769699573517\n",
            "Step 1330, Training Loss: 0.25220489501953125\n",
            "Step 1330, Training Loss: 0.13556377589702606\n",
            "Step 1330, Training Loss: 0.1476067155599594\n",
            "Step 1330, Training Loss: 0.16814403235912323\n",
            "Step 1330, Training Loss: 0.09963402152061462\n",
            "Step 1330, Training Loss: 0.19762445986270905\n",
            "Step 1330, Training Loss: 0.16756115853786469\n",
            "Step 1330, Training Loss: 0.05171689763665199\n",
            "Step 1330, Training Loss: 0.08072098344564438\n",
            "Step 1330, Training Loss: 0.15947499871253967\n",
            "Step 1330, Training Loss: 0.07470884174108505\n",
            "Step 1330, Training Loss: 0.15134446322917938\n",
            "Step 1330, Training Loss: 0.09321267157793045\n",
            "Step 1330, Training Loss: 0.010073831304907799\n",
            "Step 1330, Training Loss: 0.007210505194962025\n",
            "Step 1340, Training Loss: 0.07064345479011536\n",
            "Step 1340, Training Loss: 0.11082423478364944\n",
            "Step 1350, Training Loss: 0.027585003525018692\n",
            "Step 1350, Training Loss: 0.08952904492616653\n",
            "Step 1360, Training Loss: 0.09955140203237534\n",
            "Step 1360, Training Loss: 0.04465050622820854\n",
            "Step 1370, Training Loss: 0.027588019147515297\n",
            "Step 1370, Training Loss: 0.06733313202857971\n",
            "Step 1380, Training Loss: 0.04258745163679123\n",
            "Step 1380, Training Loss: 0.032136525958776474\n",
            "Step 1390, Training Loss: 0.03558731824159622\n",
            "Step 1390, Training Loss: 0.08645709604024887\n",
            "Step 1400, Training Loss: 0.04490415379405022\n",
            "Step 1400, Training Loss: 0.13020989298820496\n",
            "Step 1410, Training Loss: 0.10617474466562271\n",
            "Step 1410, Training Loss: 0.019239231944084167\n",
            "Step 1420, Training Loss: 0.014096006751060486\n",
            "Step 1420, Training Loss: 0.10173418372869492\n",
            "Step 1430, Training Loss: 0.04449484497308731\n",
            "Step 1430, Training Loss: 0.0702957808971405\n",
            "Step 1440, Training Loss: 0.019745031371712685\n",
            "Step 1440, Training Loss: 0.017858261242508888\n",
            "Step 1450, Training Loss: 0.03870348632335663\n",
            "Step 1450, Training Loss: 0.04371017962694168\n",
            "Step 1460, Training Loss: 0.052636127918958664\n",
            "Step 1460, Training Loss: 0.0186047051101923\n",
            "Step 1470, Training Loss: 0.04868602007627487\n",
            "Step 1470, Training Loss: 0.041352611035108566\n",
            "Step 1480, Training Loss: 0.04222086817026138\n",
            "Step 1480, Training Loss: 0.01948670856654644\n",
            "Step 1490, Training Loss: 0.026369160041213036\n",
            "Step 1490, Training Loss: 0.04994828999042511\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Saving model checkpoint to ./results/checkpoint-1500\n",
            "Configuration saved in ./results/checkpoint-1500/config.json\n",
            "Model weights saved in ./results/checkpoint-1500/pytorch_model.bin\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 1500, Training Loss: 0.02763059176504612\n",
            "Step 1500, Training Loss: 0.029019005596637726\n",
            "Step 1510, Training Loss: 0.07803946733474731\n",
            "Step 1510, Training Loss: 0.044667597860097885\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 1523\n",
            "  Batch size = 32\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 1520, Training Loss: 0.01065374631434679\n",
            "Step 1520, Training Loss: 0.22868923842906952\n",
            "Step 1520, Training Loss: 0.2433972805738449\n",
            "Step 1520, Training Loss: 0.08634742349386215\n",
            "Step 1520, Training Loss: 0.17092961072921753\n",
            "Step 1520, Training Loss: 0.1507241427898407\n",
            "Step 1520, Training Loss: 0.1968003660440445\n",
            "Step 1520, Training Loss: 0.09855679422616959\n",
            "Step 1520, Training Loss: 0.19384054839611053\n",
            "Step 1520, Training Loss: 0.08573339879512787\n",
            "Step 1520, Training Loss: 0.16689859330654144\n",
            "Step 1520, Training Loss: 0.06310251355171204\n",
            "Step 1520, Training Loss: 0.16626214981079102\n",
            "Step 1520, Training Loss: 0.12906308472156525\n",
            "Step 1520, Training Loss: 0.04853353276848793\n",
            "Step 1520, Training Loss: 0.09692969173192978\n",
            "Step 1520, Training Loss: 0.11410496383905411\n",
            "Step 1520, Training Loss: 0.15048210322856903\n",
            "Step 1520, Training Loss: 0.04195113852620125\n",
            "Step 1520, Training Loss: 0.19083324074745178\n",
            "Step 1520, Training Loss: 0.11099153757095337\n",
            "Step 1520, Training Loss: 0.21894410252571106\n",
            "Step 1520, Training Loss: 0.21112126111984253\n",
            "Step 1520, Training Loss: 0.10749685764312744\n",
            "Step 1520, Training Loss: 0.1379401832818985\n",
            "Step 1520, Training Loss: 0.2570684850215912\n",
            "Step 1520, Training Loss: 0.23489037156105042\n",
            "Step 1520, Training Loss: 0.08848641812801361\n",
            "Step 1520, Training Loss: 0.17076383531093597\n",
            "Step 1520, Training Loss: 0.19005019962787628\n",
            "Step 1520, Training Loss: 0.047462109476327896\n",
            "Step 1520, Training Loss: 0.2302194982767105\n",
            "Step 1520, Training Loss: 0.1351541131734848\n",
            "Step 1520, Training Loss: 0.177897647023201\n",
            "Step 1520, Training Loss: 0.12113730609416962\n",
            "Step 1520, Training Loss: 0.01796659082174301\n",
            "Step 1520, Training Loss: 0.24930791556835175\n",
            "Step 1520, Training Loss: 0.13298244774341583\n",
            "Step 1520, Training Loss: 0.14740917086601257\n",
            "Step 1520, Training Loss: 0.16862480342388153\n",
            "Step 1520, Training Loss: 0.10141876339912415\n",
            "Step 1520, Training Loss: 0.20045404136180878\n",
            "Step 1520, Training Loss: 0.17136815190315247\n",
            "Step 1520, Training Loss: 0.05573580414056778\n",
            "Step 1520, Training Loss: 0.08092185854911804\n",
            "Step 1520, Training Loss: 0.16668400168418884\n",
            "Step 1520, Training Loss: 0.07518967241048813\n",
            "Step 1520, Training Loss: 0.15131917595863342\n",
            "Step 1520, Training Loss: 0.09461195021867752\n",
            "Step 1520, Training Loss: 0.05018424242734909\n",
            "Step 1520, Training Loss: 0.09366606175899506\n",
            "Step 1530, Training Loss: 0.036658428609371185\n",
            "Step 1530, Training Loss: 0.10107950121164322\n",
            "Step 1540, Training Loss: 0.1256808340549469\n",
            "Step 1540, Training Loss: 0.016201429069042206\n",
            "Step 1550, Training Loss: 0.051722053438425064\n",
            "Step 1550, Training Loss: 0.07435014843940735\n",
            "Step 1560, Training Loss: 0.008759606629610062\n",
            "Step 1560, Training Loss: 0.06187717616558075\n",
            "Step 1570, Training Loss: 0.06206141412258148\n",
            "Step 1570, Training Loss: 0.03887377306818962\n",
            "Step 1580, Training Loss: 0.09631817787885666\n",
            "Step 1580, Training Loss: 0.02996690757572651\n",
            "Step 1590, Training Loss: 0.051741037517786026\n",
            "Step 1590, Training Loss: 0.05204375833272934\n",
            "Step 1600, Training Loss: 0.0068063014186918736\n",
            "Step 1600, Training Loss: 0.10672961175441742\n",
            "Step 1610, Training Loss: 0.029144862666726112\n",
            "Step 1610, Training Loss: 0.053878359496593475\n",
            "Step 1620, Training Loss: 0.1638873666524887\n",
            "Step 1620, Training Loss: 0.007311364635825157\n",
            "Step 1630, Training Loss: 0.03136628866195679\n",
            "Step 1630, Training Loss: 0.05913133919239044\n",
            "Step 1640, Training Loss: 0.023790854960680008\n",
            "Step 1640, Training Loss: 0.01608235202729702\n",
            "Step 1650, Training Loss: 0.03579862788319588\n",
            "Step 1650, Training Loss: 0.07918457686901093\n",
            "Step 1660, Training Loss: 0.03866324573755264\n",
            "Step 1660, Training Loss: 0.026958368718624115\n",
            "Step 1670, Training Loss: 0.021431805565953255\n",
            "Step 1670, Training Loss: 0.09826401621103287\n",
            "Step 1680, Training Loss: 0.09097316116094589\n",
            "Step 1680, Training Loss: 0.04197268933057785\n",
            "Step 1690, Training Loss: 0.03708576411008835\n",
            "Step 1690, Training Loss: 0.08493523299694061\n",
            "Step 1700, Training Loss: 0.04824509844183922\n",
            "Step 1700, Training Loss: 0.01870649866759777\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 1523\n",
            "  Batch size = 32\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 1710, Training Loss: 0.0471210703253746\n",
            "Step 1710, Training Loss: 0.22569362819194794\n",
            "Step 1710, Training Loss: 0.24516484141349792\n",
            "Step 1710, Training Loss: 0.0864039734005928\n",
            "Step 1710, Training Loss: 0.17127899825572968\n",
            "Step 1710, Training Loss: 0.1491575986146927\n",
            "Step 1710, Training Loss: 0.19556111097335815\n",
            "Step 1710, Training Loss: 0.09831278026103973\n",
            "Step 1710, Training Loss: 0.19369511306285858\n",
            "Step 1710, Training Loss: 0.08633854985237122\n",
            "Step 1710, Training Loss: 0.1663631647825241\n",
            "Step 1710, Training Loss: 0.06295236200094223\n",
            "Step 1710, Training Loss: 0.16520385444164276\n",
            "Step 1710, Training Loss: 0.12795303761959076\n",
            "Step 1710, Training Loss: 0.04860585182905197\n",
            "Step 1710, Training Loss: 0.09619849175214767\n",
            "Step 1710, Training Loss: 0.10943449288606644\n",
            "Step 1710, Training Loss: 0.1526222825050354\n",
            "Step 1710, Training Loss: 0.041727110743522644\n",
            "Step 1710, Training Loss: 0.18892726302146912\n",
            "Step 1710, Training Loss: 0.11371910572052002\n",
            "Step 1710, Training Loss: 0.21978825330734253\n",
            "Step 1710, Training Loss: 0.21130196750164032\n",
            "Step 1710, Training Loss: 0.10720733553171158\n",
            "Step 1710, Training Loss: 0.13898423314094543\n",
            "Step 1710, Training Loss: 0.25697916746139526\n",
            "Step 1710, Training Loss: 0.23549701273441315\n",
            "Step 1710, Training Loss: 0.08653420954942703\n",
            "Step 1710, Training Loss: 0.17048822343349457\n",
            "Step 1710, Training Loss: 0.1888870745897293\n",
            "Step 1710, Training Loss: 0.048580121248960495\n",
            "Step 1710, Training Loss: 0.22973479330539703\n",
            "Step 1710, Training Loss: 0.13364367187023163\n",
            "Step 1710, Training Loss: 0.1769624650478363\n",
            "Step 1710, Training Loss: 0.12157987058162689\n",
            "Step 1710, Training Loss: 0.018483532592654228\n",
            "Step 1710, Training Loss: 0.25087863206863403\n",
            "Step 1710, Training Loss: 0.13285206258296967\n",
            "Step 1710, Training Loss: 0.14816339313983917\n",
            "Step 1710, Training Loss: 0.16542193293571472\n",
            "Step 1710, Training Loss: 0.10134498029947281\n",
            "Step 1710, Training Loss: 0.2030891627073288\n",
            "Step 1710, Training Loss: 0.1720273792743683\n",
            "Step 1710, Training Loss: 0.05245845019817352\n",
            "Step 1710, Training Loss: 0.08080634474754333\n",
            "Step 1710, Training Loss: 0.16443127393722534\n",
            "Step 1710, Training Loss: 0.07538526505231857\n",
            "Step 1710, Training Loss: 0.15051494538784027\n",
            "Step 1710, Training Loss: 0.09329798817634583\n",
            "Step 1710, Training Loss: 0.13763155043125153\n",
            "Step 1710, Training Loss: 0.1293715536594391\n",
            "Step 1720, Training Loss: 0.01085243932902813\n",
            "Step 1720, Training Loss: 0.017127519473433495\n",
            "Step 1730, Training Loss: 0.05440567806363106\n",
            "Step 1730, Training Loss: 0.05569620057940483\n",
            "Step 1740, Training Loss: 0.014797921292483807\n",
            "Step 1740, Training Loss: 0.05228324234485626\n",
            "Step 1750, Training Loss: 0.10061654448509216\n",
            "Step 1750, Training Loss: 0.045843180269002914\n",
            "Step 1760, Training Loss: 0.048860374838113785\n",
            "Step 1760, Training Loss: 0.011621324345469475\n",
            "Step 1770, Training Loss: 0.01593410223722458\n",
            "Step 1770, Training Loss: 0.04240279272198677\n",
            "Step 1780, Training Loss: 0.13190928101539612\n",
            "Step 1780, Training Loss: 0.007784536574035883\n",
            "Step 1790, Training Loss: 0.02927299588918686\n",
            "Step 1790, Training Loss: 0.02653357945382595\n",
            "Step 1800, Training Loss: 0.049552492797374725\n",
            "Step 1800, Training Loss: 0.01858535222709179\n",
            "Step 1810, Training Loss: 0.07369967550039291\n",
            "Step 1810, Training Loss: 0.046628598123788834\n",
            "Step 1820, Training Loss: 0.07775130122900009\n",
            "Step 1820, Training Loss: 0.0904628187417984\n",
            "Step 1830, Training Loss: 0.027425430715084076\n",
            "Step 1830, Training Loss: 0.06466814875602722\n",
            "Step 1840, Training Loss: 0.04424397647380829\n",
            "Step 1840, Training Loss: 0.06411433219909668\n",
            "Step 1850, Training Loss: 0.04632476344704628\n",
            "Step 1850, Training Loss: 0.035295020788908005\n",
            "Step 1860, Training Loss: 0.058496102690696716\n",
            "Step 1860, Training Loss: 0.017908010631799698\n",
            "Step 1870, Training Loss: 0.049275871366262436\n",
            "Step 1870, Training Loss: 0.020358573645353317\n",
            "Step 1880, Training Loss: 0.02082066982984543\n",
            "Step 1880, Training Loss: 0.03873009979724884\n",
            "Step 1890, Training Loss: 0.08008541166782379\n",
            "Step 1890, Training Loss: 0.02908453531563282\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 1523\n",
            "  Batch size = 32\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 1900, Training Loss: 0.22752811014652252\n",
            "Step 1900, Training Loss: 0.24554011225700378\n",
            "Step 1900, Training Loss: 0.08665947616100311\n",
            "Step 1900, Training Loss: 0.17057786881923676\n",
            "Step 1900, Training Loss: 0.14991210401058197\n",
            "Step 1900, Training Loss: 0.1962861567735672\n",
            "Step 1900, Training Loss: 0.09856840968132019\n",
            "Step 1900, Training Loss: 0.1942855715751648\n",
            "Step 1900, Training Loss: 0.08657694607973099\n",
            "Step 1900, Training Loss: 0.16548824310302734\n",
            "Step 1900, Training Loss: 0.0634269043803215\n",
            "Step 1900, Training Loss: 0.16780677437782288\n",
            "Step 1900, Training Loss: 0.13247303664684296\n",
            "Step 1900, Training Loss: 0.04793291538953781\n",
            "Step 1900, Training Loss: 0.09584230929613113\n",
            "Step 1900, Training Loss: 0.10962218046188354\n",
            "Step 1900, Training Loss: 0.15164846181869507\n",
            "Step 1900, Training Loss: 0.04191543906927109\n",
            "Step 1900, Training Loss: 0.1894012987613678\n",
            "Step 1900, Training Loss: 0.11380139738321304\n",
            "Step 1900, Training Loss: 0.2191997915506363\n",
            "Step 1900, Training Loss: 0.21236088871955872\n",
            "Step 1900, Training Loss: 0.10738445818424225\n",
            "Step 1900, Training Loss: 0.13833938539028168\n",
            "Step 1900, Training Loss: 0.25598347187042236\n",
            "Step 1900, Training Loss: 0.23567290604114532\n",
            "Step 1900, Training Loss: 0.08629100024700165\n",
            "Step 1900, Training Loss: 0.17090699076652527\n",
            "Step 1900, Training Loss: 0.19001249969005585\n",
            "Step 1900, Training Loss: 0.04822938144207001\n",
            "Step 1900, Training Loss: 0.23003879189491272\n",
            "Step 1900, Training Loss: 0.13563600182533264\n",
            "Step 1900, Training Loss: 0.17832621932029724\n",
            "Step 1900, Training Loss: 0.12105081975460052\n",
            "Step 1900, Training Loss: 0.017597874626517296\n",
            "Step 1900, Training Loss: 0.2500140964984894\n",
            "Step 1900, Training Loss: 0.13399456441402435\n",
            "Step 1900, Training Loss: 0.147740438580513\n",
            "Step 1900, Training Loss: 0.16721829771995544\n",
            "Step 1900, Training Loss: 0.10208814591169357\n",
            "Step 1900, Training Loss: 0.20389041304588318\n",
            "Step 1900, Training Loss: 0.1730271279811859\n",
            "Step 1900, Training Loss: 0.05639775097370148\n",
            "Step 1900, Training Loss: 0.08043287694454193\n",
            "Step 1900, Training Loss: 0.1656489223241806\n",
            "Step 1900, Training Loss: 0.07567840069532394\n",
            "Step 1900, Training Loss: 0.15171007812023163\n",
            "Step 1900, Training Loss: 0.09456765651702881\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training completed in 307.51 seconds.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 1523\n",
            "  Batch size = 32\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 1900, Training Loss: 0.22752811014652252\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='48' max='48' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [48/48 00:02]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 1900, Training Loss: 0.24554011225700378\n",
            "Step 1900, Training Loss: 0.08665947616100311\n",
            "Step 1900, Training Loss: 0.17057786881923676\n",
            "Step 1900, Training Loss: 0.14991210401058197\n",
            "Step 1900, Training Loss: 0.1962861567735672\n",
            "Step 1900, Training Loss: 0.09856840968132019\n",
            "Step 1900, Training Loss: 0.1942855715751648\n",
            "Step 1900, Training Loss: 0.08657694607973099\n",
            "Step 1900, Training Loss: 0.16548824310302734\n",
            "Step 1900, Training Loss: 0.0634269043803215\n",
            "Step 1900, Training Loss: 0.16780677437782288\n",
            "Step 1900, Training Loss: 0.13247303664684296\n",
            "Step 1900, Training Loss: 0.04793291538953781\n",
            "Step 1900, Training Loss: 0.09584230929613113\n",
            "Step 1900, Training Loss: 0.10962218046188354\n",
            "Step 1900, Training Loss: 0.15164846181869507\n",
            "Step 1900, Training Loss: 0.04191543906927109\n",
            "Step 1900, Training Loss: 0.1894012987613678\n",
            "Step 1900, Training Loss: 0.11380139738321304\n",
            "Step 1900, Training Loss: 0.2191997915506363\n",
            "Step 1900, Training Loss: 0.21236088871955872\n",
            "Step 1900, Training Loss: 0.10738445818424225\n",
            "Step 1900, Training Loss: 0.13833938539028168\n",
            "Step 1900, Training Loss: 0.25598347187042236\n",
            "Step 1900, Training Loss: 0.23567290604114532\n",
            "Step 1900, Training Loss: 0.08629100024700165\n",
            "Step 1900, Training Loss: 0.17090699076652527\n",
            "Step 1900, Training Loss: 0.19001249969005585\n",
            "Step 1900, Training Loss: 0.04822938144207001\n",
            "Step 1900, Training Loss: 0.23003879189491272\n",
            "Step 1900, Training Loss: 0.13563600182533264\n",
            "Step 1900, Training Loss: 0.17832621932029724\n",
            "Step 1900, Training Loss: 0.12105081975460052\n",
            "Step 1900, Training Loss: 0.017597874626517296\n",
            "Step 1900, Training Loss: 0.2500140964984894\n",
            "Step 1900, Training Loss: 0.13399456441402435\n",
            "Step 1900, Training Loss: 0.147740438580513\n",
            "Step 1900, Training Loss: 0.16721829771995544\n",
            "Step 1900, Training Loss: 0.10208814591169357\n",
            "Step 1900, Training Loss: 0.20389041304588318\n",
            "Step 1900, Training Loss: 0.1730271279811859\n",
            "Step 1900, Training Loss: 0.05639775097370148\n",
            "Step 1900, Training Loss: 0.08043287694454193\n",
            "Step 1900, Training Loss: 0.1656489223241806\n",
            "Step 1900, Training Loss: 0.07567840069532394\n",
            "Step 1900, Training Loss: 0.15171007812023163\n",
            "Step 1900, Training Loss: 0.09456765651702881\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-12-09 19:07:14,044] Trial 3 finished with value: 0.8485331136643601 and parameters: {'learning_rate': 3.2734197057602947e-06, 'batch_size': 16, 'num_train_epochs': 10, 'weight_decay': 0.002358364056198364}. Best is trial 3 with value: 0.8485331136643601.\n",
            "<ipython-input-12-42465065092b>:2: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  learning_rate = trial.suggest_loguniform(\"learning_rate\", 1e-6, 5e-5)\n",
            "<ipython-input-12-42465065092b>:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  weight_decay = trial.suggest_loguniform(\"weight_decay\", 1e-4, 0.1)\n",
            "PyTorch: setting up devices\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n",
            "***** Running training *****\n",
            "  Num examples = 6091\n",
            "  Num Epochs = 10\n",
            "  Instantaneous batch size per device = 4\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
            "  Gradient Accumulation steps = 2\n",
            "  Total optimization steps = 7610\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting training...\n",
            "Step 0, Training Loss: 0.008787555620074272\n",
            "Step 0, Training Loss: 0.006410004571080208\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='7610' max='7610' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [7610/7610 06:48, Epoch 9/10]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "      <th>F1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>0.036700</td>\n",
              "      <td>0.147385</td>\n",
              "      <td>0.842416</td>\n",
              "      <td>0.864070</td>\n",
              "      <td>0.844673</td>\n",
              "      <td>0.846812</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.025900</td>\n",
              "      <td>0.147865</td>\n",
              "      <td>0.843073</td>\n",
              "      <td>0.865414</td>\n",
              "      <td>0.847882</td>\n",
              "      <td>0.849031</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.030800</td>\n",
              "      <td>0.145392</td>\n",
              "      <td>0.843729</td>\n",
              "      <td>0.864404</td>\n",
              "      <td>0.850449</td>\n",
              "      <td>0.851732</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.038700</td>\n",
              "      <td>0.150107</td>\n",
              "      <td>0.843729</td>\n",
              "      <td>0.857088</td>\n",
              "      <td>0.851733</td>\n",
              "      <td>0.849288</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.031100</td>\n",
              "      <td>0.147268</td>\n",
              "      <td>0.843729</td>\n",
              "      <td>0.859997</td>\n",
              "      <td>0.849807</td>\n",
              "      <td>0.851195</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>0.059300</td>\n",
              "      <td>0.149250</td>\n",
              "      <td>0.843729</td>\n",
              "      <td>0.859714</td>\n",
              "      <td>0.850449</td>\n",
              "      <td>0.851001</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>0.049000</td>\n",
              "      <td>0.149471</td>\n",
              "      <td>0.843073</td>\n",
              "      <td>0.857784</td>\n",
              "      <td>0.850449</td>\n",
              "      <td>0.850033</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>0.060600</td>\n",
              "      <td>0.150180</td>\n",
              "      <td>0.843073</td>\n",
              "      <td>0.857419</td>\n",
              "      <td>0.851091</td>\n",
              "      <td>0.850293</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>0.032900</td>\n",
              "      <td>0.148311</td>\n",
              "      <td>0.843729</td>\n",
              "      <td>0.858654</td>\n",
              "      <td>0.851733</td>\n",
              "      <td>0.851793</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>0.037000</td>\n",
              "      <td>0.147919</td>\n",
              "      <td>0.844386</td>\n",
              "      <td>0.859108</td>\n",
              "      <td>0.851091</td>\n",
              "      <td>0.851340</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 10, Training Loss: 0.006497946102172136\n",
            "Step 10, Training Loss: 0.0067041669972240925\n",
            "Step 20, Training Loss: 0.2516271770000458\n",
            "Step 20, Training Loss: 0.044441454112529755\n",
            "Step 30, Training Loss: 0.008229154162108898\n",
            "Step 30, Training Loss: 0.06227705627679825\n",
            "Step 40, Training Loss: 0.008520898409187794\n",
            "Step 40, Training Loss: 0.008468748070299625\n",
            "Step 50, Training Loss: 0.010477191768586636\n",
            "Step 50, Training Loss: 0.01327711995691061\n",
            "Step 60, Training Loss: 0.010697269812226295\n",
            "Step 60, Training Loss: 0.0077192774042487144\n",
            "Step 70, Training Loss: 0.014017203822731972\n",
            "Step 70, Training Loss: 0.04623832926154137\n",
            "Step 80, Training Loss: 0.04476635903120041\n",
            "Step 80, Training Loss: 0.04050532728433609\n",
            "Step 90, Training Loss: 0.006879331078380346\n",
            "Step 90, Training Loss: 0.013399486429989338\n",
            "Step 100, Training Loss: 0.004544976633042097\n",
            "Step 100, Training Loss: 0.005765966139733791\n",
            "Step 110, Training Loss: 0.007939155213534832\n",
            "Step 110, Training Loss: 0.008740664459764957\n",
            "Step 120, Training Loss: 0.00650807935744524\n",
            "Step 120, Training Loss: 0.08812426775693893\n",
            "Step 130, Training Loss: 0.09204938262701035\n",
            "Step 130, Training Loss: 0.007345231715589762\n",
            "Step 140, Training Loss: 0.05067328363656998\n",
            "Step 140, Training Loss: 0.11359889805316925\n",
            "Step 150, Training Loss: 0.013936654664576054\n",
            "Step 150, Training Loss: 0.10245070606470108\n",
            "Step 160, Training Loss: 0.009653430432081223\n",
            "Step 160, Training Loss: 0.02630666457116604\n",
            "Step 170, Training Loss: 0.006862424314022064\n",
            "Step 170, Training Loss: 0.006674891337752342\n",
            "Step 180, Training Loss: 0.09979917854070663\n",
            "Step 180, Training Loss: 0.24368007481098175\n",
            "Step 190, Training Loss: 0.12702594697475433\n",
            "Step 190, Training Loss: 0.05314618721604347\n",
            "Step 200, Training Loss: 0.006472253706306219\n",
            "Step 200, Training Loss: 0.006673318799585104\n",
            "Step 210, Training Loss: 0.2488298863172531\n",
            "Step 210, Training Loss: 0.09169530123472214\n",
            "Step 220, Training Loss: 0.043322738260030746\n",
            "Step 220, Training Loss: 0.16029910743236542\n",
            "Step 230, Training Loss: 0.006116154137998819\n",
            "Step 230, Training Loss: 0.006177464500069618\n",
            "Step 240, Training Loss: 0.01046009548008442\n",
            "Step 240, Training Loss: 0.005295016337186098\n",
            "Step 250, Training Loss: 0.012631387449800968\n",
            "Step 250, Training Loss: 0.04868929460644722\n",
            "Step 260, Training Loss: 0.0049011241644620895\n",
            "Step 260, Training Loss: 0.008222898468375206\n",
            "Step 270, Training Loss: 0.04481856897473335\n",
            "Step 270, Training Loss: 0.005976588930934668\n",
            "Step 280, Training Loss: 0.04232988506555557\n",
            "Step 280, Training Loss: 0.007879301905632019\n",
            "Step 290, Training Loss: 0.008710581809282303\n",
            "Step 290, Training Loss: 0.007774360943585634\n",
            "Step 300, Training Loss: 0.060661185532808304\n",
            "Step 300, Training Loss: 0.007398051209747791\n",
            "Step 310, Training Loss: 0.005639033857733011\n",
            "Step 310, Training Loss: 0.0064467270858585835\n",
            "Step 320, Training Loss: 0.005159813445061445\n",
            "Step 320, Training Loss: 0.05360127240419388\n",
            "Step 330, Training Loss: 0.006331020034849644\n",
            "Step 330, Training Loss: 0.004121893551200628\n",
            "Step 340, Training Loss: 0.0051273321732878685\n",
            "Step 340, Training Loss: 0.03792836144566536\n",
            "Step 350, Training Loss: 0.007839635014533997\n",
            "Step 350, Training Loss: 0.04845183342695236\n",
            "Step 360, Training Loss: 0.009377173148095608\n",
            "Step 360, Training Loss: 0.008690130896866322\n",
            "Step 370, Training Loss: 0.006140744313597679\n",
            "Step 370, Training Loss: 0.04505021125078201\n",
            "Step 380, Training Loss: 0.004890235606580973\n",
            "Step 380, Training Loss: 0.0545576736330986\n",
            "Step 390, Training Loss: 0.06473900377750397\n",
            "Step 390, Training Loss: 0.08115711063146591\n",
            "Step 400, Training Loss: 0.005219913087785244\n",
            "Step 400, Training Loss: 0.008797766640782356\n",
            "Step 410, Training Loss: 0.0040214406326413155\n",
            "Step 410, Training Loss: 0.0747106671333313\n",
            "Step 420, Training Loss: 0.006921918597072363\n",
            "Step 420, Training Loss: 0.06706549227237701\n",
            "Step 430, Training Loss: 0.353433221578598\n",
            "Step 430, Training Loss: 0.2779851257801056\n",
            "Step 440, Training Loss: 0.008720595389604568\n",
            "Step 440, Training Loss: 0.08707938343286514\n",
            "Step 450, Training Loss: 0.0164329893887043\n",
            "Step 450, Training Loss: 0.06024586409330368\n",
            "Step 460, Training Loss: 0.006119673140347004\n",
            "Step 460, Training Loss: 0.06654391437768936\n",
            "Step 470, Training Loss: 0.05216106399893761\n",
            "Step 470, Training Loss: 0.006104391999542713\n",
            "Step 480, Training Loss: 0.04290428385138512\n",
            "Step 480, Training Loss: 0.08837480843067169\n",
            "Step 490, Training Loss: 0.28072819113731384\n",
            "Step 490, Training Loss: 0.010081755928695202\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Saving model checkpoint to ./results/checkpoint-500\n",
            "Configuration saved in ./results/checkpoint-500/config.json\n",
            "Model weights saved in ./results/checkpoint-500/pytorch_model.bin\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 500, Training Loss: 0.07634402811527252\n",
            "Step 500, Training Loss: 0.005735232029110193\n",
            "Step 510, Training Loss: 0.04631312936544418\n",
            "Step 510, Training Loss: 0.18362100422382355\n",
            "Step 520, Training Loss: 0.10509119927883148\n",
            "Step 520, Training Loss: 0.009791304357349873\n",
            "Step 530, Training Loss: 0.04631252586841583\n",
            "Step 530, Training Loss: 0.005838590208441019\n",
            "Step 540, Training Loss: 0.004528486635535955\n",
            "Step 540, Training Loss: 0.006950741168111563\n",
            "Step 550, Training Loss: 0.1366679072380066\n",
            "Step 550, Training Loss: 0.039318546652793884\n",
            "Step 560, Training Loss: 0.08946366608142853\n",
            "Step 560, Training Loss: 0.18182958662509918\n",
            "Step 570, Training Loss: 0.006753225810825825\n",
            "Step 570, Training Loss: 0.04965716227889061\n",
            "Step 580, Training Loss: 0.04478953778743744\n",
            "Step 580, Training Loss: 0.009528792463243008\n",
            "Step 590, Training Loss: 0.005626875441521406\n",
            "Step 590, Training Loss: 0.007328901439905167\n",
            "Step 600, Training Loss: 0.1620808094739914\n",
            "Step 600, Training Loss: 0.053902287036180496\n",
            "Step 610, Training Loss: 0.06071561947464943\n",
            "Step 610, Training Loss: 0.30634284019470215\n",
            "Step 620, Training Loss: 0.14470362663269043\n",
            "Step 620, Training Loss: 0.013283242471516132\n",
            "Step 630, Training Loss: 0.005406345706433058\n",
            "Step 630, Training Loss: 0.27957791090011597\n",
            "Step 640, Training Loss: 0.005944521632045507\n",
            "Step 640, Training Loss: 0.04768481105566025\n",
            "Step 650, Training Loss: 0.008850482292473316\n",
            "Step 650, Training Loss: 0.35246655344963074\n",
            "Step 660, Training Loss: 0.04223237186670303\n",
            "Step 660, Training Loss: 0.00712464889511466\n",
            "Step 670, Training Loss: 0.00781911052763462\n",
            "Step 670, Training Loss: 0.10734035819768906\n",
            "Step 680, Training Loss: 0.03776166960597038\n",
            "Step 680, Training Loss: 0.36399510502815247\n",
            "Step 690, Training Loss: 0.045196663588285446\n",
            "Step 690, Training Loss: 0.005130650009959936\n",
            "Step 700, Training Loss: 0.01366889663040638\n",
            "Step 700, Training Loss: 0.07923229783773422\n",
            "Step 710, Training Loss: 0.07980099320411682\n",
            "Step 710, Training Loss: 0.07516228407621384\n",
            "Step 720, Training Loss: 0.006892865523695946\n",
            "Step 720, Training Loss: 0.1817627102136612\n",
            "Step 730, Training Loss: 0.006228988990187645\n",
            "Step 730, Training Loss: 0.005165614187717438\n",
            "Step 740, Training Loss: 0.0055904695764184\n",
            "Step 740, Training Loss: 0.007894987240433693\n",
            "Step 750, Training Loss: 0.006321105640381575\n",
            "Step 750, Training Loss: 0.09106481820344925\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 1523\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 760, Training Loss: 0.005173862911760807\n",
            "Step 760, Training Loss: 0.003881088923662901\n",
            "Step 770, Training Loss: 0.0057725245133042336\n",
            "Step 770, Training Loss: 0.04653089866042137\n",
            "Step 780, Training Loss: 0.007981639355421066\n",
            "Step 780, Training Loss: 0.005132687743753195\n",
            "Step 790, Training Loss: 0.004600673448294401\n",
            "Step 790, Training Loss: 0.005193701013922691\n",
            "Step 800, Training Loss: 0.0436689667403698\n",
            "Step 800, Training Loss: 0.1344711184501648\n",
            "Step 810, Training Loss: 0.006010096985846758\n",
            "Step 810, Training Loss: 0.015030747279524803\n",
            "Step 820, Training Loss: 0.011539558880031109\n",
            "Step 820, Training Loss: 0.008989240974187851\n",
            "Step 830, Training Loss: 0.10762046277523041\n",
            "Step 830, Training Loss: 0.013761455193161964\n",
            "Step 840, Training Loss: 0.04425864294171333\n",
            "Step 840, Training Loss: 0.007444169372320175\n",
            "Step 850, Training Loss: 0.008042004890739918\n",
            "Step 850, Training Loss: 0.13736015558242798\n",
            "Step 860, Training Loss: 0.005124649498611689\n",
            "Step 860, Training Loss: 0.052907779812812805\n",
            "Step 870, Training Loss: 0.06169270724058151\n",
            "Step 870, Training Loss: 0.0954025536775589\n",
            "Step 880, Training Loss: 0.00601169653236866\n",
            "Step 880, Training Loss: 0.0507524199783802\n",
            "Step 890, Training Loss: 0.009349334985017776\n",
            "Step 890, Training Loss: 0.18140263855457306\n",
            "Step 900, Training Loss: 0.09506943076848984\n",
            "Step 900, Training Loss: 0.00536379124969244\n",
            "Step 910, Training Loss: 0.1274518519639969\n",
            "Step 910, Training Loss: 0.03966771438717842\n",
            "Step 920, Training Loss: 0.08887649327516556\n",
            "Step 920, Training Loss: 0.007224911358207464\n",
            "Step 930, Training Loss: 0.007638674229383469\n",
            "Step 930, Training Loss: 0.007618374191224575\n",
            "Step 940, Training Loss: 0.004923396743834019\n",
            "Step 940, Training Loss: 0.07365264743566513\n",
            "Step 950, Training Loss: 0.04731668159365654\n",
            "Step 950, Training Loss: 0.05065535381436348\n",
            "Step 960, Training Loss: 0.059900302439928055\n",
            "Step 960, Training Loss: 0.005602384451776743\n",
            "Step 970, Training Loss: 0.004226175602525473\n",
            "Step 970, Training Loss: 0.007566758897155523\n",
            "Step 980, Training Loss: 0.2871742844581604\n",
            "Step 980, Training Loss: 0.013617613352835178\n",
            "Step 990, Training Loss: 0.18788693845272064\n",
            "Step 990, Training Loss: 0.07828216254711151\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Saving model checkpoint to ./results/checkpoint-1000\n",
            "Configuration saved in ./results/checkpoint-1000/config.json\n",
            "Model weights saved in ./results/checkpoint-1000/pytorch_model.bin\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 1000, Training Loss: 0.006097984034568071\n",
            "Step 1000, Training Loss: 0.007453261408954859\n",
            "Step 1010, Training Loss: 0.04498085007071495\n",
            "Step 1010, Training Loss: 0.004819277208298445\n",
            "Step 1020, Training Loss: 0.05547134578227997\n",
            "Step 1020, Training Loss: 0.017545921728014946\n",
            "Step 1030, Training Loss: 0.045066095888614655\n",
            "Step 1030, Training Loss: 0.049111805856227875\n",
            "Step 1040, Training Loss: 0.004305737558752298\n",
            "Step 1040, Training Loss: 0.04046157747507095\n",
            "Step 1050, Training Loss: 0.01592588797211647\n",
            "Step 1050, Training Loss: 0.046313654631376266\n",
            "Step 1060, Training Loss: 0.04373816028237343\n",
            "Step 1060, Training Loss: 0.003911723382771015\n",
            "Step 1070, Training Loss: 0.006995369680225849\n",
            "Step 1070, Training Loss: 0.051566969603300095\n",
            "Step 1080, Training Loss: 0.007707311771810055\n",
            "Step 1080, Training Loss: 0.05547047406435013\n",
            "Step 1090, Training Loss: 0.00771696213632822\n",
            "Step 1090, Training Loss: 0.004135766997933388\n",
            "Step 1100, Training Loss: 0.010219329968094826\n",
            "Step 1100, Training Loss: 0.006073938217014074\n",
            "Step 1110, Training Loss: 0.006443007383495569\n",
            "Step 1110, Training Loss: 0.006911820266395807\n",
            "Step 1120, Training Loss: 0.008463663049042225\n",
            "Step 1120, Training Loss: 0.06971776485443115\n",
            "Step 1130, Training Loss: 0.008696219883859158\n",
            "Step 1130, Training Loss: 0.01168751996010542\n",
            "Step 1140, Training Loss: 0.005082651041448116\n",
            "Step 1140, Training Loss: 0.0069300327450037\n",
            "Step 1150, Training Loss: 0.011829596012830734\n",
            "Step 1150, Training Loss: 0.36644336581230164\n",
            "Step 1160, Training Loss: 0.04812103882431984\n",
            "Step 1160, Training Loss: 0.00418313080444932\n",
            "Step 1170, Training Loss: 0.29744893312454224\n",
            "Step 1170, Training Loss: 0.07796213775873184\n",
            "Step 1180, Training Loss: 0.010464700870215893\n",
            "Step 1180, Training Loss: 0.04011086747050285\n",
            "Step 1190, Training Loss: 0.07508259266614914\n",
            "Step 1190, Training Loss: 0.10515573620796204\n",
            "Step 1200, Training Loss: 0.0457124300301075\n",
            "Step 1200, Training Loss: 0.015911322087049484\n",
            "Step 1210, Training Loss: 0.05731690302491188\n",
            "Step 1210, Training Loss: 0.011654994450509548\n",
            "Step 1220, Training Loss: 0.09310770779848099\n",
            "Step 1220, Training Loss: 0.3705947995185852\n",
            "Step 1230, Training Loss: 0.20363087952136993\n",
            "Step 1230, Training Loss: 0.039391763508319855\n",
            "Step 1240, Training Loss: 0.081942617893219\n",
            "Step 1240, Training Loss: 0.0434245839715004\n",
            "Step 1250, Training Loss: 0.004612003453075886\n",
            "Step 1250, Training Loss: 0.45719969272613525\n",
            "Step 1260, Training Loss: 0.010371576994657516\n",
            "Step 1260, Training Loss: 0.006131270434707403\n",
            "Step 1270, Training Loss: 0.1868438422679901\n",
            "Step 1270, Training Loss: 0.004355631768703461\n",
            "Step 1280, Training Loss: 0.21621057391166687\n",
            "Step 1280, Training Loss: 0.0103457598015666\n",
            "Step 1290, Training Loss: 0.006775675341486931\n",
            "Step 1290, Training Loss: 0.05170033872127533\n",
            "Step 1300, Training Loss: 0.007335278205573559\n",
            "Step 1300, Training Loss: 0.0059301345609128475\n",
            "Step 1310, Training Loss: 0.13405027985572815\n",
            "Step 1310, Training Loss: 0.425857275724411\n",
            "Step 1320, Training Loss: 0.008191529661417007\n",
            "Step 1320, Training Loss: 0.011748974211513996\n",
            "Step 1330, Training Loss: 0.140304297208786\n",
            "Step 1330, Training Loss: 0.04038329795002937\n",
            "Step 1340, Training Loss: 0.005220500752329826\n",
            "Step 1340, Training Loss: 0.003832225687801838\n",
            "Step 1350, Training Loss: 0.008538949303328991\n",
            "Step 1350, Training Loss: 0.01130647026002407\n",
            "Step 1360, Training Loss: 0.12545600533485413\n",
            "Step 1360, Training Loss: 0.1185869425535202\n",
            "Step 1370, Training Loss: 0.050253137946128845\n",
            "Step 1370, Training Loss: 0.039006203413009644\n",
            "Step 1380, Training Loss: 0.04667551442980766\n",
            "Step 1380, Training Loss: 0.0845024362206459\n",
            "Step 1390, Training Loss: 0.6057757139205933\n",
            "Step 1390, Training Loss: 0.038353487849235535\n",
            "Step 1400, Training Loss: 0.038636572659015656\n",
            "Step 1400, Training Loss: 0.04334171861410141\n",
            "Step 1410, Training Loss: 0.0053253513760864735\n",
            "Step 1410, Training Loss: 0.007024225778877735\n",
            "Step 1420, Training Loss: 0.005604204721748829\n",
            "Step 1420, Training Loss: 0.00564834289252758\n",
            "Step 1430, Training Loss: 0.10672522336244583\n",
            "Step 1430, Training Loss: 0.04552864655852318\n",
            "Step 1440, Training Loss: 0.013143406249582767\n",
            "Step 1440, Training Loss: 0.005011880304664373\n",
            "Step 1450, Training Loss: 0.006082544568926096\n",
            "Step 1450, Training Loss: 0.005167926196008921\n",
            "Step 1460, Training Loss: 0.0057993545196950436\n",
            "Step 1460, Training Loss: 0.004626802634447813\n",
            "Step 1470, Training Loss: 0.007152931299060583\n",
            "Step 1470, Training Loss: 0.12189653515815735\n",
            "Step 1480, Training Loss: 0.004690368659794331\n",
            "Step 1480, Training Loss: 0.03942152485251427\n",
            "Step 1490, Training Loss: 0.05248405411839485\n",
            "Step 1490, Training Loss: 0.0922933965921402\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Saving model checkpoint to ./results/checkpoint-1500\n",
            "Configuration saved in ./results/checkpoint-1500/config.json\n",
            "Model weights saved in ./results/checkpoint-1500/pytorch_model.bin\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 1500, Training Loss: 0.07015948742628098\n",
            "Step 1500, Training Loss: 0.08513446897268295\n",
            "Step 1510, Training Loss: 0.006873161066323519\n",
            "Step 1510, Training Loss: 0.006387824658304453\n",
            "Step 1520, Training Loss: 0.06241496279835701\n",
            "Step 1520, Training Loss: 0.008390183560550213\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 1523\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 1530, Training Loss: 0.00567219452932477\n",
            "Step 1530, Training Loss: 0.04758738726377487\n",
            "Step 1540, Training Loss: 0.006441150791943073\n",
            "Step 1540, Training Loss: 0.010207314975559711\n",
            "Step 1550, Training Loss: 0.0066515002399683\n",
            "Step 1550, Training Loss: 0.0801137387752533\n",
            "Step 1560, Training Loss: 0.00798642635345459\n",
            "Step 1560, Training Loss: 0.047625306993722916\n",
            "Step 1570, Training Loss: 0.004786253906786442\n",
            "Step 1570, Training Loss: 0.004799625836312771\n",
            "Step 1580, Training Loss: 0.06742852926254272\n",
            "Step 1580, Training Loss: 0.006979549769312143\n",
            "Step 1590, Training Loss: 0.011939538642764091\n",
            "Step 1590, Training Loss: 0.005078667309135199\n",
            "Step 1600, Training Loss: 0.012206112034618855\n",
            "Step 1600, Training Loss: 0.0177693422883749\n",
            "Step 1610, Training Loss: 0.07408502697944641\n",
            "Step 1610, Training Loss: 0.05221572145819664\n",
            "Step 1620, Training Loss: 0.03913030028343201\n",
            "Step 1620, Training Loss: 0.039707280695438385\n",
            "Step 1630, Training Loss: 0.008088766597211361\n",
            "Step 1630, Training Loss: 0.1319633275270462\n",
            "Step 1640, Training Loss: 0.006366361863911152\n",
            "Step 1640, Training Loss: 0.010849962010979652\n",
            "Step 1650, Training Loss: 0.006252002436667681\n",
            "Step 1650, Training Loss: 0.07248643040657043\n",
            "Step 1660, Training Loss: 0.008673741482198238\n",
            "Step 1660, Training Loss: 0.005909057799726725\n",
            "Step 1670, Training Loss: 0.01206760574132204\n",
            "Step 1670, Training Loss: 0.05474749207496643\n",
            "Step 1680, Training Loss: 0.006291314028203487\n",
            "Step 1680, Training Loss: 0.006824790500104427\n",
            "Step 1690, Training Loss: 0.005340268835425377\n",
            "Step 1690, Training Loss: 0.04581313207745552\n",
            "Step 1700, Training Loss: 0.0059203882701694965\n",
            "Step 1700, Training Loss: 0.0069492654874920845\n",
            "Step 1710, Training Loss: 0.004759880248457193\n",
            "Step 1710, Training Loss: 0.005451600532978773\n",
            "Step 1720, Training Loss: 0.011785190552473068\n",
            "Step 1720, Training Loss: 0.05358283966779709\n",
            "Step 1730, Training Loss: 0.05369902774691582\n",
            "Step 1730, Training Loss: 0.09466149657964706\n",
            "Step 1740, Training Loss: 0.00756370322778821\n",
            "Step 1740, Training Loss: 0.0052078766748309135\n",
            "Step 1750, Training Loss: 0.10651369392871857\n",
            "Step 1750, Training Loss: 0.08685744553804398\n",
            "Step 1760, Training Loss: 0.05465890094637871\n",
            "Step 1760, Training Loss: 0.093711718916893\n",
            "Step 1770, Training Loss: 0.03727556765079498\n",
            "Step 1770, Training Loss: 0.265448659658432\n",
            "Step 1780, Training Loss: 0.08497301489114761\n",
            "Step 1780, Training Loss: 0.004678600933402777\n",
            "Step 1790, Training Loss: 0.004933108575642109\n",
            "Step 1790, Training Loss: 0.1865086704492569\n",
            "Step 1800, Training Loss: 0.2399882972240448\n",
            "Step 1800, Training Loss: 0.00523820985108614\n",
            "Step 1810, Training Loss: 0.18857963383197784\n",
            "Step 1810, Training Loss: 0.007440393790602684\n",
            "Step 1820, Training Loss: 0.005860257428139448\n",
            "Step 1820, Training Loss: 0.009608705528080463\n",
            "Step 1830, Training Loss: 0.28540024161338806\n",
            "Step 1830, Training Loss: 0.008837277069687843\n",
            "Step 1840, Training Loss: 0.009487496688961983\n",
            "Step 1840, Training Loss: 0.04874321445822716\n",
            "Step 1850, Training Loss: 0.006898986175656319\n",
            "Step 1850, Training Loss: 0.004462081007659435\n",
            "Step 1860, Training Loss: 0.049971528351306915\n",
            "Step 1860, Training Loss: 0.12141639739274979\n",
            "Step 1870, Training Loss: 0.005094669293612242\n",
            "Step 1870, Training Loss: 0.03899097442626953\n",
            "Step 1880, Training Loss: 0.005685003008693457\n",
            "Step 1880, Training Loss: 0.016255943104624748\n",
            "Step 1890, Training Loss: 0.19146506488323212\n",
            "Step 1890, Training Loss: 0.0050046369433403015\n",
            "Step 1900, Training Loss: 0.05503328889608383\n",
            "Step 1900, Training Loss: 0.04590977355837822\n",
            "Step 1910, Training Loss: 0.007577403448522091\n",
            "Step 1910, Training Loss: 0.04368359223008156\n",
            "Step 1920, Training Loss: 0.0052891625091433525\n",
            "Step 1920, Training Loss: 0.0056575145572423935\n",
            "Step 1930, Training Loss: 0.003990878816694021\n",
            "Step 1930, Training Loss: 0.005720242392271757\n",
            "Step 1940, Training Loss: 0.0041536977514624596\n",
            "Step 1940, Training Loss: 0.006406488362699747\n",
            "Step 1950, Training Loss: 0.531049370765686\n",
            "Step 1950, Training Loss: 0.013850572519004345\n",
            "Step 1960, Training Loss: 0.004167710896581411\n",
            "Step 1960, Training Loss: 0.05214289203286171\n",
            "Step 1970, Training Loss: 0.046177130192518234\n",
            "Step 1970, Training Loss: 0.011535448022186756\n",
            "Step 1980, Training Loss: 0.005418452899903059\n",
            "Step 1980, Training Loss: 0.15351173281669617\n",
            "Step 1990, Training Loss: 0.005345841404050589\n",
            "Step 1990, Training Loss: 0.045601263642311096\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Saving model checkpoint to ./results/checkpoint-2000\n",
            "Configuration saved in ./results/checkpoint-2000/config.json\n",
            "Model weights saved in ./results/checkpoint-2000/pytorch_model.bin\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 2000, Training Loss: 0.16651026904582977\n",
            "Step 2000, Training Loss: 0.013593729585409164\n",
            "Step 2010, Training Loss: 0.010287793353199959\n",
            "Step 2010, Training Loss: 0.014053501188755035\n",
            "Step 2020, Training Loss: 0.0040437630377709866\n",
            "Step 2020, Training Loss: 0.004864661954343319\n",
            "Step 2030, Training Loss: 0.008135010488331318\n",
            "Step 2030, Training Loss: 0.3307085633277893\n",
            "Step 2040, Training Loss: 0.006203887052834034\n",
            "Step 2040, Training Loss: 0.04640323296189308\n",
            "Step 2050, Training Loss: 0.006835320498794317\n",
            "Step 2050, Training Loss: 0.007712883874773979\n",
            "Step 2060, Training Loss: 0.08447094261646271\n",
            "Step 2060, Training Loss: 0.026237856596708298\n",
            "Step 2070, Training Loss: 0.006461679004132748\n",
            "Step 2070, Training Loss: 0.05292766913771629\n",
            "Step 2080, Training Loss: 0.17889325320720673\n",
            "Step 2080, Training Loss: 0.36851540207862854\n",
            "Step 2090, Training Loss: 0.41073930263519287\n",
            "Step 2090, Training Loss: 0.013240295462310314\n",
            "Step 2100, Training Loss: 0.0781230479478836\n",
            "Step 2100, Training Loss: 0.0067932503297924995\n",
            "Step 2110, Training Loss: 0.0071592689491808414\n",
            "Step 2110, Training Loss: 0.01096881739795208\n",
            "Step 2120, Training Loss: 0.006564304698258638\n",
            "Step 2120, Training Loss: 0.006595219019800425\n",
            "Step 2130, Training Loss: 0.1392061710357666\n",
            "Step 2130, Training Loss: 0.007949022576212883\n",
            "Step 2140, Training Loss: 0.06861693412065506\n",
            "Step 2140, Training Loss: 0.005940677132457495\n",
            "Step 2150, Training Loss: 0.005503861233592033\n",
            "Step 2150, Training Loss: 0.005601334385573864\n",
            "Step 2160, Training Loss: 0.05792464315891266\n",
            "Step 2160, Training Loss: 0.11281200498342514\n",
            "Step 2170, Training Loss: 0.0223839171230793\n",
            "Step 2170, Training Loss: 0.006943403743207455\n",
            "Step 2180, Training Loss: 0.33436137437820435\n",
            "Step 2180, Training Loss: 0.005530160386115313\n",
            "Step 2190, Training Loss: 0.006400040350854397\n",
            "Step 2190, Training Loss: 0.005847118329256773\n",
            "Step 2200, Training Loss: 0.008058968931436539\n",
            "Step 2200, Training Loss: 0.004713761154562235\n",
            "Step 2210, Training Loss: 0.04575736075639725\n",
            "Step 2210, Training Loss: 0.015133526176214218\n",
            "Step 2220, Training Loss: 0.06345541030168533\n",
            "Step 2220, Training Loss: 0.046627067029476166\n",
            "Step 2230, Training Loss: 0.005693303886801004\n",
            "Step 2230, Training Loss: 0.007817612960934639\n",
            "Step 2240, Training Loss: 0.007456281688064337\n",
            "Step 2240, Training Loss: 0.01638605259358883\n",
            "Step 2250, Training Loss: 0.03877189755439758\n",
            "Step 2250, Training Loss: 0.07652651518583298\n",
            "Step 2260, Training Loss: 0.005738620646297932\n",
            "Step 2260, Training Loss: 0.009156360290944576\n",
            "Step 2270, Training Loss: 0.12257028371095657\n",
            "Step 2270, Training Loss: 0.006921593565493822\n",
            "Step 2280, Training Loss: 0.039163846522569656\n",
            "Step 2280, Training Loss: 0.05362626165151596\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 1523\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 2290, Training Loss: 0.007996918633580208\n",
            "Step 2290, Training Loss: 0.032254815101623535\n",
            "Step 2300, Training Loss: 0.004270502831786871\n",
            "Step 2300, Training Loss: 0.008242594078183174\n",
            "Step 2310, Training Loss: 0.008378800004720688\n",
            "Step 2310, Training Loss: 0.006319764535874128\n",
            "Step 2320, Training Loss: 0.0102461576461792\n",
            "Step 2320, Training Loss: 0.04631691426038742\n",
            "Step 2330, Training Loss: 0.004039878025650978\n",
            "Step 2330, Training Loss: 0.00998969841748476\n",
            "Step 2340, Training Loss: 0.006622110493481159\n",
            "Step 2340, Training Loss: 0.04729648679494858\n",
            "Step 2350, Training Loss: 0.005543866660445929\n",
            "Step 2350, Training Loss: 0.004990349989384413\n",
            "Step 2360, Training Loss: 0.005342068150639534\n",
            "Step 2360, Training Loss: 0.006111887749284506\n",
            "Step 2370, Training Loss: 0.0794462338089943\n",
            "Step 2370, Training Loss: 0.08456695824861526\n",
            "Step 2380, Training Loss: 0.055936187505722046\n",
            "Step 2380, Training Loss: 0.0039351098239421844\n",
            "Step 2390, Training Loss: 0.10901582986116409\n",
            "Step 2390, Training Loss: 0.01430843211710453\n",
            "Step 2400, Training Loss: 0.037312157452106476\n",
            "Step 2400, Training Loss: 0.004109713714569807\n",
            "Step 2410, Training Loss: 0.04692551866173744\n",
            "Step 2410, Training Loss: 0.052800606936216354\n",
            "Step 2420, Training Loss: 0.011463127098977566\n",
            "Step 2420, Training Loss: 0.01301982719451189\n",
            "Step 2430, Training Loss: 0.010413321666419506\n",
            "Step 2430, Training Loss: 0.006917334161698818\n",
            "Step 2440, Training Loss: 0.013335790485143661\n",
            "Step 2440, Training Loss: 0.005204855464398861\n",
            "Step 2450, Training Loss: 0.0050013503059744835\n",
            "Step 2450, Training Loss: 0.01849472150206566\n",
            "Step 2460, Training Loss: 0.04068531468510628\n",
            "Step 2460, Training Loss: 0.04326789081096649\n",
            "Step 2470, Training Loss: 0.358244389295578\n",
            "Step 2470, Training Loss: 0.006170577369630337\n",
            "Step 2480, Training Loss: 0.01081657875329256\n",
            "Step 2480, Training Loss: 0.006502165459096432\n",
            "Step 2490, Training Loss: 0.3457695245742798\n",
            "Step 2490, Training Loss: 0.00538369407877326\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Saving model checkpoint to ./results/checkpoint-2500\n",
            "Configuration saved in ./results/checkpoint-2500/config.json\n",
            "Model weights saved in ./results/checkpoint-2500/pytorch_model.bin\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 2500, Training Loss: 0.016861658543348312\n",
            "Step 2500, Training Loss: 0.1530667394399643\n",
            "Step 2510, Training Loss: 0.00596024701371789\n",
            "Step 2510, Training Loss: 0.006854531355202198\n",
            "Step 2520, Training Loss: 0.006024752743542194\n",
            "Step 2520, Training Loss: 0.07373803853988647\n",
            "Step 2530, Training Loss: 0.12295904755592346\n",
            "Step 2530, Training Loss: 0.0065044816583395\n",
            "Step 2540, Training Loss: 0.010293952189385891\n",
            "Step 2540, Training Loss: 0.004689905792474747\n",
            "Step 2550, Training Loss: 0.05399791896343231\n",
            "Step 2550, Training Loss: 0.057282619178295135\n",
            "Step 2560, Training Loss: 0.05789126083254814\n",
            "Step 2560, Training Loss: 0.012042817659676075\n",
            "Step 2570, Training Loss: 0.04392556473612785\n",
            "Step 2570, Training Loss: 0.012658920139074326\n",
            "Step 2580, Training Loss: 0.15677011013031006\n",
            "Step 2580, Training Loss: 0.010599309578537941\n",
            "Step 2590, Training Loss: 0.011739836074411869\n",
            "Step 2590, Training Loss: 0.0444754995405674\n",
            "Step 2600, Training Loss: 0.09654577076435089\n",
            "Step 2600, Training Loss: 0.04642326757311821\n",
            "Step 2610, Training Loss: 0.004742844495922327\n",
            "Step 2610, Training Loss: 0.19731567800045013\n",
            "Step 2620, Training Loss: 0.0787048488855362\n",
            "Step 2620, Training Loss: 0.0769474059343338\n",
            "Step 2630, Training Loss: 0.0073813702911138535\n",
            "Step 2630, Training Loss: 0.007513698190450668\n",
            "Step 2640, Training Loss: 0.04058996960520744\n",
            "Step 2640, Training Loss: 0.06473703682422638\n",
            "Step 2650, Training Loss: 0.11025085300207138\n",
            "Step 2650, Training Loss: 0.003627222264185548\n",
            "Step 2660, Training Loss: 0.00761815719306469\n",
            "Step 2660, Training Loss: 0.004643960855901241\n",
            "Step 2670, Training Loss: 0.007360969670116901\n",
            "Step 2670, Training Loss: 0.0037714592181146145\n",
            "Step 2680, Training Loss: 0.02353612706065178\n",
            "Step 2680, Training Loss: 0.006422435399144888\n",
            "Step 2690, Training Loss: 0.004468059632927179\n",
            "Step 2690, Training Loss: 0.1095382571220398\n",
            "Step 2700, Training Loss: 0.0075901770032942295\n",
            "Step 2700, Training Loss: 0.0038719226140528917\n",
            "Step 2710, Training Loss: 0.04773031920194626\n",
            "Step 2710, Training Loss: 0.11847059428691864\n",
            "Step 2720, Training Loss: 0.004562447778880596\n",
            "Step 2720, Training Loss: 0.008088004775345325\n",
            "Step 2730, Training Loss: 0.00810988899320364\n",
            "Step 2730, Training Loss: 0.00613589771091938\n",
            "Step 2740, Training Loss: 0.14463132619857788\n",
            "Step 2740, Training Loss: 0.005063076503574848\n",
            "Step 2750, Training Loss: 0.11594120413064957\n",
            "Step 2750, Training Loss: 0.31648990511894226\n",
            "Step 2760, Training Loss: 0.007563554681837559\n",
            "Step 2760, Training Loss: 0.007294800132513046\n",
            "Step 2770, Training Loss: 0.006817257963120937\n",
            "Step 2770, Training Loss: 0.0063832830637693405\n",
            "Step 2780, Training Loss: 0.08774732053279877\n",
            "Step 2780, Training Loss: 0.01099176611751318\n",
            "Step 2790, Training Loss: 0.012492042034864426\n",
            "Step 2790, Training Loss: 0.03967852145433426\n",
            "Step 2800, Training Loss: 0.006799767259508371\n",
            "Step 2800, Training Loss: 0.10898658633232117\n",
            "Step 2810, Training Loss: 0.10647498071193695\n",
            "Step 2810, Training Loss: 0.007583842147141695\n",
            "Step 2820, Training Loss: 0.008209489285945892\n",
            "Step 2820, Training Loss: 0.003930054605007172\n",
            "Step 2830, Training Loss: 0.09718688577413559\n",
            "Step 2830, Training Loss: 0.004263147246092558\n",
            "Step 2840, Training Loss: 0.005922562442719936\n",
            "Step 2840, Training Loss: 0.07877230644226074\n",
            "Step 2850, Training Loss: 0.00623896112665534\n",
            "Step 2850, Training Loss: 0.015065981075167656\n",
            "Step 2860, Training Loss: 0.005947011057287455\n",
            "Step 2860, Training Loss: 0.006601095665246248\n",
            "Step 2870, Training Loss: 0.0059553091414272785\n",
            "Step 2870, Training Loss: 0.006401073653250933\n",
            "Step 2880, Training Loss: 0.017281625419855118\n",
            "Step 2880, Training Loss: 0.0072600929997861385\n",
            "Step 2890, Training Loss: 0.055575113743543625\n",
            "Step 2890, Training Loss: 0.06376633048057556\n",
            "Step 2900, Training Loss: 0.005385256372392178\n",
            "Step 2900, Training Loss: 0.006577929016202688\n",
            "Step 2910, Training Loss: 0.005747055634856224\n",
            "Step 2910, Training Loss: 0.15753836929798126\n",
            "Step 2920, Training Loss: 0.005443883594125509\n",
            "Step 2920, Training Loss: 0.011872412636876106\n",
            "Step 2930, Training Loss: 0.01202726736664772\n",
            "Step 2930, Training Loss: 0.035821594297885895\n",
            "Step 2940, Training Loss: 0.005419152323156595\n",
            "Step 2940, Training Loss: 0.20078954100608826\n",
            "Step 2950, Training Loss: 0.03306703642010689\n",
            "Step 2950, Training Loss: 0.06982690095901489\n",
            "Step 2960, Training Loss: 0.05169543996453285\n",
            "Step 2960, Training Loss: 0.06156362220644951\n",
            "Step 2970, Training Loss: 0.006600807886570692\n",
            "Step 2970, Training Loss: 0.04748303443193436\n",
            "Step 2980, Training Loss: 0.2846042513847351\n",
            "Step 2980, Training Loss: 0.01073153130710125\n",
            "Step 2990, Training Loss: 0.09289124608039856\n",
            "Step 2990, Training Loss: 0.007952926680445671\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Saving model checkpoint to ./results/checkpoint-3000\n",
            "Configuration saved in ./results/checkpoint-3000/config.json\n",
            "Model weights saved in ./results/checkpoint-3000/pytorch_model.bin\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 3000, Training Loss: 0.04763657599687576\n",
            "Step 3000, Training Loss: 0.014235323294997215\n",
            "Step 3010, Training Loss: 0.009992619976401329\n",
            "Step 3010, Training Loss: 0.06400873512029648\n",
            "Step 3020, Training Loss: 0.3250086307525635\n",
            "Step 3020, Training Loss: 0.045374296605587006\n",
            "Step 3030, Training Loss: 0.00727772107347846\n",
            "Step 3030, Training Loss: 0.007828918285667896\n",
            "Step 3040, Training Loss: 0.005841552279889584\n",
            "Step 3040, Training Loss: 0.052587296813726425\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 1523\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 3050, Training Loss: 0.010598842985928059\n",
            "Step 3050, Training Loss: 0.005531879607588053\n",
            "Step 3060, Training Loss: 0.011793783865869045\n",
            "Step 3060, Training Loss: 0.006819501984864473\n",
            "Step 3070, Training Loss: 0.005049511324614286\n",
            "Step 3070, Training Loss: 0.13048496842384338\n",
            "Step 3080, Training Loss: 0.10794941335916519\n",
            "Step 3080, Training Loss: 0.00870669074356556\n",
            "Step 3090, Training Loss: 0.012886692769825459\n",
            "Step 3090, Training Loss: 0.007131810300052166\n",
            "Step 3100, Training Loss: 0.037889737635850906\n",
            "Step 3100, Training Loss: 0.0574028342962265\n",
            "Step 3110, Training Loss: 0.05288126692175865\n",
            "Step 3110, Training Loss: 0.005835479591041803\n",
            "Step 3120, Training Loss: 0.004393146373331547\n",
            "Step 3120, Training Loss: 0.04040871188044548\n",
            "Step 3130, Training Loss: 0.008264056406915188\n",
            "Step 3130, Training Loss: 0.009860191494226456\n",
            "Step 3140, Training Loss: 0.006438638549298048\n",
            "Step 3140, Training Loss: 0.047643501311540604\n",
            "Step 3150, Training Loss: 0.004129842855036259\n",
            "Step 3150, Training Loss: 0.0893402248620987\n",
            "Step 3160, Training Loss: 0.05959422513842583\n",
            "Step 3160, Training Loss: 0.005530037917196751\n",
            "Step 3170, Training Loss: 0.04009207710623741\n",
            "Step 3170, Training Loss: 0.008665500208735466\n",
            "Step 3180, Training Loss: 0.005273604299873114\n",
            "Step 3180, Training Loss: 0.003870041575282812\n",
            "Step 3190, Training Loss: 0.03579025715589523\n",
            "Step 3190, Training Loss: 0.004475240595638752\n",
            "Step 3200, Training Loss: 0.056459106504917145\n",
            "Step 3200, Training Loss: 0.16771677136421204\n",
            "Step 3210, Training Loss: 0.12467537075281143\n",
            "Step 3210, Training Loss: 0.10184882581233978\n",
            "Step 3220, Training Loss: 0.010590450838208199\n",
            "Step 3220, Training Loss: 0.051097117364406586\n",
            "Step 3230, Training Loss: 0.004409431479871273\n",
            "Step 3230, Training Loss: 0.006553801242262125\n",
            "Step 3240, Training Loss: 0.12257546186447144\n",
            "Step 3240, Training Loss: 0.005094450898468494\n",
            "Step 3250, Training Loss: 0.010150551795959473\n",
            "Step 3250, Training Loss: 0.23606820404529572\n",
            "Step 3260, Training Loss: 0.005884184502065182\n",
            "Step 3260, Training Loss: 0.056262094527482986\n",
            "Step 3270, Training Loss: 0.3383806645870209\n",
            "Step 3270, Training Loss: 0.14295968413352966\n",
            "Step 3280, Training Loss: 0.004982714541256428\n",
            "Step 3280, Training Loss: 0.04678819701075554\n",
            "Step 3290, Training Loss: 0.05308811739087105\n",
            "Step 3290, Training Loss: 0.005139598622918129\n",
            "Step 3300, Training Loss: 0.012074987404048443\n",
            "Step 3300, Training Loss: 0.06141761317849159\n",
            "Step 3310, Training Loss: 0.055030420422554016\n",
            "Step 3310, Training Loss: 0.004668653942644596\n",
            "Step 3320, Training Loss: 0.005800193641334772\n",
            "Step 3320, Training Loss: 0.006076773162931204\n",
            "Step 3330, Training Loss: 0.3537217974662781\n",
            "Step 3330, Training Loss: 0.00397768197581172\n",
            "Step 3340, Training Loss: 0.05058830603957176\n",
            "Step 3340, Training Loss: 0.005953760351985693\n",
            "Step 3350, Training Loss: 0.010683855973184109\n",
            "Step 3350, Training Loss: 0.24067215621471405\n",
            "Step 3360, Training Loss: 0.00610645767301321\n",
            "Step 3360, Training Loss: 0.08684106916189194\n",
            "Step 3370, Training Loss: 0.2575725317001343\n",
            "Step 3370, Training Loss: 0.012627688236534595\n",
            "Step 3380, Training Loss: 0.013269145973026752\n",
            "Step 3380, Training Loss: 0.013134862296283245\n",
            "Step 3390, Training Loss: 0.3259216845035553\n",
            "Step 3390, Training Loss: 0.007273721043020487\n",
            "Step 3400, Training Loss: 0.12608006596565247\n",
            "Step 3400, Training Loss: 0.3190161883831024\n",
            "Step 3410, Training Loss: 0.00429119635373354\n",
            "Step 3410, Training Loss: 0.005691736005246639\n",
            "Step 3420, Training Loss: 0.010549766942858696\n",
            "Step 3420, Training Loss: 0.004952562041580677\n",
            "Step 3430, Training Loss: 0.05879269167780876\n",
            "Step 3430, Training Loss: 0.006635838653892279\n",
            "Step 3440, Training Loss: 0.006196905858814716\n",
            "Step 3440, Training Loss: 0.10458415001630783\n",
            "Step 3450, Training Loss: 0.011501566506922245\n",
            "Step 3450, Training Loss: 0.04593238607048988\n",
            "Step 3460, Training Loss: 0.050284456461668015\n",
            "Step 3460, Training Loss: 0.0053816912695765495\n",
            "Step 3470, Training Loss: 0.009899322874844074\n",
            "Step 3470, Training Loss: 0.04117470607161522\n",
            "Step 3480, Training Loss: 0.005633997265249491\n",
            "Step 3480, Training Loss: 0.03722948580980301\n",
            "Step 3490, Training Loss: 0.07839617133140564\n",
            "Step 3490, Training Loss: 0.004726123996078968\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Saving model checkpoint to ./results/checkpoint-3500\n",
            "Configuration saved in ./results/checkpoint-3500/config.json\n",
            "Model weights saved in ./results/checkpoint-3500/pytorch_model.bin\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 3500, Training Loss: 0.006553367245942354\n",
            "Step 3500, Training Loss: 0.010698975995182991\n",
            "Step 3510, Training Loss: 0.04889243096113205\n",
            "Step 3510, Training Loss: 0.16628843545913696\n",
            "Step 3520, Training Loss: 0.22002047300338745\n",
            "Step 3520, Training Loss: 0.005017976742237806\n",
            "Step 3530, Training Loss: 0.2126733809709549\n",
            "Step 3530, Training Loss: 0.006244082469493151\n",
            "Step 3540, Training Loss: 0.04714221879839897\n",
            "Step 3540, Training Loss: 0.005969219841063023\n",
            "Step 3550, Training Loss: 0.006317478604614735\n",
            "Step 3550, Training Loss: 0.05428477004170418\n",
            "Step 3560, Training Loss: 0.03609296306967735\n",
            "Step 3560, Training Loss: 0.005777718964964151\n",
            "Step 3570, Training Loss: 0.006311853416264057\n",
            "Step 3570, Training Loss: 0.007119313348084688\n",
            "Step 3580, Training Loss: 0.005082016810774803\n",
            "Step 3580, Training Loss: 0.008591926656663418\n",
            "Step 3590, Training Loss: 0.0064641074277460575\n",
            "Step 3590, Training Loss: 0.044523708522319794\n",
            "Step 3600, Training Loss: 0.005420467350631952\n",
            "Step 3600, Training Loss: 0.1090787947177887\n",
            "Step 3610, Training Loss: 0.061467867344617844\n",
            "Step 3610, Training Loss: 0.01113736443221569\n",
            "Step 3620, Training Loss: 0.2472340166568756\n",
            "Step 3620, Training Loss: 0.009853595867753029\n",
            "Step 3630, Training Loss: 0.11339948326349258\n",
            "Step 3630, Training Loss: 0.34211841225624084\n",
            "Step 3640, Training Loss: 0.10798594355583191\n",
            "Step 3640, Training Loss: 0.011060974560678005\n",
            "Step 3650, Training Loss: 0.1487785428762436\n",
            "Step 3650, Training Loss: 0.04445609077811241\n",
            "Step 3660, Training Loss: 0.005290676839649677\n",
            "Step 3660, Training Loss: 0.011272449046373367\n",
            "Step 3670, Training Loss: 0.008540338836610317\n",
            "Step 3670, Training Loss: 0.1691737323999405\n",
            "Step 3680, Training Loss: 0.0055151735432446\n",
            "Step 3680, Training Loss: 0.20390920341014862\n",
            "Step 3690, Training Loss: 0.00683761527761817\n",
            "Step 3690, Training Loss: 0.39015042781829834\n",
            "Step 3700, Training Loss: 0.01079186424612999\n",
            "Step 3700, Training Loss: 0.28441736102104187\n",
            "Step 3710, Training Loss: 0.055767614394426346\n",
            "Step 3710, Training Loss: 0.004906912334263325\n",
            "Step 3720, Training Loss: 0.004652522504329681\n",
            "Step 3720, Training Loss: 0.018807917833328247\n",
            "Step 3730, Training Loss: 0.004722787998616695\n",
            "Step 3730, Training Loss: 0.03961850330233574\n",
            "Step 3740, Training Loss: 0.11635124683380127\n",
            "Step 3740, Training Loss: 0.005638290662318468\n",
            "Step 3750, Training Loss: 0.006950633134692907\n",
            "Step 3750, Training Loss: 0.004831661470234394\n",
            "Step 3760, Training Loss: 0.005089125595986843\n",
            "Step 3760, Training Loss: 0.005456435494124889\n",
            "Step 3770, Training Loss: 0.013361318036913872\n",
            "Step 3770, Training Loss: 0.005325780715793371\n",
            "Step 3780, Training Loss: 0.004224891308695078\n",
            "Step 3780, Training Loss: 0.005599589552730322\n",
            "Step 3790, Training Loss: 0.007315579801797867\n",
            "Step 3790, Training Loss: 0.006595714949071407\n",
            "Step 3800, Training Loss: 0.009850827045738697\n",
            "Step 3800, Training Loss: 0.04958077892661095\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 1523\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 3810, Training Loss: 0.0066313957795500755\n",
            "Step 3810, Training Loss: 0.005127168260514736\n",
            "Step 3820, Training Loss: 0.005653294734656811\n",
            "Step 3820, Training Loss: 0.005867787636816502\n",
            "Step 3830, Training Loss: 0.08454839140176773\n",
            "Step 3830, Training Loss: 0.051581405103206635\n",
            "Step 3840, Training Loss: 0.09748121351003647\n",
            "Step 3840, Training Loss: 0.005137708969414234\n",
            "Step 3850, Training Loss: 0.05062144249677658\n",
            "Step 3850, Training Loss: 0.10747668147087097\n",
            "Step 3860, Training Loss: 0.06480242311954498\n",
            "Step 3860, Training Loss: 0.005852353293448687\n",
            "Step 3870, Training Loss: 0.005627365317195654\n",
            "Step 3870, Training Loss: 0.010229106992483139\n",
            "Step 3880, Training Loss: 0.18234308063983917\n",
            "Step 3880, Training Loss: 0.005894904490560293\n",
            "Step 3890, Training Loss: 0.03454569727182388\n",
            "Step 3890, Training Loss: 0.007197693455964327\n",
            "Step 3900, Training Loss: 0.049059394747018814\n",
            "Step 3900, Training Loss: 0.00447805505245924\n",
            "Step 3910, Training Loss: 0.00772103713825345\n",
            "Step 3910, Training Loss: 0.08409322798252106\n",
            "Step 3920, Training Loss: 0.004771524574607611\n",
            "Step 3920, Training Loss: 0.00960603915154934\n",
            "Step 3930, Training Loss: 0.00603849021717906\n",
            "Step 3930, Training Loss: 0.046622030436992645\n",
            "Step 3940, Training Loss: 0.04005571827292442\n",
            "Step 3940, Training Loss: 0.020403960719704628\n",
            "Step 3950, Training Loss: 0.09938149899244308\n",
            "Step 3950, Training Loss: 0.00679237162694335\n",
            "Step 3960, Training Loss: 0.008055881597101688\n",
            "Step 3960, Training Loss: 0.009884609840810299\n",
            "Step 3970, Training Loss: 0.011208908632397652\n",
            "Step 3970, Training Loss: 0.11302012950181961\n",
            "Step 3980, Training Loss: 0.004946759436279535\n",
            "Step 3980, Training Loss: 0.05071104317903519\n",
            "Step 3990, Training Loss: 0.08405277132987976\n",
            "Step 3990, Training Loss: 0.005464800167828798\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Saving model checkpoint to ./results/checkpoint-4000\n",
            "Configuration saved in ./results/checkpoint-4000/config.json\n",
            "Model weights saved in ./results/checkpoint-4000/pytorch_model.bin\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 4000, Training Loss: 0.006714516784995794\n",
            "Step 4000, Training Loss: 0.00548954214900732\n",
            "Step 4010, Training Loss: 0.006697176955640316\n",
            "Step 4010, Training Loss: 0.007278396748006344\n",
            "Step 4020, Training Loss: 0.006087609101086855\n",
            "Step 4020, Training Loss: 0.05140924081206322\n",
            "Step 4030, Training Loss: 0.10586405545473099\n",
            "Step 4030, Training Loss: 0.005147608462721109\n",
            "Step 4040, Training Loss: 0.055189985781908035\n",
            "Step 4040, Training Loss: 0.00434652715921402\n",
            "Step 4050, Training Loss: 0.037646159529685974\n",
            "Step 4050, Training Loss: 0.005487293470650911\n",
            "Step 4060, Training Loss: 0.005742753855884075\n",
            "Step 4060, Training Loss: 0.10822811722755432\n",
            "Step 4070, Training Loss: 0.24212893843650818\n",
            "Step 4070, Training Loss: 0.10849551856517792\n",
            "Step 4080, Training Loss: 0.318954199552536\n",
            "Step 4080, Training Loss: 0.10949130356311798\n",
            "Step 4090, Training Loss: 0.04014485329389572\n",
            "Step 4090, Training Loss: 0.006773661822080612\n",
            "Step 4100, Training Loss: 0.008878572843968868\n",
            "Step 4100, Training Loss: 0.005116511136293411\n",
            "Step 4110, Training Loss: 0.005443019326776266\n",
            "Step 4110, Training Loss: 0.03661595657467842\n",
            "Step 4120, Training Loss: 0.03904007747769356\n",
            "Step 4120, Training Loss: 0.07546855509281158\n",
            "Step 4130, Training Loss: 0.38014549016952515\n",
            "Step 4130, Training Loss: 0.04640135541558266\n",
            "Step 4140, Training Loss: 0.17969070374965668\n",
            "Step 4140, Training Loss: 0.0069862110540270805\n",
            "Step 4150, Training Loss: 0.007245090790092945\n",
            "Step 4150, Training Loss: 0.17695747315883636\n",
            "Step 4160, Training Loss: 0.05811342969536781\n",
            "Step 4160, Training Loss: 0.010313865728676319\n",
            "Step 4170, Training Loss: 0.009882951155304909\n",
            "Step 4170, Training Loss: 0.07816645503044128\n",
            "Step 4180, Training Loss: 0.03653940185904503\n",
            "Step 4180, Training Loss: 0.013738775625824928\n",
            "Step 4190, Training Loss: 0.04236869141459465\n",
            "Step 4190, Training Loss: 0.046027131378650665\n",
            "Step 4200, Training Loss: 0.00785430334508419\n",
            "Step 4200, Training Loss: 0.06946103274822235\n",
            "Step 4210, Training Loss: 0.04050147533416748\n",
            "Step 4210, Training Loss: 0.05148633196949959\n",
            "Step 4220, Training Loss: 0.17943045496940613\n",
            "Step 4220, Training Loss: 0.004273007158190012\n",
            "Step 4230, Training Loss: 0.013588805682957172\n",
            "Step 4230, Training Loss: 0.10139831900596619\n",
            "Step 4240, Training Loss: 0.009841471910476685\n",
            "Step 4240, Training Loss: 0.20543554425239563\n",
            "Step 4250, Training Loss: 0.0046469965018332005\n",
            "Step 4250, Training Loss: 0.041719790548086166\n",
            "Step 4260, Training Loss: 0.033949896693229675\n",
            "Step 4260, Training Loss: 0.005284411832690239\n",
            "Step 4270, Training Loss: 0.004717120435088873\n",
            "Step 4270, Training Loss: 0.00415815319865942\n",
            "Step 4280, Training Loss: 0.005946615245193243\n",
            "Step 4280, Training Loss: 0.051570259034633636\n",
            "Step 4290, Training Loss: 0.29031944274902344\n",
            "Step 4290, Training Loss: 0.046815477311611176\n",
            "Step 4300, Training Loss: 0.04814064875245094\n",
            "Step 4300, Training Loss: 0.07651245594024658\n",
            "Step 4310, Training Loss: 0.04070286080241203\n",
            "Step 4310, Training Loss: 0.012616481631994247\n",
            "Step 4320, Training Loss: 0.04174173250794411\n",
            "Step 4320, Training Loss: 0.00401320168748498\n",
            "Step 4330, Training Loss: 0.0054288203828036785\n",
            "Step 4330, Training Loss: 0.18706278502941132\n",
            "Step 4340, Training Loss: 0.04080948978662491\n",
            "Step 4340, Training Loss: 0.07024192810058594\n",
            "Step 4350, Training Loss: 0.009007011540234089\n",
            "Step 4350, Training Loss: 0.0356639102101326\n",
            "Step 4360, Training Loss: 0.06188569590449333\n",
            "Step 4360, Training Loss: 0.04778600484132767\n",
            "Step 4370, Training Loss: 0.05777203291654587\n",
            "Step 4370, Training Loss: 0.03924078494310379\n",
            "Step 4380, Training Loss: 0.00376032548956573\n",
            "Step 4380, Training Loss: 0.06371849775314331\n",
            "Step 4390, Training Loss: 0.004413248971104622\n",
            "Step 4390, Training Loss: 0.012273406609892845\n",
            "Step 4400, Training Loss: 0.004892596509307623\n",
            "Step 4400, Training Loss: 0.08814973384141922\n",
            "Step 4410, Training Loss: 0.005517979152500629\n",
            "Step 4410, Training Loss: 0.00552730169147253\n",
            "Step 4420, Training Loss: 0.009060293436050415\n",
            "Step 4420, Training Loss: 0.008665233850479126\n",
            "Step 4430, Training Loss: 0.03738503158092499\n",
            "Step 4430, Training Loss: 0.057870637625455856\n",
            "Step 4440, Training Loss: 0.04547318443655968\n",
            "Step 4440, Training Loss: 0.14762330055236816\n",
            "Step 4450, Training Loss: 0.11364850401878357\n",
            "Step 4450, Training Loss: 0.25129780173301697\n",
            "Step 4460, Training Loss: 0.005304259248077869\n",
            "Step 4460, Training Loss: 0.005332621280103922\n",
            "Step 4470, Training Loss: 0.007799853105098009\n",
            "Step 4470, Training Loss: 0.08823186904191971\n",
            "Step 4480, Training Loss: 0.12982861697673798\n",
            "Step 4480, Training Loss: 0.007871678099036217\n",
            "Step 4490, Training Loss: 0.07266225665807724\n",
            "Step 4490, Training Loss: 0.006148445885628462\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Saving model checkpoint to ./results/checkpoint-4500\n",
            "Configuration saved in ./results/checkpoint-4500/config.json\n",
            "Model weights saved in ./results/checkpoint-4500/pytorch_model.bin\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 4500, Training Loss: 0.009954196400940418\n",
            "Step 4500, Training Loss: 0.02482597902417183\n",
            "Step 4510, Training Loss: 0.4146372377872467\n",
            "Step 4510, Training Loss: 0.003738834522664547\n",
            "Step 4520, Training Loss: 0.15537650883197784\n",
            "Step 4520, Training Loss: 0.21364066004753113\n",
            "Step 4530, Training Loss: 0.06873428076505661\n",
            "Step 4530, Training Loss: 0.006108713336288929\n",
            "Step 4540, Training Loss: 0.05545411258935928\n",
            "Step 4540, Training Loss: 0.005183136090636253\n",
            "Step 4550, Training Loss: 0.05602754279971123\n",
            "Step 4550, Training Loss: 0.008304743096232414\n",
            "Step 4560, Training Loss: 0.004381065722554922\n",
            "Step 4560, Training Loss: 0.011685836128890514\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 1523\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 4570, Training Loss: 0.06157897040247917\n",
            "Step 4570, Training Loss: 0.0034798390697687864\n",
            "Step 4580, Training Loss: 0.05192285403609276\n",
            "Step 4580, Training Loss: 0.16643640398979187\n",
            "Step 4590, Training Loss: 0.004416581243276596\n",
            "Step 4590, Training Loss: 0.04032701253890991\n",
            "Step 4600, Training Loss: 0.042448870837688446\n",
            "Step 4600, Training Loss: 0.011587534099817276\n",
            "Step 4610, Training Loss: 0.00672870920971036\n",
            "Step 4610, Training Loss: 0.007566791959106922\n",
            "Step 4620, Training Loss: 0.011086593382060528\n",
            "Step 4620, Training Loss: 0.2289772480726242\n",
            "Step 4630, Training Loss: 0.04967813938856125\n",
            "Step 4630, Training Loss: 0.003719094442203641\n",
            "Step 4640, Training Loss: 0.011351034976541996\n",
            "Step 4640, Training Loss: 0.11439929157495499\n",
            "Step 4650, Training Loss: 0.005770646966993809\n",
            "Step 4650, Training Loss: 0.003995486069470644\n",
            "Step 4660, Training Loss: 0.03532867506146431\n",
            "Step 4660, Training Loss: 0.1260746568441391\n",
            "Step 4670, Training Loss: 0.010037588886916637\n",
            "Step 4670, Training Loss: 0.035328224301338196\n",
            "Step 4680, Training Loss: 0.006425454281270504\n",
            "Step 4680, Training Loss: 0.011747922748327255\n",
            "Step 4690, Training Loss: 0.052197884768247604\n",
            "Step 4690, Training Loss: 0.0037855948321521282\n",
            "Step 4700, Training Loss: 0.12863057851791382\n",
            "Step 4700, Training Loss: 0.05937417224049568\n",
            "Step 4710, Training Loss: 0.058753978461027145\n",
            "Step 4710, Training Loss: 0.005268127657473087\n",
            "Step 4720, Training Loss: 0.04296507313847542\n",
            "Step 4720, Training Loss: 0.09973879903554916\n",
            "Step 4730, Training Loss: 0.01018407940864563\n",
            "Step 4730, Training Loss: 0.006235841196030378\n",
            "Step 4740, Training Loss: 0.005834956653416157\n",
            "Step 4740, Training Loss: 0.010405240580439568\n",
            "Step 4750, Training Loss: 0.0048262327909469604\n",
            "Step 4750, Training Loss: 0.038254912942647934\n",
            "Step 4760, Training Loss: 0.0042816572822630405\n",
            "Step 4760, Training Loss: 0.004744263365864754\n",
            "Step 4770, Training Loss: 0.005775070749223232\n",
            "Step 4770, Training Loss: 0.08813972026109695\n",
            "Step 4780, Training Loss: 0.012025744654238224\n",
            "Step 4780, Training Loss: 0.006679831072688103\n",
            "Step 4790, Training Loss: 0.008007611148059368\n",
            "Step 4790, Training Loss: 0.2212918996810913\n",
            "Step 4800, Training Loss: 0.004442261997610331\n",
            "Step 4800, Training Loss: 0.003972390666604042\n",
            "Step 4810, Training Loss: 0.05004492774605751\n",
            "Step 4810, Training Loss: 0.006474535446614027\n",
            "Step 4820, Training Loss: 0.12910857796669006\n",
            "Step 4820, Training Loss: 0.11590124666690826\n",
            "Step 4830, Training Loss: 0.009072314947843552\n",
            "Step 4830, Training Loss: 0.008723549544811249\n",
            "Step 4840, Training Loss: 0.07213102281093597\n",
            "Step 4840, Training Loss: 0.00554475374519825\n",
            "Step 4850, Training Loss: 0.0052468543872237206\n",
            "Step 4850, Training Loss: 0.2787320017814636\n",
            "Step 4860, Training Loss: 0.17868603765964508\n",
            "Step 4860, Training Loss: 0.006212509237229824\n",
            "Step 4870, Training Loss: 0.009904161095619202\n",
            "Step 4870, Training Loss: 0.0061040823347866535\n",
            "Step 4880, Training Loss: 0.03593387082219124\n",
            "Step 4880, Training Loss: 0.013517003506422043\n",
            "Step 4890, Training Loss: 0.08121983706951141\n",
            "Step 4890, Training Loss: 0.16609154641628265\n",
            "Step 4900, Training Loss: 0.008597249165177345\n",
            "Step 4900, Training Loss: 0.007533254101872444\n",
            "Step 4910, Training Loss: 0.009484793059527874\n",
            "Step 4910, Training Loss: 0.008171666413545609\n",
            "Step 4920, Training Loss: 0.003881023498252034\n",
            "Step 4920, Training Loss: 0.011602685786783695\n",
            "Step 4930, Training Loss: 0.006670629605650902\n",
            "Step 4930, Training Loss: 0.03888987749814987\n",
            "Step 4940, Training Loss: 0.08909005671739578\n",
            "Step 4940, Training Loss: 0.009876176714897156\n",
            "Step 4950, Training Loss: 0.010570939630270004\n",
            "Step 4950, Training Loss: 0.0058684032410383224\n",
            "Step 4960, Training Loss: 0.038943249732255936\n",
            "Step 4960, Training Loss: 0.1830815225839615\n",
            "Step 4970, Training Loss: 0.003968183882534504\n",
            "Step 4970, Training Loss: 0.007300928235054016\n",
            "Step 4980, Training Loss: 0.012228261679410934\n",
            "Step 4980, Training Loss: 0.15451525151729584\n",
            "Step 4990, Training Loss: 0.05748072266578674\n",
            "Step 4990, Training Loss: 0.0052403612062335014\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Saving model checkpoint to ./results/checkpoint-5000\n",
            "Configuration saved in ./results/checkpoint-5000/config.json\n",
            "Model weights saved in ./results/checkpoint-5000/pytorch_model.bin\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 5000, Training Loss: 0.006390095222741365\n",
            "Step 5000, Training Loss: 0.004476936999708414\n",
            "Step 5010, Training Loss: 0.0067457593977451324\n",
            "Step 5010, Training Loss: 0.037104617804288864\n",
            "Step 5020, Training Loss: 0.004501793067902327\n",
            "Step 5020, Training Loss: 0.018523775041103363\n",
            "Step 5030, Training Loss: 0.03677595779299736\n",
            "Step 5030, Training Loss: 0.008276771754026413\n",
            "Step 5040, Training Loss: 0.08876015245914459\n",
            "Step 5040, Training Loss: 0.003897786373272538\n",
            "Step 5050, Training Loss: 0.00621046731248498\n",
            "Step 5050, Training Loss: 0.005740534048527479\n",
            "Step 5060, Training Loss: 0.07968245446681976\n",
            "Step 5060, Training Loss: 0.339211642742157\n",
            "Step 5070, Training Loss: 0.04478708654642105\n",
            "Step 5070, Training Loss: 0.00453098863363266\n",
            "Step 5080, Training Loss: 0.007773193065077066\n",
            "Step 5080, Training Loss: 0.06959690898656845\n",
            "Step 5090, Training Loss: 0.006640644744038582\n",
            "Step 5090, Training Loss: 0.006596543826162815\n",
            "Step 5100, Training Loss: 0.00789149571210146\n",
            "Step 5100, Training Loss: 0.008651870302855968\n",
            "Step 5110, Training Loss: 0.004722265060991049\n",
            "Step 5110, Training Loss: 0.007074159570038319\n",
            "Step 5120, Training Loss: 0.19207528233528137\n",
            "Step 5120, Training Loss: 0.04062601551413536\n",
            "Step 5130, Training Loss: 0.007078955415636301\n",
            "Step 5130, Training Loss: 0.006225393619388342\n",
            "Step 5140, Training Loss: 0.0037730925250798464\n",
            "Step 5140, Training Loss: 0.2758239507675171\n",
            "Step 5150, Training Loss: 0.00764126144349575\n",
            "Step 5150, Training Loss: 0.0062180194072425365\n",
            "Step 5160, Training Loss: 0.006275900639593601\n",
            "Step 5160, Training Loss: 0.0049625965766608715\n",
            "Step 5170, Training Loss: 0.04505573958158493\n",
            "Step 5170, Training Loss: 0.011844818480312824\n",
            "Step 5180, Training Loss: 0.007012015208601952\n",
            "Step 5180, Training Loss: 0.11057959496974945\n",
            "Step 5190, Training Loss: 0.010732470080256462\n",
            "Step 5190, Training Loss: 0.0052479407750070095\n",
            "Step 5200, Training Loss: 0.01549556478857994\n",
            "Step 5200, Training Loss: 0.09637464582920074\n",
            "Step 5210, Training Loss: 0.09496007859706879\n",
            "Step 5210, Training Loss: 0.06300513446331024\n",
            "Step 5220, Training Loss: 0.00608493946492672\n",
            "Step 5220, Training Loss: 0.07093729823827744\n",
            "Step 5230, Training Loss: 0.0044047897681593895\n",
            "Step 5230, Training Loss: 0.17452219128608704\n",
            "Step 5240, Training Loss: 0.006905933376401663\n",
            "Step 5240, Training Loss: 0.004458548966795206\n",
            "Step 5250, Training Loss: 0.046058084815740585\n",
            "Step 5250, Training Loss: 0.007045750040560961\n",
            "Step 5260, Training Loss: 0.006909013260155916\n",
            "Step 5260, Training Loss: 0.07843153923749924\n",
            "Step 5270, Training Loss: 0.003539425553753972\n",
            "Step 5270, Training Loss: 0.0047261640429496765\n",
            "Step 5280, Training Loss: 0.005964565556496382\n",
            "Step 5280, Training Loss: 0.006409871857613325\n",
            "Step 5290, Training Loss: 0.004856695421040058\n",
            "Step 5290, Training Loss: 0.05518263578414917\n",
            "Step 5300, Training Loss: 0.005852038040757179\n",
            "Step 5300, Training Loss: 0.03695486858487129\n",
            "Step 5310, Training Loss: 0.006167619023472071\n",
            "Step 5310, Training Loss: 0.005612482316792011\n",
            "Step 5320, Training Loss: 0.09128757566213608\n",
            "Step 5320, Training Loss: 0.005301081575453281\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 1523\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 5330, Training Loss: 0.005128434859216213\n",
            "Step 5330, Training Loss: 0.009818409569561481\n",
            "Step 5340, Training Loss: 0.06473197042942047\n",
            "Step 5340, Training Loss: 0.005330035928636789\n",
            "Step 5350, Training Loss: 0.005917112808674574\n",
            "Step 5350, Training Loss: 0.0049980152398347855\n",
            "Step 5360, Training Loss: 0.005215668585151434\n",
            "Step 5360, Training Loss: 0.06564637273550034\n",
            "Step 5370, Training Loss: 0.004377712961286306\n",
            "Step 5370, Training Loss: 0.3736609220504761\n",
            "Step 5380, Training Loss: 0.00354743842035532\n",
            "Step 5380, Training Loss: 0.06448984146118164\n",
            "Step 5390, Training Loss: 0.05021296441555023\n",
            "Step 5390, Training Loss: 0.005242730025202036\n",
            "Step 5400, Training Loss: 0.0073369950987398624\n",
            "Step 5400, Training Loss: 0.11107894033193588\n",
            "Step 5410, Training Loss: 0.009548178873956203\n",
            "Step 5410, Training Loss: 0.06806616485118866\n",
            "Step 5420, Training Loss: 0.04413288086652756\n",
            "Step 5420, Training Loss: 0.0058920374140143394\n",
            "Step 5430, Training Loss: 0.049901269376277924\n",
            "Step 5430, Training Loss: 0.007783430628478527\n",
            "Step 5440, Training Loss: 0.014570147730410099\n",
            "Step 5440, Training Loss: 0.10161403566598892\n",
            "Step 5450, Training Loss: 0.0045782653614878654\n",
            "Step 5450, Training Loss: 0.05065693333745003\n",
            "Step 5460, Training Loss: 0.07979331910610199\n",
            "Step 5460, Training Loss: 0.0867246687412262\n",
            "Step 5470, Training Loss: 0.011295227333903313\n",
            "Step 5470, Training Loss: 0.08761567622423172\n",
            "Step 5480, Training Loss: 0.03854430466890335\n",
            "Step 5480, Training Loss: 0.010048255324363708\n",
            "Step 5490, Training Loss: 0.07526811957359314\n",
            "Step 5490, Training Loss: 0.04155817627906799\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Saving model checkpoint to ./results/checkpoint-5500\n",
            "Configuration saved in ./results/checkpoint-5500/config.json\n",
            "Model weights saved in ./results/checkpoint-5500/pytorch_model.bin\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 5500, Training Loss: 0.0758899450302124\n",
            "Step 5500, Training Loss: 0.04453970119357109\n",
            "Step 5510, Training Loss: 0.00793997012078762\n",
            "Step 5510, Training Loss: 0.011217085644602776\n",
            "Step 5520, Training Loss: 0.004976463504135609\n",
            "Step 5520, Training Loss: 0.012234736233949661\n",
            "Step 5530, Training Loss: 0.013350413180887699\n",
            "Step 5530, Training Loss: 0.005936886183917522\n",
            "Step 5540, Training Loss: 0.03629719465970993\n",
            "Step 5540, Training Loss: 0.004438699688762426\n",
            "Step 5550, Training Loss: 0.007043749094009399\n",
            "Step 5550, Training Loss: 0.005005912389606237\n",
            "Step 5560, Training Loss: 0.008027488365769386\n",
            "Step 5560, Training Loss: 0.005161583423614502\n",
            "Step 5570, Training Loss: 0.05439477413892746\n",
            "Step 5570, Training Loss: 0.07200214266777039\n",
            "Step 5580, Training Loss: 0.03461410105228424\n",
            "Step 5580, Training Loss: 0.003970326855778694\n",
            "Step 5590, Training Loss: 0.3133680522441864\n",
            "Step 5590, Training Loss: 0.006340111140161753\n",
            "Step 5600, Training Loss: 0.012110363692045212\n",
            "Step 5600, Training Loss: 0.35191479325294495\n",
            "Step 5610, Training Loss: 0.039465054869651794\n",
            "Step 5610, Training Loss: 0.008440511301159859\n",
            "Step 5620, Training Loss: 0.27464061975479126\n",
            "Step 5620, Training Loss: 0.007665715180337429\n",
            "Step 5630, Training Loss: 0.005246188957244158\n",
            "Step 5630, Training Loss: 0.031929489225149155\n",
            "Step 5640, Training Loss: 0.06412412971258163\n",
            "Step 5640, Training Loss: 0.04601072520017624\n",
            "Step 5650, Training Loss: 0.006900782231241465\n",
            "Step 5650, Training Loss: 0.010201205499470234\n",
            "Step 5660, Training Loss: 0.005802773404866457\n",
            "Step 5660, Training Loss: 0.06328801065683365\n",
            "Step 5670, Training Loss: 0.009779601357877254\n",
            "Step 5670, Training Loss: 0.33225107192993164\n",
            "Step 5680, Training Loss: 0.13433246314525604\n",
            "Step 5680, Training Loss: 0.30645930767059326\n",
            "Step 5690, Training Loss: 0.02757658250629902\n",
            "Step 5690, Training Loss: 0.006456366740167141\n",
            "Step 5700, Training Loss: 0.00506064435467124\n",
            "Step 5700, Training Loss: 0.004319682717323303\n",
            "Step 5710, Training Loss: 0.057464852929115295\n",
            "Step 5710, Training Loss: 0.01361649576574564\n",
            "Step 5720, Training Loss: 0.31215915083885193\n",
            "Step 5720, Training Loss: 0.06181199103593826\n",
            "Step 5730, Training Loss: 0.034441083669662476\n",
            "Step 5730, Training Loss: 0.04348388686776161\n",
            "Step 5740, Training Loss: 0.010209335014224052\n",
            "Step 5740, Training Loss: 0.033762168139219284\n",
            "Step 5750, Training Loss: 0.1329742670059204\n",
            "Step 5750, Training Loss: 0.05732382833957672\n",
            "Step 5760, Training Loss: 0.005333134438842535\n",
            "Step 5760, Training Loss: 0.006403056439012289\n",
            "Step 5770, Training Loss: 0.046375926584005356\n",
            "Step 5770, Training Loss: 0.006980732083320618\n",
            "Step 5780, Training Loss: 0.1331603229045868\n",
            "Step 5780, Training Loss: 0.013873589225113392\n",
            "Step 5790, Training Loss: 0.042152684181928635\n",
            "Step 5790, Training Loss: 0.010295538231730461\n",
            "Step 5800, Training Loss: 0.004595756530761719\n",
            "Step 5800, Training Loss: 0.0065406146459281445\n",
            "Step 5810, Training Loss: 0.0380493588745594\n",
            "Step 5810, Training Loss: 0.010683555155992508\n",
            "Step 5820, Training Loss: 0.07828661799430847\n",
            "Step 5820, Training Loss: 0.006504436954855919\n",
            "Step 5830, Training Loss: 0.006032087374478579\n",
            "Step 5830, Training Loss: 0.04632245749235153\n",
            "Step 5840, Training Loss: 0.0054779513739049435\n",
            "Step 5840, Training Loss: 0.19733378291130066\n",
            "Step 5850, Training Loss: 0.0052771237678825855\n",
            "Step 5850, Training Loss: 0.005986700300127268\n",
            "Step 5860, Training Loss: 0.00895693339407444\n",
            "Step 5860, Training Loss: 0.00643071997910738\n",
            "Step 5870, Training Loss: 0.005563079845160246\n",
            "Step 5870, Training Loss: 0.08291804790496826\n",
            "Step 5880, Training Loss: 0.006500331684947014\n",
            "Step 5880, Training Loss: 0.0776582881808281\n",
            "Step 5890, Training Loss: 0.013898158445954323\n",
            "Step 5890, Training Loss: 0.03718837723135948\n",
            "Step 5900, Training Loss: 0.045226261019706726\n",
            "Step 5900, Training Loss: 0.034810055047273636\n",
            "Step 5910, Training Loss: 0.040181394666433334\n",
            "Step 5910, Training Loss: 0.031149622052907944\n",
            "Step 5920, Training Loss: 0.00820435956120491\n",
            "Step 5920, Training Loss: 0.005253429990261793\n",
            "Step 5930, Training Loss: 0.04970972612500191\n",
            "Step 5930, Training Loss: 0.005351012572646141\n",
            "Step 5940, Training Loss: 0.004720880649983883\n",
            "Step 5940, Training Loss: 0.006112326867878437\n",
            "Step 5950, Training Loss: 0.0727219432592392\n",
            "Step 5950, Training Loss: 0.0552792064845562\n",
            "Step 5960, Training Loss: 0.008144639432430267\n",
            "Step 5960, Training Loss: 0.06986561417579651\n",
            "Step 5970, Training Loss: 0.03105485625565052\n",
            "Step 5970, Training Loss: 0.005183895584195852\n",
            "Step 5980, Training Loss: 0.005283921025693417\n",
            "Step 5980, Training Loss: 0.003964104689657688\n",
            "Step 5990, Training Loss: 0.006413737777620554\n",
            "Step 5990, Training Loss: 0.10367259383201599\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Saving model checkpoint to ./results/checkpoint-6000\n",
            "Configuration saved in ./results/checkpoint-6000/config.json\n",
            "Model weights saved in ./results/checkpoint-6000/pytorch_model.bin\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 6000, Training Loss: 0.003815398784354329\n",
            "Step 6000, Training Loss: 0.12714771926403046\n",
            "Step 6010, Training Loss: 0.09412362426519394\n",
            "Step 6010, Training Loss: 0.012207621708512306\n",
            "Step 6020, Training Loss: 0.006172923371195793\n",
            "Step 6020, Training Loss: 0.2530924379825592\n",
            "Step 6030, Training Loss: 0.0482216514647007\n",
            "Step 6030, Training Loss: 0.0036958553828299046\n",
            "Step 6040, Training Loss: 0.0066743572242558\n",
            "Step 6040, Training Loss: 0.004356594756245613\n",
            "Step 6050, Training Loss: 0.36246076226234436\n",
            "Step 6050, Training Loss: 0.04584076628088951\n",
            "Step 6060, Training Loss: 0.043115537613630295\n",
            "Step 6060, Training Loss: 0.007363588083535433\n",
            "Step 6070, Training Loss: 0.06319441646337509\n",
            "Step 6070, Training Loss: 0.004460647702217102\n",
            "Step 6080, Training Loss: 0.03839047625660896\n",
            "Step 6080, Training Loss: 0.004367541521787643\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 1523\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 6090, Training Loss: 0.003917172085493803\n",
            "Step 6090, Training Loss: 0.003671640995889902\n",
            "Step 6100, Training Loss: 0.1041327640414238\n",
            "Step 6100, Training Loss: 0.05713394284248352\n",
            "Step 6110, Training Loss: 0.06702640652656555\n",
            "Step 6110, Training Loss: 0.00626996299251914\n",
            "Step 6120, Training Loss: 0.00473323930054903\n",
            "Step 6120, Training Loss: 0.021793324500322342\n",
            "Step 6130, Training Loss: 0.03284666687250137\n",
            "Step 6130, Training Loss: 0.07754385471343994\n",
            "Step 6140, Training Loss: 0.10193987190723419\n",
            "Step 6140, Training Loss: 0.03560228645801544\n",
            "Step 6150, Training Loss: 0.004434471484273672\n",
            "Step 6150, Training Loss: 0.005915799643844366\n",
            "Step 6160, Training Loss: 0.05805054306983948\n",
            "Step 6160, Training Loss: 0.0070172268897295\n",
            "Step 6170, Training Loss: 0.00687031727284193\n",
            "Step 6170, Training Loss: 0.01118866540491581\n",
            "Step 6180, Training Loss: 0.007383843418210745\n",
            "Step 6180, Training Loss: 0.06127467006444931\n",
            "Step 6190, Training Loss: 0.006237146444618702\n",
            "Step 6190, Training Loss: 0.04244942590594292\n",
            "Step 6200, Training Loss: 0.04075717180967331\n",
            "Step 6200, Training Loss: 0.5588955283164978\n",
            "Step 6210, Training Loss: 0.03878152370452881\n",
            "Step 6210, Training Loss: 0.00467510474845767\n",
            "Step 6220, Training Loss: 0.3281562328338623\n",
            "Step 6220, Training Loss: 0.05149644985795021\n",
            "Step 6230, Training Loss: 0.1676325649023056\n",
            "Step 6230, Training Loss: 0.03759218379855156\n",
            "Step 6240, Training Loss: 0.005496434401720762\n",
            "Step 6240, Training Loss: 0.010990430600941181\n",
            "Step 6250, Training Loss: 0.00765813235193491\n",
            "Step 6250, Training Loss: 0.005627540871500969\n",
            "Step 6260, Training Loss: 0.0734184980392456\n",
            "Step 6260, Training Loss: 0.010639959014952183\n",
            "Step 6270, Training Loss: 0.043907687067985535\n",
            "Step 6270, Training Loss: 0.007405384443700314\n",
            "Step 6280, Training Loss: 0.005253793206065893\n",
            "Step 6280, Training Loss: 0.12060785293579102\n",
            "Step 6290, Training Loss: 0.005076329689472914\n",
            "Step 6290, Training Loss: 0.029762690886855125\n",
            "Step 6300, Training Loss: 0.010129413567483425\n",
            "Step 6300, Training Loss: 0.008187617175281048\n",
            "Step 6310, Training Loss: 0.08786768466234207\n",
            "Step 6310, Training Loss: 0.013428401201963425\n",
            "Step 6320, Training Loss: 0.0655972883105278\n",
            "Step 6320, Training Loss: 0.00428564241155982\n",
            "Step 6330, Training Loss: 0.004403295926749706\n",
            "Step 6330, Training Loss: 0.009490004740655422\n",
            "Step 6340, Training Loss: 0.008903330191969872\n",
            "Step 6340, Training Loss: 0.005560467950999737\n",
            "Step 6350, Training Loss: 0.004181574564427137\n",
            "Step 6350, Training Loss: 0.011944311670958996\n",
            "Step 6360, Training Loss: 0.006928500719368458\n",
            "Step 6360, Training Loss: 0.06750054657459259\n",
            "Step 6370, Training Loss: 0.011416123248636723\n",
            "Step 6370, Training Loss: 0.25307831168174744\n",
            "Step 6380, Training Loss: 0.004323075991123915\n",
            "Step 6380, Training Loss: 0.0053627947345376015\n",
            "Step 6390, Training Loss: 0.04053347185254097\n",
            "Step 6390, Training Loss: 0.040627967566251755\n",
            "Step 6400, Training Loss: 0.003701320616528392\n",
            "Step 6400, Training Loss: 0.05931051820516586\n",
            "Step 6410, Training Loss: 0.033564936369657516\n",
            "Step 6410, Training Loss: 0.006385632324963808\n",
            "Step 6420, Training Loss: 0.07513768970966339\n",
            "Step 6420, Training Loss: 0.04634317010641098\n",
            "Step 6430, Training Loss: 0.053682658821344376\n",
            "Step 6430, Training Loss: 0.05320078879594803\n",
            "Step 6440, Training Loss: 0.04762484133243561\n",
            "Step 6440, Training Loss: 0.04180331155657768\n",
            "Step 6450, Training Loss: 0.09269116818904877\n",
            "Step 6450, Training Loss: 0.006059662904590368\n",
            "Step 6460, Training Loss: 0.005015847273170948\n",
            "Step 6460, Training Loss: 0.34060782194137573\n",
            "Step 6470, Training Loss: 0.00676753930747509\n",
            "Step 6470, Training Loss: 0.07121080905199051\n",
            "Step 6480, Training Loss: 0.015111159533262253\n",
            "Step 6480, Training Loss: 0.020915096625685692\n",
            "Step 6490, Training Loss: 0.009007999673485756\n",
            "Step 6490, Training Loss: 0.004150504246354103\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Saving model checkpoint to ./results/checkpoint-6500\n",
            "Configuration saved in ./results/checkpoint-6500/config.json\n",
            "Model weights saved in ./results/checkpoint-6500/pytorch_model.bin\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 6500, Training Loss: 0.03607269749045372\n",
            "Step 6500, Training Loss: 0.044654443860054016\n",
            "Step 6510, Training Loss: 0.010605982504785061\n",
            "Step 6510, Training Loss: 0.028140120208263397\n",
            "Step 6520, Training Loss: 0.0710034891963005\n",
            "Step 6520, Training Loss: 0.09941890090703964\n",
            "Step 6530, Training Loss: 0.04259318858385086\n",
            "Step 6530, Training Loss: 0.16220490634441376\n",
            "Step 6540, Training Loss: 0.0942029058933258\n",
            "Step 6540, Training Loss: 0.14353488385677338\n",
            "Step 6550, Training Loss: 0.005368491634726524\n",
            "Step 6550, Training Loss: 0.006311857607215643\n",
            "Step 6560, Training Loss: 0.04076017066836357\n",
            "Step 6560, Training Loss: 0.05426165461540222\n",
            "Step 6570, Training Loss: 0.006998227443546057\n",
            "Step 6570, Training Loss: 0.006117966491729021\n",
            "Step 6580, Training Loss: 0.18301163613796234\n",
            "Step 6580, Training Loss: 0.07515745609998703\n",
            "Step 6590, Training Loss: 0.005031144246459007\n",
            "Step 6590, Training Loss: 0.04846793785691261\n",
            "Step 6600, Training Loss: 0.05626027658581734\n",
            "Step 6600, Training Loss: 0.031897228211164474\n",
            "Step 6610, Training Loss: 0.31624075770378113\n",
            "Step 6610, Training Loss: 0.005667251534759998\n",
            "Step 6620, Training Loss: 0.00645124725997448\n",
            "Step 6620, Training Loss: 0.007937830872833729\n",
            "Step 6630, Training Loss: 0.005330703221261501\n",
            "Step 6630, Training Loss: 0.005048797931522131\n",
            "Step 6640, Training Loss: 0.006602072156965733\n",
            "Step 6640, Training Loss: 0.04057454317808151\n",
            "Step 6650, Training Loss: 0.0067832376807928085\n",
            "Step 6650, Training Loss: 0.06778435409069061\n",
            "Step 6660, Training Loss: 0.006380317732691765\n",
            "Step 6660, Training Loss: 0.03223235160112381\n",
            "Step 6670, Training Loss: 0.31354305148124695\n",
            "Step 6670, Training Loss: 0.005388807039707899\n",
            "Step 6680, Training Loss: 0.058868441730737686\n",
            "Step 6680, Training Loss: 0.0468592494726181\n",
            "Step 6690, Training Loss: 0.009114458225667477\n",
            "Step 6690, Training Loss: 0.10408897697925568\n",
            "Step 6700, Training Loss: 0.010987977497279644\n",
            "Step 6700, Training Loss: 0.026753434911370277\n",
            "Step 6710, Training Loss: 0.028280409052968025\n",
            "Step 6710, Training Loss: 0.009613421745598316\n",
            "Step 6720, Training Loss: 0.01206508744508028\n",
            "Step 6720, Training Loss: 0.007330876309424639\n",
            "Step 6730, Training Loss: 0.07880917936563492\n",
            "Step 6730, Training Loss: 0.007298591546714306\n",
            "Step 6740, Training Loss: 0.007659134455025196\n",
            "Step 6740, Training Loss: 0.02001696079969406\n",
            "Step 6750, Training Loss: 0.07396208494901657\n",
            "Step 6750, Training Loss: 0.0889369323849678\n",
            "Step 6760, Training Loss: 0.08228860795497894\n",
            "Step 6760, Training Loss: 0.041394930332899094\n",
            "Step 6770, Training Loss: 0.004758937284350395\n",
            "Step 6770, Training Loss: 0.0813530683517456\n",
            "Step 6780, Training Loss: 0.011007009074091911\n",
            "Step 6780, Training Loss: 0.01188808772712946\n",
            "Step 6790, Training Loss: 0.01031600870192051\n",
            "Step 6790, Training Loss: 0.035179782658815384\n",
            "Step 6800, Training Loss: 0.05000592768192291\n",
            "Step 6800, Training Loss: 0.005215066485106945\n",
            "Step 6810, Training Loss: 0.03775554895401001\n",
            "Step 6810, Training Loss: 0.004925449378788471\n",
            "Step 6820, Training Loss: 0.08780208975076675\n",
            "Step 6820, Training Loss: 0.011772013269364834\n",
            "Step 6830, Training Loss: 0.04201176390051842\n",
            "Step 6830, Training Loss: 0.012455706484615803\n",
            "Step 6840, Training Loss: 0.2415565401315689\n",
            "Step 6840, Training Loss: 0.006459773052483797\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 1523\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 6850, Training Loss: 0.05224569886922836\n",
            "Step 6850, Training Loss: 0.05841170996427536\n",
            "Step 6860, Training Loss: 0.00392193254083395\n",
            "Step 6860, Training Loss: 0.007253647316247225\n",
            "Step 6870, Training Loss: 0.03672685846686363\n",
            "Step 6870, Training Loss: 0.005749153438955545\n",
            "Step 6880, Training Loss: 0.004938813392072916\n",
            "Step 6880, Training Loss: 0.0038310345262289047\n",
            "Step 6890, Training Loss: 0.010826810263097286\n",
            "Step 6890, Training Loss: 0.009939518757164478\n",
            "Step 6900, Training Loss: 0.005283494479954243\n",
            "Step 6900, Training Loss: 0.19568921625614166\n",
            "Step 6910, Training Loss: 0.007047024089843035\n",
            "Step 6910, Training Loss: 0.005231847986578941\n",
            "Step 6920, Training Loss: 0.005711241625249386\n",
            "Step 6920, Training Loss: 0.13745716214179993\n",
            "Step 6930, Training Loss: 0.12486163526773453\n",
            "Step 6930, Training Loss: 0.004619236569851637\n",
            "Step 6940, Training Loss: 0.10992881655693054\n",
            "Step 6940, Training Loss: 0.04195573553442955\n",
            "Step 6950, Training Loss: 0.004729295149445534\n",
            "Step 6950, Training Loss: 0.007932664826512337\n",
            "Step 6960, Training Loss: 0.037688177078962326\n",
            "Step 6960, Training Loss: 0.007274080067873001\n",
            "Step 6970, Training Loss: 0.005263908300548792\n",
            "Step 6970, Training Loss: 0.041141774505376816\n",
            "Step 6980, Training Loss: 0.010339532978832722\n",
            "Step 6980, Training Loss: 0.08833853900432587\n",
            "Step 6990, Training Loss: 0.037796225398778915\n",
            "Step 6990, Training Loss: 0.0042765261605381966\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Saving model checkpoint to ./results/checkpoint-7000\n",
            "Configuration saved in ./results/checkpoint-7000/config.json\n",
            "Model weights saved in ./results/checkpoint-7000/pytorch_model.bin\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 7000, Training Loss: 0.03785553202033043\n",
            "Step 7000, Training Loss: 0.037286318838596344\n",
            "Step 7010, Training Loss: 0.005022258963435888\n",
            "Step 7010, Training Loss: 0.15845882892608643\n",
            "Step 7020, Training Loss: 0.07803449779748917\n",
            "Step 7020, Training Loss: 0.054771073162555695\n",
            "Step 7030, Training Loss: 0.05076732859015465\n",
            "Step 7030, Training Loss: 0.03858893737196922\n",
            "Step 7040, Training Loss: 0.0710391029715538\n",
            "Step 7040, Training Loss: 0.17296525835990906\n",
            "Step 7050, Training Loss: 0.04333948716521263\n",
            "Step 7050, Training Loss: 0.16321513056755066\n",
            "Step 7060, Training Loss: 0.09755092114210129\n",
            "Step 7060, Training Loss: 0.0053895446471869946\n",
            "Step 7070, Training Loss: 0.054826680570840836\n",
            "Step 7070, Training Loss: 0.06839912384748459\n",
            "Step 7080, Training Loss: 0.00577488774433732\n",
            "Step 7080, Training Loss: 0.05514748394489288\n",
            "Step 7090, Training Loss: 0.005106853786855936\n",
            "Step 7090, Training Loss: 0.04022412374615669\n",
            "Step 7100, Training Loss: 0.006993024609982967\n",
            "Step 7100, Training Loss: 0.2478410303592682\n",
            "Step 7110, Training Loss: 0.09985251724720001\n",
            "Step 7110, Training Loss: 0.05032536759972572\n",
            "Step 7120, Training Loss: 0.005315833725035191\n",
            "Step 7120, Training Loss: 0.03987440466880798\n",
            "Step 7130, Training Loss: 0.005484191235154867\n",
            "Step 7130, Training Loss: 0.3445214629173279\n",
            "Step 7140, Training Loss: 0.041701775044202805\n",
            "Step 7140, Training Loss: 0.03743134066462517\n",
            "Step 7150, Training Loss: 0.011487242765724659\n",
            "Step 7150, Training Loss: 0.005195925943553448\n",
            "Step 7160, Training Loss: 0.005944474600255489\n",
            "Step 7160, Training Loss: 0.010558308102190495\n",
            "Step 7170, Training Loss: 0.004013597033917904\n",
            "Step 7170, Training Loss: 0.004634689074009657\n",
            "Step 7180, Training Loss: 0.0076255290769040585\n",
            "Step 7180, Training Loss: 0.005955939646810293\n",
            "Step 7190, Training Loss: 0.09105554968118668\n",
            "Step 7190, Training Loss: 0.0077824373729527\n",
            "Step 7200, Training Loss: 0.0356537364423275\n",
            "Step 7200, Training Loss: 0.08353453129529953\n",
            "Step 7210, Training Loss: 0.11035455018281937\n",
            "Step 7210, Training Loss: 0.010997706092894077\n",
            "Step 7220, Training Loss: 0.005080982577055693\n",
            "Step 7220, Training Loss: 0.012384268455207348\n",
            "Step 7230, Training Loss: 0.014117283746600151\n",
            "Step 7230, Training Loss: 0.04162168502807617\n",
            "Step 7240, Training Loss: 0.11275041848421097\n",
            "Step 7240, Training Loss: 0.004733401816338301\n",
            "Step 7250, Training Loss: 0.042769744992256165\n",
            "Step 7250, Training Loss: 0.05868937447667122\n",
            "Step 7260, Training Loss: 0.0058491285890340805\n",
            "Step 7260, Training Loss: 0.059902261942625046\n",
            "Step 7270, Training Loss: 0.004930821247398853\n",
            "Step 7270, Training Loss: 0.008979123085737228\n",
            "Step 7280, Training Loss: 0.006756712682545185\n",
            "Step 7280, Training Loss: 0.007577179465442896\n",
            "Step 7290, Training Loss: 0.0893491581082344\n",
            "Step 7290, Training Loss: 0.059667639434337616\n",
            "Step 7300, Training Loss: 0.23443405330181122\n",
            "Step 7300, Training Loss: 0.005150175653398037\n",
            "Step 7310, Training Loss: 0.0072110723704099655\n",
            "Step 7310, Training Loss: 0.21435770392417908\n",
            "Step 7320, Training Loss: 0.008590882644057274\n",
            "Step 7320, Training Loss: 0.007326096761971712\n",
            "Step 7330, Training Loss: 0.008986607193946838\n",
            "Step 7330, Training Loss: 0.039063237607479095\n",
            "Step 7340, Training Loss: 0.06184865161776543\n",
            "Step 7340, Training Loss: 0.014230367727577686\n",
            "Step 7350, Training Loss: 0.013704263605177402\n",
            "Step 7350, Training Loss: 0.005901264492422342\n",
            "Step 7360, Training Loss: 0.005372198764234781\n",
            "Step 7360, Training Loss: 0.059416383504867554\n",
            "Step 7370, Training Loss: 0.07197562605142593\n",
            "Step 7370, Training Loss: 0.05715765058994293\n",
            "Step 7380, Training Loss: 0.04522410407662392\n",
            "Step 7380, Training Loss: 0.04592115059494972\n",
            "Step 7390, Training Loss: 0.00442008301615715\n",
            "Step 7390, Training Loss: 0.03960435464978218\n",
            "Step 7400, Training Loss: 0.0053955381736159325\n",
            "Step 7400, Training Loss: 0.005462247412651777\n",
            "Step 7410, Training Loss: 0.07412062585353851\n",
            "Step 7410, Training Loss: 0.05930695682764053\n",
            "Step 7420, Training Loss: 0.009231865406036377\n",
            "Step 7420, Training Loss: 0.015561358071863651\n",
            "Step 7430, Training Loss: 0.07793083786964417\n",
            "Step 7430, Training Loss: 0.007055410649627447\n",
            "Step 7440, Training Loss: 0.012427517212927341\n",
            "Step 7440, Training Loss: 0.05401778593659401\n",
            "Step 7450, Training Loss: 0.005614912137389183\n",
            "Step 7450, Training Loss: 0.17918948829174042\n",
            "Step 7460, Training Loss: 0.008518984541296959\n",
            "Step 7460, Training Loss: 0.006373847834765911\n",
            "Step 7470, Training Loss: 0.03518972918391228\n",
            "Step 7470, Training Loss: 0.004314398393034935\n",
            "Step 7480, Training Loss: 0.03639347106218338\n",
            "Step 7480, Training Loss: 0.05607849732041359\n",
            "Step 7490, Training Loss: 0.005454984959214926\n",
            "Step 7490, Training Loss: 0.006302460562437773\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Saving model checkpoint to ./results/checkpoint-7500\n",
            "Configuration saved in ./results/checkpoint-7500/config.json\n",
            "Model weights saved in ./results/checkpoint-7500/pytorch_model.bin\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 7500, Training Loss: 0.01842380128800869\n",
            "Step 7500, Training Loss: 0.00400808360427618\n",
            "Step 7510, Training Loss: 0.05153132602572441\n",
            "Step 7510, Training Loss: 0.006417775992304087\n",
            "Step 7520, Training Loss: 0.1557471603155136\n",
            "Step 7520, Training Loss: 0.005998833104968071\n",
            "Step 7530, Training Loss: 0.034044403582811356\n",
            "Step 7530, Training Loss: 0.004940273705869913\n",
            "Step 7540, Training Loss: 0.01211834792047739\n",
            "Step 7540, Training Loss: 0.07373132556676865\n",
            "Step 7550, Training Loss: 0.012620986439287663\n",
            "Step 7550, Training Loss: 0.006370307877659798\n",
            "Step 7560, Training Loss: 0.09431444853544235\n",
            "Step 7560, Training Loss: 0.2771693468093872\n",
            "Step 7570, Training Loss: 0.00595357920974493\n",
            "Step 7570, Training Loss: 0.2635539472103119\n",
            "Step 7580, Training Loss: 0.0075760199688375\n",
            "Step 7580, Training Loss: 0.004359942860901356\n",
            "Step 7590, Training Loss: 0.04474973306059837\n",
            "Step 7590, Training Loss: 0.03941754996776581\n",
            "Step 7600, Training Loss: 0.004556312691420317\n",
            "Step 7600, Training Loss: 0.10882821679115295\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 1523\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 7610, Training Loss: 0.38420847058296204\n",
            "Step 7610, Training Loss: 0.37743550539016724\n",
            "Step 7610, Training Loss: 0.1720101535320282\n",
            "Step 7610, Training Loss: 0.02779061160981655\n",
            "Step 7610, Training Loss: 0.2523842453956604\n",
            "Step 7610, Training Loss: 0.38900935649871826\n",
            "Step 7610, Training Loss: 0.2863045632839203\n",
            "Step 7610, Training Loss: 0.0737265944480896\n",
            "Step 7610, Training Loss: 0.02636643871665001\n",
            "Step 7610, Training Loss: 0.04097771272063255\n",
            "Step 7610, Training Loss: 0.19253374636173248\n",
            "Step 7610, Training Loss: 0.08555585891008377\n",
            "Step 7610, Training Loss: 0.18729275465011597\n",
            "Step 7610, Training Loss: 0.19809669256210327\n",
            "Step 7610, Training Loss: 0.13559600710868835\n",
            "Step 7610, Training Loss: 0.1802290976047516\n",
            "Step 7610, Training Loss: 0.2979230284690857\n",
            "Step 7610, Training Loss: 0.10495457053184509\n",
            "Step 7610, Training Loss: 0.0772710070014\n",
            "Step 7610, Training Loss: 0.10195980966091156\n",
            "Step 7610, Training Loss: 0.2087683379650116\n",
            "Step 7610, Training Loss: 0.005655771587044001\n",
            "Step 7610, Training Loss: 0.29738277196884155\n",
            "Step 7610, Training Loss: 0.2899573743343353\n",
            "Step 7610, Training Loss: 0.0038563297130167484\n",
            "Step 7610, Training Loss: 0.2044796198606491\n",
            "Step 7610, Training Loss: 0.1726052463054657\n",
            "Step 7610, Training Loss: 0.022028561681509018\n",
            "Step 7610, Training Loss: 0.0069378092885017395\n",
            "Step 7610, Training Loss: 0.37600964307785034\n",
            "Step 7610, Training Loss: 0.006086050532758236\n",
            "Step 7610, Training Loss: 0.3956940770149231\n",
            "Step 7610, Training Loss: 0.013124671764671803\n",
            "Step 7610, Training Loss: 0.03244398906826973\n",
            "Step 7610, Training Loss: 0.0049929022789001465\n",
            "Step 7610, Training Loss: 0.28598150610923767\n",
            "Step 7610, Training Loss: 0.0987921878695488\n",
            "Step 7610, Training Loss: 0.022321296855807304\n",
            "Step 7610, Training Loss: 0.16090892255306244\n",
            "Step 7610, Training Loss: 0.3649238049983978\n",
            "Step 7610, Training Loss: 0.21525156497955322\n",
            "Step 7610, Training Loss: 0.005513234529644251\n",
            "Step 7610, Training Loss: 0.01915724202990532\n",
            "Step 7610, Training Loss: 0.006717090029269457\n",
            "Step 7610, Training Loss: 0.19223763048648834\n",
            "Step 7610, Training Loss: 0.06348685920238495\n",
            "Step 7610, Training Loss: 0.20266737043857574\n",
            "Step 7610, Training Loss: 0.3016323447227478\n",
            "Step 7610, Training Loss: 0.14854390919208527\n",
            "Step 7610, Training Loss: 0.2661772668361664\n",
            "Step 7610, Training Loss: 0.006126304157078266\n",
            "Step 7610, Training Loss: 0.14784416556358337\n",
            "Step 7610, Training Loss: 0.11952672153711319\n",
            "Step 7610, Training Loss: 0.012295391410589218\n",
            "Step 7610, Training Loss: 0.029476311057806015\n",
            "Step 7610, Training Loss: 0.02046997658908367\n",
            "Step 7610, Training Loss: 0.24235089123249054\n",
            "Step 7610, Training Loss: 0.0046091764234006405\n",
            "Step 7610, Training Loss: 0.05563018098473549\n",
            "Step 7610, Training Loss: 0.06746473908424377\n",
            "Step 7610, Training Loss: 0.023488778620958328\n",
            "Step 7610, Training Loss: 0.10113779455423355\n",
            "Step 7610, Training Loss: 0.24807924032211304\n",
            "Step 7610, Training Loss: 0.06278140842914581\n",
            "Step 7610, Training Loss: 0.3276265561580658\n",
            "Step 7610, Training Loss: 0.00794192310422659\n",
            "Step 7610, Training Loss: 0.1359192281961441\n",
            "Step 7610, Training Loss: 0.1799081414937973\n",
            "Step 7610, Training Loss: 0.006165957543998957\n",
            "Step 7610, Training Loss: 0.020963646471500397\n",
            "Step 7610, Training Loss: 0.024165907874703407\n",
            "Step 7610, Training Loss: 0.12563665211200714\n",
            "Step 7610, Training Loss: 0.3636336624622345\n",
            "Step 7610, Training Loss: 0.05744003504514694\n",
            "Step 7610, Training Loss: 0.30256980657577515\n",
            "Step 7610, Training Loss: 0.05367100611329079\n",
            "Step 7610, Training Loss: 0.02564331702888012\n",
            "Step 7610, Training Loss: 0.06998695433139801\n",
            "Step 7610, Training Loss: 0.1239733099937439\n",
            "Step 7610, Training Loss: 0.2423587143421173\n",
            "Step 7610, Training Loss: 0.24159452319145203\n",
            "Step 7610, Training Loss: 0.13419675827026367\n",
            "Step 7610, Training Loss: 0.292625367641449\n",
            "Step 7610, Training Loss: 0.23990710079669952\n",
            "Step 7610, Training Loss: 0.3097880482673645\n",
            "Step 7610, Training Loss: 0.30983462929725647\n",
            "Step 7610, Training Loss: 0.02100188471376896\n",
            "Step 7610, Training Loss: 0.22746969759464264\n",
            "Step 7610, Training Loss: 0.18168026208877563\n",
            "Step 7610, Training Loss: 0.024819208309054375\n",
            "Step 7610, Training Loss: 0.022695889696478844\n",
            "Step 7610, Training Loss: 0.2005484402179718\n",
            "Step 7610, Training Loss: 0.3110036849975586\n",
            "Step 7610, Training Loss: 0.022044701501727104\n",
            "Step 7610, Training Loss: 0.032995063811540604\n",
            "Step 7610, Training Loss: 0.18936516344547272\n",
            "Step 7610, Training Loss: 0.23826617002487183\n",
            "Step 7610, Training Loss: 0.14756354689598083\n",
            "Step 7610, Training Loss: 0.4449996054172516\n",
            "Step 7610, Training Loss: 0.23763436079025269\n",
            "Step 7610, Training Loss: 0.2333279550075531\n",
            "Step 7610, Training Loss: 0.2227235585451126\n",
            "Step 7610, Training Loss: 0.04501791298389435\n",
            "Step 7610, Training Loss: 0.48587465286254883\n",
            "Step 7610, Training Loss: 0.19921797513961792\n",
            "Step 7610, Training Loss: 0.022020060569047928\n",
            "Step 7610, Training Loss: 0.029203062877058983\n",
            "Step 7610, Training Loss: 0.08390673249959946\n",
            "Step 7610, Training Loss: 0.2004719227552414\n",
            "Step 7610, Training Loss: 0.1889662891626358\n",
            "Step 7610, Training Loss: 0.2774151861667633\n",
            "Step 7610, Training Loss: 0.03797660022974014\n",
            "Step 7610, Training Loss: 0.03963588550686836\n",
            "Step 7610, Training Loss: 0.3779104948043823\n",
            "Step 7610, Training Loss: 0.3406704068183899\n",
            "Step 7610, Training Loss: 0.024319691583514214\n",
            "Step 7610, Training Loss: 0.02635420300066471\n",
            "Step 7610, Training Loss: 0.13342365622520447\n",
            "Step 7610, Training Loss: 0.004195520654320717\n",
            "Step 7610, Training Loss: 0.027426335960626602\n",
            "Step 7610, Training Loss: 0.37127259373664856\n",
            "Step 7610, Training Loss: 0.3431728482246399\n",
            "Step 7610, Training Loss: 0.03656180575489998\n",
            "Step 7610, Training Loss: 0.19161783158779144\n",
            "Step 7610, Training Loss: 0.08348052948713303\n",
            "Step 7610, Training Loss: 0.15903587639331818\n",
            "Step 7610, Training Loss: 0.05604449659585953\n",
            "Step 7610, Training Loss: 0.2304372936487198\n",
            "Step 7610, Training Loss: 0.25227221846580505\n",
            "Step 7610, Training Loss: 0.1909574717283249\n",
            "Step 7610, Training Loss: 0.07869724929332733\n",
            "Step 7610, Training Loss: 0.18062002956867218\n",
            "Step 7610, Training Loss: 0.18882118165493011\n",
            "Step 7610, Training Loss: 0.186228945851326\n",
            "Step 7610, Training Loss: 0.0066123176366090775\n",
            "Step 7610, Training Loss: 0.11014451086521149\n",
            "Step 7610, Training Loss: 0.008653709664940834\n",
            "Step 7610, Training Loss: 0.007877903990447521\n",
            "Step 7610, Training Loss: 0.005904511548578739\n",
            "Step 7610, Training Loss: 0.008797523565590382\n",
            "Step 7610, Training Loss: 0.1737472265958786\n",
            "Step 7610, Training Loss: 0.3243584632873535\n",
            "Step 7610, Training Loss: 0.3275962471961975\n",
            "Step 7610, Training Loss: 0.24899497628211975\n",
            "Step 7610, Training Loss: 0.1447521299123764\n",
            "Step 7610, Training Loss: 0.09543368220329285\n",
            "Step 7610, Training Loss: 0.2084737867116928\n",
            "Step 7610, Training Loss: 0.1076594665646553\n",
            "Step 7610, Training Loss: 0.06895986944437027\n",
            "Step 7610, Training Loss: 0.3020877540111542\n",
            "Step 7610, Training Loss: 0.023016836494207382\n",
            "Step 7610, Training Loss: 0.1901317983865738\n",
            "Step 7610, Training Loss: 0.0047615705989301205\n",
            "Step 7610, Training Loss: 0.49757814407348633\n",
            "Step 7610, Training Loss: 0.18087704479694366\n",
            "Step 7610, Training Loss: 0.02222735621035099\n",
            "Step 7610, Training Loss: 0.08198986202478409\n",
            "Step 7610, Training Loss: 0.04952708259224892\n",
            "Step 7610, Training Loss: 0.21653775870800018\n",
            "Step 7610, Training Loss: 0.06952855736017227\n",
            "Step 7610, Training Loss: 0.10847929120063782\n",
            "Step 7610, Training Loss: 0.45093706250190735\n",
            "Step 7610, Training Loss: 0.18960754573345184\n",
            "Step 7610, Training Loss: 0.14469504356384277\n",
            "Step 7610, Training Loss: 0.4359985589981079\n",
            "Step 7610, Training Loss: 0.11925056576728821\n",
            "Step 7610, Training Loss: 0.15698713064193726\n",
            "Step 7610, Training Loss: 0.06780051440000534\n",
            "Step 7610, Training Loss: 0.11170230060815811\n",
            "Step 7610, Training Loss: 0.006459654774516821\n",
            "Step 7610, Training Loss: 0.1423831582069397\n",
            "Step 7610, Training Loss: 0.006708240136504173\n",
            "Step 7610, Training Loss: 0.2703293561935425\n",
            "Step 7610, Training Loss: 0.03980233892798424\n",
            "Step 7610, Training Loss: 0.00642024539411068\n",
            "Step 7610, Training Loss: 0.006218857131898403\n",
            "Step 7610, Training Loss: 0.09363832324743271\n",
            "Step 7610, Training Loss: 0.25015687942504883\n",
            "Step 7610, Training Loss: 0.24492979049682617\n",
            "Step 7610, Training Loss: 0.09410487115383148\n",
            "Step 7610, Training Loss: 0.005501226522028446\n",
            "Step 7610, Training Loss: 0.06858542561531067\n",
            "Step 7610, Training Loss: 0.10506685823202133\n",
            "Step 7610, Training Loss: 0.12155202031135559\n",
            "Step 7610, Training Loss: 0.006657454650849104\n",
            "Step 7610, Training Loss: 0.06762684881687164\n",
            "Step 7610, Training Loss: 0.1612909585237503\n",
            "Step 7610, Training Loss: 0.390103280544281\n",
            "Step 7610, Training Loss: 0.07531320303678513\n",
            "Step 7610, Training Loss: 0.08654504269361496\n",
            "Step 7610, Training Loss: 0.17300473153591156\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 1523\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training completed in 408.75 seconds.\n",
            "Step 7610, Training Loss: 0.38420847058296204\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='191' max='191' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [191/191 00:03]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 7610, Training Loss: 0.37743550539016724\n",
            "Step 7610, Training Loss: 0.1720101535320282\n",
            "Step 7610, Training Loss: 0.02779061160981655\n",
            "Step 7610, Training Loss: 0.2523842453956604\n",
            "Step 7610, Training Loss: 0.38900935649871826\n",
            "Step 7610, Training Loss: 0.2863045632839203\n",
            "Step 7610, Training Loss: 0.0737265944480896\n",
            "Step 7610, Training Loss: 0.02636643871665001\n",
            "Step 7610, Training Loss: 0.04097771272063255\n",
            "Step 7610, Training Loss: 0.19253374636173248\n",
            "Step 7610, Training Loss: 0.08555585891008377\n",
            "Step 7610, Training Loss: 0.18729275465011597\n",
            "Step 7610, Training Loss: 0.19809669256210327\n",
            "Step 7610, Training Loss: 0.13559600710868835\n",
            "Step 7610, Training Loss: 0.1802290976047516\n",
            "Step 7610, Training Loss: 0.2979230284690857\n",
            "Step 7610, Training Loss: 0.10495457053184509\n",
            "Step 7610, Training Loss: 0.0772710070014\n",
            "Step 7610, Training Loss: 0.10195980966091156\n",
            "Step 7610, Training Loss: 0.2087683379650116\n",
            "Step 7610, Training Loss: 0.005655771587044001\n",
            "Step 7610, Training Loss: 0.29738277196884155\n",
            "Step 7610, Training Loss: 0.2899573743343353\n",
            "Step 7610, Training Loss: 0.0038563297130167484\n",
            "Step 7610, Training Loss: 0.2044796198606491\n",
            "Step 7610, Training Loss: 0.1726052463054657\n",
            "Step 7610, Training Loss: 0.022028561681509018\n",
            "Step 7610, Training Loss: 0.0069378092885017395\n",
            "Step 7610, Training Loss: 0.37600964307785034\n",
            "Step 7610, Training Loss: 0.006086050532758236\n",
            "Step 7610, Training Loss: 0.3956940770149231\n",
            "Step 7610, Training Loss: 0.013124671764671803\n",
            "Step 7610, Training Loss: 0.03244398906826973\n",
            "Step 7610, Training Loss: 0.0049929022789001465\n",
            "Step 7610, Training Loss: 0.28598150610923767\n",
            "Step 7610, Training Loss: 0.0987921878695488\n",
            "Step 7610, Training Loss: 0.022321296855807304\n",
            "Step 7610, Training Loss: 0.16090892255306244\n",
            "Step 7610, Training Loss: 0.3649238049983978\n",
            "Step 7610, Training Loss: 0.21525156497955322\n",
            "Step 7610, Training Loss: 0.005513234529644251\n",
            "Step 7610, Training Loss: 0.01915724202990532\n",
            "Step 7610, Training Loss: 0.006717090029269457\n",
            "Step 7610, Training Loss: 0.19223763048648834\n",
            "Step 7610, Training Loss: 0.06348685920238495\n",
            "Step 7610, Training Loss: 0.20266737043857574\n",
            "Step 7610, Training Loss: 0.3016323447227478\n",
            "Step 7610, Training Loss: 0.14854390919208527\n",
            "Step 7610, Training Loss: 0.2661772668361664\n",
            "Step 7610, Training Loss: 0.006126304157078266\n",
            "Step 7610, Training Loss: 0.14784416556358337\n",
            "Step 7610, Training Loss: 0.11952672153711319\n",
            "Step 7610, Training Loss: 0.012295391410589218\n",
            "Step 7610, Training Loss: 0.029476311057806015\n",
            "Step 7610, Training Loss: 0.02046997658908367\n",
            "Step 7610, Training Loss: 0.24235089123249054\n",
            "Step 7610, Training Loss: 0.0046091764234006405\n",
            "Step 7610, Training Loss: 0.05563018098473549\n",
            "Step 7610, Training Loss: 0.06746473908424377\n",
            "Step 7610, Training Loss: 0.023488778620958328\n",
            "Step 7610, Training Loss: 0.10113779455423355\n",
            "Step 7610, Training Loss: 0.24807924032211304\n",
            "Step 7610, Training Loss: 0.06278140842914581\n",
            "Step 7610, Training Loss: 0.3276265561580658\n",
            "Step 7610, Training Loss: 0.00794192310422659\n",
            "Step 7610, Training Loss: 0.1359192281961441\n",
            "Step 7610, Training Loss: 0.1799081414937973\n",
            "Step 7610, Training Loss: 0.006165957543998957\n",
            "Step 7610, Training Loss: 0.020963646471500397\n",
            "Step 7610, Training Loss: 0.024165907874703407\n",
            "Step 7610, Training Loss: 0.12563665211200714\n",
            "Step 7610, Training Loss: 0.3636336624622345\n",
            "Step 7610, Training Loss: 0.05744003504514694\n",
            "Step 7610, Training Loss: 0.30256980657577515\n",
            "Step 7610, Training Loss: 0.05367100611329079\n",
            "Step 7610, Training Loss: 0.02564331702888012\n",
            "Step 7610, Training Loss: 0.06998695433139801\n",
            "Step 7610, Training Loss: 0.1239733099937439\n",
            "Step 7610, Training Loss: 0.2423587143421173\n",
            "Step 7610, Training Loss: 0.24159452319145203\n",
            "Step 7610, Training Loss: 0.13419675827026367\n",
            "Step 7610, Training Loss: 0.292625367641449\n",
            "Step 7610, Training Loss: 0.23990710079669952\n",
            "Step 7610, Training Loss: 0.3097880482673645\n",
            "Step 7610, Training Loss: 0.30983462929725647\n",
            "Step 7610, Training Loss: 0.02100188471376896\n",
            "Step 7610, Training Loss: 0.22746969759464264\n",
            "Step 7610, Training Loss: 0.18168026208877563\n",
            "Step 7610, Training Loss: 0.024819208309054375\n",
            "Step 7610, Training Loss: 0.022695889696478844\n",
            "Step 7610, Training Loss: 0.2005484402179718\n",
            "Step 7610, Training Loss: 0.3110036849975586\n",
            "Step 7610, Training Loss: 0.022044701501727104\n",
            "Step 7610, Training Loss: 0.032995063811540604\n",
            "Step 7610, Training Loss: 0.18936516344547272\n",
            "Step 7610, Training Loss: 0.23826617002487183\n",
            "Step 7610, Training Loss: 0.14756354689598083\n",
            "Step 7610, Training Loss: 0.4449996054172516\n",
            "Step 7610, Training Loss: 0.23763436079025269\n",
            "Step 7610, Training Loss: 0.2333279550075531\n",
            "Step 7610, Training Loss: 0.2227235585451126\n",
            "Step 7610, Training Loss: 0.04501791298389435\n",
            "Step 7610, Training Loss: 0.48587465286254883\n",
            "Step 7610, Training Loss: 0.19921797513961792\n",
            "Step 7610, Training Loss: 0.022020060569047928\n",
            "Step 7610, Training Loss: 0.029203062877058983\n",
            "Step 7610, Training Loss: 0.08390673249959946\n",
            "Step 7610, Training Loss: 0.2004719227552414\n",
            "Step 7610, Training Loss: 0.1889662891626358\n",
            "Step 7610, Training Loss: 0.2774151861667633\n",
            "Step 7610, Training Loss: 0.03797660022974014\n",
            "Step 7610, Training Loss: 0.03963588550686836\n",
            "Step 7610, Training Loss: 0.3779104948043823\n",
            "Step 7610, Training Loss: 0.3406704068183899\n",
            "Step 7610, Training Loss: 0.024319691583514214\n",
            "Step 7610, Training Loss: 0.02635420300066471\n",
            "Step 7610, Training Loss: 0.13342365622520447\n",
            "Step 7610, Training Loss: 0.004195520654320717\n",
            "Step 7610, Training Loss: 0.027426335960626602\n",
            "Step 7610, Training Loss: 0.37127259373664856\n",
            "Step 7610, Training Loss: 0.3431728482246399\n",
            "Step 7610, Training Loss: 0.03656180575489998\n",
            "Step 7610, Training Loss: 0.19161783158779144\n",
            "Step 7610, Training Loss: 0.08348052948713303\n",
            "Step 7610, Training Loss: 0.15903587639331818\n",
            "Step 7610, Training Loss: 0.05604449659585953\n",
            "Step 7610, Training Loss: 0.2304372936487198\n",
            "Step 7610, Training Loss: 0.25227221846580505\n",
            "Step 7610, Training Loss: 0.1909574717283249\n",
            "Step 7610, Training Loss: 0.07869724929332733\n",
            "Step 7610, Training Loss: 0.18062002956867218\n",
            "Step 7610, Training Loss: 0.18882118165493011\n",
            "Step 7610, Training Loss: 0.186228945851326\n",
            "Step 7610, Training Loss: 0.0066123176366090775\n",
            "Step 7610, Training Loss: 0.11014451086521149\n",
            "Step 7610, Training Loss: 0.008653709664940834\n",
            "Step 7610, Training Loss: 0.007877903990447521\n",
            "Step 7610, Training Loss: 0.005904511548578739\n",
            "Step 7610, Training Loss: 0.008797523565590382\n",
            "Step 7610, Training Loss: 0.1737472265958786\n",
            "Step 7610, Training Loss: 0.3243584632873535\n",
            "Step 7610, Training Loss: 0.3275962471961975\n",
            "Step 7610, Training Loss: 0.24899497628211975\n",
            "Step 7610, Training Loss: 0.1447521299123764\n",
            "Step 7610, Training Loss: 0.09543368220329285\n",
            "Step 7610, Training Loss: 0.2084737867116928\n",
            "Step 7610, Training Loss: 0.1076594665646553\n",
            "Step 7610, Training Loss: 0.06895986944437027\n",
            "Step 7610, Training Loss: 0.3020877540111542\n",
            "Step 7610, Training Loss: 0.023016836494207382\n",
            "Step 7610, Training Loss: 0.1901317983865738\n",
            "Step 7610, Training Loss: 0.0047615705989301205\n",
            "Step 7610, Training Loss: 0.49757814407348633\n",
            "Step 7610, Training Loss: 0.18087704479694366\n",
            "Step 7610, Training Loss: 0.02222735621035099\n",
            "Step 7610, Training Loss: 0.08198986202478409\n",
            "Step 7610, Training Loss: 0.04952708259224892\n",
            "Step 7610, Training Loss: 0.21653775870800018\n",
            "Step 7610, Training Loss: 0.06952855736017227\n",
            "Step 7610, Training Loss: 0.10847929120063782\n",
            "Step 7610, Training Loss: 0.45093706250190735\n",
            "Step 7610, Training Loss: 0.18960754573345184\n",
            "Step 7610, Training Loss: 0.14469504356384277\n",
            "Step 7610, Training Loss: 0.4359985589981079\n",
            "Step 7610, Training Loss: 0.11925056576728821\n",
            "Step 7610, Training Loss: 0.15698713064193726\n",
            "Step 7610, Training Loss: 0.06780051440000534\n",
            "Step 7610, Training Loss: 0.11170230060815811\n",
            "Step 7610, Training Loss: 0.006459654774516821\n",
            "Step 7610, Training Loss: 0.1423831582069397\n",
            "Step 7610, Training Loss: 0.006708240136504173\n",
            "Step 7610, Training Loss: 0.2703293561935425\n",
            "Step 7610, Training Loss: 0.03980233892798424\n",
            "Step 7610, Training Loss: 0.00642024539411068\n",
            "Step 7610, Training Loss: 0.006218857131898403\n",
            "Step 7610, Training Loss: 0.09363832324743271\n",
            "Step 7610, Training Loss: 0.25015687942504883\n",
            "Step 7610, Training Loss: 0.24492979049682617\n",
            "Step 7610, Training Loss: 0.09410487115383148\n",
            "Step 7610, Training Loss: 0.005501226522028446\n",
            "Step 7610, Training Loss: 0.06858542561531067\n",
            "Step 7610, Training Loss: 0.10506685823202133\n",
            "Step 7610, Training Loss: 0.12155202031135559\n",
            "Step 7610, Training Loss: 0.006657454650849104\n",
            "Step 7610, Training Loss: 0.06762684881687164\n",
            "Step 7610, Training Loss: 0.1612909585237503\n",
            "Step 7610, Training Loss: 0.390103280544281\n",
            "Step 7610, Training Loss: 0.07531320303678513\n",
            "Step 7610, Training Loss: 0.08654504269361496\n",
            "Step 7610, Training Loss: 0.17300473153591156\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-12-09 19:14:06,196] Trial 4 finished with value: 0.8513399803594174 and parameters: {'learning_rate': 1.4778967436657266e-06, 'batch_size': 4, 'num_train_epochs': 10, 'weight_decay': 0.05206252656731796}. Best is trial 4 with value: 0.8513399803594174.\n",
            "<ipython-input-12-42465065092b>:2: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  learning_rate = trial.suggest_loguniform(\"learning_rate\", 1e-6, 5e-5)\n",
            "<ipython-input-12-42465065092b>:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  weight_decay = trial.suggest_loguniform(\"weight_decay\", 1e-4, 0.1)\n",
            "PyTorch: setting up devices\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n",
            "***** Running training *****\n",
            "  Num examples = 6091\n",
            "  Num Epochs = 4\n",
            "  Instantaneous batch size per device = 4\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
            "  Gradient Accumulation steps = 2\n",
            "  Total optimization steps = 3044\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 0, Training Loss: 0.007473161444067955\n",
            "Step 0, Training Loss: 0.005644978955388069\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='3044' max='3044' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [3044/3044 02:41, Epoch 3/4]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "      <th>F1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>0.029700</td>\n",
              "      <td>0.159865</td>\n",
              "      <td>0.848326</td>\n",
              "      <td>0.864537</td>\n",
              "      <td>0.854942</td>\n",
              "      <td>0.854775</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.023300</td>\n",
              "      <td>0.153592</td>\n",
              "      <td>0.849639</td>\n",
              "      <td>0.858674</td>\n",
              "      <td>0.854942</td>\n",
              "      <td>0.853392</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.020600</td>\n",
              "      <td>0.151825</td>\n",
              "      <td>0.850295</td>\n",
              "      <td>0.856939</td>\n",
              "      <td>0.859435</td>\n",
              "      <td>0.855869</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.030200</td>\n",
              "      <td>0.157038</td>\n",
              "      <td>0.850952</td>\n",
              "      <td>0.857790</td>\n",
              "      <td>0.856226</td>\n",
              "      <td>0.853810</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 10, Training Loss: 0.005288909189403057\n",
            "Step 10, Training Loss: 0.0055931331589818\n",
            "Step 20, Training Loss: 0.2542423605918884\n",
            "Step 20, Training Loss: 0.04275144264101982\n",
            "Step 30, Training Loss: 0.0063588726334273815\n",
            "Step 30, Training Loss: 0.057863712310791016\n",
            "Step 40, Training Loss: 0.007481374777853489\n",
            "Step 40, Training Loss: 0.007251695264130831\n",
            "Step 50, Training Loss: 0.009241381660103798\n",
            "Step 50, Training Loss: 0.011353602632880211\n",
            "Step 60, Training Loss: 0.00850983988493681\n",
            "Step 60, Training Loss: 0.005868378095328808\n",
            "Step 70, Training Loss: 0.00979310180991888\n",
            "Step 70, Training Loss: 0.039570339024066925\n",
            "Step 80, Training Loss: 0.03605879098176956\n",
            "Step 80, Training Loss: 0.03559432551264763\n",
            "Step 90, Training Loss: 0.0057816836051642895\n",
            "Step 90, Training Loss: 0.011552052572369576\n",
            "Step 100, Training Loss: 0.003786760149523616\n",
            "Step 100, Training Loss: 0.004918849095702171\n",
            "Step 110, Training Loss: 0.006747139617800713\n",
            "Step 110, Training Loss: 0.007479213643819094\n",
            "Step 120, Training Loss: 0.004633018746972084\n",
            "Step 120, Training Loss: 0.09405918419361115\n",
            "Step 130, Training Loss: 0.1057211235165596\n",
            "Step 130, Training Loss: 0.00600934075191617\n",
            "Step 140, Training Loss: 0.04372655972838402\n",
            "Step 140, Training Loss: 0.09180789440870285\n",
            "Step 150, Training Loss: 0.01212421152740717\n",
            "Step 150, Training Loss: 0.1955685317516327\n",
            "Step 160, Training Loss: 0.014976123347878456\n",
            "Step 160, Training Loss: 0.016613662242889404\n",
            "Step 170, Training Loss: 0.005893126595765352\n",
            "Step 170, Training Loss: 0.005614866502583027\n",
            "Step 180, Training Loss: 0.07998232543468475\n",
            "Step 180, Training Loss: 0.25018686056137085\n",
            "Step 190, Training Loss: 0.12122958898544312\n",
            "Step 190, Training Loss: 0.04464087635278702\n",
            "Step 200, Training Loss: 0.0055426559410989285\n",
            "Step 200, Training Loss: 0.004998510703444481\n",
            "Step 210, Training Loss: 0.25937339663505554\n",
            "Step 210, Training Loss: 0.05613488331437111\n",
            "Step 220, Training Loss: 0.03624029830098152\n",
            "Step 220, Training Loss: 0.1476542353630066\n",
            "Step 230, Training Loss: 0.005352621898055077\n",
            "Step 230, Training Loss: 0.005211185198277235\n",
            "Step 240, Training Loss: 0.008967801928520203\n",
            "Step 240, Training Loss: 0.004513564985245466\n",
            "Step 250, Training Loss: 0.013558821752667427\n",
            "Step 250, Training Loss: 0.04533664509654045\n",
            "Step 260, Training Loss: 0.004050863906741142\n",
            "Step 260, Training Loss: 0.007019596640020609\n",
            "Step 270, Training Loss: 0.0349431149661541\n",
            "Step 270, Training Loss: 0.004670280963182449\n",
            "Step 280, Training Loss: 0.03558032214641571\n",
            "Step 280, Training Loss: 0.006276017054915428\n",
            "Step 290, Training Loss: 0.007473234552890062\n",
            "Step 290, Training Loss: 0.006681269966065884\n",
            "Step 300, Training Loss: 0.06420253962278366\n",
            "Step 300, Training Loss: 0.006216241978108883\n",
            "Step 310, Training Loss: 0.006762081757187843\n",
            "Step 310, Training Loss: 0.00544992508366704\n",
            "Step 320, Training Loss: 0.004009708762168884\n",
            "Step 320, Training Loss: 0.044846635311841965\n",
            "Step 330, Training Loss: 0.005344743374735117\n",
            "Step 330, Training Loss: 0.003425437491387129\n",
            "Step 340, Training Loss: 0.004340232815593481\n",
            "Step 340, Training Loss: 0.029142722487449646\n",
            "Step 350, Training Loss: 0.009037919342517853\n",
            "Step 350, Training Loss: 0.038389187306165695\n",
            "Step 360, Training Loss: 0.010775811038911343\n",
            "Step 360, Training Loss: 0.007329302839934826\n",
            "Step 370, Training Loss: 0.004848241340368986\n",
            "Step 370, Training Loss: 0.03690095990896225\n",
            "Step 380, Training Loss: 0.004093073308467865\n",
            "Step 380, Training Loss: 0.04684758931398392\n",
            "Step 390, Training Loss: 0.06274783611297607\n",
            "Step 390, Training Loss: 0.0926450863480568\n",
            "Step 400, Training Loss: 0.004361369647085667\n",
            "Step 400, Training Loss: 0.00747161079198122\n",
            "Step 410, Training Loss: 0.003289826912805438\n",
            "Step 410, Training Loss: 0.06475953012704849\n",
            "Step 420, Training Loss: 0.005530965514481068\n",
            "Step 420, Training Loss: 0.06462019681930542\n",
            "Step 430, Training Loss: 0.37728267908096313\n",
            "Step 430, Training Loss: 0.28642717003822327\n",
            "Step 440, Training Loss: 0.008037328720092773\n",
            "Step 440, Training Loss: 0.08062492311000824\n",
            "Step 450, Training Loss: 0.012634743936359882\n",
            "Step 450, Training Loss: 0.038199663162231445\n",
            "Step 460, Training Loss: 0.0050819613970816135\n",
            "Step 460, Training Loss: 0.07825139164924622\n",
            "Step 470, Training Loss: 0.039423488080501556\n",
            "Step 470, Training Loss: 0.004964551888406277\n",
            "Step 480, Training Loss: 0.038070835173130035\n",
            "Step 480, Training Loss: 0.07403125613927841\n",
            "Step 490, Training Loss: 0.2803367078304291\n",
            "Step 490, Training Loss: 0.007901124656200409\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Saving model checkpoint to ./results/checkpoint-500\n",
            "Configuration saved in ./results/checkpoint-500/config.json\n",
            "Model weights saved in ./results/checkpoint-500/pytorch_model.bin\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 500, Training Loss: 0.05767257884144783\n",
            "Step 500, Training Loss: 0.004714668728411198\n",
            "Step 510, Training Loss: 0.039104968309402466\n",
            "Step 510, Training Loss: 0.1783592402935028\n",
            "Step 520, Training Loss: 0.09014297276735306\n",
            "Step 520, Training Loss: 0.008298598229885101\n",
            "Step 530, Training Loss: 0.041549790650606155\n",
            "Step 530, Training Loss: 0.004801345057785511\n",
            "Step 540, Training Loss: 0.0036553258541971445\n",
            "Step 540, Training Loss: 0.0058008586056530476\n",
            "Step 550, Training Loss: 0.13049761950969696\n",
            "Step 550, Training Loss: 0.02915879711508751\n",
            "Step 560, Training Loss: 0.0694892406463623\n",
            "Step 560, Training Loss: 0.19529806077480316\n",
            "Step 570, Training Loss: 0.005527464672923088\n",
            "Step 570, Training Loss: 0.03879530727863312\n",
            "Step 580, Training Loss: 0.034868672490119934\n",
            "Step 580, Training Loss: 0.006892410106956959\n",
            "Step 590, Training Loss: 0.004484979435801506\n",
            "Step 590, Training Loss: 0.005991614889353514\n",
            "Step 600, Training Loss: 0.17302606999874115\n",
            "Step 600, Training Loss: 0.04099496826529503\n",
            "Step 610, Training Loss: 0.04311543330550194\n",
            "Step 610, Training Loss: 0.3287121057510376\n",
            "Step 620, Training Loss: 0.15585699677467346\n",
            "Step 620, Training Loss: 0.01031102892011404\n",
            "Step 630, Training Loss: 0.004113722126930952\n",
            "Step 630, Training Loss: 0.21171621978282928\n",
            "Step 640, Training Loss: 0.004665608983486891\n",
            "Step 640, Training Loss: 0.04346548020839691\n",
            "Step 650, Training Loss: 0.00714811822399497\n",
            "Step 650, Training Loss: 0.35803094506263733\n",
            "Step 660, Training Loss: 0.031991537660360336\n",
            "Step 660, Training Loss: 0.0058176093734800816\n",
            "Step 670, Training Loss: 0.006401184480637312\n",
            "Step 670, Training Loss: 0.0647694319486618\n",
            "Step 680, Training Loss: 0.0339837521314621\n",
            "Step 680, Training Loss: 0.38362839818000793\n",
            "Step 690, Training Loss: 0.03747354447841644\n",
            "Step 690, Training Loss: 0.004138141870498657\n",
            "Step 700, Training Loss: 0.01115760300308466\n",
            "Step 700, Training Loss: 0.06131882965564728\n",
            "Step 710, Training Loss: 0.09481117874383926\n",
            "Step 710, Training Loss: 0.051135193556547165\n",
            "Step 720, Training Loss: 0.005468540359288454\n",
            "Step 720, Training Loss: 0.20186857879161835\n",
            "Step 730, Training Loss: 0.005059329327195883\n",
            "Step 730, Training Loss: 0.004207993857562542\n",
            "Step 740, Training Loss: 0.011193899437785149\n",
            "Step 740, Training Loss: 0.006222332362085581\n",
            "Step 750, Training Loss: 0.004698524717241526\n",
            "Step 750, Training Loss: 0.06550770998001099\n",
            "Step 760, Training Loss: 0.004181923344731331\n",
            "Step 760, Training Loss: 0.002950900001451373\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 1523\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 770, Training Loss: 0.00453424546867609\n",
            "Step 770, Training Loss: 0.04800058528780937\n",
            "Step 780, Training Loss: 0.005330818705260754\n",
            "Step 780, Training Loss: 0.004009014926850796\n",
            "Step 790, Training Loss: 0.0036733062006533146\n",
            "Step 790, Training Loss: 0.004188330378383398\n",
            "Step 800, Training Loss: 0.040316298604011536\n",
            "Step 800, Training Loss: 0.13274720311164856\n",
            "Step 810, Training Loss: 0.004790033679455519\n",
            "Step 810, Training Loss: 0.006893163546919823\n",
            "Step 820, Training Loss: 0.009457647800445557\n",
            "Step 820, Training Loss: 0.006960887461900711\n",
            "Step 830, Training Loss: 0.09935847669839859\n",
            "Step 830, Training Loss: 0.010731481947004795\n",
            "Step 840, Training Loss: 0.03136942908167839\n",
            "Step 840, Training Loss: 0.0059822965413331985\n",
            "Step 850, Training Loss: 0.006577609106898308\n",
            "Step 850, Training Loss: 0.12140091508626938\n",
            "Step 860, Training Loss: 0.004090023692697287\n",
            "Step 860, Training Loss: 0.03733431547880173\n",
            "Step 870, Training Loss: 0.04752975329756737\n",
            "Step 870, Training Loss: 0.07215826958417892\n",
            "Step 880, Training Loss: 0.004817171022295952\n",
            "Step 880, Training Loss: 0.03745495527982712\n",
            "Step 890, Training Loss: 0.006552607286721468\n",
            "Step 890, Training Loss: 0.1808968335390091\n",
            "Step 900, Training Loss: 0.1769663393497467\n",
            "Step 900, Training Loss: 0.004334920085966587\n",
            "Step 910, Training Loss: 0.07696409523487091\n",
            "Step 910, Training Loss: 0.029704511165618896\n",
            "Step 920, Training Loss: 0.06684114784002304\n",
            "Step 920, Training Loss: 0.005969142075628042\n",
            "Step 930, Training Loss: 0.011859042569994926\n",
            "Step 930, Training Loss: 0.006125567480921745\n",
            "Step 940, Training Loss: 0.003911362029612064\n",
            "Step 940, Training Loss: 0.08596622198820114\n",
            "Step 950, Training Loss: 0.03547753766179085\n",
            "Step 950, Training Loss: 0.03255728632211685\n",
            "Step 960, Training Loss: 0.04589638486504555\n",
            "Step 960, Training Loss: 0.00452581187710166\n",
            "Step 970, Training Loss: 0.003287283005192876\n",
            "Step 970, Training Loss: 0.005443349946290255\n",
            "Step 980, Training Loss: 0.2964489161968231\n",
            "Step 980, Training Loss: 0.011757166124880314\n",
            "Step 990, Training Loss: 0.18883638083934784\n",
            "Step 990, Training Loss: 0.08168177306652069\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Saving model checkpoint to ./results/checkpoint-1000\n",
            "Configuration saved in ./results/checkpoint-1000/config.json\n",
            "Model weights saved in ./results/checkpoint-1000/pytorch_model.bin\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 1000, Training Loss: 0.004885310772806406\n",
            "Step 1000, Training Loss: 0.0059966398403048515\n",
            "Step 1010, Training Loss: 0.03346636891365051\n",
            "Step 1010, Training Loss: 0.00383650790899992\n",
            "Step 1020, Training Loss: 0.06026841327548027\n",
            "Step 1020, Training Loss: 0.007952429354190826\n",
            "Step 1030, Training Loss: 0.030609633773565292\n",
            "Step 1030, Training Loss: 0.037752654403448105\n",
            "Step 1040, Training Loss: 0.003351982217282057\n",
            "Step 1040, Training Loss: 0.027384601533412933\n",
            "Step 1050, Training Loss: 0.012836352922022343\n",
            "Step 1050, Training Loss: 0.03338726982474327\n",
            "Step 1060, Training Loss: 0.02729291282594204\n",
            "Step 1060, Training Loss: 0.003039890667423606\n",
            "Step 1070, Training Loss: 0.0055036600679159164\n",
            "Step 1070, Training Loss: 0.03627762943506241\n",
            "Step 1080, Training Loss: 0.006216918583959341\n",
            "Step 1080, Training Loss: 0.03678742051124573\n",
            "Step 1090, Training Loss: 0.005728757940232754\n",
            "Step 1090, Training Loss: 0.0032145390287041664\n",
            "Step 1100, Training Loss: 0.006980749312788248\n",
            "Step 1100, Training Loss: 0.004775685723870993\n",
            "Step 1110, Training Loss: 0.004884672816842794\n",
            "Step 1110, Training Loss: 0.005509659182280302\n",
            "Step 1120, Training Loss: 0.006668328307569027\n",
            "Step 1120, Training Loss: 0.09318500012159348\n",
            "Step 1130, Training Loss: 0.00826245080679655\n",
            "Step 1130, Training Loss: 0.009409700520336628\n",
            "Step 1140, Training Loss: 0.0038766092620790005\n",
            "Step 1140, Training Loss: 0.005563953425735235\n",
            "Step 1150, Training Loss: 0.009618997573852539\n",
            "Step 1150, Training Loss: 0.38365259766578674\n",
            "Step 1160, Training Loss: 0.05313493683934212\n",
            "Step 1160, Training Loss: 0.0031858549918979406\n",
            "Step 1170, Training Loss: 0.3143949508666992\n",
            "Step 1170, Training Loss: 0.05297239497303963\n",
            "Step 1180, Training Loss: 0.008418192155659199\n",
            "Step 1180, Training Loss: 0.02986997365951538\n",
            "Step 1190, Training Loss: 0.07124776393175125\n",
            "Step 1190, Training Loss: 0.10539042204618454\n",
            "Step 1200, Training Loss: 0.03149700164794922\n",
            "Step 1200, Training Loss: 0.02911221608519554\n",
            "Step 1210, Training Loss: 0.0582953579723835\n",
            "Step 1210, Training Loss: 0.009306743741035461\n",
            "Step 1220, Training Loss: 0.13033747673034668\n",
            "Step 1220, Training Loss: 0.41310200095176697\n",
            "Step 1230, Training Loss: 0.23690715432167053\n",
            "Step 1230, Training Loss: 0.029398631304502487\n",
            "Step 1240, Training Loss: 0.07889293134212494\n",
            "Step 1240, Training Loss: 0.03209938853979111\n",
            "Step 1250, Training Loss: 0.0036619091406464577\n",
            "Step 1250, Training Loss: 0.5778993964195251\n",
            "Step 1260, Training Loss: 0.008335507474839687\n",
            "Step 1260, Training Loss: 0.0049604009836912155\n",
            "Step 1270, Training Loss: 0.2066548764705658\n",
            "Step 1270, Training Loss: 0.0033129805233329535\n",
            "Step 1280, Training Loss: 0.12859199941158295\n",
            "Step 1280, Training Loss: 0.008231828920543194\n",
            "Step 1290, Training Loss: 0.005382372997701168\n",
            "Step 1290, Training Loss: 0.034231942147016525\n",
            "Step 1300, Training Loss: 0.005704275332391262\n",
            "Step 1300, Training Loss: 0.004801454022526741\n",
            "Step 1310, Training Loss: 0.12469489127397537\n",
            "Step 1310, Training Loss: 0.4427878260612488\n",
            "Step 1320, Training Loss: 0.006683646701276302\n",
            "Step 1320, Training Loss: 0.008178556337952614\n",
            "Step 1330, Training Loss: 0.09814006835222244\n",
            "Step 1330, Training Loss: 0.026244167238473892\n",
            "Step 1340, Training Loss: 0.004059813916683197\n",
            "Step 1340, Training Loss: 0.002925471169874072\n",
            "Step 1350, Training Loss: 0.006811690516769886\n",
            "Step 1350, Training Loss: 0.0084042027592659\n",
            "Step 1360, Training Loss: 0.11278513818979263\n",
            "Step 1360, Training Loss: 0.08502189069986343\n",
            "Step 1370, Training Loss: 0.08543314039707184\n",
            "Step 1370, Training Loss: 0.02649988979101181\n",
            "Step 1380, Training Loss: 0.03830813243985176\n",
            "Step 1380, Training Loss: 0.08448556810617447\n",
            "Step 1390, Training Loss: 0.5975803732872009\n",
            "Step 1390, Training Loss: 0.026918374001979828\n",
            "Step 1400, Training Loss: 0.0249109398573637\n",
            "Step 1400, Training Loss: 0.029269004240632057\n",
            "Step 1410, Training Loss: 0.0042576780542731285\n",
            "Step 1410, Training Loss: 0.0056922598741948605\n",
            "Step 1420, Training Loss: 0.004501869902014732\n",
            "Step 1420, Training Loss: 0.004509453661739826\n",
            "Step 1430, Training Loss: 0.09387104958295822\n",
            "Step 1430, Training Loss: 0.03287811204791069\n",
            "Step 1440, Training Loss: 0.010315346531569958\n",
            "Step 1440, Training Loss: 0.0038848489057272673\n",
            "Step 1450, Training Loss: 0.0046226950362324715\n",
            "Step 1450, Training Loss: 0.00400203512981534\n",
            "Step 1460, Training Loss: 0.0044350577518343925\n",
            "Step 1460, Training Loss: 0.0035279064904898405\n",
            "Step 1470, Training Loss: 0.005722259171307087\n",
            "Step 1470, Training Loss: 0.07359621673822403\n",
            "Step 1480, Training Loss: 0.003582803765311837\n",
            "Step 1480, Training Loss: 0.02691659890115261\n",
            "Step 1490, Training Loss: 0.049710433930158615\n",
            "Step 1490, Training Loss: 0.07369142770767212\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Saving model checkpoint to ./results/checkpoint-1500\n",
            "Configuration saved in ./results/checkpoint-1500/config.json\n",
            "Model weights saved in ./results/checkpoint-1500/pytorch_model.bin\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 1500, Training Loss: 0.049300048500299454\n",
            "Step 1500, Training Loss: 0.10908707231283188\n",
            "Step 1510, Training Loss: 0.00544809689745307\n",
            "Step 1510, Training Loss: 0.00490769324824214\n",
            "Step 1520, Training Loss: 0.04647152125835419\n",
            "Step 1520, Training Loss: 0.006572450045496225\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 1523\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 1530, Training Loss: 0.004399831872433424\n",
            "Step 1530, Training Loss: 0.03387467563152313\n",
            "Step 1540, Training Loss: 0.005134300794452429\n",
            "Step 1540, Training Loss: 0.007973305881023407\n",
            "Step 1550, Training Loss: 0.005223130341619253\n",
            "Step 1550, Training Loss: 0.1117650493979454\n",
            "Step 1560, Training Loss: 0.00631294958293438\n",
            "Step 1560, Training Loss: 0.03420174866914749\n",
            "Step 1570, Training Loss: 0.0036350309383124113\n",
            "Step 1570, Training Loss: 0.0036518212873488665\n",
            "Step 1580, Training Loss: 0.06787707656621933\n",
            "Step 1580, Training Loss: 0.0053691924549639225\n",
            "Step 1590, Training Loss: 0.00885054375976324\n",
            "Step 1590, Training Loss: 0.003913305699825287\n",
            "Step 1600, Training Loss: 0.009443161077797413\n",
            "Step 1600, Training Loss: 0.013996221125125885\n",
            "Step 1610, Training Loss: 0.08118877559900284\n",
            "Step 1610, Training Loss: 0.038474977016448975\n",
            "Step 1620, Training Loss: 0.025579137727618217\n",
            "Step 1620, Training Loss: 0.028332408517599106\n",
            "Step 1630, Training Loss: 0.006069103721529245\n",
            "Step 1630, Training Loss: 0.0895453467965126\n",
            "Step 1640, Training Loss: 0.004221731796860695\n",
            "Step 1640, Training Loss: 0.008081617765128613\n",
            "Step 1650, Training Loss: 0.004616928286850452\n",
            "Step 1650, Training Loss: 0.06285309046506882\n",
            "Step 1660, Training Loss: 0.0064304787665605545\n",
            "Step 1660, Training Loss: 0.0045165447518229485\n",
            "Step 1670, Training Loss: 0.009351092390716076\n",
            "Step 1670, Training Loss: 0.0504230372607708\n",
            "Step 1680, Training Loss: 0.004773314576596022\n",
            "Step 1680, Training Loss: 0.0046754213981330395\n",
            "Step 1690, Training Loss: 0.004046867601573467\n",
            "Step 1690, Training Loss: 0.03221851587295532\n",
            "Step 1700, Training Loss: 0.00448532123118639\n",
            "Step 1700, Training Loss: 0.005301118362694979\n",
            "Step 1710, Training Loss: 0.003585954662412405\n",
            "Step 1710, Training Loss: 0.003999736160039902\n",
            "Step 1720, Training Loss: 0.009182849898934364\n",
            "Step 1720, Training Loss: 0.036301594227552414\n",
            "Step 1730, Training Loss: 0.03672054037451744\n",
            "Step 1730, Training Loss: 0.051908574998378754\n",
            "Step 1740, Training Loss: 0.005766647402197123\n",
            "Step 1740, Training Loss: 0.0038849192205816507\n",
            "Step 1750, Training Loss: 0.18086113035678864\n",
            "Step 1750, Training Loss: 0.05658893287181854\n",
            "Step 1760, Training Loss: 0.0461718887090683\n",
            "Step 1760, Training Loss: 0.07177937030792236\n",
            "Step 1770, Training Loss: 0.023053528741002083\n",
            "Step 1770, Training Loss: 0.29064199328422546\n",
            "Step 1780, Training Loss: 0.05473516136407852\n",
            "Step 1780, Training Loss: 0.0035344127099961042\n",
            "Step 1790, Training Loss: 0.003678775392472744\n",
            "Step 1790, Training Loss: 0.17230536043643951\n",
            "Step 1800, Training Loss: 0.23475809395313263\n",
            "Step 1800, Training Loss: 0.004293930716812611\n",
            "Step 1810, Training Loss: 0.19854244589805603\n",
            "Step 1810, Training Loss: 0.005684332922101021\n",
            "Step 1820, Training Loss: 0.0063118417747318745\n",
            "Step 1820, Training Loss: 0.007502288091927767\n",
            "Step 1830, Training Loss: 0.13324207067489624\n",
            "Step 1830, Training Loss: 0.006797586567699909\n",
            "Step 1840, Training Loss: 0.007311630062758923\n",
            "Step 1840, Training Loss: 0.03316115960478783\n",
            "Step 1850, Training Loss: 0.005172329489141703\n",
            "Step 1850, Training Loss: 0.0033037944231182337\n",
            "Step 1860, Training Loss: 0.036047570407390594\n",
            "Step 1860, Training Loss: 0.11584305763244629\n",
            "Step 1870, Training Loss: 0.003718034364283085\n",
            "Step 1870, Training Loss: 0.03989730775356293\n",
            "Step 1880, Training Loss: 0.004410945810377598\n",
            "Step 1880, Training Loss: 0.012662545777857304\n",
            "Step 1890, Training Loss: 0.19115620851516724\n",
            "Step 1890, Training Loss: 0.0038643593434244394\n",
            "Step 1900, Training Loss: 0.040830232203006744\n",
            "Step 1900, Training Loss: 0.053689878433942795\n",
            "Step 1910, Training Loss: 0.0057927826419472694\n",
            "Step 1910, Training Loss: 0.030522100627422333\n",
            "Step 1920, Training Loss: 0.003985568881034851\n",
            "Step 1920, Training Loss: 0.004267015494406223\n",
            "Step 1930, Training Loss: 0.003041784279048443\n",
            "Step 1930, Training Loss: 0.004260831046849489\n",
            "Step 1940, Training Loss: 0.003111794823780656\n",
            "Step 1940, Training Loss: 0.004886284004896879\n",
            "Step 1950, Training Loss: 0.5299407243728638\n",
            "Step 1950, Training Loss: 0.010739754885435104\n",
            "Step 1960, Training Loss: 0.0031133960001170635\n",
            "Step 1960, Training Loss: 0.06565123051404953\n",
            "Step 1970, Training Loss: 0.03243944048881531\n",
            "Step 1970, Training Loss: 0.008993979543447495\n",
            "Step 1980, Training Loss: 0.004123556427657604\n",
            "Step 1980, Training Loss: 0.25359272956848145\n",
            "Step 1990, Training Loss: 0.004086419008672237\n",
            "Step 1990, Training Loss: 0.032556939870119095\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Saving model checkpoint to ./results/checkpoint-2000\n",
            "Configuration saved in ./results/checkpoint-2000/config.json\n",
            "Model weights saved in ./results/checkpoint-2000/pytorch_model.bin\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 2000, Training Loss: 0.17342832684516907\n",
            "Step 2000, Training Loss: 0.010119873099029064\n",
            "Step 2010, Training Loss: 0.008002062328159809\n",
            "Step 2010, Training Loss: 0.011319907382130623\n",
            "Step 2020, Training Loss: 0.0030615434516221285\n",
            "Step 2020, Training Loss: 0.003661530092358589\n",
            "Step 2030, Training Loss: 0.10957355797290802\n",
            "Step 2030, Training Loss: 0.3429182171821594\n",
            "Step 2040, Training Loss: 0.004668570123612881\n",
            "Step 2040, Training Loss: 0.03842009976506233\n",
            "Step 2050, Training Loss: 0.005100357811897993\n",
            "Step 2050, Training Loss: 0.007361190859228373\n",
            "Step 2060, Training Loss: 0.08172731846570969\n",
            "Step 2060, Training Loss: 0.007500591222196817\n",
            "Step 2070, Training Loss: 0.005014835391193628\n",
            "Step 2070, Training Loss: 0.03974347561597824\n",
            "Step 2080, Training Loss: 0.19632843136787415\n",
            "Step 2080, Training Loss: 0.3682064712047577\n",
            "Step 2090, Training Loss: 0.4174317717552185\n",
            "Step 2090, Training Loss: 0.0073201521299779415\n",
            "Step 2100, Training Loss: 0.07047619670629501\n",
            "Step 2100, Training Loss: 0.005065562203526497\n",
            "Step 2110, Training Loss: 0.005404382012784481\n",
            "Step 2110, Training Loss: 0.007533437572419643\n",
            "Step 2120, Training Loss: 0.004920972511172295\n",
            "Step 2120, Training Loss: 0.005041200201958418\n",
            "Step 2130, Training Loss: 0.14148901402950287\n",
            "Step 2130, Training Loss: 0.0055425032041966915\n",
            "Step 2140, Training Loss: 0.011297387070953846\n",
            "Step 2140, Training Loss: 0.004494217224419117\n",
            "Step 2150, Training Loss: 0.004190831910818815\n",
            "Step 2150, Training Loss: 0.004262032452970743\n",
            "Step 2160, Training Loss: 0.03678819537162781\n",
            "Step 2160, Training Loss: 0.07588590681552887\n",
            "Step 2170, Training Loss: 0.02549908682703972\n",
            "Step 2170, Training Loss: 0.005316937807947397\n",
            "Step 2180, Training Loss: 0.33015525341033936\n",
            "Step 2180, Training Loss: 0.00422998471185565\n",
            "Step 2190, Training Loss: 0.004836125299334526\n",
            "Step 2190, Training Loss: 0.004299392458051443\n",
            "Step 2200, Training Loss: 0.00564951216802001\n",
            "Step 2200, Training Loss: 0.0035495529882609844\n",
            "Step 2210, Training Loss: 0.029426639899611473\n",
            "Step 2210, Training Loss: 0.010697652585804462\n",
            "Step 2220, Training Loss: 0.0347294956445694\n",
            "Step 2220, Training Loss: 0.03628961369395256\n",
            "Step 2230, Training Loss: 0.0043603284284472466\n",
            "Step 2230, Training Loss: 0.006010142154991627\n",
            "Step 2240, Training Loss: 0.005581433419138193\n",
            "Step 2240, Training Loss: 0.01720600016415119\n",
            "Step 2250, Training Loss: 0.023285580798983574\n",
            "Step 2250, Training Loss: 0.0307378638535738\n",
            "Step 2260, Training Loss: 0.004319219384342432\n",
            "Step 2260, Training Loss: 0.00700157368555665\n",
            "Step 2270, Training Loss: 0.08816024661064148\n",
            "Step 2270, Training Loss: 0.005429460201412439\n",
            "Step 2280, Training Loss: 0.02392948418855667\n",
            "Step 2280, Training Loss: 0.03717532381415367\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 1523\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 2290, Training Loss: 0.006071799900382757\n",
            "Step 2290, Training Loss: 0.011674378998577595\n",
            "Step 2300, Training Loss: 0.003129693679511547\n",
            "Step 2300, Training Loss: 0.007357741240411997\n",
            "Step 2310, Training Loss: 0.005794382654130459\n",
            "Step 2310, Training Loss: 0.004829177167266607\n",
            "Step 2320, Training Loss: 0.007372213061898947\n",
            "Step 2320, Training Loss: 0.034508828073740005\n",
            "Step 2330, Training Loss: 0.002962848637253046\n",
            "Step 2330, Training Loss: 0.007632286287844181\n",
            "Step 2340, Training Loss: 0.004914173390716314\n",
            "Step 2340, Training Loss: 0.0292831864207983\n",
            "Step 2350, Training Loss: 0.004116292577236891\n",
            "Step 2350, Training Loss: 0.0037876577116549015\n",
            "Step 2360, Training Loss: 0.004086498636752367\n",
            "Step 2360, Training Loss: 0.004648121539503336\n",
            "Step 2370, Training Loss: 0.10686453431844711\n",
            "Step 2370, Training Loss: 0.050258006900548935\n",
            "Step 2380, Training Loss: 0.03677209094166756\n",
            "Step 2380, Training Loss: 0.0029111735057085752\n",
            "Step 2390, Training Loss: 0.06242617964744568\n",
            "Step 2390, Training Loss: 0.011271449737250805\n",
            "Step 2400, Training Loss: 0.02347983978688717\n",
            "Step 2400, Training Loss: 0.003047656500712037\n",
            "Step 2410, Training Loss: 0.031116163358092308\n",
            "Step 2410, Training Loss: 0.031044842675328255\n",
            "Step 2420, Training Loss: 0.008486795239150524\n",
            "Step 2420, Training Loss: 0.010119709186255932\n",
            "Step 2430, Training Loss: 0.006076108198612928\n",
            "Step 2430, Training Loss: 0.005185006186366081\n",
            "Step 2440, Training Loss: 0.010187545791268349\n",
            "Step 2440, Training Loss: 0.003910830244421959\n",
            "Step 2450, Training Loss: 0.0036449122708290815\n",
            "Step 2450, Training Loss: 0.01648258976638317\n",
            "Step 2460, Training Loss: 0.026494229212403297\n",
            "Step 2460, Training Loss: 0.033375333994627\n",
            "Step 2470, Training Loss: 0.33215266466140747\n",
            "Step 2470, Training Loss: 0.00464756041765213\n",
            "Step 2480, Training Loss: 0.008390086703002453\n",
            "Step 2480, Training Loss: 0.004850952886044979\n",
            "Step 2490, Training Loss: 0.35703912377357483\n",
            "Step 2490, Training Loss: 0.0040029785595834255\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Saving model checkpoint to ./results/checkpoint-2500\n",
            "Configuration saved in ./results/checkpoint-2500/config.json\n",
            "Model weights saved in ./results/checkpoint-2500/pytorch_model.bin\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 2500, Training Loss: 0.005559243727475405\n",
            "Step 2500, Training Loss: 0.16988800466060638\n",
            "Step 2510, Training Loss: 0.0044740671291947365\n",
            "Step 2510, Training Loss: 0.005139077082276344\n",
            "Step 2520, Training Loss: 0.0042552221566438675\n",
            "Step 2520, Training Loss: 0.07033504545688629\n",
            "Step 2530, Training Loss: 0.09598206728696823\n",
            "Step 2530, Training Loss: 0.004962190520018339\n",
            "Step 2540, Training Loss: 0.007817777805030346\n",
            "Step 2540, Training Loss: 0.003506597364321351\n",
            "Step 2550, Training Loss: 0.04745430871844292\n",
            "Step 2550, Training Loss: 0.033134907484054565\n",
            "Step 2560, Training Loss: 0.03976922854781151\n",
            "Step 2560, Training Loss: 0.009121997281908989\n",
            "Step 2570, Training Loss: 0.028378650546073914\n",
            "Step 2570, Training Loss: 0.009845408610999584\n",
            "Step 2580, Training Loss: 0.16100047528743744\n",
            "Step 2580, Training Loss: 0.004954812116920948\n",
            "Step 2590, Training Loss: 0.0090062590315938\n",
            "Step 2590, Training Loss: 0.03016066737473011\n",
            "Step 2600, Training Loss: 0.06597553938627243\n",
            "Step 2600, Training Loss: 0.034686051309108734\n",
            "Step 2610, Training Loss: 0.0036269580014050007\n",
            "Step 2610, Training Loss: 0.20350660383701324\n",
            "Step 2620, Training Loss: 0.0643487498164177\n",
            "Step 2620, Training Loss: 0.07106576859951019\n",
            "Step 2630, Training Loss: 0.006391717121005058\n",
            "Step 2630, Training Loss: 0.005959943402558565\n",
            "Step 2640, Training Loss: 0.03712441772222519\n",
            "Step 2640, Training Loss: 0.0401935875415802\n",
            "Step 2650, Training Loss: 0.08677386492490768\n",
            "Step 2650, Training Loss: 0.002688157605007291\n",
            "Step 2660, Training Loss: 0.005836821161210537\n",
            "Step 2660, Training Loss: 0.0034575024619698524\n",
            "Step 2670, Training Loss: 0.005659660790115595\n",
            "Step 2670, Training Loss: 0.0027922706212848425\n",
            "Step 2680, Training Loss: 0.03876715153455734\n",
            "Step 2680, Training Loss: 0.004679636564105749\n",
            "Step 2690, Training Loss: 0.0033660123590379953\n",
            "Step 2690, Training Loss: 0.09401539713144302\n",
            "Step 2700, Training Loss: 0.005789162591099739\n",
            "Step 2700, Training Loss: 0.0028486205264925957\n",
            "Step 2710, Training Loss: 0.030025027692317963\n",
            "Step 2710, Training Loss: 0.08609307557344437\n",
            "Step 2720, Training Loss: 0.0034407880157232285\n",
            "Step 2720, Training Loss: 0.005778491962701082\n",
            "Step 2730, Training Loss: 0.006197201553732157\n",
            "Step 2730, Training Loss: 0.004745136015117168\n",
            "Step 2740, Training Loss: 0.09104731678962708\n",
            "Step 2740, Training Loss: 0.0038348250091075897\n",
            "Step 2750, Training Loss: 0.05923762172460556\n",
            "Step 2750, Training Loss: 0.30203109979629517\n",
            "Step 2760, Training Loss: 0.005646673031151295\n",
            "Step 2760, Training Loss: 0.005642947740852833\n",
            "Step 2770, Training Loss: 0.005243200343102217\n",
            "Step 2770, Training Loss: 0.004843498580157757\n",
            "Step 2780, Training Loss: 0.09162489324808121\n",
            "Step 2780, Training Loss: 0.008930789306759834\n",
            "Step 2790, Training Loss: 0.009374076500535011\n",
            "Step 2790, Training Loss: 0.022338716313242912\n",
            "Step 2800, Training Loss: 0.005229343194514513\n",
            "Step 2800, Training Loss: 0.12093506008386612\n",
            "Step 2810, Training Loss: 0.0767078846693039\n",
            "Step 2810, Training Loss: 0.0056908889673650265\n",
            "Step 2820, Training Loss: 0.006236018147319555\n",
            "Step 2820, Training Loss: 0.0029363478533923626\n",
            "Step 2830, Training Loss: 0.09692691266536713\n",
            "Step 2830, Training Loss: 0.003152395598590374\n",
            "Step 2840, Training Loss: 0.0044333855621516705\n",
            "Step 2840, Training Loss: 0.08943906426429749\n",
            "Step 2850, Training Loss: 0.004666121210902929\n",
            "Step 2850, Training Loss: 0.011568947695195675\n",
            "Step 2860, Training Loss: 0.004922737367451191\n",
            "Step 2860, Training Loss: 0.005059843882918358\n",
            "Step 2870, Training Loss: 0.004508648999035358\n",
            "Step 2870, Training Loss: 0.004711995366960764\n",
            "Step 2880, Training Loss: 0.013524672016501427\n",
            "Step 2880, Training Loss: 0.005609935149550438\n",
            "Step 2890, Training Loss: 0.03384603559970856\n",
            "Step 2890, Training Loss: 0.043421488255262375\n",
            "Step 2900, Training Loss: 0.004086640663444996\n",
            "Step 2900, Training Loss: 0.00503193773329258\n",
            "Step 2910, Training Loss: 0.0043676248751580715\n",
            "Step 2910, Training Loss: 0.16545026004314423\n",
            "Step 2920, Training Loss: 0.004107112996280193\n",
            "Step 2920, Training Loss: 0.009118763729929924\n",
            "Step 2930, Training Loss: 0.00946519523859024\n",
            "Step 2930, Training Loss: 0.02295105904340744\n",
            "Step 2940, Training Loss: 0.004024585243314505\n",
            "Step 2940, Training Loss: 0.18109031021595\n",
            "Step 2950, Training Loss: 0.04951923340559006\n",
            "Step 2950, Training Loss: 0.08895111829042435\n",
            "Step 2960, Training Loss: 0.03668690845370293\n",
            "Step 2960, Training Loss: 0.033994026482105255\n",
            "Step 2970, Training Loss: 0.005033077206462622\n",
            "Step 2970, Training Loss: 0.02914104238152504\n",
            "Step 2980, Training Loss: 0.2889622449874878\n",
            "Step 2980, Training Loss: 0.00830754917114973\n",
            "Step 2990, Training Loss: 0.08901975303888321\n",
            "Step 2990, Training Loss: 0.005280753597617149\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Saving model checkpoint to ./results/checkpoint-3000\n",
            "Configuration saved in ./results/checkpoint-3000/config.json\n",
            "Model weights saved in ./results/checkpoint-3000/pytorch_model.bin\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 3000, Training Loss: 0.0305524542927742\n",
            "Step 3000, Training Loss: 0.007349895313382149\n",
            "Step 3010, Training Loss: 0.019546478986740112\n",
            "Step 3010, Training Loss: 0.04257132485508919\n",
            "Step 3020, Training Loss: 0.33164307475090027\n",
            "Step 3020, Training Loss: 0.03113224171102047\n",
            "Step 3030, Training Loss: 0.0056742215529084206\n",
            "Step 3030, Training Loss: 0.005999532528221607\n",
            "Step 3040, Training Loss: 0.004414567723870277\n",
            "Step 3040, Training Loss: 0.03741434961557388\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 1523\n",
            "  Batch size = 8\n",
            "\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 1523\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training completed in 161.10 seconds.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='191' max='191' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [191/191 00:03]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-12-09 19:16:50,444] Trial 5 finished with value: 0.8538103838325353 and parameters: {'learning_rate': 7.729041227551434e-06, 'batch_size': 4, 'num_train_epochs': 4, 'weight_decay': 0.03470290754677491}. Best is trial 5 with value: 0.8538103838325353.\n",
            "<ipython-input-12-42465065092b>:2: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  learning_rate = trial.suggest_loguniform(\"learning_rate\", 1e-6, 5e-5)\n",
            "<ipython-input-12-42465065092b>:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  weight_decay = trial.suggest_loguniform(\"weight_decay\", 1e-4, 0.1)\n",
            "PyTorch: setting up devices\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n",
            "***** Running training *****\n",
            "  Num examples = 6091\n",
            "  Num Epochs = 8\n",
            "  Instantaneous batch size per device = 4\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
            "  Gradient Accumulation steps = 2\n",
            "  Total optimization steps = 6088\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting training...\n",
            "Step 0, Training Loss: 0.00602645892649889\n",
            "Step 0, Training Loss: 0.004525281488895416\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='6088' max='6088' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [6088/6088 05:24, Epoch 7/8]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "      <th>F1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>0.020400</td>\n",
              "      <td>0.154805</td>\n",
              "      <td>0.850952</td>\n",
              "      <td>0.857232</td>\n",
              "      <td>0.862003</td>\n",
              "      <td>0.857491</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.015900</td>\n",
              "      <td>0.159063</td>\n",
              "      <td>0.852265</td>\n",
              "      <td>0.857455</td>\n",
              "      <td>0.856868</td>\n",
              "      <td>0.853479</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.015900</td>\n",
              "      <td>0.153559</td>\n",
              "      <td>0.851609</td>\n",
              "      <td>0.860390</td>\n",
              "      <td>0.860077</td>\n",
              "      <td>0.858119</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.026400</td>\n",
              "      <td>0.165290</td>\n",
              "      <td>0.850295</td>\n",
              "      <td>0.856092</td>\n",
              "      <td>0.855584</td>\n",
              "      <td>0.851731</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.016300</td>\n",
              "      <td>0.157132</td>\n",
              "      <td>0.850295</td>\n",
              "      <td>0.858346</td>\n",
              "      <td>0.855584</td>\n",
              "      <td>0.854009</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>0.042900</td>\n",
              "      <td>0.159138</td>\n",
              "      <td>0.849639</td>\n",
              "      <td>0.858015</td>\n",
              "      <td>0.858151</td>\n",
              "      <td>0.855135</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>0.032100</td>\n",
              "      <td>0.159713</td>\n",
              "      <td>0.854235</td>\n",
              "      <td>0.859910</td>\n",
              "      <td>0.858793</td>\n",
              "      <td>0.856155</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>0.053800</td>\n",
              "      <td>0.160372</td>\n",
              "      <td>0.851609</td>\n",
              "      <td>0.859076</td>\n",
              "      <td>0.858793</td>\n",
              "      <td>0.855937</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 10, Training Loss: 0.0035174826625734568\n",
            "Step 10, Training Loss: 0.004355572629719973\n",
            "Step 20, Training Loss: 0.1717141568660736\n",
            "Step 20, Training Loss: 0.033826228231191635\n",
            "Step 30, Training Loss: 0.00482560507953167\n",
            "Step 30, Training Loss: 0.04118172824382782\n",
            "Step 40, Training Loss: 0.00601164111867547\n",
            "Step 40, Training Loss: 0.005871147848665714\n",
            "Step 50, Training Loss: 0.007362978998571634\n",
            "Step 50, Training Loss: 0.009305148385465145\n",
            "Step 60, Training Loss: 0.006794717162847519\n",
            "Step 60, Training Loss: 0.0047804065980017185\n",
            "Step 70, Training Loss: 0.008216029964387417\n",
            "Step 70, Training Loss: 0.028768066316843033\n",
            "Step 80, Training Loss: 0.026976602151989937\n",
            "Step 80, Training Loss: 0.022762726992368698\n",
            "Step 90, Training Loss: 0.004679685924202204\n",
            "Step 90, Training Loss: 0.009317437186837196\n",
            "Step 100, Training Loss: 0.0030275792814791203\n",
            "Step 100, Training Loss: 0.004037787672132254\n",
            "Step 110, Training Loss: 0.005290949251502752\n",
            "Step 110, Training Loss: 0.00610135355964303\n",
            "Step 120, Training Loss: 0.0038660212885588408\n",
            "Step 120, Training Loss: 0.05942351743578911\n",
            "Step 130, Training Loss: 0.01541760005056858\n",
            "Step 130, Training Loss: 0.004850772209465504\n",
            "Step 140, Training Loss: 0.032946329563856125\n",
            "Step 140, Training Loss: 0.05280717834830284\n",
            "Step 150, Training Loss: 0.010048866271972656\n",
            "Step 150, Training Loss: 0.1040467917919159\n",
            "Step 160, Training Loss: 0.0055335755459964275\n",
            "Step 160, Training Loss: 0.0054371594451367855\n",
            "Step 170, Training Loss: 0.00477417791262269\n",
            "Step 170, Training Loss: 0.004597787745296955\n",
            "Step 180, Training Loss: 0.05272538587450981\n",
            "Step 180, Training Loss: 0.25781553983688354\n",
            "Step 190, Training Loss: 0.09948251396417618\n",
            "Step 190, Training Loss: 0.030317146331071854\n",
            "Step 200, Training Loss: 0.004494260530918837\n",
            "Step 200, Training Loss: 0.004685621242970228\n",
            "Step 210, Training Loss: 0.1309059113264084\n",
            "Step 210, Training Loss: 0.0433313362300396\n",
            "Step 220, Training Loss: 0.023895466700196266\n",
            "Step 220, Training Loss: 0.12314363569021225\n",
            "Step 230, Training Loss: 0.004300236236304045\n",
            "Step 230, Training Loss: 0.004188248421996832\n",
            "Step 240, Training Loss: 0.007332823239266872\n",
            "Step 240, Training Loss: 0.0037456389982253313\n",
            "Step 250, Training Loss: 0.00912581104785204\n",
            "Step 250, Training Loss: 0.027819916605949402\n",
            "Step 260, Training Loss: 0.003294873284175992\n",
            "Step 260, Training Loss: 0.005766755901277065\n",
            "Step 270, Training Loss: 0.02365717478096485\n",
            "Step 270, Training Loss: 0.003802150720730424\n",
            "Step 280, Training Loss: 0.024561306461691856\n",
            "Step 280, Training Loss: 0.029793204739689827\n",
            "Step 290, Training Loss: 0.006017924752086401\n",
            "Step 290, Training Loss: 0.005450012162327766\n",
            "Step 300, Training Loss: 0.04091570898890495\n",
            "Step 300, Training Loss: 0.0052274479530751705\n",
            "Step 310, Training Loss: 0.0036305012181401253\n",
            "Step 310, Training Loss: 0.004223471507430077\n",
            "Step 320, Training Loss: 0.0032136058434844017\n",
            "Step 320, Training Loss: 0.03025195747613907\n",
            "Step 330, Training Loss: 0.004358920268714428\n",
            "Step 330, Training Loss: 0.0027290419675409794\n",
            "Step 340, Training Loss: 0.0035132402554154396\n",
            "Step 340, Training Loss: 0.020799467340111732\n",
            "Step 350, Training Loss: 0.005380454007536173\n",
            "Step 350, Training Loss: 0.0315205417573452\n",
            "Step 360, Training Loss: 0.005082958377897739\n",
            "Step 360, Training Loss: 0.006094759330153465\n",
            "Step 370, Training Loss: 0.003903042059391737\n",
            "Step 370, Training Loss: 0.02690320648252964\n",
            "Step 380, Training Loss: 0.0033771253656595945\n",
            "Step 380, Training Loss: 0.03279975801706314\n",
            "Step 390, Training Loss: 0.04231077805161476\n",
            "Step 390, Training Loss: 0.006804503500461578\n",
            "Step 400, Training Loss: 0.003589126281440258\n",
            "Step 400, Training Loss: 0.004717831499874592\n",
            "Step 410, Training Loss: 0.002663874765858054\n",
            "Step 410, Training Loss: 0.031111471354961395\n",
            "Step 420, Training Loss: 0.0045821284875273705\n",
            "Step 420, Training Loss: 0.03707941249012947\n",
            "Step 430, Training Loss: 0.3508722484111786\n",
            "Step 430, Training Loss: 0.18437975645065308\n",
            "Step 440, Training Loss: 0.006108387373387814\n",
            "Step 440, Training Loss: 0.03667416051030159\n",
            "Step 450, Training Loss: 0.007051720283925533\n",
            "Step 450, Training Loss: 0.024706168100237846\n",
            "Step 460, Training Loss: 0.004212908446788788\n",
            "Step 460, Training Loss: 0.06095244735479355\n",
            "Step 470, Training Loss: 0.02635393664240837\n",
            "Step 470, Training Loss: 0.004113650880753994\n",
            "Step 480, Training Loss: 0.029345812276005745\n",
            "Step 480, Training Loss: 0.045728638768196106\n",
            "Step 490, Training Loss: 0.2876318097114563\n",
            "Step 490, Training Loss: 0.0075839729979634285\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Saving model checkpoint to ./results/checkpoint-500\n",
            "Configuration saved in ./results/checkpoint-500/config.json\n",
            "Model weights saved in ./results/checkpoint-500/pytorch_model.bin\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 500, Training Loss: 0.038363017141819\n",
            "Step 500, Training Loss: 0.003932353109121323\n",
            "Step 510, Training Loss: 0.037002064287662506\n",
            "Step 510, Training Loss: 0.1289805918931961\n",
            "Step 520, Training Loss: 0.05485956370830536\n",
            "Step 520, Training Loss: 0.00670570507645607\n",
            "Step 530, Training Loss: 0.029521338641643524\n",
            "Step 530, Training Loss: 0.0037731716874986887\n",
            "Step 540, Training Loss: 0.0030335099436342716\n",
            "Step 540, Training Loss: 0.004870097152888775\n",
            "Step 550, Training Loss: 0.09180508553981781\n",
            "Step 550, Training Loss: 0.019090259447693825\n",
            "Step 560, Training Loss: 0.05207807198166847\n",
            "Step 560, Training Loss: 0.1760348528623581\n",
            "Step 570, Training Loss: 0.004653234034776688\n",
            "Step 570, Training Loss: 0.025394229218363762\n",
            "Step 580, Training Loss: 0.02820161171257496\n",
            "Step 580, Training Loss: 0.005834835581481457\n",
            "Step 590, Training Loss: 0.0037859883159399033\n",
            "Step 590, Training Loss: 0.005066788289695978\n",
            "Step 600, Training Loss: 0.11185500025749207\n",
            "Step 600, Training Loss: 0.033416859805583954\n",
            "Step 610, Training Loss: 0.031566306948661804\n",
            "Step 610, Training Loss: 0.3300982415676117\n",
            "Step 620, Training Loss: 0.1013716533780098\n",
            "Step 620, Training Loss: 0.008845618925988674\n",
            "Step 630, Training Loss: 0.0033453868236392736\n",
            "Step 630, Training Loss: 0.06139669939875603\n",
            "Step 640, Training Loss: 0.0040092929266393185\n",
            "Step 640, Training Loss: 0.021764609962701797\n",
            "Step 650, Training Loss: 0.0037374284584075212\n",
            "Step 650, Training Loss: 0.3664303719997406\n",
            "Step 660, Training Loss: 0.023961199447512627\n",
            "Step 660, Training Loss: 0.004945984575897455\n",
            "Step 670, Training Loss: 0.0054039256647229195\n",
            "Step 670, Training Loss: 0.03560137376189232\n",
            "Step 680, Training Loss: 0.018439657986164093\n",
            "Step 680, Training Loss: 0.39114049077033997\n",
            "Step 690, Training Loss: 0.024823524057865143\n",
            "Step 690, Training Loss: 0.003491305746138096\n",
            "Step 700, Training Loss: 0.009368209168314934\n",
            "Step 700, Training Loss: 0.03801392391324043\n",
            "Step 710, Training Loss: 0.0860888883471489\n",
            "Step 710, Training Loss: 0.02570704184472561\n",
            "Step 720, Training Loss: 0.004434892907738686\n",
            "Step 720, Training Loss: 0.14112551510334015\n",
            "Step 730, Training Loss: 0.004227028228342533\n",
            "Step 730, Training Loss: 0.0034897879231721163\n",
            "Step 740, Training Loss: 0.00375966913998127\n",
            "Step 740, Training Loss: 0.005172581411898136\n",
            "Step 750, Training Loss: 0.0039622061885893345\n",
            "Step 750, Training Loss: 0.04656357318162918\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 1523\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 760, Training Loss: 0.0035320869646966457\n",
            "Step 760, Training Loss: 0.002648201771080494\n",
            "Step 770, Training Loss: 0.0038652338553220034\n",
            "Step 770, Training Loss: 0.026528675109148026\n",
            "Step 780, Training Loss: 0.004553158767521381\n",
            "Step 780, Training Loss: 0.00339475367218256\n",
            "Step 790, Training Loss: 0.0031038830056786537\n",
            "Step 790, Training Loss: 0.0035306839272379875\n",
            "Step 800, Training Loss: 0.023680295795202255\n",
            "Step 800, Training Loss: 0.030728623270988464\n",
            "Step 810, Training Loss: 0.0041123079136013985\n",
            "Step 810, Training Loss: 0.006323751527816057\n",
            "Step 820, Training Loss: 0.008082060143351555\n",
            "Step 820, Training Loss: 0.005362153984606266\n",
            "Step 830, Training Loss: 0.008688144385814667\n",
            "Step 830, Training Loss: 0.009349181316792965\n",
            "Step 840, Training Loss: 0.020129410549998283\n",
            "Step 840, Training Loss: 0.005098605994135141\n",
            "Step 850, Training Loss: 0.005629447754472494\n",
            "Step 850, Training Loss: 0.04532211273908615\n",
            "Step 860, Training Loss: 0.0033782871905714273\n",
            "Step 860, Training Loss: 0.027356546372175217\n",
            "Step 870, Training Loss: 0.0321669802069664\n",
            "Step 870, Training Loss: 0.056746941059827805\n",
            "Step 880, Training Loss: 0.004226114600896835\n",
            "Step 880, Training Loss: 0.0285628754645586\n",
            "Step 890, Training Loss: 0.005467248149216175\n",
            "Step 890, Training Loss: 0.11681761592626572\n",
            "Step 900, Training Loss: 0.10344661772251129\n",
            "Step 900, Training Loss: 0.003654514905065298\n",
            "Step 910, Training Loss: 0.05116515979170799\n",
            "Step 910, Training Loss: 0.0206398144364357\n",
            "Step 920, Training Loss: 0.04923947528004646\n",
            "Step 920, Training Loss: 0.004816732835024595\n",
            "Step 930, Training Loss: 0.004979436751455069\n",
            "Step 930, Training Loss: 0.005111898295581341\n",
            "Step 940, Training Loss: 0.0032670446671545506\n",
            "Step 940, Training Loss: 0.07948087900876999\n",
            "Step 950, Training Loss: 0.024173865094780922\n",
            "Step 950, Training Loss: 0.022206967696547508\n",
            "Step 960, Training Loss: 0.03563569486141205\n",
            "Step 960, Training Loss: 0.0038596841040998697\n",
            "Step 970, Training Loss: 0.002793292049318552\n",
            "Step 970, Training Loss: 0.004353612195700407\n",
            "Step 980, Training Loss: 0.30452805757522583\n",
            "Step 980, Training Loss: 0.007832164876163006\n",
            "Step 990, Training Loss: 0.17651444673538208\n",
            "Step 990, Training Loss: 0.05846194177865982\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Saving model checkpoint to ./results/checkpoint-1000\n",
            "Configuration saved in ./results/checkpoint-1000/config.json\n",
            "Model weights saved in ./results/checkpoint-1000/pytorch_model.bin\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 1000, Training Loss: 0.004144451580941677\n",
            "Step 1000, Training Loss: 0.005122813396155834\n",
            "Step 1010, Training Loss: 0.02596919611096382\n",
            "Step 1010, Training Loss: 0.0033311699517071247\n",
            "Step 1020, Training Loss: 0.058311644941568375\n",
            "Step 1020, Training Loss: 0.004534667357802391\n",
            "Step 1030, Training Loss: 0.02008265070617199\n",
            "Step 1030, Training Loss: 0.03159938007593155\n",
            "Step 1040, Training Loss: 0.0028183339163661003\n",
            "Step 1040, Training Loss: 0.018815314397215843\n",
            "Step 1050, Training Loss: 0.010992324911057949\n",
            "Step 1050, Training Loss: 0.024886494502425194\n",
            "Step 1060, Training Loss: 0.019268764182925224\n",
            "Step 1060, Training Loss: 0.002589436015114188\n",
            "Step 1070, Training Loss: 0.004775366745889187\n",
            "Step 1070, Training Loss: 0.026703201234340668\n",
            "Step 1080, Training Loss: 0.00517124542966485\n",
            "Step 1080, Training Loss: 0.026256443932652473\n",
            "Step 1090, Training Loss: 0.00505815539509058\n",
            "Step 1090, Training Loss: 0.0027493112720549107\n",
            "Step 1100, Training Loss: 0.004881867673248053\n",
            "Step 1100, Training Loss: 0.004092535004019737\n",
            "Step 1110, Training Loss: 0.004411166999489069\n",
            "Step 1110, Training Loss: 0.0047417678870260715\n",
            "Step 1120, Training Loss: 0.005724020767956972\n",
            "Step 1120, Training Loss: 0.1096075028181076\n",
            "Step 1130, Training Loss: 0.006603382993489504\n",
            "Step 1130, Training Loss: 0.008113816380500793\n",
            "Step 1140, Training Loss: 0.0032928676810115576\n",
            "Step 1140, Training Loss: 0.004577815067023039\n",
            "Step 1150, Training Loss: 0.008357461541891098\n",
            "Step 1150, Training Loss: 0.38875052332878113\n",
            "Step 1160, Training Loss: 0.029832106083631516\n",
            "Step 1160, Training Loss: 0.002654680283740163\n",
            "Step 1170, Training Loss: 0.2856937348842621\n",
            "Step 1170, Training Loss: 0.02572406269609928\n",
            "Step 1180, Training Loss: 0.007289965637028217\n",
            "Step 1180, Training Loss: 0.022690845653414726\n",
            "Step 1190, Training Loss: 0.06820909678936005\n",
            "Step 1190, Training Loss: 0.10428187996149063\n",
            "Step 1200, Training Loss: 0.019239608198404312\n",
            "Step 1200, Training Loss: 0.010417921468615532\n",
            "Step 1210, Training Loss: 0.02798977494239807\n",
            "Step 1210, Training Loss: 0.008036183193325996\n",
            "Step 1220, Training Loss: 0.11952552199363708\n",
            "Step 1220, Training Loss: 0.10694137960672379\n",
            "Step 1230, Training Loss: 0.15129297971725464\n",
            "Step 1230, Training Loss: 0.021940860897302628\n",
            "Step 1240, Training Loss: 0.0317244753241539\n",
            "Step 1240, Training Loss: 0.026984820142388344\n",
            "Step 1250, Training Loss: 0.0031081971246749163\n",
            "Step 1250, Training Loss: 0.40917637944221497\n",
            "Step 1260, Training Loss: 0.0071449619717895985\n",
            "Step 1260, Training Loss: 0.004071894101798534\n",
            "Step 1270, Training Loss: 0.21348366141319275\n",
            "Step 1270, Training Loss: 0.002837103558704257\n",
            "Step 1280, Training Loss: 0.08158287405967712\n",
            "Step 1280, Training Loss: 0.00708407536149025\n",
            "Step 1290, Training Loss: 0.004566581454128027\n",
            "Step 1290, Training Loss: 0.02402736060321331\n",
            "Step 1300, Training Loss: 0.0049270628951489925\n",
            "Step 1300, Training Loss: 0.004407945089042187\n",
            "Step 1310, Training Loss: 0.0891403928399086\n",
            "Step 1310, Training Loss: 0.43254008889198303\n",
            "Step 1320, Training Loss: 0.005785023793578148\n",
            "Step 1320, Training Loss: 0.007473375648260117\n",
            "Step 1330, Training Loss: 0.05226197466254234\n",
            "Step 1330, Training Loss: 0.01958276890218258\n",
            "Step 1340, Training Loss: 0.0034507636446505785\n",
            "Step 1340, Training Loss: 0.0025090829003602266\n",
            "Step 1350, Training Loss: 0.005377242807298899\n",
            "Step 1350, Training Loss: 0.008023380301892757\n",
            "Step 1360, Training Loss: 0.059930868446826935\n",
            "Step 1360, Training Loss: 0.0719464123249054\n",
            "Step 1370, Training Loss: 0.050754111260175705\n",
            "Step 1370, Training Loss: 0.019533460959792137\n",
            "Step 1380, Training Loss: 0.031370628625154495\n",
            "Step 1380, Training Loss: 0.06617289036512375\n",
            "Step 1390, Training Loss: 0.5636179447174072\n",
            "Step 1390, Training Loss: 0.020131774246692657\n",
            "Step 1400, Training Loss: 0.018437378108501434\n",
            "Step 1400, Training Loss: 0.020004818215966225\n",
            "Step 1410, Training Loss: 0.003677347907796502\n",
            "Step 1410, Training Loss: 0.004945894703269005\n",
            "Step 1420, Training Loss: 0.0038934703916311264\n",
            "Step 1420, Training Loss: 0.003850427223369479\n",
            "Step 1430, Training Loss: 0.07396133244037628\n",
            "Step 1430, Training Loss: 0.02701522782444954\n",
            "Step 1440, Training Loss: 0.009018129669129848\n",
            "Step 1440, Training Loss: 0.0040288446471095085\n",
            "Step 1450, Training Loss: 0.0040662349201738834\n",
            "Step 1450, Training Loss: 0.003453262848779559\n",
            "Step 1460, Training Loss: 0.003833206370472908\n",
            "Step 1460, Training Loss: 0.003065455239266157\n",
            "Step 1470, Training Loss: 0.004964049439877272\n",
            "Step 1470, Training Loss: 0.01633879914879799\n",
            "Step 1480, Training Loss: 0.003202692838385701\n",
            "Step 1480, Training Loss: 0.020796168595552444\n",
            "Step 1490, Training Loss: 0.026723425835371017\n",
            "Step 1490, Training Loss: 0.06173090264201164\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Saving model checkpoint to ./results/checkpoint-1500\n",
            "Configuration saved in ./results/checkpoint-1500/config.json\n",
            "Model weights saved in ./results/checkpoint-1500/pytorch_model.bin\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 1500, Training Loss: 0.03539394587278366\n",
            "Step 1500, Training Loss: 0.12847229838371277\n",
            "Step 1510, Training Loss: 0.0047663552686572075\n",
            "Step 1510, Training Loss: 0.004266923293471336\n",
            "Step 1520, Training Loss: 0.02796081453561783\n",
            "Step 1520, Training Loss: 0.005793761927634478\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 1523\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 1530, Training Loss: 0.00383848138153553\n",
            "Step 1530, Training Loss: 0.02407204546034336\n",
            "Step 1540, Training Loss: 0.004392234142869711\n",
            "Step 1540, Training Loss: 0.006865070667117834\n",
            "Step 1550, Training Loss: 0.004483788274228573\n",
            "Step 1550, Training Loss: 0.12372604757547379\n",
            "Step 1560, Training Loss: 0.005507391411811113\n",
            "Step 1560, Training Loss: 0.026937803253531456\n",
            "Step 1570, Training Loss: 0.0031363607849925756\n",
            "Step 1570, Training Loss: 0.0028378425631672144\n",
            "Step 1580, Training Loss: 0.0445236898958683\n",
            "Step 1580, Training Loss: 0.004689115099608898\n",
            "Step 1590, Training Loss: 0.007864627055823803\n",
            "Step 1590, Training Loss: 0.003393394872546196\n",
            "Step 1600, Training Loss: 0.008258633315563202\n",
            "Step 1600, Training Loss: 0.012389559298753738\n",
            "Step 1610, Training Loss: 0.05114351958036423\n",
            "Step 1610, Training Loss: 0.0255439393222332\n",
            "Step 1620, Training Loss: 0.018745874986052513\n",
            "Step 1620, Training Loss: 0.019055098295211792\n",
            "Step 1630, Training Loss: 0.005239512305706739\n",
            "Step 1630, Training Loss: 0.06856285780668259\n",
            "Step 1640, Training Loss: 0.004115929827094078\n",
            "Step 1640, Training Loss: 0.007015596143901348\n",
            "Step 1650, Training Loss: 0.003991208039224148\n",
            "Step 1650, Training Loss: 0.05001939460635185\n",
            "Step 1660, Training Loss: 0.00487010320648551\n",
            "Step 1660, Training Loss: 0.003723583649843931\n",
            "Step 1670, Training Loss: 0.008178389631211758\n",
            "Step 1670, Training Loss: 0.03481188043951988\n",
            "Step 1680, Training Loss: 0.0041443645022809505\n",
            "Step 1680, Training Loss: 0.0042159054428339005\n",
            "Step 1690, Training Loss: 0.0035137541126459837\n",
            "Step 1690, Training Loss: 0.026751918718218803\n",
            "Step 1700, Training Loss: 0.003892419161275029\n",
            "Step 1700, Training Loss: 0.004618711303919554\n",
            "Step 1710, Training Loss: 0.0030519613064825535\n",
            "Step 1710, Training Loss: 0.0034846412017941475\n",
            "Step 1720, Training Loss: 0.008090350776910782\n",
            "Step 1720, Training Loss: 0.027634114027023315\n",
            "Step 1730, Training Loss: 0.028098542243242264\n",
            "Step 1730, Training Loss: 0.034780897200107574\n",
            "Step 1740, Training Loss: 0.005061844829469919\n",
            "Step 1740, Training Loss: 0.003401112975552678\n",
            "Step 1750, Training Loss: 0.15727490186691284\n",
            "Step 1750, Training Loss: 0.0403653047978878\n",
            "Step 1760, Training Loss: 0.0340379923582077\n",
            "Step 1760, Training Loss: 0.058775074779987335\n",
            "Step 1770, Training Loss: 0.01728786900639534\n",
            "Step 1770, Training Loss: 0.2223576456308365\n",
            "Step 1780, Training Loss: 0.06371811032295227\n",
            "Step 1780, Training Loss: 0.0030614559073001146\n",
            "Step 1790, Training Loss: 0.0031741480343043804\n",
            "Step 1790, Training Loss: 0.16344335675239563\n",
            "Step 1800, Training Loss: 0.13437522947788239\n",
            "Step 1800, Training Loss: 0.003513730363920331\n",
            "Step 1810, Training Loss: 0.17584697902202606\n",
            "Step 1810, Training Loss: 0.004904827103018761\n",
            "Step 1820, Training Loss: 0.004301588516682386\n",
            "Step 1820, Training Loss: 0.0065972451120615005\n",
            "Step 1830, Training Loss: 0.09178560227155685\n",
            "Step 1830, Training Loss: 0.005964608397334814\n",
            "Step 1840, Training Loss: 0.006332234013825655\n",
            "Step 1840, Training Loss: 0.02473348006606102\n",
            "Step 1850, Training Loss: 0.004514084663242102\n",
            "Step 1850, Training Loss: 0.002852210309356451\n",
            "Step 1860, Training Loss: 0.029316658154129982\n",
            "Step 1860, Training Loss: 0.10232194513082504\n",
            "Step 1870, Training Loss: 0.0032296606805175543\n",
            "Step 1870, Training Loss: 0.023503687232732773\n",
            "Step 1880, Training Loss: 0.0038543606642633677\n",
            "Step 1880, Training Loss: 0.01112814899533987\n",
            "Step 1890, Training Loss: 0.18288961052894592\n",
            "Step 1890, Training Loss: 0.003416557563468814\n",
            "Step 1900, Training Loss: 0.026683984324336052\n",
            "Step 1900, Training Loss: 0.031331177800893784\n",
            "Step 1910, Training Loss: 0.005109061487019062\n",
            "Step 1910, Training Loss: 0.023375971242785454\n",
            "Step 1920, Training Loss: 0.00351134967058897\n",
            "Step 1920, Training Loss: 0.0036918469704687595\n",
            "Step 1930, Training Loss: 0.0026729644741863012\n",
            "Step 1930, Training Loss: 0.0037275846116244793\n",
            "Step 1940, Training Loss: 0.0026948475278913975\n",
            "Step 1940, Training Loss: 0.004278318956494331\n",
            "Step 1950, Training Loss: 0.5111396312713623\n",
            "Step 1950, Training Loss: 0.009416662156581879\n",
            "Step 1960, Training Loss: 0.0026962854899466038\n",
            "Step 1960, Training Loss: 0.04615895077586174\n",
            "Step 1970, Training Loss: 0.02728494256734848\n",
            "Step 1970, Training Loss: 0.007823207415640354\n",
            "Step 1980, Training Loss: 0.0035742088221013546\n",
            "Step 1980, Training Loss: 0.1903204470872879\n",
            "Step 1990, Training Loss: 0.003576404880732298\n",
            "Step 1990, Training Loss: 0.030722083523869514\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Saving model checkpoint to ./results/checkpoint-2000\n",
            "Configuration saved in ./results/checkpoint-2000/config.json\n",
            "Model weights saved in ./results/checkpoint-2000/pytorch_model.bin\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 2000, Training Loss: 0.17688021063804626\n",
            "Step 2000, Training Loss: 0.009474007412791252\n",
            "Step 2010, Training Loss: 0.007018637377768755\n",
            "Step 2010, Training Loss: 0.006956418044865131\n",
            "Step 2020, Training Loss: 0.0026283913757652044\n",
            "Step 2020, Training Loss: 0.00321027310565114\n",
            "Step 2030, Training Loss: 0.05531446263194084\n",
            "Step 2030, Training Loss: 0.2923107147216797\n",
            "Step 2040, Training Loss: 0.004086821340024471\n",
            "Step 2040, Training Loss: 0.026241352781653404\n",
            "Step 2050, Training Loss: 0.004433990456163883\n",
            "Step 2050, Training Loss: 0.005624174140393734\n",
            "Step 2060, Training Loss: 0.07365380972623825\n",
            "Step 2060, Training Loss: 0.0053854831494390965\n",
            "Step 2070, Training Loss: 0.004359787795692682\n",
            "Step 2070, Training Loss: 0.027641907334327698\n",
            "Step 2080, Training Loss: 0.19644466042518616\n",
            "Step 2080, Training Loss: 0.36223286390304565\n",
            "Step 2090, Training Loss: 0.39759570360183716\n",
            "Step 2090, Training Loss: 0.006915587931871414\n",
            "Step 2100, Training Loss: 0.061105381697416306\n",
            "Step 2100, Training Loss: 0.004432613030076027\n",
            "Step 2110, Training Loss: 0.004798380192369223\n",
            "Step 2110, Training Loss: 0.006139630917459726\n",
            "Step 2120, Training Loss: 0.004304409958422184\n",
            "Step 2120, Training Loss: 0.004420335404574871\n",
            "Step 2130, Training Loss: 0.10432032495737076\n",
            "Step 2130, Training Loss: 0.004748760722577572\n",
            "Step 2140, Training Loss: 0.003860805183649063\n",
            "Step 2140, Training Loss: 0.003912176005542278\n",
            "Step 2150, Training Loss: 0.003641041461378336\n",
            "Step 2150, Training Loss: 0.0037428056821227074\n",
            "Step 2160, Training Loss: 0.028923217207193375\n",
            "Step 2160, Training Loss: 0.059900831431150436\n",
            "Step 2170, Training Loss: 0.021955925971269608\n",
            "Step 2170, Training Loss: 0.004670500755310059\n",
            "Step 2180, Training Loss: 0.2959374189376831\n",
            "Step 2180, Training Loss: 0.0036968709900975227\n",
            "Step 2190, Training Loss: 0.0042633917182683945\n",
            "Step 2190, Training Loss: 0.00374743458814919\n",
            "Step 2200, Training Loss: 0.00487933773547411\n",
            "Step 2200, Training Loss: 0.0030882039573043585\n",
            "Step 2210, Training Loss: 0.02092752791941166\n",
            "Step 2210, Training Loss: 0.009899723343551159\n",
            "Step 2220, Training Loss: 0.020809194073081017\n",
            "Step 2220, Training Loss: 0.03144042193889618\n",
            "Step 2230, Training Loss: 0.0038335330318659544\n",
            "Step 2230, Training Loss: 0.005257992539554834\n",
            "Step 2240, Training Loss: 0.004897670354694128\n",
            "Step 2240, Training Loss: 0.008745821192860603\n",
            "Step 2250, Training Loss: 0.01789247617125511\n",
            "Step 2250, Training Loss: 0.014591493643820286\n",
            "Step 2260, Training Loss: 0.003805904183536768\n",
            "Step 2260, Training Loss: 0.005966325290501118\n",
            "Step 2270, Training Loss: 0.061834730207920074\n",
            "Step 2270, Training Loss: 0.00488789938390255\n",
            "Step 2280, Training Loss: 0.017732907086610794\n",
            "Step 2280, Training Loss: 0.02801414579153061\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 1523\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 2290, Training Loss: 0.005029441323131323\n",
            "Step 2290, Training Loss: 0.009967302903532982\n",
            "Step 2300, Training Loss: 0.0027083156164735556\n",
            "Step 2300, Training Loss: 0.006392438430339098\n",
            "Step 2310, Training Loss: 0.004915526136755943\n",
            "Step 2310, Training Loss: 0.004224071744829416\n",
            "Step 2320, Training Loss: 0.006193650886416435\n",
            "Step 2320, Training Loss: 0.02474471740424633\n",
            "Step 2330, Training Loss: 0.0025662737898528576\n",
            "Step 2330, Training Loss: 0.006693453993648291\n",
            "Step 2340, Training Loss: 0.004268587101250887\n",
            "Step 2340, Training Loss: 0.02229059487581253\n",
            "Step 2350, Training Loss: 0.0035703396424651146\n",
            "Step 2350, Training Loss: 0.003267157357186079\n",
            "Step 2360, Training Loss: 0.0035802191123366356\n",
            "Step 2360, Training Loss: 0.004070958122611046\n",
            "Step 2370, Training Loss: 0.12393242120742798\n",
            "Step 2370, Training Loss: 0.0428340919315815\n",
            "Step 2380, Training Loss: 0.027362501248717308\n",
            "Step 2380, Training Loss: 0.0025218012742698193\n",
            "Step 2390, Training Loss: 0.054811883717775345\n",
            "Step 2390, Training Loss: 0.007742443587630987\n",
            "Step 2400, Training Loss: 0.01984550431370735\n",
            "Step 2400, Training Loss: 0.002648830646649003\n",
            "Step 2410, Training Loss: 0.0256673451513052\n",
            "Step 2410, Training Loss: 0.033054426312446594\n",
            "Step 2420, Training Loss: 0.007379189599305391\n",
            "Step 2420, Training Loss: 0.008932407014071941\n",
            "Step 2430, Training Loss: 0.0035820151679217815\n",
            "Step 2430, Training Loss: 0.004530392587184906\n",
            "Step 2440, Training Loss: 0.008944368921220303\n",
            "Step 2440, Training Loss: 0.0033919482957571745\n",
            "Step 2450, Training Loss: 0.003113757586106658\n",
            "Step 2450, Training Loss: 0.010149545036256313\n",
            "Step 2460, Training Loss: 0.019583627581596375\n",
            "Step 2460, Training Loss: 0.02202555723488331\n",
            "Step 2470, Training Loss: 0.32835593819618225\n",
            "Step 2470, Training Loss: 0.004147322848439217\n",
            "Step 2480, Training Loss: 0.00733809033408761\n",
            "Step 2480, Training Loss: 0.004205575678497553\n",
            "Step 2490, Training Loss: 0.35873717069625854\n",
            "Step 2490, Training Loss: 0.0034708280581980944\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Saving model checkpoint to ./results/checkpoint-2500\n",
            "Configuration saved in ./results/checkpoint-2500/config.json\n",
            "Model weights saved in ./results/checkpoint-2500/pytorch_model.bin\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 2500, Training Loss: 0.004578312858939171\n",
            "Step 2500, Training Loss: 0.17571710050106049\n",
            "Step 2510, Training Loss: 0.0038903451059013605\n",
            "Step 2510, Training Loss: 0.0045099505223333836\n",
            "Step 2520, Training Loss: 0.0036640337202697992\n",
            "Step 2520, Training Loss: 0.07801501452922821\n",
            "Step 2530, Training Loss: 0.07178910076618195\n",
            "Step 2530, Training Loss: 0.004347627516835928\n",
            "Step 2540, Training Loss: 0.006861541420221329\n",
            "Step 2540, Training Loss: 0.0030645192600786686\n",
            "Step 2550, Training Loss: 0.04054558277130127\n",
            "Step 2550, Training Loss: 0.024853432551026344\n",
            "Step 2560, Training Loss: 0.030499741435050964\n",
            "Step 2560, Training Loss: 0.00802934542298317\n",
            "Step 2570, Training Loss: 0.020069951191544533\n",
            "Step 2570, Training Loss: 0.008454794064164162\n",
            "Step 2580, Training Loss: 0.16453222930431366\n",
            "Step 2580, Training Loss: 0.003941831178963184\n",
            "Step 2590, Training Loss: 0.007906015031039715\n",
            "Step 2590, Training Loss: 0.0253988578915596\n",
            "Step 2600, Training Loss: 0.04809166118502617\n",
            "Step 2600, Training Loss: 0.02940717525780201\n",
            "Step 2610, Training Loss: 0.003182586980983615\n",
            "Step 2610, Training Loss: 0.20697881281375885\n",
            "Step 2620, Training Loss: 0.042602378875017166\n",
            "Step 2620, Training Loss: 0.05812859907746315\n",
            "Step 2630, Training Loss: 0.0054429373703897\n",
            "Step 2630, Training Loss: 0.0052712648175656796\n",
            "Step 2640, Training Loss: 0.04217495024204254\n",
            "Step 2640, Training Loss: 0.02990294061601162\n",
            "Step 2650, Training Loss: 0.06329485774040222\n",
            "Step 2650, Training Loss: 0.002339296042919159\n",
            "Step 2660, Training Loss: 0.005123979412019253\n",
            "Step 2660, Training Loss: 0.003012092085555196\n",
            "Step 2670, Training Loss: 0.004970373120158911\n",
            "Step 2670, Training Loss: 0.002427662257105112\n",
            "Step 2680, Training Loss: 0.026252921670675278\n",
            "Step 2680, Training Loss: 0.004071585834026337\n",
            "Step 2690, Training Loss: 0.0029627440962940454\n",
            "Step 2690, Training Loss: 0.06742982566356659\n",
            "Step 2700, Training Loss: 0.005116564687341452\n",
            "Step 2700, Training Loss: 0.0024822631385177374\n",
            "Step 2710, Training Loss: 0.023312222212553024\n",
            "Step 2710, Training Loss: 0.08531494438648224\n",
            "Step 2720, Training Loss: 0.0029673671815544367\n",
            "Step 2720, Training Loss: 0.004795385524630547\n",
            "Step 2730, Training Loss: 0.005406036041676998\n",
            "Step 2730, Training Loss: 0.004181857220828533\n",
            "Step 2740, Training Loss: 0.058117419481277466\n",
            "Step 2740, Training Loss: 0.003357694949954748\n",
            "Step 2750, Training Loss: 0.07121773809194565\n",
            "Step 2750, Training Loss: 0.2817689776420593\n",
            "Step 2760, Training Loss: 0.004867366049438715\n",
            "Step 2760, Training Loss: 0.0051026190631091595\n",
            "Step 2770, Training Loss: 0.004613739904016256\n",
            "Step 2770, Training Loss: 0.0042323097586631775\n",
            "Step 2780, Training Loss: 0.05965743958950043\n",
            "Step 2780, Training Loss: 0.007864335551857948\n",
            "Step 2790, Training Loss: 0.007952388375997543\n",
            "Step 2790, Training Loss: 0.01667395979166031\n",
            "Step 2800, Training Loss: 0.004600293468683958\n",
            "Step 2800, Training Loss: 0.05890969559550285\n",
            "Step 2810, Training Loss: 0.0805215984582901\n",
            "Step 2810, Training Loss: 0.004971698857843876\n",
            "Step 2820, Training Loss: 0.005351398605853319\n",
            "Step 2820, Training Loss: 0.002552392892539501\n",
            "Step 2830, Training Loss: 0.09709306806325912\n",
            "Step 2830, Training Loss: 0.0027190239634364843\n",
            "Step 2840, Training Loss: 0.003856795374304056\n",
            "Step 2840, Training Loss: 0.08917880803346634\n",
            "Step 2850, Training Loss: 0.004056874196976423\n",
            "Step 2850, Training Loss: 0.010082576423883438\n",
            "Step 2860, Training Loss: 0.005577647592872381\n",
            "Step 2860, Training Loss: 0.004417258780449629\n",
            "Step 2870, Training Loss: 0.003923362120985985\n",
            "Step 2870, Training Loss: 0.004089078865945339\n",
            "Step 2880, Training Loss: 0.011889230459928513\n",
            "Step 2880, Training Loss: 0.0049089896492660046\n",
            "Step 2890, Training Loss: 0.024509450420737267\n",
            "Step 2890, Training Loss: 0.029976041987538338\n",
            "Step 2900, Training Loss: 0.0035998194944113493\n",
            "Step 2900, Training Loss: 0.0043280841782689095\n",
            "Step 2910, Training Loss: 0.003806385677307844\n",
            "Step 2910, Training Loss: 0.1712297797203064\n",
            "Step 2920, Training Loss: 0.003495588432997465\n",
            "Step 2920, Training Loss: 0.008051896467804909\n",
            "Step 2930, Training Loss: 0.008213957771658897\n",
            "Step 2930, Training Loss: 0.016899671405553818\n",
            "Step 2940, Training Loss: 0.0033729062415659428\n",
            "Step 2940, Training Loss: 0.15921013057231903\n",
            "Step 2950, Training Loss: 0.013411356136202812\n",
            "Step 2950, Training Loss: 0.1124282255768776\n",
            "Step 2960, Training Loss: 0.030350778251886368\n",
            "Step 2960, Training Loss: 0.030575592070817947\n",
            "Step 2970, Training Loss: 0.004403701983392239\n",
            "Step 2970, Training Loss: 0.021903591230511665\n",
            "Step 2980, Training Loss: 0.26304036378860474\n",
            "Step 2980, Training Loss: 0.007204823195934296\n",
            "Step 2990, Training Loss: 0.06917720288038254\n",
            "Step 2990, Training Loss: 0.004511108156293631\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Saving model checkpoint to ./results/checkpoint-3000\n",
            "Configuration saved in ./results/checkpoint-3000/config.json\n",
            "Model weights saved in ./results/checkpoint-3000/pytorch_model.bin\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 3000, Training Loss: 0.02360442467033863\n",
            "Step 3000, Training Loss: 0.005758472718298435\n",
            "Step 3010, Training Loss: 0.011251727119088173\n",
            "Step 3010, Training Loss: 0.029674045741558075\n",
            "Step 3020, Training Loss: 0.33447733521461487\n",
            "Step 3020, Training Loss: 0.023392362520098686\n",
            "Step 3030, Training Loss: 0.00493013858795166\n",
            "Step 3030, Training Loss: 0.005505135282874107\n",
            "Step 3040, Training Loss: 0.003798247082158923\n",
            "Step 3040, Training Loss: 0.031230347231030464\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 1523\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 3050, Training Loss: 0.0069308956153690815\n",
            "Step 3050, Training Loss: 0.003743598936125636\n",
            "Step 3060, Training Loss: 0.007989587262272835\n",
            "Step 3060, Training Loss: 0.004513198975473642\n",
            "Step 3070, Training Loss: 0.0032662979792803526\n",
            "Step 3070, Training Loss: 0.08002018928527832\n",
            "Step 3080, Training Loss: 0.04804347828030586\n",
            "Step 3080, Training Loss: 0.0058066765777766705\n",
            "Step 3090, Training Loss: 0.008915810845792294\n",
            "Step 3090, Training Loss: 0.004791910760104656\n",
            "Step 3100, Training Loss: 0.016747837886214256\n",
            "Step 3100, Training Loss: 0.05339411273598671\n",
            "Step 3110, Training Loss: 0.021055491641163826\n",
            "Step 3110, Training Loss: 0.00384198734536767\n",
            "Step 3120, Training Loss: 0.002872566692531109\n",
            "Step 3120, Training Loss: 0.024851953610777855\n",
            "Step 3130, Training Loss: 0.005468686111271381\n",
            "Step 3130, Training Loss: 0.006413266994059086\n",
            "Step 3140, Training Loss: 0.0042203268967568874\n",
            "Step 3140, Training Loss: 0.020249327644705772\n",
            "Step 3150, Training Loss: 0.002703455975279212\n",
            "Step 3150, Training Loss: 0.036757007241249084\n",
            "Step 3160, Training Loss: 0.06173969432711601\n",
            "Step 3160, Training Loss: 0.0036747492849826813\n",
            "Step 3170, Training Loss: 0.01763179898262024\n",
            "Step 3170, Training Loss: 0.004632934927940369\n",
            "Step 3180, Training Loss: 0.0034363954328000546\n",
            "Step 3180, Training Loss: 0.0024625055957585573\n",
            "Step 3190, Training Loss: 0.015379436314105988\n",
            "Step 3190, Training Loss: 0.0029654554091393948\n",
            "Step 3200, Training Loss: 0.09999474138021469\n",
            "Step 3200, Training Loss: 0.1794147789478302\n",
            "Step 3210, Training Loss: 0.15393492579460144\n",
            "Step 3210, Training Loss: 0.04850056394934654\n",
            "Step 3220, Training Loss: 0.0072305817157030106\n",
            "Step 3220, Training Loss: 0.02192601189017296\n",
            "Step 3230, Training Loss: 0.002830761717632413\n",
            "Step 3230, Training Loss: 0.004216252826154232\n",
            "Step 3240, Training Loss: 0.05024654418230057\n",
            "Step 3240, Training Loss: 0.003307776292786002\n",
            "Step 3250, Training Loss: 0.006117978598922491\n",
            "Step 3250, Training Loss: 0.1583520472049713\n",
            "Step 3260, Training Loss: 0.00363513664342463\n",
            "Step 3260, Training Loss: 0.02184327132999897\n",
            "Step 3270, Training Loss: 0.36425694823265076\n",
            "Step 3270, Training Loss: 0.04561679810285568\n",
            "Step 3280, Training Loss: 0.0032711823005229235\n",
            "Step 3280, Training Loss: 0.045873384922742844\n",
            "Step 3290, Training Loss: 0.025444449856877327\n",
            "Step 3290, Training Loss: 0.003371995408087969\n",
            "Step 3300, Training Loss: 0.008032896555960178\n",
            "Step 3300, Training Loss: 0.024131355807185173\n",
            "Step 3310, Training Loss: 0.03266973793506622\n",
            "Step 3310, Training Loss: 0.0030215950682759285\n",
            "Step 3320, Training Loss: 0.00407089339569211\n",
            "Step 3320, Training Loss: 0.0038856850005686283\n",
            "Step 3330, Training Loss: 0.36499011516571045\n",
            "Step 3330, Training Loss: 0.002551804529502988\n",
            "Step 3340, Training Loss: 0.022400831803679466\n",
            "Step 3340, Training Loss: 0.004110670182853937\n",
            "Step 3350, Training Loss: 0.007138124201446772\n",
            "Step 3350, Training Loss: 0.2305546998977661\n",
            "Step 3360, Training Loss: 0.004016574006527662\n",
            "Step 3360, Training Loss: 0.08198926597833633\n",
            "Step 3370, Training Loss: 0.29910269379615784\n",
            "Step 3370, Training Loss: 0.011409622617065907\n",
            "Step 3380, Training Loss: 0.008929206058382988\n",
            "Step 3380, Training Loss: 0.008970106951892376\n",
            "Step 3390, Training Loss: 0.34367382526397705\n",
            "Step 3390, Training Loss: 0.005080775357782841\n",
            "Step 3400, Training Loss: 0.09719978272914886\n",
            "Step 3400, Training Loss: 0.3390102982521057\n",
            "Step 3410, Training Loss: 0.0026588919572532177\n",
            "Step 3410, Training Loss: 0.0036089634522795677\n",
            "Step 3420, Training Loss: 0.006753914523869753\n",
            "Step 3420, Training Loss: 0.0032289994414895773\n",
            "Step 3430, Training Loss: 0.028948256745934486\n",
            "Step 3430, Training Loss: 0.004403029568493366\n",
            "Step 3440, Training Loss: 0.004072115756571293\n",
            "Step 3440, Training Loss: 0.07074330002069473\n",
            "Step 3450, Training Loss: 0.00525358971208334\n",
            "Step 3450, Training Loss: 0.029016122221946716\n",
            "Step 3460, Training Loss: 0.029339978471398354\n",
            "Step 3460, Training Loss: 0.0035561632830649614\n",
            "Step 3470, Training Loss: 0.014728945679962635\n",
            "Step 3470, Training Loss: 0.017775030806660652\n",
            "Step 3480, Training Loss: 0.003651499282568693\n",
            "Step 3480, Training Loss: 0.017579663544893265\n",
            "Step 3490, Training Loss: 0.03181616961956024\n",
            "Step 3490, Training Loss: 0.0030895594973117113\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Saving model checkpoint to ./results/checkpoint-3500\n",
            "Configuration saved in ./results/checkpoint-3500/config.json\n",
            "Model weights saved in ./results/checkpoint-3500/pytorch_model.bin\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 3500, Training Loss: 0.004316948354244232\n",
            "Step 3500, Training Loss: 0.007096689194440842\n",
            "Step 3510, Training Loss: 0.050270453095436096\n",
            "Step 3510, Training Loss: 0.15596675872802734\n",
            "Step 3520, Training Loss: 0.23368661105632782\n",
            "Step 3520, Training Loss: 0.0032786247320473194\n",
            "Step 3530, Training Loss: 0.20844224095344543\n",
            "Step 3530, Training Loss: 0.0041145579889416695\n",
            "Step 3540, Training Loss: 0.023071976378560066\n",
            "Step 3540, Training Loss: 0.0039024818688631058\n",
            "Step 3550, Training Loss: 0.004108268767595291\n",
            "Step 3550, Training Loss: 0.02615448646247387\n",
            "Step 3560, Training Loss: 0.015384169295430183\n",
            "Step 3560, Training Loss: 0.0038549560122191906\n",
            "Step 3570, Training Loss: 0.00414996175095439\n",
            "Step 3570, Training Loss: 0.00480115320533514\n",
            "Step 3580, Training Loss: 0.003357240930199623\n",
            "Step 3580, Training Loss: 0.0028968784026801586\n",
            "Step 3590, Training Loss: 0.0041971998289227486\n",
            "Step 3590, Training Loss: 0.024813812226057053\n",
            "Step 3600, Training Loss: 0.0035544263664633036\n",
            "Step 3600, Training Loss: 0.2736670970916748\n",
            "Step 3610, Training Loss: 0.02343890257179737\n",
            "Step 3610, Training Loss: 0.006908838637173176\n",
            "Step 3620, Training Loss: 0.22593334317207336\n",
            "Step 3620, Training Loss: 0.006472388748079538\n",
            "Step 3630, Training Loss: 0.15854492783546448\n",
            "Step 3630, Training Loss: 0.3179721236228943\n",
            "Step 3640, Training Loss: 0.04931914433836937\n",
            "Step 3640, Training Loss: 0.0046395608223974705\n",
            "Step 3650, Training Loss: 0.1242697685956955\n",
            "Step 3650, Training Loss: 0.0234841201454401\n",
            "Step 3660, Training Loss: 0.0034288393799215555\n",
            "Step 3660, Training Loss: 0.007503627799451351\n",
            "Step 3670, Training Loss: 0.006626359652727842\n",
            "Step 3670, Training Loss: 0.14317485690116882\n",
            "Step 3680, Training Loss: 0.0035876084584742785\n",
            "Step 3680, Training Loss: 0.1539614051580429\n",
            "Step 3690, Training Loss: 0.004063294734805822\n",
            "Step 3690, Training Loss: 0.38608983159065247\n",
            "Step 3700, Training Loss: 0.007221200969070196\n",
            "Step 3700, Training Loss: 0.08745153248310089\n",
            "Step 3710, Training Loss: 0.0288145262748003\n",
            "Step 3710, Training Loss: 0.0032183423172682524\n",
            "Step 3720, Training Loss: 0.0036631114780902863\n",
            "Step 3720, Training Loss: 0.01274158526211977\n",
            "Step 3730, Training Loss: 0.0028301537968218327\n",
            "Step 3730, Training Loss: 0.01724052242934704\n",
            "Step 3740, Training Loss: 0.12996308505535126\n",
            "Step 3740, Training Loss: 0.003465324640274048\n",
            "Step 3750, Training Loss: 0.004552437923848629\n",
            "Step 3750, Training Loss: 0.0030999283771961927\n",
            "Step 3760, Training Loss: 0.0032503074035048485\n",
            "Step 3760, Training Loss: 0.003411646466702223\n",
            "Step 3770, Training Loss: 0.00906809326261282\n",
            "Step 3770, Training Loss: 0.0034523229114711285\n",
            "Step 3780, Training Loss: 0.0027107656933367252\n",
            "Step 3780, Training Loss: 0.003458709456026554\n",
            "Step 3790, Training Loss: 0.004700744990259409\n",
            "Step 3790, Training Loss: 0.0042540221475064754\n",
            "Step 3800, Training Loss: 0.006789593957364559\n",
            "Step 3800, Training Loss: 0.022492628544569016\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 1523\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 3810, Training Loss: 0.0043272399343550205\n",
            "Step 3810, Training Loss: 0.003460229141637683\n",
            "Step 3820, Training Loss: 0.00364200328476727\n",
            "Step 3820, Training Loss: 0.003826454747468233\n",
            "Step 3830, Training Loss: 0.037041645497083664\n",
            "Step 3830, Training Loss: 0.06405537575483322\n",
            "Step 3840, Training Loss: 0.08306774497032166\n",
            "Step 3840, Training Loss: 0.0033438217360526323\n",
            "Step 3850, Training Loss: 0.024313094094395638\n",
            "Step 3850, Training Loss: 0.08327070623636246\n",
            "Step 3860, Training Loss: 0.055973414331674576\n",
            "Step 3860, Training Loss: 0.003885444952175021\n",
            "Step 3870, Training Loss: 0.0036679012700915337\n",
            "Step 3870, Training Loss: 0.006948552094399929\n",
            "Step 3880, Training Loss: 0.13765837252140045\n",
            "Step 3880, Training Loss: 0.003830697387456894\n",
            "Step 3890, Training Loss: 0.014320495538413525\n",
            "Step 3890, Training Loss: 0.005000091157853603\n",
            "Step 3900, Training Loss: 0.0151679627597332\n",
            "Step 3900, Training Loss: 0.0029167896136641502\n",
            "Step 3910, Training Loss: 0.004836392588913441\n",
            "Step 3910, Training Loss: 0.03529895097017288\n",
            "Step 3920, Training Loss: 0.0030926153995096684\n",
            "Step 3920, Training Loss: 0.0063233221881091595\n",
            "Step 3930, Training Loss: 0.0042274086736142635\n",
            "Step 3930, Training Loss: 0.021460605785250664\n",
            "Step 3940, Training Loss: 0.020720455795526505\n",
            "Step 3940, Training Loss: 0.02640775963664055\n",
            "Step 3950, Training Loss: 0.05859779566526413\n",
            "Step 3950, Training Loss: 0.004491972271353006\n",
            "Step 3960, Training Loss: 0.004318262916058302\n",
            "Step 3960, Training Loss: 0.006239410024136305\n",
            "Step 3970, Training Loss: 0.007599491626024246\n",
            "Step 3970, Training Loss: 0.05102088674902916\n",
            "Step 3980, Training Loss: 0.0031736898235976696\n",
            "Step 3980, Training Loss: 0.03170153126120567\n",
            "Step 3990, Training Loss: 0.05188944563269615\n",
            "Step 3990, Training Loss: 0.0035200626589357853\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Saving model checkpoint to ./results/checkpoint-4000\n",
            "Configuration saved in ./results/checkpoint-4000/config.json\n",
            "Model weights saved in ./results/checkpoint-4000/pytorch_model.bin\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 4000, Training Loss: 0.00445007486268878\n",
            "Step 4000, Training Loss: 0.003589842701330781\n",
            "Step 4010, Training Loss: 0.0043493169359862804\n",
            "Step 4010, Training Loss: 0.004818196874111891\n",
            "Step 4020, Training Loss: 0.0039820498786866665\n",
            "Step 4020, Training Loss: 0.019941605627536774\n",
            "Step 4030, Training Loss: 0.04225544258952141\n",
            "Step 4030, Training Loss: 0.003389518242329359\n",
            "Step 4040, Training Loss: 0.02244315855205059\n",
            "Step 4040, Training Loss: 0.0027781417593359947\n",
            "Step 4050, Training Loss: 0.019701173529028893\n",
            "Step 4050, Training Loss: 0.0036010059993714094\n",
            "Step 4060, Training Loss: 0.0037884714547544718\n",
            "Step 4060, Training Loss: 0.12510007619857788\n",
            "Step 4070, Training Loss: 0.12369044870138168\n",
            "Step 4070, Training Loss: 0.035474713891744614\n",
            "Step 4080, Training Loss: 0.32847529649734497\n",
            "Step 4080, Training Loss: 0.12484326958656311\n",
            "Step 4090, Training Loss: 0.023617710918188095\n",
            "Step 4090, Training Loss: 0.004594281315803528\n",
            "Step 4100, Training Loss: 0.005905534140765667\n",
            "Step 4100, Training Loss: 0.0033098796848207712\n",
            "Step 4110, Training Loss: 0.0034840242005884647\n",
            "Step 4110, Training Loss: 0.014184512197971344\n",
            "Step 4120, Training Loss: 0.01812482252717018\n",
            "Step 4120, Training Loss: 0.11301116645336151\n",
            "Step 4130, Training Loss: 0.3405959904193878\n",
            "Step 4130, Training Loss: 0.019240479916334152\n",
            "Step 4140, Training Loss: 0.03496646508574486\n",
            "Step 4140, Training Loss: 0.004501413553953171\n",
            "Step 4150, Training Loss: 0.003546064952388406\n",
            "Step 4150, Training Loss: 0.13629816472530365\n",
            "Step 4160, Training Loss: 0.034423816949129105\n",
            "Step 4160, Training Loss: 0.004971309099346399\n",
            "Step 4170, Training Loss: 0.005964883603155613\n",
            "Step 4170, Training Loss: 0.04508613795042038\n",
            "Step 4180, Training Loss: 0.015796761959791183\n",
            "Step 4180, Training Loss: 0.009224457666277885\n",
            "Step 4190, Training Loss: 0.016687653958797455\n",
            "Step 4190, Training Loss: 0.01744999550282955\n",
            "Step 4200, Training Loss: 0.005160666536539793\n",
            "Step 4200, Training Loss: 0.02503032423555851\n",
            "Step 4210, Training Loss: 0.01154589094221592\n",
            "Step 4210, Training Loss: 0.03640454635024071\n",
            "Step 4220, Training Loss: 0.09878850728273392\n",
            "Step 4220, Training Loss: 0.0027183913625776768\n",
            "Step 4230, Training Loss: 0.005530067253857851\n",
            "Step 4230, Training Loss: 0.014625219628214836\n",
            "Step 4240, Training Loss: 0.00661763409152627\n",
            "Step 4240, Training Loss: 0.17193540930747986\n",
            "Step 4250, Training Loss: 0.0029812196735292673\n",
            "Step 4250, Training Loss: 0.01935231313109398\n",
            "Step 4260, Training Loss: 0.01479838602244854\n",
            "Step 4260, Training Loss: 0.0035296801943331957\n",
            "Step 4270, Training Loss: 0.003051056759431958\n",
            "Step 4270, Training Loss: 0.0026168932672590017\n",
            "Step 4280, Training Loss: 0.0039398884400725365\n",
            "Step 4280, Training Loss: 0.028784241527318954\n",
            "Step 4290, Training Loss: 0.2927670180797577\n",
            "Step 4290, Training Loss: 0.02025732770562172\n",
            "Step 4300, Training Loss: 0.024938154965639114\n",
            "Step 4300, Training Loss: 0.03331298381090164\n",
            "Step 4310, Training Loss: 0.023635277524590492\n",
            "Step 4310, Training Loss: 0.008185800164937973\n",
            "Step 4320, Training Loss: 0.0656890943646431\n",
            "Step 4320, Training Loss: 0.0024851770140230656\n",
            "Step 4330, Training Loss: 0.004288812167942524\n",
            "Step 4330, Training Loss: 0.18288174271583557\n",
            "Step 4340, Training Loss: 0.01641993224620819\n",
            "Step 4340, Training Loss: 0.03588917851448059\n",
            "Step 4350, Training Loss: 0.00607107300311327\n",
            "Step 4350, Training Loss: 0.014678049832582474\n",
            "Step 4360, Training Loss: 0.027357924729585648\n",
            "Step 4360, Training Loss: 0.02745984122157097\n",
            "Step 4370, Training Loss: 0.03492949157953262\n",
            "Step 4370, Training Loss: 0.018689317628741264\n",
            "Step 4380, Training Loss: 0.0023971633054316044\n",
            "Step 4380, Training Loss: 0.02619122341275215\n",
            "Step 4390, Training Loss: 0.0028243539854884148\n",
            "Step 4390, Training Loss: 0.008138327859342098\n",
            "Step 4400, Training Loss: 0.0032064353581517935\n",
            "Step 4400, Training Loss: 0.036796726286411285\n",
            "Step 4410, Training Loss: 0.003950414713472128\n",
            "Step 4410, Training Loss: 0.0036849260795861483\n",
            "Step 4420, Training Loss: 0.006017826963216066\n",
            "Step 4420, Training Loss: 0.008734236471354961\n",
            "Step 4430, Training Loss: 0.016797984018921852\n",
            "Step 4430, Training Loss: 0.024581994861364365\n",
            "Step 4440, Training Loss: 0.028782621026039124\n",
            "Step 4440, Training Loss: 0.052843619138002396\n",
            "Step 4450, Training Loss: 0.07414539158344269\n",
            "Step 4450, Training Loss: 0.051700517535209656\n",
            "Step 4460, Training Loss: 0.0027678341139107943\n",
            "Step 4460, Training Loss: 0.0033880043774843216\n",
            "Step 4470, Training Loss: 0.005852441769093275\n",
            "Step 4470, Training Loss: 0.0635317862033844\n",
            "Step 4480, Training Loss: 0.0039021719712764025\n",
            "Step 4480, Training Loss: 0.004763966426253319\n",
            "Step 4490, Training Loss: 0.03100733831524849\n",
            "Step 4490, Training Loss: 0.0040076845325529575\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Saving model checkpoint to ./results/checkpoint-4500\n",
            "Configuration saved in ./results/checkpoint-4500/config.json\n",
            "Model weights saved in ./results/checkpoint-4500/pytorch_model.bin\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 4500, Training Loss: 0.006876485887914896\n",
            "Step 4500, Training Loss: 0.02545209601521492\n",
            "Step 4510, Training Loss: 0.17554457485675812\n",
            "Step 4510, Training Loss: 0.002376757562160492\n",
            "Step 4520, Training Loss: 0.051499880850315094\n",
            "Step 4520, Training Loss: 0.2135201245546341\n",
            "Step 4530, Training Loss: 0.026601340621709824\n",
            "Step 4530, Training Loss: 0.005429425742477179\n",
            "Step 4540, Training Loss: 0.023487335070967674\n",
            "Step 4540, Training Loss: 0.0033843417186290026\n",
            "Step 4550, Training Loss: 0.028062451630830765\n",
            "Step 4550, Training Loss: 0.005356408189982176\n",
            "Step 4560, Training Loss: 0.0028123115189373493\n",
            "Step 4560, Training Loss: 0.00803944282233715\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 1523\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 4570, Training Loss: 0.025121323764324188\n",
            "Step 4570, Training Loss: 0.002141993958503008\n",
            "Step 4580, Training Loss: 0.025816665962338448\n",
            "Step 4580, Training Loss: 0.14640553295612335\n",
            "Step 4590, Training Loss: 0.0028495958540588617\n",
            "Step 4590, Training Loss: 0.013923541642725468\n",
            "Step 4600, Training Loss: 0.015193728730082512\n",
            "Step 4600, Training Loss: 0.007648163009434938\n",
            "Step 4610, Training Loss: 0.004378313664346933\n",
            "Step 4610, Training Loss: 0.0050040557980537415\n",
            "Step 4620, Training Loss: 0.007425723597407341\n",
            "Step 4620, Training Loss: 0.09425146877765656\n",
            "Step 4630, Training Loss: 0.021659065037965775\n",
            "Step 4630, Training Loss: 0.0022709087934345007\n",
            "Step 4640, Training Loss: 0.007469487376511097\n",
            "Step 4640, Training Loss: 0.09252282977104187\n",
            "Step 4650, Training Loss: 0.003729521529749036\n",
            "Step 4650, Training Loss: 0.0025645175483077765\n",
            "Step 4660, Training Loss: 0.015585463494062424\n",
            "Step 4660, Training Loss: 0.10876031965017319\n",
            "Step 4670, Training Loss: 0.006513479631394148\n",
            "Step 4670, Training Loss: 0.014991754665970802\n",
            "Step 4680, Training Loss: 0.004266543313860893\n",
            "Step 4680, Training Loss: 0.008317242376506329\n",
            "Step 4690, Training Loss: 0.030233727768063545\n",
            "Step 4690, Training Loss: 0.0023713514674454927\n",
            "Step 4700, Training Loss: 0.006666770204901695\n",
            "Step 4700, Training Loss: 0.022327225655317307\n",
            "Step 4710, Training Loss: 0.0349089689552784\n",
            "Step 4710, Training Loss: 0.003407025709748268\n",
            "Step 4720, Training Loss: 0.019925860688090324\n",
            "Step 4720, Training Loss: 0.04044057056307793\n",
            "Step 4730, Training Loss: 0.017750004306435585\n",
            "Step 4730, Training Loss: 0.004123940132558346\n",
            "Step 4740, Training Loss: 0.0038115328643471003\n",
            "Step 4740, Training Loss: 0.006884273141622543\n",
            "Step 4750, Training Loss: 0.0032466412521898746\n",
            "Step 4750, Training Loss: 0.03606875613331795\n",
            "Step 4760, Training Loss: 0.0027432048227638006\n",
            "Step 4760, Training Loss: 0.003030969761312008\n",
            "Step 4770, Training Loss: 0.003772192867472768\n",
            "Step 4770, Training Loss: 0.043033305555582047\n",
            "Step 4780, Training Loss: 0.008288128301501274\n",
            "Step 4780, Training Loss: 0.0042646341025829315\n",
            "Step 4790, Training Loss: 0.00499887578189373\n",
            "Step 4790, Training Loss: 0.2507697641849518\n",
            "Step 4800, Training Loss: 0.0026851261500269175\n",
            "Step 4800, Training Loss: 0.0024911416694521904\n",
            "Step 4810, Training Loss: 0.021041296422481537\n",
            "Step 4810, Training Loss: 0.004330799914896488\n",
            "Step 4820, Training Loss: 0.17040561139583588\n",
            "Step 4820, Training Loss: 0.0524849072098732\n",
            "Step 4830, Training Loss: 0.005883294157683849\n",
            "Step 4830, Training Loss: 0.005726768635213375\n",
            "Step 4840, Training Loss: 0.025696786120533943\n",
            "Step 4840, Training Loss: 0.0036157912109047174\n",
            "Step 4850, Training Loss: 0.003469951916486025\n",
            "Step 4850, Training Loss: 0.11596234887838364\n",
            "Step 4860, Training Loss: 0.08251669257879257\n",
            "Step 4860, Training Loss: 0.004104256629943848\n",
            "Step 4870, Training Loss: 0.00662162434309721\n",
            "Step 4870, Training Loss: 0.0039798058569431305\n",
            "Step 4880, Training Loss: 0.016907384619116783\n",
            "Step 4880, Training Loss: 0.019220270216464996\n",
            "Step 4890, Training Loss: 0.03804502636194229\n",
            "Step 4890, Training Loss: 0.08732154965400696\n",
            "Step 4900, Training Loss: 0.005642038304358721\n",
            "Step 4900, Training Loss: 0.0050358884036540985\n",
            "Step 4910, Training Loss: 0.0054183779284358025\n",
            "Step 4910, Training Loss: 0.004647399298846722\n",
            "Step 4920, Training Loss: 0.0024564776103943586\n",
            "Step 4920, Training Loss: 0.007166298571974039\n",
            "Step 4930, Training Loss: 0.004269738681614399\n",
            "Step 4930, Training Loss: 0.020585797727108\n",
            "Step 4940, Training Loss: 0.07143877446651459\n",
            "Step 4940, Training Loss: 0.006514181382954121\n",
            "Step 4950, Training Loss: 0.006849692668765783\n",
            "Step 4950, Training Loss: 0.0037496548611670732\n",
            "Step 4960, Training Loss: 0.017761506140232086\n",
            "Step 4960, Training Loss: 0.172581285238266\n",
            "Step 4970, Training Loss: 0.0024991873651742935\n",
            "Step 4970, Training Loss: 0.004861971363425255\n",
            "Step 4980, Training Loss: 0.008167630061507225\n",
            "Step 4980, Training Loss: 0.059911563992500305\n",
            "Step 4990, Training Loss: 0.021900620311498642\n",
            "Step 4990, Training Loss: 0.0033396880608052015\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Saving model checkpoint to ./results/checkpoint-5000\n",
            "Configuration saved in ./results/checkpoint-5000/config.json\n",
            "Model weights saved in ./results/checkpoint-5000/pytorch_model.bin\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 5000, Training Loss: 0.004165230318903923\n",
            "Step 5000, Training Loss: 0.00285756541416049\n",
            "Step 5010, Training Loss: 0.004416896961629391\n",
            "Step 5010, Training Loss: 0.017269684001803398\n",
            "Step 5020, Training Loss: 0.002928831847384572\n",
            "Step 5020, Training Loss: 0.009671769104897976\n",
            "Step 5030, Training Loss: 0.015445874072611332\n",
            "Step 5030, Training Loss: 0.006597128231078386\n",
            "Step 5040, Training Loss: 0.050750307738780975\n",
            "Step 5040, Training Loss: 0.00252133677713573\n",
            "Step 5050, Training Loss: 0.004022954031825066\n",
            "Step 5050, Training Loss: 0.0037750385235995054\n",
            "Step 5060, Training Loss: 0.04728369414806366\n",
            "Step 5060, Training Loss: 0.3528152406215668\n",
            "Step 5070, Training Loss: 0.019776996225118637\n",
            "Step 5070, Training Loss: 0.0028892054688185453\n",
            "Step 5080, Training Loss: 0.0050888340920209885\n",
            "Step 5080, Training Loss: 0.02795065939426422\n",
            "Step 5090, Training Loss: 0.0043626087717711926\n",
            "Step 5090, Training Loss: 0.004122091457247734\n",
            "Step 5100, Training Loss: 0.006344485562294722\n",
            "Step 5100, Training Loss: 0.005642917938530445\n",
            "Step 5110, Training Loss: 0.002946376334875822\n",
            "Step 5110, Training Loss: 0.004256336949765682\n",
            "Step 5120, Training Loss: 0.31355252861976624\n",
            "Step 5120, Training Loss: 0.018203863874077797\n",
            "Step 5130, Training Loss: 0.004766227211803198\n",
            "Step 5130, Training Loss: 0.004063221625983715\n",
            "Step 5140, Training Loss: 0.002389125060290098\n",
            "Step 5140, Training Loss: 0.12580667436122894\n",
            "Step 5150, Training Loss: 0.004043614491820335\n",
            "Step 5150, Training Loss: 0.004110635723918676\n",
            "Step 5160, Training Loss: 0.003918026573956013\n",
            "Step 5160, Training Loss: 0.003250785870477557\n",
            "Step 5170, Training Loss: 0.01875009573996067\n",
            "Step 5170, Training Loss: 0.0077704330906271935\n",
            "Step 5180, Training Loss: 0.00457912078127265\n",
            "Step 5180, Training Loss: 0.053248222917318344\n",
            "Step 5190, Training Loss: 0.007170392666012049\n",
            "Step 5190, Training Loss: 0.003374523250386119\n",
            "Step 5200, Training Loss: 0.010368511080741882\n",
            "Step 5200, Training Loss: 0.010262973606586456\n",
            "Step 5210, Training Loss: 0.06824713945388794\n",
            "Step 5210, Training Loss: 0.03069145418703556\n",
            "Step 5220, Training Loss: 0.0038905125111341476\n",
            "Step 5220, Training Loss: 0.04249348118901253\n",
            "Step 5230, Training Loss: 0.0029423723462969065\n",
            "Step 5230, Training Loss: 0.09353624284267426\n",
            "Step 5240, Training Loss: 0.004586374387145042\n",
            "Step 5240, Training Loss: 0.002898976905271411\n",
            "Step 5250, Training Loss: 0.016858872026205063\n",
            "Step 5250, Training Loss: 0.004685624968260527\n",
            "Step 5260, Training Loss: 0.004596208222210407\n",
            "Step 5260, Training Loss: 0.022779783234000206\n",
            "Step 5270, Training Loss: 0.002263710368424654\n",
            "Step 5270, Training Loss: 0.003042457392439246\n",
            "Step 5280, Training Loss: 0.003904657904058695\n",
            "Step 5280, Training Loss: 0.004231173545122147\n",
            "Step 5290, Training Loss: 0.0031734504736959934\n",
            "Step 5290, Training Loss: 0.027592595666646957\n",
            "Step 5300, Training Loss: 0.0038067561108618975\n",
            "Step 5300, Training Loss: 0.014679597690701485\n",
            "Step 5310, Training Loss: 0.004106295295059681\n",
            "Step 5310, Training Loss: 0.0036839472595602274\n",
            "Step 5320, Training Loss: 0.13723208010196686\n",
            "Step 5320, Training Loss: 0.0033974982798099518\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 1523\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 5330, Training Loss: 0.0033415143843740225\n",
            "Step 5330, Training Loss: 0.006627752911299467\n",
            "Step 5340, Training Loss: 0.020861996337771416\n",
            "Step 5340, Training Loss: 0.0035294238477945328\n",
            "Step 5350, Training Loss: 0.004234265070408583\n",
            "Step 5350, Training Loss: 0.0033029469195753336\n",
            "Step 5360, Training Loss: 0.0033388170413672924\n",
            "Step 5360, Training Loss: 0.03255466744303703\n",
            "Step 5370, Training Loss: 0.002766047138720751\n",
            "Step 5370, Training Loss: 0.4041883647441864\n",
            "Step 5380, Training Loss: 0.002211214043200016\n",
            "Step 5380, Training Loss: 0.026629826053977013\n",
            "Step 5390, Training Loss: 0.08137984573841095\n",
            "Step 5390, Training Loss: 0.0034124692901968956\n",
            "Step 5400, Training Loss: 0.005353166256099939\n",
            "Step 5400, Training Loss: 0.07524771988391876\n",
            "Step 5410, Training Loss: 0.006469107698649168\n",
            "Step 5410, Training Loss: 0.03711210563778877\n",
            "Step 5420, Training Loss: 0.025366805493831635\n",
            "Step 5420, Training Loss: 0.0038617330137640238\n",
            "Step 5430, Training Loss: 0.027130406349897385\n",
            "Step 5430, Training Loss: 0.005077528301626444\n",
            "Step 5440, Training Loss: 0.008882572874426842\n",
            "Step 5440, Training Loss: 0.33413800597190857\n",
            "Step 5450, Training Loss: 0.002981452737003565\n",
            "Step 5450, Training Loss: 0.023141656070947647\n",
            "Step 5460, Training Loss: 0.10471399873495102\n",
            "Step 5460, Training Loss: 0.03153065964579582\n",
            "Step 5470, Training Loss: 0.006082388572394848\n",
            "Step 5470, Training Loss: 0.09954212605953217\n",
            "Step 5480, Training Loss: 0.01644766516983509\n",
            "Step 5480, Training Loss: 0.04248299077153206\n",
            "Step 5490, Training Loss: 0.04295887425541878\n",
            "Step 5490, Training Loss: 0.015560757368803024\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Saving model checkpoint to ./results/checkpoint-5500\n",
            "Configuration saved in ./results/checkpoint-5500/config.json\n",
            "Model weights saved in ./results/checkpoint-5500/pytorch_model.bin\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 5500, Training Loss: 0.05551601201295853\n",
            "Step 5500, Training Loss: 0.019965823739767075\n",
            "Step 5510, Training Loss: 0.005252729170024395\n",
            "Step 5510, Training Loss: 0.00748868752270937\n",
            "Step 5520, Training Loss: 0.003157335566356778\n",
            "Step 5520, Training Loss: 0.018191400915384293\n",
            "Step 5530, Training Loss: 0.009028898552060127\n",
            "Step 5530, Training Loss: 0.003971458412706852\n",
            "Step 5540, Training Loss: 0.015848351642489433\n",
            "Step 5540, Training Loss: 0.0027777410577982664\n",
            "Step 5550, Training Loss: 0.004645197652280331\n",
            "Step 5550, Training Loss: 0.0033010083716362715\n",
            "Step 5560, Training Loss: 0.0051784091629087925\n",
            "Step 5560, Training Loss: 0.003349459730088711\n",
            "Step 5570, Training Loss: 0.0374220572412014\n",
            "Step 5570, Training Loss: 0.09441565722227097\n",
            "Step 5580, Training Loss: 0.013311399146914482\n",
            "Step 5580, Training Loss: 0.0024948727805167437\n",
            "Step 5590, Training Loss: 0.3312482535839081\n",
            "Step 5590, Training Loss: 0.0041347164660692215\n",
            "Step 5600, Training Loss: 0.007427452597767115\n",
            "Step 5600, Training Loss: 0.2937149107456207\n",
            "Step 5610, Training Loss: 0.02365712635219097\n",
            "Step 5610, Training Loss: 0.00561019591987133\n",
            "Step 5620, Training Loss: 0.10163053870201111\n",
            "Step 5620, Training Loss: 0.005010460969060659\n",
            "Step 5630, Training Loss: 0.0031780588906258345\n",
            "Step 5630, Training Loss: 0.014113092795014381\n",
            "Step 5640, Training Loss: 0.025340069085359573\n",
            "Step 5640, Training Loss: 0.025015169754624367\n",
            "Step 5650, Training Loss: 0.004640643484890461\n",
            "Step 5650, Training Loss: 0.006012492813169956\n",
            "Step 5660, Training Loss: 0.003796565579250455\n",
            "Step 5660, Training Loss: 0.041299473494291306\n",
            "Step 5670, Training Loss: 0.006721173413097858\n",
            "Step 5670, Training Loss: 0.35304930806159973\n",
            "Step 5680, Training Loss: 0.11773951351642609\n",
            "Step 5680, Training Loss: 0.3349076807498932\n",
            "Step 5690, Training Loss: 0.003928720485419035\n",
            "Step 5690, Training Loss: 0.004190362058579922\n",
            "Step 5700, Training Loss: 0.0033034842927008867\n",
            "Step 5700, Training Loss: 0.0028339079581201077\n",
            "Step 5710, Training Loss: 0.022978482767939568\n",
            "Step 5710, Training Loss: 0.009085058234632015\n",
            "Step 5720, Training Loss: 0.2626318335533142\n",
            "Step 5720, Training Loss: 0.04053094983100891\n",
            "Step 5730, Training Loss: 0.015252356417477131\n",
            "Step 5730, Training Loss: 0.01672477275133133\n",
            "Step 5740, Training Loss: 0.006972199305891991\n",
            "Step 5740, Training Loss: 0.016498442739248276\n",
            "Step 5750, Training Loss: 0.10524231940507889\n",
            "Step 5750, Training Loss: 0.024252645671367645\n",
            "Step 5760, Training Loss: 0.0034526849631220102\n",
            "Step 5760, Training Loss: 0.0041704485192894936\n",
            "Step 5770, Training Loss: 0.02487361989915371\n",
            "Step 5770, Training Loss: 0.00451735220849514\n",
            "Step 5780, Training Loss: 0.03681980445981026\n",
            "Step 5780, Training Loss: 0.009431791491806507\n",
            "Step 5790, Training Loss: 0.01958204060792923\n",
            "Step 5790, Training Loss: 0.0071173072792589664\n",
            "Step 5800, Training Loss: 0.0029408440459519625\n",
            "Step 5800, Training Loss: 0.004287343937903643\n",
            "Step 5810, Training Loss: 0.01573970913887024\n",
            "Step 5810, Training Loss: 0.0070986696518957615\n",
            "Step 5820, Training Loss: 0.03697260469198227\n",
            "Step 5820, Training Loss: 0.004214762710034847\n",
            "Step 5830, Training Loss: 0.003945166245102882\n",
            "Step 5830, Training Loss: 0.027259843423962593\n",
            "Step 5840, Training Loss: 0.003352029714733362\n",
            "Step 5840, Training Loss: 0.07479878515005112\n",
            "Step 5850, Training Loss: 0.0037641669623553753\n",
            "Step 5850, Training Loss: 0.0039182910695672035\n",
            "Step 5860, Training Loss: 0.005993555299937725\n",
            "Step 5860, Training Loss: 0.004303095396608114\n",
            "Step 5870, Training Loss: 0.0036490443162620068\n",
            "Step 5870, Training Loss: 0.07445690035820007\n",
            "Step 5880, Training Loss: 0.004247178323566914\n",
            "Step 5880, Training Loss: 0.02934546209871769\n",
            "Step 5890, Training Loss: 0.008717508986592293\n",
            "Step 5890, Training Loss: 0.01867692917585373\n",
            "Step 5900, Training Loss: 0.02367793209850788\n",
            "Step 5900, Training Loss: 0.015410488471388817\n",
            "Step 5910, Training Loss: 0.029846535995602608\n",
            "Step 5910, Training Loss: 0.012179483659565449\n",
            "Step 5920, Training Loss: 0.006327526643872261\n",
            "Step 5920, Training Loss: 0.003429729025810957\n",
            "Step 5930, Training Loss: 0.023030055686831474\n",
            "Step 5930, Training Loss: 0.0034471258986741304\n",
            "Step 5940, Training Loss: 0.002928179455921054\n",
            "Step 5940, Training Loss: 0.0040212757885456085\n",
            "Step 5950, Training Loss: 0.040659099817276\n",
            "Step 5950, Training Loss: 0.021675558760762215\n",
            "Step 5960, Training Loss: 0.00531469052657485\n",
            "Step 5960, Training Loss: 0.04531971737742424\n",
            "Step 5970, Training Loss: 0.013660538010299206\n",
            "Step 5970, Training Loss: 0.0034682343248277903\n",
            "Step 5980, Training Loss: 0.00348716601729393\n",
            "Step 5980, Training Loss: 0.002586435992270708\n",
            "Step 5990, Training Loss: 0.0041105179116129875\n",
            "Step 5990, Training Loss: 0.030082518234848976\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Saving model checkpoint to ./results/checkpoint-6000\n",
            "Configuration saved in ./results/checkpoint-6000/config.json\n",
            "Model weights saved in ./results/checkpoint-6000/pytorch_model.bin\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 6000, Training Loss: 0.0024256128817796707\n",
            "Step 6000, Training Loss: 0.12293196469545364\n",
            "Step 6010, Training Loss: 0.10457488149404526\n",
            "Step 6010, Training Loss: 0.008210977539420128\n",
            "Step 6020, Training Loss: 0.004036358091980219\n",
            "Step 6020, Training Loss: 0.2967929244041443\n",
            "Step 6030, Training Loss: 0.023974096402525902\n",
            "Step 6030, Training Loss: 0.00233844225294888\n",
            "Step 6040, Training Loss: 0.004366288892924786\n",
            "Step 6040, Training Loss: 0.002807461656630039\n",
            "Step 6050, Training Loss: 0.3711372911930084\n",
            "Step 6050, Training Loss: 0.020456740632653236\n",
            "Step 6060, Training Loss: 0.022075410932302475\n",
            "Step 6060, Training Loss: 0.0047376761212944984\n",
            "Step 6070, Training Loss: 0.09000436961650848\n",
            "Step 6070, Training Loss: 0.0028562925290316343\n",
            "Step 6080, Training Loss: 0.02410714142024517\n",
            "Step 6080, Training Loss: 0.0028144936077296734\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 1523\n",
            "  Batch size = 8\n",
            "\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 1523\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training completed in 324.88 seconds.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='191' max='191' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [191/191 00:02]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-12-09 19:22:18,347] Trial 6 finished with value: 0.8559372738098081 and parameters: {'learning_rate': 3.121688224807886e-06, 'batch_size': 4, 'num_train_epochs': 8, 'weight_decay': 0.0021495137507074624}. Best is trial 6 with value: 0.8559372738098081.\n",
            "<ipython-input-12-42465065092b>:2: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  learning_rate = trial.suggest_loguniform(\"learning_rate\", 1e-6, 5e-5)\n",
            "<ipython-input-12-42465065092b>:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  weight_decay = trial.suggest_loguniform(\"weight_decay\", 1e-4, 0.1)\n",
            "PyTorch: setting up devices\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n",
            "***** Running training *****\n",
            "  Num examples = 6091\n",
            "  Num Epochs = 9\n",
            "  Instantaneous batch size per device = 16\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
            "  Gradient Accumulation steps = 2\n",
            "  Total optimization steps = 1710\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting training...\n",
            "Step 0, Training Loss: 0.009554348886013031\n",
            "Step 0, Training Loss: 0.045910026878118515\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1710' max='1710' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1710/1710 04:41, Epoch 8/9]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "      <th>F1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>0.023400</td>\n",
              "      <td>0.160607</td>\n",
              "      <td>0.847012</td>\n",
              "      <td>0.855672</td>\n",
              "      <td>0.856226</td>\n",
              "      <td>0.852893</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.018000</td>\n",
              "      <td>0.155537</td>\n",
              "      <td>0.851609</td>\n",
              "      <td>0.858648</td>\n",
              "      <td>0.862003</td>\n",
              "      <td>0.857590</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.032600</td>\n",
              "      <td>0.156058</td>\n",
              "      <td>0.848982</td>\n",
              "      <td>0.856713</td>\n",
              "      <td>0.860077</td>\n",
              "      <td>0.855721</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.033400</td>\n",
              "      <td>0.155402</td>\n",
              "      <td>0.848326</td>\n",
              "      <td>0.859658</td>\n",
              "      <td>0.857510</td>\n",
              "      <td>0.856331</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.028200</td>\n",
              "      <td>0.157497</td>\n",
              "      <td>0.848326</td>\n",
              "      <td>0.858367</td>\n",
              "      <td>0.858793</td>\n",
              "      <td>0.856284</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>0.031800</td>\n",
              "      <td>0.156046</td>\n",
              "      <td>0.850295</td>\n",
              "      <td>0.858103</td>\n",
              "      <td>0.857510</td>\n",
              "      <td>0.855227</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>0.021400</td>\n",
              "      <td>0.155534</td>\n",
              "      <td>0.850295</td>\n",
              "      <td>0.858573</td>\n",
              "      <td>0.857510</td>\n",
              "      <td>0.855375</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>0.035400</td>\n",
              "      <td>0.155466</td>\n",
              "      <td>0.849639</td>\n",
              "      <td>0.859171</td>\n",
              "      <td>0.856868</td>\n",
              "      <td>0.855435</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>0.029400</td>\n",
              "      <td>0.157019</td>\n",
              "      <td>0.849639</td>\n",
              "      <td>0.857580</td>\n",
              "      <td>0.857510</td>\n",
              "      <td>0.854915</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 10, Training Loss: 0.0047466205433011055\n",
            "Step 10, Training Loss: 0.02255837991833687\n",
            "Step 20, Training Loss: 0.025128493085503578\n",
            "Step 20, Training Loss: 0.006840710062533617\n",
            "Step 30, Training Loss: 0.026615044102072716\n",
            "Step 30, Training Loss: 0.07078629732131958\n",
            "Step 40, Training Loss: 0.0035813706927001476\n",
            "Step 40, Training Loss: 0.003040214302018285\n",
            "Step 50, Training Loss: 0.0042435172945261\n",
            "Step 50, Training Loss: 0.01650448702275753\n",
            "Step 60, Training Loss: 0.004992262925952673\n",
            "Step 60, Training Loss: 0.10090753436088562\n",
            "Step 70, Training Loss: 0.05253808945417404\n",
            "Step 70, Training Loss: 0.044130243360996246\n",
            "Step 80, Training Loss: 0.01293498557060957\n",
            "Step 80, Training Loss: 0.015113112516701221\n",
            "Step 90, Training Loss: 0.006730388849973679\n",
            "Step 90, Training Loss: 0.030451111495494843\n",
            "Step 100, Training Loss: 0.0137171084061265\n",
            "Step 100, Training Loss: 0.003980483394116163\n",
            "Step 110, Training Loss: 0.03860543295741081\n",
            "Step 110, Training Loss: 0.01724475435912609\n",
            "Step 120, Training Loss: 0.017852291464805603\n",
            "Step 120, Training Loss: 0.006472054403275251\n",
            "Step 130, Training Loss: 0.03298303484916687\n",
            "Step 130, Training Loss: 0.009150499477982521\n",
            "Step 140, Training Loss: 0.048849545419216156\n",
            "Step 140, Training Loss: 0.03165465593338013\n",
            "Step 150, Training Loss: 0.057319287210702896\n",
            "Step 150, Training Loss: 0.06226330250501633\n",
            "Step 160, Training Loss: 0.11157077550888062\n",
            "Step 160, Training Loss: 0.05391040816903114\n",
            "Step 170, Training Loss: 0.11877867579460144\n",
            "Step 170, Training Loss: 0.037197232246398926\n",
            "Step 180, Training Loss: 0.042243994772434235\n",
            "Step 180, Training Loss: 0.044474128633737564\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 1523\n",
            "  Batch size = 32\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 190, Training Loss: 0.043901968747377396\n",
            "Step 190, Training Loss: 0.2625839412212372\n",
            "Step 190, Training Loss: 0.2835221588611603\n",
            "Step 190, Training Loss: 0.09886137396097183\n",
            "Step 190, Training Loss: 0.17065444588661194\n",
            "Step 190, Training Loss: 0.1425393521785736\n",
            "Step 190, Training Loss: 0.19599196314811707\n",
            "Step 190, Training Loss: 0.1039995327591896\n",
            "Step 190, Training Loss: 0.1740499585866928\n",
            "Step 190, Training Loss: 0.0854780375957489\n",
            "Step 190, Training Loss: 0.1783803552389145\n",
            "Step 190, Training Loss: 0.05836193636059761\n",
            "Step 190, Training Loss: 0.20864041149616241\n",
            "Step 190, Training Loss: 0.17090007662773132\n",
            "Step 190, Training Loss: 0.046741727739572525\n",
            "Step 190, Training Loss: 0.07734113186597824\n",
            "Step 190, Training Loss: 0.13735122978687286\n",
            "Step 190, Training Loss: 0.17280255258083344\n",
            "Step 190, Training Loss: 0.0621735081076622\n",
            "Step 190, Training Loss: 0.2010488659143448\n",
            "Step 190, Training Loss: 0.12174666672945023\n",
            "Step 190, Training Loss: 0.2501460909843445\n",
            "Step 190, Training Loss: 0.19920243322849274\n",
            "Step 190, Training Loss: 0.11147654801607132\n",
            "Step 190, Training Loss: 0.15194150805473328\n",
            "Step 190, Training Loss: 0.3225693702697754\n",
            "Step 190, Training Loss: 0.2747681736946106\n",
            "Step 190, Training Loss: 0.15119975805282593\n",
            "Step 190, Training Loss: 0.20560429990291595\n",
            "Step 190, Training Loss: 0.21183142066001892\n",
            "Step 190, Training Loss: 0.028716986998915672\n",
            "Step 190, Training Loss: 0.22520501911640167\n",
            "Step 190, Training Loss: 0.1653779149055481\n",
            "Step 190, Training Loss: 0.21328911185264587\n",
            "Step 190, Training Loss: 0.12967297434806824\n",
            "Step 190, Training Loss: 0.008350119926035404\n",
            "Step 190, Training Loss: 0.2587164342403412\n",
            "Step 190, Training Loss: 0.12817412614822388\n",
            "Step 190, Training Loss: 0.15218757092952728\n",
            "Step 190, Training Loss: 0.17824988067150116\n",
            "Step 190, Training Loss: 0.09294793009757996\n",
            "Step 190, Training Loss: 0.26422733068466187\n",
            "Step 190, Training Loss: 0.20178291201591492\n",
            "Step 190, Training Loss: 0.09901995956897736\n",
            "Step 190, Training Loss: 0.1160389631986618\n",
            "Step 190, Training Loss: 0.22698897123336792\n",
            "Step 190, Training Loss: 0.0802178755402565\n",
            "Step 190, Training Loss: 0.17414675652980804\n",
            "Step 190, Training Loss: 0.11568103730678558\n",
            "Step 190, Training Loss: 0.023209990933537483\n",
            "Step 190, Training Loss: 0.016463302075862885\n",
            "Step 200, Training Loss: 0.005400846712291241\n",
            "Step 200, Training Loss: 0.015576363541185856\n",
            "Step 210, Training Loss: 0.03871534392237663\n",
            "Step 210, Training Loss: 0.01292476337403059\n",
            "Step 220, Training Loss: 0.017160572111606598\n",
            "Step 220, Training Loss: 0.025942450389266014\n",
            "Step 230, Training Loss: 0.005132910795509815\n",
            "Step 230, Training Loss: 0.009273570962250233\n",
            "Step 240, Training Loss: 0.04873328655958176\n",
            "Step 240, Training Loss: 0.04846303164958954\n",
            "Step 250, Training Loss: 0.0061201779171824455\n",
            "Step 250, Training Loss: 0.03492724895477295\n",
            "Step 260, Training Loss: 0.012269781902432442\n",
            "Step 260, Training Loss: 0.06687235832214355\n",
            "Step 270, Training Loss: 0.021183336153626442\n",
            "Step 270, Training Loss: 0.012469134293496609\n",
            "Step 280, Training Loss: 0.007501286454498768\n",
            "Step 280, Training Loss: 0.03691090643405914\n",
            "Step 290, Training Loss: 0.06272196769714355\n",
            "Step 290, Training Loss: 0.0661529079079628\n",
            "Step 300, Training Loss: 0.08243067562580109\n",
            "Step 300, Training Loss: 0.022503424435853958\n",
            "Step 310, Training Loss: 0.01454242318868637\n",
            "Step 310, Training Loss: 0.017816919833421707\n",
            "Step 320, Training Loss: 0.003341555129736662\n",
            "Step 320, Training Loss: 0.013272952288389206\n",
            "Step 330, Training Loss: 0.013774131424725056\n",
            "Step 330, Training Loss: 0.017518965527415276\n",
            "Step 340, Training Loss: 0.003603102173656225\n",
            "Step 340, Training Loss: 0.014951184391975403\n",
            "Step 350, Training Loss: 0.016704389825463295\n",
            "Step 350, Training Loss: 0.07519719004631042\n",
            "Step 360, Training Loss: 0.028222473338246346\n",
            "Step 360, Training Loss: 0.014399201609194279\n",
            "Step 370, Training Loss: 0.0052161370404064655\n",
            "Step 370, Training Loss: 0.013847064226865768\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 1523\n",
            "  Batch size = 32\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 380, Training Loss: 0.015519680455327034\n",
            "Step 380, Training Loss: 0.24957722425460815\n",
            "Step 380, Training Loss: 0.27982351183891296\n",
            "Step 380, Training Loss: 0.0964701846241951\n",
            "Step 380, Training Loss: 0.16437387466430664\n",
            "Step 380, Training Loss: 0.14394502341747284\n",
            "Step 380, Training Loss: 0.19204331934452057\n",
            "Step 380, Training Loss: 0.10389924794435501\n",
            "Step 380, Training Loss: 0.1724690943956375\n",
            "Step 380, Training Loss: 0.08402570337057114\n",
            "Step 380, Training Loss: 0.1690852791070938\n",
            "Step 380, Training Loss: 0.05805637314915657\n",
            "Step 380, Training Loss: 0.18795238435268402\n",
            "Step 380, Training Loss: 0.15585526823997498\n",
            "Step 380, Training Loss: 0.0545303151011467\n",
            "Step 380, Training Loss: 0.08204011619091034\n",
            "Step 380, Training Loss: 0.12697599828243256\n",
            "Step 380, Training Loss: 0.17478415369987488\n",
            "Step 380, Training Loss: 0.06175690144300461\n",
            "Step 380, Training Loss: 0.20026551187038422\n",
            "Step 380, Training Loss: 0.11582837998867035\n",
            "Step 380, Training Loss: 0.23626162111759186\n",
            "Step 380, Training Loss: 0.20704655349254608\n",
            "Step 380, Training Loss: 0.11044205725193024\n",
            "Step 380, Training Loss: 0.14930133521556854\n",
            "Step 380, Training Loss: 0.3215467035770416\n",
            "Step 380, Training Loss: 0.2715780735015869\n",
            "Step 380, Training Loss: 0.09277310967445374\n",
            "Step 380, Training Loss: 0.19250448048114777\n",
            "Step 380, Training Loss: 0.2099687159061432\n",
            "Step 380, Training Loss: 0.033119238913059235\n",
            "Step 380, Training Loss: 0.2228546440601349\n",
            "Step 380, Training Loss: 0.13055214285850525\n",
            "Step 380, Training Loss: 0.2134333997964859\n",
            "Step 380, Training Loss: 0.1312713623046875\n",
            "Step 380, Training Loss: 0.008548954501748085\n",
            "Step 380, Training Loss: 0.27885672450065613\n",
            "Step 380, Training Loss: 0.13322566449642181\n",
            "Step 380, Training Loss: 0.1525673270225525\n",
            "Step 380, Training Loss: 0.1831340491771698\n",
            "Step 380, Training Loss: 0.08978570252656937\n",
            "Step 380, Training Loss: 0.2521270215511322\n",
            "Step 380, Training Loss: 0.1975450962781906\n",
            "Step 380, Training Loss: 0.08117999136447906\n",
            "Step 380, Training Loss: 0.09094201773405075\n",
            "Step 380, Training Loss: 0.2121814787387848\n",
            "Step 380, Training Loss: 0.07995335012674332\n",
            "Step 380, Training Loss: 0.17098039388656616\n",
            "Step 380, Training Loss: 0.1266031712293625\n",
            "Step 380, Training Loss: 0.013125180266797543\n",
            "Step 380, Training Loss: 0.035227976739406586\n",
            "Step 390, Training Loss: 0.005863038823008537\n",
            "Step 390, Training Loss: 0.029697611927986145\n",
            "Step 400, Training Loss: 0.01349828951060772\n",
            "Step 400, Training Loss: 0.02430974692106247\n",
            "Step 410, Training Loss: 0.027167852967977524\n",
            "Step 410, Training Loss: 0.015048767440021038\n",
            "Step 420, Training Loss: 0.02308783121407032\n",
            "Step 420, Training Loss: 0.05448714643716812\n",
            "Step 430, Training Loss: 0.017322981730103493\n",
            "Step 430, Training Loss: 0.09470750391483307\n",
            "Step 440, Training Loss: 0.11400434374809265\n",
            "Step 440, Training Loss: 0.014030985534191132\n",
            "Step 450, Training Loss: 0.03934066742658615\n",
            "Step 450, Training Loss: 0.010050594806671143\n",
            "Step 460, Training Loss: 0.013276408426463604\n",
            "Step 460, Training Loss: 0.03277168422937393\n",
            "Step 470, Training Loss: 0.01670243963599205\n",
            "Step 470, Training Loss: 0.06107339262962341\n",
            "Step 480, Training Loss: 0.047770433127880096\n",
            "Step 480, Training Loss: 0.11629468947649002\n",
            "Step 490, Training Loss: 0.037465743720531464\n",
            "Step 490, Training Loss: 0.01932911016047001\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Saving model checkpoint to ./results/checkpoint-500\n",
            "Configuration saved in ./results/checkpoint-500/config.json\n",
            "Model weights saved in ./results/checkpoint-500/pytorch_model.bin\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 500, Training Loss: 0.046036235988140106\n",
            "Step 500, Training Loss: 0.04145633801817894\n",
            "Step 510, Training Loss: 0.006809436250478029\n",
            "Step 510, Training Loss: 0.024648945778608322\n",
            "Step 520, Training Loss: 0.013835427351295948\n",
            "Step 520, Training Loss: 0.014915392734110355\n",
            "Step 530, Training Loss: 0.01091559324413538\n",
            "Step 530, Training Loss: 0.07062115520238876\n",
            "Step 540, Training Loss: 0.06706000864505768\n",
            "Step 540, Training Loss: 0.03697086125612259\n",
            "Step 550, Training Loss: 0.01735430583357811\n",
            "Step 550, Training Loss: 0.02935326099395752\n",
            "Step 560, Training Loss: 0.016530105844140053\n",
            "Step 560, Training Loss: 0.050673708319664\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 1523\n",
            "  Batch size = 32\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 570, Training Loss: 0.048887815326452255\n",
            "Step 570, Training Loss: 0.23857828974723816\n",
            "Step 570, Training Loss: 0.2819104790687561\n",
            "Step 570, Training Loss: 0.0965932309627533\n",
            "Step 570, Training Loss: 0.16622203588485718\n",
            "Step 570, Training Loss: 0.1415274441242218\n",
            "Step 570, Training Loss: 0.18118815124034882\n",
            "Step 570, Training Loss: 0.10413099825382233\n",
            "Step 570, Training Loss: 0.17073403298854828\n",
            "Step 570, Training Loss: 0.08577665686607361\n",
            "Step 570, Training Loss: 0.1657087355852127\n",
            "Step 570, Training Loss: 0.05783312767744064\n",
            "Step 570, Training Loss: 0.1912306696176529\n",
            "Step 570, Training Loss: 0.16727806627750397\n",
            "Step 570, Training Loss: 0.04677971079945564\n",
            "Step 570, Training Loss: 0.07217756658792496\n",
            "Step 570, Training Loss: 0.12641917169094086\n",
            "Step 570, Training Loss: 0.17276541888713837\n",
            "Step 570, Training Loss: 0.06183457747101784\n",
            "Step 570, Training Loss: 0.19500483572483063\n",
            "Step 570, Training Loss: 0.13140861690044403\n",
            "Step 570, Training Loss: 0.24091224372386932\n",
            "Step 570, Training Loss: 0.21158821880817413\n",
            "Step 570, Training Loss: 0.11011207848787308\n",
            "Step 570, Training Loss: 0.1524556428194046\n",
            "Step 570, Training Loss: 0.3234027028083801\n",
            "Step 570, Training Loss: 0.27223464846611023\n",
            "Step 570, Training Loss: 0.09417864680290222\n",
            "Step 570, Training Loss: 0.19801343977451324\n",
            "Step 570, Training Loss: 0.2128010243177414\n",
            "Step 570, Training Loss: 0.035433925688266754\n",
            "Step 570, Training Loss: 0.22506898641586304\n",
            "Step 570, Training Loss: 0.1343059539794922\n",
            "Step 570, Training Loss: 0.21432359516620636\n",
            "Step 570, Training Loss: 0.12983983755111694\n",
            "Step 570, Training Loss: 0.010491492226719856\n",
            "Step 570, Training Loss: 0.2766072452068329\n",
            "Step 570, Training Loss: 0.13904312252998352\n",
            "Step 570, Training Loss: 0.15459603071212769\n",
            "Step 570, Training Loss: 0.17769305408000946\n",
            "Step 570, Training Loss: 0.09377742558717728\n",
            "Step 570, Training Loss: 0.25873374938964844\n",
            "Step 570, Training Loss: 0.19978822767734528\n",
            "Step 570, Training Loss: 0.07302077114582062\n",
            "Step 570, Training Loss: 0.08976095169782639\n",
            "Step 570, Training Loss: 0.21218153834342957\n",
            "Step 570, Training Loss: 0.08131750673055649\n",
            "Step 570, Training Loss: 0.17688174545764923\n",
            "Step 570, Training Loss: 0.12417307496070862\n",
            "Step 570, Training Loss: 0.01921781711280346\n",
            "Step 570, Training Loss: 0.026918970048427582\n",
            "Step 580, Training Loss: 0.023337163031101227\n",
            "Step 580, Training Loss: 0.051672257483005524\n",
            "Step 590, Training Loss: 0.020990272983908653\n",
            "Step 590, Training Loss: 0.03187444433569908\n",
            "Step 600, Training Loss: 0.022079626098275185\n",
            "Step 600, Training Loss: 0.012239139527082443\n",
            "Step 610, Training Loss: 0.02652638405561447\n",
            "Step 610, Training Loss: 0.0030869028996676207\n",
            "Step 620, Training Loss: 0.025822559371590614\n",
            "Step 620, Training Loss: 0.03326542302966118\n",
            "Step 630, Training Loss: 0.0464986115694046\n",
            "Step 630, Training Loss: 0.007996521890163422\n",
            "Step 640, Training Loss: 0.027562683448195457\n",
            "Step 640, Training Loss: 0.09592441469430923\n",
            "Step 650, Training Loss: 0.03895590826869011\n",
            "Step 650, Training Loss: 0.01497756876051426\n",
            "Step 660, Training Loss: 0.00445247208699584\n",
            "Step 660, Training Loss: 0.032933901995420456\n",
            "Step 670, Training Loss: 0.17182089388370514\n",
            "Step 670, Training Loss: 0.04405740648508072\n",
            "Step 680, Training Loss: 0.08431247621774673\n",
            "Step 680, Training Loss: 0.01740591786801815\n",
            "Step 690, Training Loss: 0.009755905717611313\n",
            "Step 690, Training Loss: 0.008530941791832447\n",
            "Step 700, Training Loss: 0.011120283044874668\n",
            "Step 700, Training Loss: 0.10785356909036636\n",
            "Step 710, Training Loss: 0.02825123816728592\n",
            "Step 710, Training Loss: 0.01018010824918747\n",
            "Step 720, Training Loss: 0.02369207702577114\n",
            "Step 720, Training Loss: 0.010381761007010937\n",
            "Step 730, Training Loss: 0.025540776550769806\n",
            "Step 730, Training Loss: 0.02333647385239601\n",
            "Step 740, Training Loss: 0.03717275336384773\n",
            "Step 740, Training Loss: 0.011214232072234154\n",
            "Step 750, Training Loss: 0.059376854449510574\n",
            "Step 750, Training Loss: 0.10600841790437698\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 1523\n",
            "  Batch size = 32\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 760, Training Loss: 0.008176138624548912\n",
            "Step 760, Training Loss: 0.24032896757125854\n",
            "Step 760, Training Loss: 0.28345954418182373\n",
            "Step 760, Training Loss: 0.09627772122621536\n",
            "Step 760, Training Loss: 0.16907034814357758\n",
            "Step 760, Training Loss: 0.15199518203735352\n",
            "Step 760, Training Loss: 0.18424168229103088\n",
            "Step 760, Training Loss: 0.10424970090389252\n",
            "Step 760, Training Loss: 0.17278426885604858\n",
            "Step 760, Training Loss: 0.08432174474000931\n",
            "Step 760, Training Loss: 0.1701100617647171\n",
            "Step 760, Training Loss: 0.05808959901332855\n",
            "Step 760, Training Loss: 0.1810881495475769\n",
            "Step 760, Training Loss: 0.16145816445350647\n",
            "Step 760, Training Loss: 0.05312607064843178\n",
            "Step 760, Training Loss: 0.07869350165128708\n",
            "Step 760, Training Loss: 0.1378743201494217\n",
            "Step 760, Training Loss: 0.1624481976032257\n",
            "Step 760, Training Loss: 0.06138601154088974\n",
            "Step 760, Training Loss: 0.19447234272956848\n",
            "Step 760, Training Loss: 0.11130692809820175\n",
            "Step 760, Training Loss: 0.23096461594104767\n",
            "Step 760, Training Loss: 0.20611725747585297\n",
            "Step 760, Training Loss: 0.11187160760164261\n",
            "Step 760, Training Loss: 0.1518820971250534\n",
            "Step 760, Training Loss: 0.30534622073173523\n",
            "Step 760, Training Loss: 0.2769039273262024\n",
            "Step 760, Training Loss: 0.08953902125358582\n",
            "Step 760, Training Loss: 0.19667473435401917\n",
            "Step 760, Training Loss: 0.20862171053886414\n",
            "Step 760, Training Loss: 0.031184963881969452\n",
            "Step 760, Training Loss: 0.22417914867401123\n",
            "Step 760, Training Loss: 0.12844140827655792\n",
            "Step 760, Training Loss: 0.2151457965373993\n",
            "Step 760, Training Loss: 0.13129514455795288\n",
            "Step 760, Training Loss: 0.01389412023127079\n",
            "Step 760, Training Loss: 0.27665039896965027\n",
            "Step 760, Training Loss: 0.14544136822223663\n",
            "Step 760, Training Loss: 0.1505511850118637\n",
            "Step 760, Training Loss: 0.1805342584848404\n",
            "Step 760, Training Loss: 0.09312394261360168\n",
            "Step 760, Training Loss: 0.24223273992538452\n",
            "Step 760, Training Loss: 0.21004214882850647\n",
            "Step 760, Training Loss: 0.08968912065029144\n",
            "Step 760, Training Loss: 0.09086474031209946\n",
            "Step 760, Training Loss: 0.20748525857925415\n",
            "Step 760, Training Loss: 0.08284737169742584\n",
            "Step 760, Training Loss: 0.170409694314003\n",
            "Step 760, Training Loss: 0.13040322065353394\n",
            "Step 760, Training Loss: 0.011252352967858315\n",
            "Step 760, Training Loss: 0.006301708985120058\n",
            "Step 770, Training Loss: 0.004819796420633793\n",
            "Step 770, Training Loss: 0.009495850652456284\n",
            "Step 780, Training Loss: 0.004063331987708807\n",
            "Step 780, Training Loss: 0.05194590613245964\n",
            "Step 790, Training Loss: 0.0071858931332826614\n",
            "Step 790, Training Loss: 0.011197218671441078\n",
            "Step 800, Training Loss: 0.013366990722715855\n",
            "Step 800, Training Loss: 0.011554686352610588\n",
            "Step 810, Training Loss: 0.0585378035902977\n",
            "Step 810, Training Loss: 0.049366023391485214\n",
            "Step 820, Training Loss: 0.0190153606235981\n",
            "Step 820, Training Loss: 0.022266695275902748\n",
            "Step 830, Training Loss: 0.017063258215785027\n",
            "Step 830, Training Loss: 0.02899159863591194\n",
            "Step 840, Training Loss: 0.04068131744861603\n",
            "Step 840, Training Loss: 0.012555900029838085\n",
            "Step 850, Training Loss: 0.01793651096522808\n",
            "Step 850, Training Loss: 0.007681851740926504\n",
            "Step 860, Training Loss: 0.005580132827162743\n",
            "Step 860, Training Loss: 0.012022307142615318\n",
            "Step 870, Training Loss: 0.018854528665542603\n",
            "Step 870, Training Loss: 0.03351442143321037\n",
            "Step 880, Training Loss: 0.012925407849252224\n",
            "Step 880, Training Loss: 0.014522927813231945\n",
            "Step 890, Training Loss: 0.07271235436201096\n",
            "Step 890, Training Loss: 0.01848584972321987\n",
            "Step 900, Training Loss: 0.12925207614898682\n",
            "Step 900, Training Loss: 0.014345000497996807\n",
            "Step 910, Training Loss: 0.0035590101033449173\n",
            "Step 910, Training Loss: 0.013851216062903404\n",
            "Step 920, Training Loss: 0.007190007716417313\n",
            "Step 920, Training Loss: 0.03778395801782608\n",
            "Step 930, Training Loss: 0.06441838294267654\n",
            "Step 930, Training Loss: 0.010880098678171635\n",
            "Step 940, Training Loss: 0.0870814248919487\n",
            "Step 940, Training Loss: 0.017447244375944138\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 1523\n",
            "  Batch size = 32\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 950, Training Loss: 0.004432578571140766\n",
            "Step 950, Training Loss: 0.2515498101711273\n",
            "Step 950, Training Loss: 0.29018908739089966\n",
            "Step 950, Training Loss: 0.09795112907886505\n",
            "Step 950, Training Loss: 0.17048220336437225\n",
            "Step 950, Training Loss: 0.15102480351924896\n",
            "Step 950, Training Loss: 0.19268925487995148\n",
            "Step 950, Training Loss: 0.10434183478355408\n",
            "Step 950, Training Loss: 0.17389719188213348\n",
            "Step 950, Training Loss: 0.08506704866886139\n",
            "Step 950, Training Loss: 0.17779484391212463\n",
            "Step 950, Training Loss: 0.05831149220466614\n",
            "Step 950, Training Loss: 0.19348102807998657\n",
            "Step 950, Training Loss: 0.16632837057113647\n",
            "Step 950, Training Loss: 0.04800637066364288\n",
            "Step 950, Training Loss: 0.07653136551380157\n",
            "Step 950, Training Loss: 0.14089486002922058\n",
            "Step 950, Training Loss: 0.16601689159870148\n",
            "Step 950, Training Loss: 0.061560943722724915\n",
            "Step 950, Training Loss: 0.19589625298976898\n",
            "Step 950, Training Loss: 0.11574245244264603\n",
            "Step 950, Training Loss: 0.23616307973861694\n",
            "Step 950, Training Loss: 0.20684753358364105\n",
            "Step 950, Training Loss: 0.11107084155082703\n",
            "Step 950, Training Loss: 0.1506362408399582\n",
            "Step 950, Training Loss: 0.31120479106903076\n",
            "Step 950, Training Loss: 0.2800526022911072\n",
            "Step 950, Training Loss: 0.0885874405503273\n",
            "Step 950, Training Loss: 0.20201663672924042\n",
            "Step 950, Training Loss: 0.2092011421918869\n",
            "Step 950, Training Loss: 0.030613357201218605\n",
            "Step 950, Training Loss: 0.2233026772737503\n",
            "Step 950, Training Loss: 0.1354416161775589\n",
            "Step 950, Training Loss: 0.21522611379623413\n",
            "Step 950, Training Loss: 0.1301823854446411\n",
            "Step 950, Training Loss: 0.009790271520614624\n",
            "Step 950, Training Loss: 0.2739393413066864\n",
            "Step 950, Training Loss: 0.14108803868293762\n",
            "Step 950, Training Loss: 0.14948853850364685\n",
            "Step 950, Training Loss: 0.17357152700424194\n",
            "Step 950, Training Loss: 0.09573230892419815\n",
            "Step 950, Training Loss: 0.25429701805114746\n",
            "Step 950, Training Loss: 0.21407555043697357\n",
            "Step 950, Training Loss: 0.09791190177202225\n",
            "Step 950, Training Loss: 0.09098072350025177\n",
            "Step 950, Training Loss: 0.20874223113059998\n",
            "Step 950, Training Loss: 0.08263536542654037\n",
            "Step 950, Training Loss: 0.17499440908432007\n",
            "Step 950, Training Loss: 0.13527603447437286\n",
            "Step 950, Training Loss: 0.017661359161138535\n",
            "Step 950, Training Loss: 0.04316480830311775\n",
            "Step 960, Training Loss: 0.004689342807978392\n",
            "Step 960, Training Loss: 0.017888985574245453\n",
            "Step 970, Training Loss: 0.007370194420218468\n",
            "Step 970, Training Loss: 0.023105671629309654\n",
            "Step 980, Training Loss: 0.01027671154588461\n",
            "Step 980, Training Loss: 0.0495411641895771\n",
            "Step 990, Training Loss: 0.017462417483329773\n",
            "Step 990, Training Loss: 0.011241385713219643\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Saving model checkpoint to ./results/checkpoint-1000\n",
            "Configuration saved in ./results/checkpoint-1000/config.json\n",
            "Model weights saved in ./results/checkpoint-1000/pytorch_model.bin\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 1000, Training Loss: 0.01771511696279049\n",
            "Step 1000, Training Loss: 0.022149708122015\n",
            "Step 1010, Training Loss: 0.014532305300235748\n",
            "Step 1010, Training Loss: 0.01925010420382023\n",
            "Step 1020, Training Loss: 0.01911206915974617\n",
            "Step 1020, Training Loss: 0.02350146882236004\n",
            "Step 1030, Training Loss: 0.036581214517354965\n",
            "Step 1030, Training Loss: 0.03533416986465454\n",
            "Step 1040, Training Loss: 0.012559342198073864\n",
            "Step 1040, Training Loss: 0.01550756674259901\n",
            "Step 1050, Training Loss: 0.013136256486177444\n",
            "Step 1050, Training Loss: 0.02349952422082424\n",
            "Step 1060, Training Loss: 0.027778178453445435\n",
            "Step 1060, Training Loss: 0.12063521146774292\n",
            "Step 1070, Training Loss: 0.08647706359624863\n",
            "Step 1070, Training Loss: 0.013348451815545559\n",
            "Step 1080, Training Loss: 0.0035072476603090763\n",
            "Step 1080, Training Loss: 0.011050102300941944\n",
            "Step 1090, Training Loss: 0.05258546024560928\n",
            "Step 1090, Training Loss: 0.008796739391982555\n",
            "Step 1100, Training Loss: 0.02005007304251194\n",
            "Step 1100, Training Loss: 0.03158767148852348\n",
            "Step 1110, Training Loss: 0.02289656735956669\n",
            "Step 1110, Training Loss: 0.014493533410131931\n",
            "Step 1120, Training Loss: 0.03659870848059654\n",
            "Step 1120, Training Loss: 0.012621930800378323\n",
            "Step 1130, Training Loss: 0.07179280370473862\n",
            "Step 1130, Training Loss: 0.0028531209100037813\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 1523\n",
            "  Batch size = 32\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 1140, Training Loss: 0.18306705355644226\n",
            "Step 1140, Training Loss: 0.24209146201610565\n",
            "Step 1140, Training Loss: 0.2847316563129425\n",
            "Step 1140, Training Loss: 0.09704034775495529\n",
            "Step 1140, Training Loss: 0.1674780547618866\n",
            "Step 1140, Training Loss: 0.15272341668605804\n",
            "Step 1140, Training Loss: 0.1830534040927887\n",
            "Step 1140, Training Loss: 0.1042170375585556\n",
            "Step 1140, Training Loss: 0.17297063767910004\n",
            "Step 1140, Training Loss: 0.08501534909009933\n",
            "Step 1140, Training Loss: 0.16799874603748322\n",
            "Step 1140, Training Loss: 0.05813576281070709\n",
            "Step 1140, Training Loss: 0.1834377646446228\n",
            "Step 1140, Training Loss: 0.16672345995903015\n",
            "Step 1140, Training Loss: 0.047738216817379\n",
            "Step 1140, Training Loss: 0.07480096071958542\n",
            "Step 1140, Training Loss: 0.13801626861095428\n",
            "Step 1140, Training Loss: 0.1698266863822937\n",
            "Step 1140, Training Loss: 0.06147969141602516\n",
            "Step 1140, Training Loss: 0.1948689967393875\n",
            "Step 1140, Training Loss: 0.11490587890148163\n",
            "Step 1140, Training Loss: 0.23672690987586975\n",
            "Step 1140, Training Loss: 0.20451083779335022\n",
            "Step 1140, Training Loss: 0.11108828336000443\n",
            "Step 1140, Training Loss: 0.15159961581230164\n",
            "Step 1140, Training Loss: 0.31486567854881287\n",
            "Step 1140, Training Loss: 0.2801017165184021\n",
            "Step 1140, Training Loss: 0.08498255908489227\n",
            "Step 1140, Training Loss: 0.19994336366653442\n",
            "Step 1140, Training Loss: 0.20850758254528046\n",
            "Step 1140, Training Loss: 0.030732646584510803\n",
            "Step 1140, Training Loss: 0.22517430782318115\n",
            "Step 1140, Training Loss: 0.13357752561569214\n",
            "Step 1140, Training Loss: 0.2156866490840912\n",
            "Step 1140, Training Loss: 0.1306181401014328\n",
            "Step 1140, Training Loss: 0.009465097449719906\n",
            "Step 1140, Training Loss: 0.2742343544960022\n",
            "Step 1140, Training Loss: 0.13882574439048767\n",
            "Step 1140, Training Loss: 0.15108899772167206\n",
            "Step 1140, Training Loss: 0.17482559382915497\n",
            "Step 1140, Training Loss: 0.09358295798301697\n",
            "Step 1140, Training Loss: 0.2567153573036194\n",
            "Step 1140, Training Loss: 0.21079270541667938\n",
            "Step 1140, Training Loss: 0.08419433981180191\n",
            "Step 1140, Training Loss: 0.09228454530239105\n",
            "Step 1140, Training Loss: 0.20970354974269867\n",
            "Step 1140, Training Loss: 0.08213750272989273\n",
            "Step 1140, Training Loss: 0.17381884157657623\n",
            "Step 1140, Training Loss: 0.13432098925113678\n",
            "Step 1140, Training Loss: 0.004300987813621759\n",
            "Step 1140, Training Loss: 0.014890420250594616\n",
            "Step 1150, Training Loss: 0.034570641815662384\n",
            "Step 1150, Training Loss: 0.010168629698455334\n",
            "Step 1160, Training Loss: 0.13644865155220032\n",
            "Step 1160, Training Loss: 0.011982718482613564\n",
            "Step 1170, Training Loss: 0.1015733852982521\n",
            "Step 1170, Training Loss: 0.012840553186833858\n",
            "Step 1180, Training Loss: 0.015581595711410046\n",
            "Step 1180, Training Loss: 0.003587824059650302\n",
            "Step 1190, Training Loss: 0.014690791256725788\n",
            "Step 1190, Training Loss: 0.006190781947225332\n",
            "Step 1200, Training Loss: 0.016616802662611008\n",
            "Step 1200, Training Loss: 0.020984718576073647\n",
            "Step 1210, Training Loss: 0.0307947788387537\n",
            "Step 1210, Training Loss: 0.03225594758987427\n",
            "Step 1220, Training Loss: 0.018596086651086807\n",
            "Step 1220, Training Loss: 0.04540460184216499\n",
            "Step 1230, Training Loss: 0.01252490933984518\n",
            "Step 1230, Training Loss: 0.05369029939174652\n",
            "Step 1240, Training Loss: 0.018163302913308144\n",
            "Step 1240, Training Loss: 0.011959140188992023\n",
            "Step 1250, Training Loss: 0.0207657590508461\n",
            "Step 1250, Training Loss: 0.01730576902627945\n",
            "Step 1260, Training Loss: 0.007167452946305275\n",
            "Step 1260, Training Loss: 0.013205383904278278\n",
            "Step 1270, Training Loss: 0.07651593536138535\n",
            "Step 1270, Training Loss: 0.06507240235805511\n",
            "Step 1280, Training Loss: 0.05962060019373894\n",
            "Step 1280, Training Loss: 0.0044264174066483974\n",
            "Step 1290, Training Loss: 0.05390700697898865\n",
            "Step 1290, Training Loss: 0.0193218681961298\n",
            "Step 1300, Training Loss: 0.008275289088487625\n",
            "Step 1300, Training Loss: 0.0071531301364302635\n",
            "Step 1310, Training Loss: 0.01023421622812748\n",
            "Step 1310, Training Loss: 0.010414640419185162\n",
            "Step 1320, Training Loss: 0.010764828883111477\n",
            "Step 1320, Training Loss: 0.036993466317653656\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 1523\n",
            "  Batch size = 32\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 1330, Training Loss: 0.1302030235528946\n",
            "Step 1330, Training Loss: 0.24017438292503357\n",
            "Step 1330, Training Loss: 0.28217625617980957\n",
            "Step 1330, Training Loss: 0.09682860225439072\n",
            "Step 1330, Training Loss: 0.1648896187543869\n",
            "Step 1330, Training Loss: 0.15329262614250183\n",
            "Step 1330, Training Loss: 0.18258793652057648\n",
            "Step 1330, Training Loss: 0.10402058809995651\n",
            "Step 1330, Training Loss: 0.17275021970272064\n",
            "Step 1330, Training Loss: 0.0851944237947464\n",
            "Step 1330, Training Loss: 0.16577918827533722\n",
            "Step 1330, Training Loss: 0.05802438035607338\n",
            "Step 1330, Training Loss: 0.1799071878194809\n",
            "Step 1330, Training Loss: 0.16603368520736694\n",
            "Step 1330, Training Loss: 0.048533979803323746\n",
            "Step 1330, Training Loss: 0.07501187920570374\n",
            "Step 1330, Training Loss: 0.1380918323993683\n",
            "Step 1330, Training Loss: 0.17103211581707\n",
            "Step 1330, Training Loss: 0.06125814840197563\n",
            "Step 1330, Training Loss: 0.19499322772026062\n",
            "Step 1330, Training Loss: 0.11618999391794205\n",
            "Step 1330, Training Loss: 0.23481862246990204\n",
            "Step 1330, Training Loss: 0.20607437193393707\n",
            "Step 1330, Training Loss: 0.11110709607601166\n",
            "Step 1330, Training Loss: 0.15147125720977783\n",
            "Step 1330, Training Loss: 0.314101904630661\n",
            "Step 1330, Training Loss: 0.2793992757797241\n",
            "Step 1330, Training Loss: 0.08399877697229385\n",
            "Step 1330, Training Loss: 0.19921091198921204\n",
            "Step 1330, Training Loss: 0.20797239243984222\n",
            "Step 1330, Training Loss: 0.030573831871151924\n",
            "Step 1330, Training Loss: 0.22391419112682343\n",
            "Step 1330, Training Loss: 0.1339080035686493\n",
            "Step 1330, Training Loss: 0.21548902988433838\n",
            "Step 1330, Training Loss: 0.13061806559562683\n",
            "Step 1330, Training Loss: 0.009285186417400837\n",
            "Step 1330, Training Loss: 0.27591472864151\n",
            "Step 1330, Training Loss: 0.13804206252098083\n",
            "Step 1330, Training Loss: 0.15171988308429718\n",
            "Step 1330, Training Loss: 0.17316165566444397\n",
            "Step 1330, Training Loss: 0.09391964226961136\n",
            "Step 1330, Training Loss: 0.2554027736186981\n",
            "Step 1330, Training Loss: 0.20809559524059296\n",
            "Step 1330, Training Loss: 0.07986797392368317\n",
            "Step 1330, Training Loss: 0.09212157130241394\n",
            "Step 1330, Training Loss: 0.20911012589931488\n",
            "Step 1330, Training Loss: 0.08192058652639389\n",
            "Step 1330, Training Loss: 0.17410758137702942\n",
            "Step 1330, Training Loss: 0.1353328973054886\n",
            "Step 1330, Training Loss: 0.006954230833798647\n",
            "Step 1330, Training Loss: 0.004020498599857092\n",
            "Step 1340, Training Loss: 0.05581256002187729\n",
            "Step 1340, Training Loss: 0.12116730958223343\n",
            "Step 1350, Training Loss: 0.011475405655801296\n",
            "Step 1350, Training Loss: 0.0830669105052948\n",
            "Step 1360, Training Loss: 0.08059854805469513\n",
            "Step 1360, Training Loss: 0.013193856924772263\n",
            "Step 1370, Training Loss: 0.009578448720276356\n",
            "Step 1370, Training Loss: 0.041838034987449646\n",
            "Step 1380, Training Loss: 0.016968807205557823\n",
            "Step 1380, Training Loss: 0.03247421979904175\n",
            "Step 1390, Training Loss: 0.014037320390343666\n",
            "Step 1390, Training Loss: 0.05133189633488655\n",
            "Step 1400, Training Loss: 0.022899040952324867\n",
            "Step 1400, Training Loss: 0.044818028807640076\n",
            "Step 1410, Training Loss: 0.0880696102976799\n",
            "Step 1410, Training Loss: 0.009284835308790207\n",
            "Step 1420, Training Loss: 0.008115828037261963\n",
            "Step 1420, Training Loss: 0.08435385674238205\n",
            "Step 1430, Training Loss: 0.03354503586888313\n",
            "Step 1430, Training Loss: 0.03985754773020744\n",
            "Step 1440, Training Loss: 0.008194351568818092\n",
            "Step 1440, Training Loss: 0.008811785839498043\n",
            "Step 1450, Training Loss: 0.03463508188724518\n",
            "Step 1450, Training Loss: 0.018229106441140175\n",
            "Step 1460, Training Loss: 0.020505953580141068\n",
            "Step 1460, Training Loss: 0.008892112411558628\n",
            "Step 1470, Training Loss: 0.03743235021829605\n",
            "Step 1470, Training Loss: 0.018581144511699677\n",
            "Step 1480, Training Loss: 0.015740586444735527\n",
            "Step 1480, Training Loss: 0.007217247039079666\n",
            "Step 1490, Training Loss: 0.008794340305030346\n",
            "Step 1490, Training Loss: 0.023706583306193352\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Saving model checkpoint to ./results/checkpoint-1500\n",
            "Configuration saved in ./results/checkpoint-1500/config.json\n",
            "Model weights saved in ./results/checkpoint-1500/pytorch_model.bin\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 1500, Training Loss: 0.012774533592164516\n",
            "Step 1500, Training Loss: 0.03054114803671837\n",
            "Step 1510, Training Loss: 0.05470473691821098\n",
            "Step 1510, Training Loss: 0.011165066622197628\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 1523\n",
            "  Batch size = 32\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 1520, Training Loss: 0.006286303512752056\n",
            "Step 1520, Training Loss: 0.2375890016555786\n",
            "Step 1520, Training Loss: 0.28281909227371216\n",
            "Step 1520, Training Loss: 0.09751475602388382\n",
            "Step 1520, Training Loss: 0.16469036042690277\n",
            "Step 1520, Training Loss: 0.15252932906150818\n",
            "Step 1520, Training Loss: 0.1813012659549713\n",
            "Step 1520, Training Loss: 0.10390215367078781\n",
            "Step 1520, Training Loss: 0.17190244793891907\n",
            "Step 1520, Training Loss: 0.08616815507411957\n",
            "Step 1520, Training Loss: 0.16417650878429413\n",
            "Step 1520, Training Loss: 0.05804932117462158\n",
            "Step 1520, Training Loss: 0.17862941324710846\n",
            "Step 1520, Training Loss: 0.1673152595758438\n",
            "Step 1520, Training Loss: 0.050502877682447433\n",
            "Step 1520, Training Loss: 0.07679131627082825\n",
            "Step 1520, Training Loss: 0.1372680515050888\n",
            "Step 1520, Training Loss: 0.1720903515815735\n",
            "Step 1520, Training Loss: 0.061213888227939606\n",
            "Step 1520, Training Loss: 0.19414225220680237\n",
            "Step 1520, Training Loss: 0.12035951018333435\n",
            "Step 1520, Training Loss: 0.235780268907547\n",
            "Step 1520, Training Loss: 0.2073371708393097\n",
            "Step 1520, Training Loss: 0.1110985279083252\n",
            "Step 1520, Training Loss: 0.15229733288288116\n",
            "Step 1520, Training Loss: 0.31702035665512085\n",
            "Step 1520, Training Loss: 0.2779136002063751\n",
            "Step 1520, Training Loss: 0.085611492395401\n",
            "Step 1520, Training Loss: 0.19907407462596893\n",
            "Step 1520, Training Loss: 0.20759113132953644\n",
            "Step 1520, Training Loss: 0.03144226595759392\n",
            "Step 1520, Training Loss: 0.22348642349243164\n",
            "Step 1520, Training Loss: 0.1367231160402298\n",
            "Step 1520, Training Loss: 0.215352863073349\n",
            "Step 1520, Training Loss: 0.1312927007675171\n",
            "Step 1520, Training Loss: 0.01048033032566309\n",
            "Step 1520, Training Loss: 0.27525442838668823\n",
            "Step 1520, Training Loss: 0.13613945245742798\n",
            "Step 1520, Training Loss: 0.15287372469902039\n",
            "Step 1520, Training Loss: 0.17079026997089386\n",
            "Step 1520, Training Loss: 0.0932978168129921\n",
            "Step 1520, Training Loss: 0.25299498438835144\n",
            "Step 1520, Training Loss: 0.20536300539970398\n",
            "Step 1520, Training Loss: 0.07615305483341217\n",
            "Step 1520, Training Loss: 0.09238728135824203\n",
            "Step 1520, Training Loss: 0.2102866917848587\n",
            "Step 1520, Training Loss: 0.0819169282913208\n",
            "Step 1520, Training Loss: 0.17174361646175385\n",
            "Step 1520, Training Loss: 0.13229258358478546\n",
            "Step 1520, Training Loss: 0.019092507660388947\n",
            "Step 1520, Training Loss: 0.008550065569579601\n",
            "Step 1530, Training Loss: 0.009563982486724854\n",
            "Step 1530, Training Loss: 0.10339181125164032\n",
            "Step 1540, Training Loss: 0.08106007426977158\n",
            "Step 1540, Training Loss: 0.011205310933291912\n",
            "Step 1550, Training Loss: 0.018477896228432655\n",
            "Step 1550, Training Loss: 0.04772466793656349\n",
            "Step 1560, Training Loss: 0.004447329789400101\n",
            "Step 1560, Training Loss: 0.029191231355071068\n",
            "Step 1570, Training Loss: 0.023241551592946053\n",
            "Step 1570, Training Loss: 0.025878017768263817\n",
            "Step 1580, Training Loss: 0.06054467335343361\n",
            "Step 1580, Training Loss: 0.01286845188587904\n",
            "Step 1590, Training Loss: 0.034494247287511826\n",
            "Step 1590, Training Loss: 0.012840943410992622\n",
            "Step 1600, Training Loss: 0.0037013525143265724\n",
            "Step 1600, Training Loss: 0.033602576702833176\n",
            "Step 1610, Training Loss: 0.012818263843655586\n",
            "Step 1610, Training Loss: 0.019976375624537468\n",
            "Step 1620, Training Loss: 0.1577887386083603\n",
            "Step 1620, Training Loss: 0.003982501104474068\n",
            "Step 1630, Training Loss: 0.014623204246163368\n",
            "Step 1630, Training Loss: 0.05359012261033058\n",
            "Step 1640, Training Loss: 0.013529917225241661\n",
            "Step 1640, Training Loss: 0.006985780782997608\n",
            "Step 1650, Training Loss: 0.01468090433627367\n",
            "Step 1650, Training Loss: 0.011466318741440773\n",
            "Step 1660, Training Loss: 0.01649152673780918\n",
            "Step 1660, Training Loss: 0.013473170809447765\n",
            "Step 1670, Training Loss: 0.009846551343798637\n",
            "Step 1670, Training Loss: 0.0853331908583641\n",
            "Step 1680, Training Loss: 0.06117302551865578\n",
            "Step 1680, Training Loss: 0.014849837869405746\n",
            "Step 1690, Training Loss: 0.01617995835840702\n",
            "Step 1690, Training Loss: 0.04293401911854744\n",
            "Step 1700, Training Loss: 0.026903070509433746\n",
            "Step 1700, Training Loss: 0.00793310534209013\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 1523\n",
            "  Batch size = 32\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 1710, Training Loss: 0.2437208890914917\n",
            "Step 1710, Training Loss: 0.2880776524543762\n",
            "Step 1710, Training Loss: 0.09774444997310638\n",
            "Step 1710, Training Loss: 0.16500237584114075\n",
            "Step 1710, Training Loss: 0.15054543316364288\n",
            "Step 1710, Training Loss: 0.18497978150844574\n",
            "Step 1710, Training Loss: 0.10411711782217026\n",
            "Step 1710, Training Loss: 0.17271097004413605\n",
            "Step 1710, Training Loss: 0.08634354919195175\n",
            "Step 1710, Training Loss: 0.1692558377981186\n",
            "Step 1710, Training Loss: 0.058212436735630035\n",
            "Step 1710, Training Loss: 0.18696658313274384\n",
            "Step 1710, Training Loss: 0.1711510568857193\n",
            "Step 1710, Training Loss: 0.04783612862229347\n",
            "Step 1710, Training Loss: 0.07425841689109802\n",
            "Step 1710, Training Loss: 0.137391597032547\n",
            "Step 1710, Training Loss: 0.17117147147655487\n",
            "Step 1710, Training Loss: 0.06161544471979141\n",
            "Step 1710, Training Loss: 0.19430574774742126\n",
            "Step 1710, Training Loss: 0.12045344710350037\n",
            "Step 1710, Training Loss: 0.23599518835544586\n",
            "Step 1710, Training Loss: 0.2089659422636032\n",
            "Step 1710, Training Loss: 0.11115090548992157\n",
            "Step 1710, Training Loss: 0.15126550197601318\n",
            "Step 1710, Training Loss: 0.31655311584472656\n",
            "Step 1710, Training Loss: 0.2790929675102234\n",
            "Step 1710, Training Loss: 0.08570925146341324\n",
            "Step 1710, Training Loss: 0.20168501138687134\n",
            "Step 1710, Training Loss: 0.20860891044139862\n",
            "Step 1710, Training Loss: 0.03159455955028534\n",
            "Step 1710, Training Loss: 0.22587743401527405\n",
            "Step 1710, Training Loss: 0.14000394940376282\n",
            "Step 1710, Training Loss: 0.21604186296463013\n",
            "Step 1710, Training Loss: 0.13006173074245453\n",
            "Step 1710, Training Loss: 0.008595387451350689\n",
            "Step 1710, Training Loss: 0.27515706419944763\n",
            "Step 1710, Training Loss: 0.13887959718704224\n",
            "Step 1710, Training Loss: 0.1521749347448349\n",
            "Step 1710, Training Loss: 0.17455635964870453\n",
            "Step 1710, Training Loss: 0.09606596827507019\n",
            "Step 1710, Training Loss: 0.256949782371521\n",
            "Step 1710, Training Loss: 0.21055342257022858\n",
            "Step 1710, Training Loss: 0.08673497289419174\n",
            "Step 1710, Training Loss: 0.09166058897972107\n",
            "Step 1710, Training Loss: 0.211923286318779\n",
            "Step 1710, Training Loss: 0.08195377886295319\n",
            "Step 1710, Training Loss: 0.17684611678123474\n",
            "Step 1710, Training Loss: 0.13911928236484528\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 1523\n",
            "  Batch size = 32\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training completed in 281.37 seconds.\n",
            "Step 1710, Training Loss: 0.2437208890914917\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='48' max='48' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [48/48 00:02]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 1710, Training Loss: 0.2880776524543762\n",
            "Step 1710, Training Loss: 0.09774444997310638\n",
            "Step 1710, Training Loss: 0.16500237584114075\n",
            "Step 1710, Training Loss: 0.15054543316364288\n",
            "Step 1710, Training Loss: 0.18497978150844574\n",
            "Step 1710, Training Loss: 0.10411711782217026\n",
            "Step 1710, Training Loss: 0.17271097004413605\n",
            "Step 1710, Training Loss: 0.08634354919195175\n",
            "Step 1710, Training Loss: 0.1692558377981186\n",
            "Step 1710, Training Loss: 0.058212436735630035\n",
            "Step 1710, Training Loss: 0.18696658313274384\n",
            "Step 1710, Training Loss: 0.1711510568857193\n",
            "Step 1710, Training Loss: 0.04783612862229347\n",
            "Step 1710, Training Loss: 0.07425841689109802\n",
            "Step 1710, Training Loss: 0.137391597032547\n",
            "Step 1710, Training Loss: 0.17117147147655487\n",
            "Step 1710, Training Loss: 0.06161544471979141\n",
            "Step 1710, Training Loss: 0.19430574774742126\n",
            "Step 1710, Training Loss: 0.12045344710350037\n",
            "Step 1710, Training Loss: 0.23599518835544586\n",
            "Step 1710, Training Loss: 0.2089659422636032\n",
            "Step 1710, Training Loss: 0.11115090548992157\n",
            "Step 1710, Training Loss: 0.15126550197601318\n",
            "Step 1710, Training Loss: 0.31655311584472656\n",
            "Step 1710, Training Loss: 0.2790929675102234\n",
            "Step 1710, Training Loss: 0.08570925146341324\n",
            "Step 1710, Training Loss: 0.20168501138687134\n",
            "Step 1710, Training Loss: 0.20860891044139862\n",
            "Step 1710, Training Loss: 0.03159455955028534\n",
            "Step 1710, Training Loss: 0.22587743401527405\n",
            "Step 1710, Training Loss: 0.14000394940376282\n",
            "Step 1710, Training Loss: 0.21604186296463013\n",
            "Step 1710, Training Loss: 0.13006173074245453\n",
            "Step 1710, Training Loss: 0.008595387451350689\n",
            "Step 1710, Training Loss: 0.27515706419944763\n",
            "Step 1710, Training Loss: 0.13887959718704224\n",
            "Step 1710, Training Loss: 0.1521749347448349\n",
            "Step 1710, Training Loss: 0.17455635964870453\n",
            "Step 1710, Training Loss: 0.09606596827507019\n",
            "Step 1710, Training Loss: 0.256949782371521\n",
            "Step 1710, Training Loss: 0.21055342257022858\n",
            "Step 1710, Training Loss: 0.08673497289419174\n",
            "Step 1710, Training Loss: 0.09166058897972107\n",
            "Step 1710, Training Loss: 0.211923286318779\n",
            "Step 1710, Training Loss: 0.08195377886295319\n",
            "Step 1710, Training Loss: 0.17684611678123474\n",
            "Step 1710, Training Loss: 0.13911928236484528\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-12-09 19:27:02,456] Trial 7 finished with value: 0.8549150922550887 and parameters: {'learning_rate': 2.2684757405421545e-06, 'batch_size': 16, 'num_train_epochs': 9, 'weight_decay': 0.002918533807233721}. Best is trial 6 with value: 0.8559372738098081.\n",
            "<ipython-input-12-42465065092b>:2: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  learning_rate = trial.suggest_loguniform(\"learning_rate\", 1e-6, 5e-5)\n",
            "<ipython-input-12-42465065092b>:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  weight_decay = trial.suggest_loguniform(\"weight_decay\", 1e-4, 0.1)\n",
            "PyTorch: setting up devices\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n",
            "***** Running training *****\n",
            "  Num examples = 6091\n",
            "  Num Epochs = 5\n",
            "  Instantaneous batch size per device = 16\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
            "  Gradient Accumulation steps = 2\n",
            "  Total optimization steps = 950\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting training...\n",
            "Step 0, Training Loss: 0.008752989582717419\n",
            "Step 0, Training Loss: 0.03306811302900314\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='950' max='950' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [950/950 02:35, Epoch 4/5]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "      <th>F1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>0.019600</td>\n",
              "      <td>0.159296</td>\n",
              "      <td>0.846356</td>\n",
              "      <td>0.855554</td>\n",
              "      <td>0.855584</td>\n",
              "      <td>0.852630</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.015900</td>\n",
              "      <td>0.156442</td>\n",
              "      <td>0.848982</td>\n",
              "      <td>0.858501</td>\n",
              "      <td>0.856868</td>\n",
              "      <td>0.855012</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.028700</td>\n",
              "      <td>0.156974</td>\n",
              "      <td>0.850295</td>\n",
              "      <td>0.857442</td>\n",
              "      <td>0.858151</td>\n",
              "      <td>0.854872</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.029200</td>\n",
              "      <td>0.157916</td>\n",
              "      <td>0.848326</td>\n",
              "      <td>0.856205</td>\n",
              "      <td>0.856226</td>\n",
              "      <td>0.853457</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.025700</td>\n",
              "      <td>0.157019</td>\n",
              "      <td>0.847012</td>\n",
              "      <td>0.855294</td>\n",
              "      <td>0.855584</td>\n",
              "      <td>0.852840</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 10, Training Loss: 0.004842395428568125\n",
            "Step 10, Training Loss: 0.02569926530122757\n",
            "Step 20, Training Loss: 0.019107401371002197\n",
            "Step 20, Training Loss: 0.006252435967326164\n",
            "Step 30, Training Loss: 0.020039580762386322\n",
            "Step 30, Training Loss: 0.06634719669818878\n",
            "Step 40, Training Loss: 0.003593250410631299\n",
            "Step 40, Training Loss: 0.0029598181135952473\n",
            "Step 50, Training Loss: 0.004205803852528334\n",
            "Step 50, Training Loss: 0.015126908197999\n",
            "Step 60, Training Loss: 0.004857413005083799\n",
            "Step 60, Training Loss: 0.10117930918931961\n",
            "Step 70, Training Loss: 0.04689675197005272\n",
            "Step 70, Training Loss: 0.023645387962460518\n",
            "Step 80, Training Loss: 0.011835538782179356\n",
            "Step 80, Training Loss: 0.014458071440458298\n",
            "Step 90, Training Loss: 0.0064705172553658485\n",
            "Step 90, Training Loss: 0.0123031260445714\n",
            "Step 100, Training Loss: 0.013074266724288464\n",
            "Step 100, Training Loss: 0.003985582385212183\n",
            "Step 110, Training Loss: 0.03265928104519844\n",
            "Step 110, Training Loss: 0.015082143247127533\n",
            "Step 120, Training Loss: 0.0170094333589077\n",
            "Step 120, Training Loss: 0.006568167824298143\n",
            "Step 130, Training Loss: 0.029100626707077026\n",
            "Step 130, Training Loss: 0.008861396461725235\n",
            "Step 140, Training Loss: 0.04082035645842552\n",
            "Step 140, Training Loss: 0.024070626124739647\n",
            "Step 150, Training Loss: 0.0553465262055397\n",
            "Step 150, Training Loss: 0.059931255877017975\n",
            "Step 160, Training Loss: 0.10902128368616104\n",
            "Step 160, Training Loss: 0.05385637283325195\n",
            "Step 170, Training Loss: 0.11345511674880981\n",
            "Step 170, Training Loss: 0.030888386070728302\n",
            "Step 180, Training Loss: 0.03356602415442467\n",
            "Step 180, Training Loss: 0.03984188288450241\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 1523\n",
            "  Batch size = 32\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 190, Training Loss: 0.036359142512083054\n",
            "Step 190, Training Loss: 0.25034424662590027\n",
            "Step 190, Training Loss: 0.28815317153930664\n",
            "Step 190, Training Loss: 0.09872592240571976\n",
            "Step 190, Training Loss: 0.16988418996334076\n",
            "Step 190, Training Loss: 0.14779570698738098\n",
            "Step 190, Training Loss: 0.19190900027751923\n",
            "Step 190, Training Loss: 0.10414014756679535\n",
            "Step 190, Training Loss: 0.1732773631811142\n",
            "Step 190, Training Loss: 0.08619067817926407\n",
            "Step 190, Training Loss: 0.18322055041790009\n",
            "Step 190, Training Loss: 0.058561548590660095\n",
            "Step 190, Training Loss: 0.19860345125198364\n",
            "Step 190, Training Loss: 0.171300008893013\n",
            "Step 190, Training Loss: 0.04786089435219765\n",
            "Step 190, Training Loss: 0.07542552053928375\n",
            "Step 190, Training Loss: 0.14009074866771698\n",
            "Step 190, Training Loss: 0.17543858289718628\n",
            "Step 190, Training Loss: 0.061897579580545425\n",
            "Step 190, Training Loss: 0.19570323824882507\n",
            "Step 190, Training Loss: 0.11954393237829208\n",
            "Step 190, Training Loss: 0.24786540865898132\n",
            "Step 190, Training Loss: 0.20312584936618805\n",
            "Step 190, Training Loss: 0.1114993691444397\n",
            "Step 190, Training Loss: 0.15242226421833038\n",
            "Step 190, Training Loss: 0.3205869197845459\n",
            "Step 190, Training Loss: 0.27959567308425903\n",
            "Step 190, Training Loss: 0.10934694111347198\n",
            "Step 190, Training Loss: 0.20918700098991394\n",
            "Step 190, Training Loss: 0.20912855863571167\n",
            "Step 190, Training Loss: 0.028625423088669777\n",
            "Step 190, Training Loss: 0.22444412112236023\n",
            "Step 190, Training Loss: 0.15254269540309906\n",
            "Step 190, Training Loss: 0.21586966514587402\n",
            "Step 190, Training Loss: 0.13038058578968048\n",
            "Step 190, Training Loss: 0.007111771497875452\n",
            "Step 190, Training Loss: 0.2644701898097992\n",
            "Step 190, Training Loss: 0.13379517197608948\n",
            "Step 190, Training Loss: 0.1519804745912552\n",
            "Step 190, Training Loss: 0.17223678529262543\n",
            "Step 190, Training Loss: 0.09429387003183365\n",
            "Step 190, Training Loss: 0.2630210518836975\n",
            "Step 190, Training Loss: 0.20957276225090027\n",
            "Step 190, Training Loss: 0.09901364892721176\n",
            "Step 190, Training Loss: 0.10093944519758224\n",
            "Step 190, Training Loss: 0.21320490539073944\n",
            "Step 190, Training Loss: 0.08178004622459412\n",
            "Step 190, Training Loss: 0.17591899633407593\n",
            "Step 190, Training Loss: 0.13719166815280914\n",
            "Step 190, Training Loss: 0.02075015753507614\n",
            "Step 190, Training Loss: 0.014565707184374332\n",
            "Step 200, Training Loss: 0.005279159173369408\n",
            "Step 200, Training Loss: 0.014638298191130161\n",
            "Step 210, Training Loss: 0.03087233379483223\n",
            "Step 210, Training Loss: 0.01205108966678381\n",
            "Step 220, Training Loss: 0.008849574252963066\n",
            "Step 220, Training Loss: 0.023502977564930916\n",
            "Step 230, Training Loss: 0.005033946596086025\n",
            "Step 230, Training Loss: 0.00891962368041277\n",
            "Step 240, Training Loss: 0.045701369643211365\n",
            "Step 240, Training Loss: 0.037752486765384674\n",
            "Step 250, Training Loss: 0.007023932412266731\n",
            "Step 250, Training Loss: 0.03157810866832733\n",
            "Step 260, Training Loss: 0.011227007955312729\n",
            "Step 260, Training Loss: 0.05408044904470444\n",
            "Step 270, Training Loss: 0.018492722883820534\n",
            "Step 270, Training Loss: 0.010900172404944897\n",
            "Step 280, Training Loss: 0.007068852428346872\n",
            "Step 280, Training Loss: 0.033255062997341156\n",
            "Step 290, Training Loss: 0.05643969774246216\n",
            "Step 290, Training Loss: 0.06348979473114014\n",
            "Step 300, Training Loss: 0.0761408731341362\n",
            "Step 300, Training Loss: 0.017240002751350403\n",
            "Step 310, Training Loss: 0.013309591449797153\n",
            "Step 310, Training Loss: 0.0157599076628685\n",
            "Step 320, Training Loss: 0.0032751192338764668\n",
            "Step 320, Training Loss: 0.013092474080622196\n",
            "Step 330, Training Loss: 0.013178815133869648\n",
            "Step 330, Training Loss: 0.011094270274043083\n",
            "Step 340, Training Loss: 0.0035371435806155205\n",
            "Step 340, Training Loss: 0.014254618436098099\n",
            "Step 350, Training Loss: 0.015954356640577316\n",
            "Step 350, Training Loss: 0.06873388588428497\n",
            "Step 360, Training Loss: 0.024874689057469368\n",
            "Step 360, Training Loss: 0.013161059468984604\n",
            "Step 370, Training Loss: 0.005101307295262814\n",
            "Step 370, Training Loss: 0.011587121523916721\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 1523\n",
            "  Batch size = 32\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 380, Training Loss: 0.015156215056777\n",
            "Step 380, Training Loss: 0.24388056993484497\n",
            "Step 380, Training Loss: 0.2821418344974518\n",
            "Step 380, Training Loss: 0.09719529747962952\n",
            "Step 380, Training Loss: 0.16545334458351135\n",
            "Step 380, Training Loss: 0.14940229058265686\n",
            "Step 380, Training Loss: 0.18732868134975433\n",
            "Step 380, Training Loss: 0.10404190421104431\n",
            "Step 380, Training Loss: 0.17203651368618011\n",
            "Step 380, Training Loss: 0.08499220758676529\n",
            "Step 380, Training Loss: 0.17602486908435822\n",
            "Step 380, Training Loss: 0.05822146311402321\n",
            "Step 380, Training Loss: 0.18201392889022827\n",
            "Step 380, Training Loss: 0.1619315892457962\n",
            "Step 380, Training Loss: 0.05310780927538872\n",
            "Step 380, Training Loss: 0.07894124835729599\n",
            "Step 380, Training Loss: 0.13477788865566254\n",
            "Step 380, Training Loss: 0.17637476325035095\n",
            "Step 380, Training Loss: 0.06158720701932907\n",
            "Step 380, Training Loss: 0.1961277276277542\n",
            "Step 380, Training Loss: 0.11505556106567383\n",
            "Step 380, Training Loss: 0.24264657497406006\n",
            "Step 380, Training Loss: 0.20328781008720398\n",
            "Step 380, Training Loss: 0.11113323271274567\n",
            "Step 380, Training Loss: 0.15190596878528595\n",
            "Step 380, Training Loss: 0.31997954845428467\n",
            "Step 380, Training Loss: 0.2785087525844574\n",
            "Step 380, Training Loss: 0.09274900704622269\n",
            "Step 380, Training Loss: 0.1997220814228058\n",
            "Step 380, Training Loss: 0.20841370522975922\n",
            "Step 380, Training Loss: 0.029796507209539413\n",
            "Step 380, Training Loss: 0.22335512936115265\n",
            "Step 380, Training Loss: 0.1342943161725998\n",
            "Step 380, Training Loss: 0.21574260294437408\n",
            "Step 380, Training Loss: 0.13131754100322723\n",
            "Step 380, Training Loss: 0.00768236443400383\n",
            "Step 380, Training Loss: 0.27242520451545715\n",
            "Step 380, Training Loss: 0.13518306612968445\n",
            "Step 380, Training Loss: 0.15244337916374207\n",
            "Step 380, Training Loss: 0.17560335993766785\n",
            "Step 380, Training Loss: 0.09201540797948837\n",
            "Step 380, Training Loss: 0.2566315531730652\n",
            "Step 380, Training Loss: 0.20660892128944397\n",
            "Step 380, Training Loss: 0.08372791111469269\n",
            "Step 380, Training Loss: 0.09417185187339783\n",
            "Step 380, Training Loss: 0.21079929172992706\n",
            "Step 380, Training Loss: 0.0815492570400238\n",
            "Step 380, Training Loss: 0.17171809077262878\n",
            "Step 380, Training Loss: 0.13745972514152527\n",
            "Step 380, Training Loss: 0.011798995546996593\n",
            "Step 380, Training Loss: 0.02960263192653656\n",
            "Step 390, Training Loss: 0.005531589034944773\n",
            "Step 390, Training Loss: 0.025444751605391502\n",
            "Step 400, Training Loss: 0.011761358007788658\n",
            "Step 400, Training Loss: 0.021823706105351448\n",
            "Step 410, Training Loss: 0.025130638852715492\n",
            "Step 410, Training Loss: 0.014273462817072868\n",
            "Step 420, Training Loss: 0.02359669841825962\n",
            "Step 420, Training Loss: 0.03520120307803154\n",
            "Step 430, Training Loss: 0.015398398041725159\n",
            "Step 430, Training Loss: 0.09058059006929398\n",
            "Step 440, Training Loss: 0.09864882379770279\n",
            "Step 440, Training Loss: 0.012624721974134445\n",
            "Step 450, Training Loss: 0.03836265951395035\n",
            "Step 450, Training Loss: 0.009485943242907524\n",
            "Step 460, Training Loss: 0.012129654176533222\n",
            "Step 460, Training Loss: 0.028851067647337914\n",
            "Step 470, Training Loss: 0.015398812480270863\n",
            "Step 470, Training Loss: 0.05465014651417732\n",
            "Step 480, Training Loss: 0.047592200338840485\n",
            "Step 480, Training Loss: 0.11384782940149307\n",
            "Step 490, Training Loss: 0.026867153123021126\n",
            "Step 490, Training Loss: 0.01624363474547863\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Saving model checkpoint to ./results/checkpoint-500\n",
            "Configuration saved in ./results/checkpoint-500/config.json\n",
            "Model weights saved in ./results/checkpoint-500/pytorch_model.bin\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 500, Training Loss: 0.039094503968954086\n",
            "Step 500, Training Loss: 0.03711014986038208\n",
            "Step 510, Training Loss: 0.006589415017515421\n",
            "Step 510, Training Loss: 0.023353807628154755\n",
            "Step 520, Training Loss: 0.012997472658753395\n",
            "Step 520, Training Loss: 0.014893580228090286\n",
            "Step 530, Training Loss: 0.01063966192305088\n",
            "Step 530, Training Loss: 0.06809597462415695\n",
            "Step 540, Training Loss: 0.06520910561084747\n",
            "Step 540, Training Loss: 0.033624373376369476\n",
            "Step 550, Training Loss: 0.016407573595643044\n",
            "Step 550, Training Loss: 0.022861627861857414\n",
            "Step 560, Training Loss: 0.0156535841524601\n",
            "Step 560, Training Loss: 0.04094360023736954\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 1523\n",
            "  Batch size = 32\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 570, Training Loss: 0.023762991651892662\n",
            "Step 570, Training Loss: 0.24072392284870148\n",
            "Step 570, Training Loss: 0.284374862909317\n",
            "Step 570, Training Loss: 0.09732679277658463\n",
            "Step 570, Training Loss: 0.16617989540100098\n",
            "Step 570, Training Loss: 0.1468692421913147\n",
            "Step 570, Training Loss: 0.18628384172916412\n",
            "Step 570, Training Loss: 0.10414180904626846\n",
            "Step 570, Training Loss: 0.17116983234882355\n",
            "Step 570, Training Loss: 0.08606704324483871\n",
            "Step 570, Training Loss: 0.17564992606639862\n",
            "Step 570, Training Loss: 0.05802921950817108\n",
            "Step 570, Training Loss: 0.18413369357585907\n",
            "Step 570, Training Loss: 0.16657036542892456\n",
            "Step 570, Training Loss: 0.049400683492422104\n",
            "Step 570, Training Loss: 0.07572460174560547\n",
            "Step 570, Training Loss: 0.13419818878173828\n",
            "Step 570, Training Loss: 0.17848679423332214\n",
            "Step 570, Training Loss: 0.061745334416627884\n",
            "Step 570, Training Loss: 0.1942429095506668\n",
            "Step 570, Training Loss: 0.12416410446166992\n",
            "Step 570, Training Loss: 0.24524757266044617\n",
            "Step 570, Training Loss: 0.2055976837873459\n",
            "Step 570, Training Loss: 0.11072766780853271\n",
            "Step 570, Training Loss: 0.15323732793331146\n",
            "Step 570, Training Loss: 0.32453852891921997\n",
            "Step 570, Training Loss: 0.2777671813964844\n",
            "Step 570, Training Loss: 0.09494254738092422\n",
            "Step 570, Training Loss: 0.20299820601940155\n",
            "Step 570, Training Loss: 0.20953013002872467\n",
            "Step 570, Training Loss: 0.031898435205221176\n",
            "Step 570, Training Loss: 0.2230774462223053\n",
            "Step 570, Training Loss: 0.1380544751882553\n",
            "Step 570, Training Loss: 0.2158443033695221\n",
            "Step 570, Training Loss: 0.13087615370750427\n",
            "Step 570, Training Loss: 0.007815493270754814\n",
            "Step 570, Training Loss: 0.2723585069179535\n",
            "Step 570, Training Loss: 0.1345142126083374\n",
            "Step 570, Training Loss: 0.1530030518770218\n",
            "Step 570, Training Loss: 0.17188195884227753\n",
            "Step 570, Training Loss: 0.09450095146894455\n",
            "Step 570, Training Loss: 0.2613833546638489\n",
            "Step 570, Training Loss: 0.20702844858169556\n",
            "Step 570, Training Loss: 0.07850377261638641\n",
            "Step 570, Training Loss: 0.09374504536390305\n",
            "Step 570, Training Loss: 0.21102888882160187\n",
            "Step 570, Training Loss: 0.08129493147134781\n",
            "Step 570, Training Loss: 0.17350174486637115\n",
            "Step 570, Training Loss: 0.13572949171066284\n",
            "Step 570, Training Loss: 0.016621869057416916\n",
            "Step 570, Training Loss: 0.02201117016375065\n",
            "Step 580, Training Loss: 0.021460847929120064\n",
            "Step 580, Training Loss: 0.04837528616189957\n",
            "Step 590, Training Loss: 0.019020136445760727\n",
            "Step 590, Training Loss: 0.02858605608344078\n",
            "Step 600, Training Loss: 0.013863605447113514\n",
            "Step 600, Training Loss: 0.01182318851351738\n",
            "Step 610, Training Loss: 0.022715263068675995\n",
            "Step 610, Training Loss: 0.0029894940089434385\n",
            "Step 620, Training Loss: 0.022888613864779472\n",
            "Step 620, Training Loss: 0.031058451160788536\n",
            "Step 630, Training Loss: 0.025591248646378517\n",
            "Step 630, Training Loss: 0.007768483366817236\n",
            "Step 640, Training Loss: 0.02384021133184433\n",
            "Step 640, Training Loss: 0.09021937102079391\n",
            "Step 650, Training Loss: 0.032598987221717834\n",
            "Step 650, Training Loss: 0.013384741730988026\n",
            "Step 660, Training Loss: 0.004408569075167179\n",
            "Step 660, Training Loss: 0.027294326573610306\n",
            "Step 670, Training Loss: 0.17133694887161255\n",
            "Step 670, Training Loss: 0.03744548186659813\n",
            "Step 680, Training Loss: 0.07425754517316818\n",
            "Step 680, Training Loss: 0.015571428462862968\n",
            "Step 690, Training Loss: 0.009750250726938248\n",
            "Step 690, Training Loss: 0.00765299191698432\n",
            "Step 700, Training Loss: 0.007838490419089794\n",
            "Step 700, Training Loss: 0.10561961680650711\n",
            "Step 710, Training Loss: 0.024386096745729446\n",
            "Step 710, Training Loss: 0.00978644099086523\n",
            "Step 720, Training Loss: 0.020936163142323494\n",
            "Step 720, Training Loss: 0.009521272964775562\n",
            "Step 730, Training Loss: 0.02432802878320217\n",
            "Step 730, Training Loss: 0.02055845595896244\n",
            "Step 740, Training Loss: 0.03234810754656792\n",
            "Step 740, Training Loss: 0.011200451292097569\n",
            "Step 750, Training Loss: 0.0588708221912384\n",
            "Step 750, Training Loss: 0.08026500046253204\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 1523\n",
            "  Batch size = 32\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 760, Training Loss: 0.00797693245112896\n",
            "Step 760, Training Loss: 0.24130968749523163\n",
            "Step 760, Training Loss: 0.2860298156738281\n",
            "Step 760, Training Loss: 0.09808890521526337\n",
            "Step 760, Training Loss: 0.16830779612064362\n",
            "Step 760, Training Loss: 0.1483079046010971\n",
            "Step 760, Training Loss: 0.1878051906824112\n",
            "Step 760, Training Loss: 0.10425971448421478\n",
            "Step 760, Training Loss: 0.17199470102787018\n",
            "Step 760, Training Loss: 0.08638427406549454\n",
            "Step 760, Training Loss: 0.17950941622257233\n",
            "Step 760, Training Loss: 0.058301374316215515\n",
            "Step 760, Training Loss: 0.1871267408132553\n",
            "Step 760, Training Loss: 0.16970311105251312\n",
            "Step 760, Training Loss: 0.04921185225248337\n",
            "Step 760, Training Loss: 0.07464848458766937\n",
            "Step 760, Training Loss: 0.13672885298728943\n",
            "Step 760, Training Loss: 0.17618058621883392\n",
            "Step 760, Training Loss: 0.06183129549026489\n",
            "Step 760, Training Loss: 0.19382308423519135\n",
            "Step 760, Training Loss: 0.12415208667516708\n",
            "Step 760, Training Loss: 0.246482253074646\n",
            "Step 760, Training Loss: 0.20538358390331268\n",
            "Step 760, Training Loss: 0.11101105064153671\n",
            "Step 760, Training Loss: 0.15286999940872192\n",
            "Step 760, Training Loss: 0.32150131464004517\n",
            "Step 760, Training Loss: 0.2783672511577606\n",
            "Step 760, Training Loss: 0.09604280441999435\n",
            "Step 760, Training Loss: 0.20540092885494232\n",
            "Step 760, Training Loss: 0.20929227769374847\n",
            "Step 760, Training Loss: 0.03142351657152176\n",
            "Step 760, Training Loss: 0.2249424159526825\n",
            "Step 760, Training Loss: 0.1411208063364029\n",
            "Step 760, Training Loss: 0.2165995091199875\n",
            "Step 760, Training Loss: 0.13066433370113373\n",
            "Step 760, Training Loss: 0.008239577524363995\n",
            "Step 760, Training Loss: 0.27094870805740356\n",
            "Step 760, Training Loss: 0.13782130181789398\n",
            "Step 760, Training Loss: 0.1528322398662567\n",
            "Step 760, Training Loss: 0.17524144053459167\n",
            "Step 760, Training Loss: 0.09586653858423233\n",
            "Step 760, Training Loss: 0.26107197999954224\n",
            "Step 760, Training Loss: 0.20898158848285675\n",
            "Step 760, Training Loss: 0.08596525341272354\n",
            "Step 760, Training Loss: 0.0937993973493576\n",
            "Step 760, Training Loss: 0.21103344857692719\n",
            "Step 760, Training Loss: 0.08188174664974213\n",
            "Step 760, Training Loss: 0.17504964768886566\n",
            "Step 760, Training Loss: 0.13859988749027252\n",
            "Step 760, Training Loss: 0.010153274983167648\n",
            "Step 760, Training Loss: 0.006216477137058973\n",
            "Step 770, Training Loss: 0.004769187420606613\n",
            "Step 770, Training Loss: 0.009357332251966\n",
            "Step 780, Training Loss: 0.003950371406972408\n",
            "Step 780, Training Loss: 0.04120013862848282\n",
            "Step 790, Training Loss: 0.007092073559761047\n",
            "Step 790, Training Loss: 0.010619274340569973\n",
            "Step 800, Training Loss: 0.012872869148850441\n",
            "Step 800, Training Loss: 0.009792651049792767\n",
            "Step 810, Training Loss: 0.05744079500436783\n",
            "Step 810, Training Loss: 0.04864249378442764\n",
            "Step 820, Training Loss: 0.017896361649036407\n",
            "Step 820, Training Loss: 0.013050423003733158\n",
            "Step 830, Training Loss: 0.016168592497706413\n",
            "Step 830, Training Loss: 0.028061125427484512\n",
            "Step 840, Training Loss: 0.02325255423784256\n",
            "Step 840, Training Loss: 0.011694143526256084\n",
            "Step 850, Training Loss: 0.01637176238000393\n",
            "Step 850, Training Loss: 0.0074791922233998775\n",
            "Step 860, Training Loss: 0.005557143595069647\n",
            "Step 860, Training Loss: 0.011418696492910385\n",
            "Step 870, Training Loss: 0.018097514286637306\n",
            "Step 870, Training Loss: 0.02995552122592926\n",
            "Step 880, Training Loss: 0.011493859812617302\n",
            "Step 880, Training Loss: 0.014934527687728405\n",
            "Step 890, Training Loss: 0.05560517683625221\n",
            "Step 890, Training Loss: 0.01995091885328293\n",
            "Step 900, Training Loss: 0.12474554777145386\n",
            "Step 900, Training Loss: 0.014161357656121254\n",
            "Step 910, Training Loss: 0.003491312963888049\n",
            "Step 910, Training Loss: 0.0137750543653965\n",
            "Step 920, Training Loss: 0.00682508759200573\n",
            "Step 920, Training Loss: 0.03170280158519745\n",
            "Step 930, Training Loss: 0.06416591256856918\n",
            "Step 930, Training Loss: 0.010836823843419552\n",
            "Step 940, Training Loss: 0.07956363260746002\n",
            "Step 940, Training Loss: 0.017512226477265358\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 1523\n",
            "  Batch size = 32\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 950, Training Loss: 0.238149955868721\n",
            "Step 950, Training Loss: 0.2831413745880127\n",
            "Step 950, Training Loss: 0.09855605661869049\n",
            "Step 950, Training Loss: 0.1687164455652237\n",
            "Step 950, Training Loss: 0.14939144253730774\n",
            "Step 950, Training Loss: 0.18574856221675873\n",
            "Step 950, Training Loss: 0.10414699465036392\n",
            "Step 950, Training Loss: 0.17119745910167694\n",
            "Step 950, Training Loss: 0.0867045447230339\n",
            "Step 950, Training Loss: 0.1764664500951767\n",
            "Step 950, Training Loss: 0.05830734968185425\n",
            "Step 950, Training Loss: 0.18153022229671478\n",
            "Step 950, Training Loss: 0.16830335557460785\n",
            "Step 950, Training Loss: 0.05090360343456268\n",
            "Step 950, Training Loss: 0.07683190703392029\n",
            "Step 950, Training Loss: 0.13600286841392517\n",
            "Step 950, Training Loss: 0.17669937014579773\n",
            "Step 950, Training Loss: 0.061750125139951706\n",
            "Step 950, Training Loss: 0.19315603375434875\n",
            "Step 950, Training Loss: 0.1255330592393875\n",
            "Step 950, Training Loss: 0.24587471783161163\n",
            "Step 950, Training Loss: 0.20476755499839783\n",
            "Step 950, Training Loss: 0.11093473434448242\n",
            "Step 950, Training Loss: 0.15396049618721008\n",
            "Step 950, Training Loss: 0.32264384627342224\n",
            "Step 950, Training Loss: 0.27742043137550354\n",
            "Step 950, Training Loss: 0.09492292255163193\n",
            "Step 950, Training Loss: 0.2033420354127884\n",
            "Step 950, Training Loss: 0.2084912657737732\n",
            "Step 950, Training Loss: 0.031859274953603745\n",
            "Step 950, Training Loss: 0.22322002053260803\n",
            "Step 950, Training Loss: 0.1394856721162796\n",
            "Step 950, Training Loss: 0.21621212363243103\n",
            "Step 950, Training Loss: 0.13182446360588074\n",
            "Step 950, Training Loss: 0.009931985288858414\n",
            "Step 950, Training Loss: 0.27151989936828613\n",
            "Step 950, Training Loss: 0.1358186900615692\n",
            "Step 950, Training Loss: 0.153282031416893\n",
            "Step 950, Training Loss: 0.17105616629123688\n",
            "Step 950, Training Loss: 0.09354956448078156\n",
            "Step 950, Training Loss: 0.2585567533969879\n",
            "Step 950, Training Loss: 0.20573128759860992\n",
            "Step 950, Training Loss: 0.0789332389831543\n",
            "Step 950, Training Loss: 0.09385183453559875\n",
            "Step 950, Training Loss: 0.21126551926136017\n",
            "Step 950, Training Loss: 0.08192498981952667\n",
            "Step 950, Training Loss: 0.171858012676239\n",
            "Step 950, Training Loss: 0.1341206580400467\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 1523\n",
            "  Batch size = 32\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training completed in 155.96 seconds.\n",
            "Step 950, Training Loss: 0.238149955868721\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='48' max='48' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [48/48 00:02]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 950, Training Loss: 0.2831413745880127\n",
            "Step 950, Training Loss: 0.09855605661869049\n",
            "Step 950, Training Loss: 0.1687164455652237\n",
            "Step 950, Training Loss: 0.14939144253730774\n",
            "Step 950, Training Loss: 0.18574856221675873\n",
            "Step 950, Training Loss: 0.10414699465036392\n",
            "Step 950, Training Loss: 0.17119745910167694\n",
            "Step 950, Training Loss: 0.0867045447230339\n",
            "Step 950, Training Loss: 0.1764664500951767\n",
            "Step 950, Training Loss: 0.05830734968185425\n",
            "Step 950, Training Loss: 0.18153022229671478\n",
            "Step 950, Training Loss: 0.16830335557460785\n",
            "Step 950, Training Loss: 0.05090360343456268\n",
            "Step 950, Training Loss: 0.07683190703392029\n",
            "Step 950, Training Loss: 0.13600286841392517\n",
            "Step 950, Training Loss: 0.17669937014579773\n",
            "Step 950, Training Loss: 0.061750125139951706\n",
            "Step 950, Training Loss: 0.19315603375434875\n",
            "Step 950, Training Loss: 0.1255330592393875\n",
            "Step 950, Training Loss: 0.24587471783161163\n",
            "Step 950, Training Loss: 0.20476755499839783\n",
            "Step 950, Training Loss: 0.11093473434448242\n",
            "Step 950, Training Loss: 0.15396049618721008\n",
            "Step 950, Training Loss: 0.32264384627342224\n",
            "Step 950, Training Loss: 0.27742043137550354\n",
            "Step 950, Training Loss: 0.09492292255163193\n",
            "Step 950, Training Loss: 0.2033420354127884\n",
            "Step 950, Training Loss: 0.2084912657737732\n",
            "Step 950, Training Loss: 0.031859274953603745\n",
            "Step 950, Training Loss: 0.22322002053260803\n",
            "Step 950, Training Loss: 0.1394856721162796\n",
            "Step 950, Training Loss: 0.21621212363243103\n",
            "Step 950, Training Loss: 0.13182446360588074\n",
            "Step 950, Training Loss: 0.009931985288858414\n",
            "Step 950, Training Loss: 0.27151989936828613\n",
            "Step 950, Training Loss: 0.1358186900615692\n",
            "Step 950, Training Loss: 0.153282031416893\n",
            "Step 950, Training Loss: 0.17105616629123688\n",
            "Step 950, Training Loss: 0.09354956448078156\n",
            "Step 950, Training Loss: 0.2585567533969879\n",
            "Step 950, Training Loss: 0.20573128759860992\n",
            "Step 950, Training Loss: 0.0789332389831543\n",
            "Step 950, Training Loss: 0.09385183453559875\n",
            "Step 950, Training Loss: 0.21126551926136017\n",
            "Step 950, Training Loss: 0.08192498981952667\n",
            "Step 950, Training Loss: 0.171858012676239\n",
            "Step 950, Training Loss: 0.1341206580400467\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-12-09 19:29:41,254] Trial 8 finished with value: 0.8528395761429663 and parameters: {'learning_rate': 1.1241907060526185e-06, 'batch_size': 16, 'num_train_epochs': 5, 'weight_decay': 0.00012588582315295504}. Best is trial 6 with value: 0.8559372738098081.\n",
            "<ipython-input-12-42465065092b>:2: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  learning_rate = trial.suggest_loguniform(\"learning_rate\", 1e-6, 5e-5)\n",
            "<ipython-input-12-42465065092b>:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  weight_decay = trial.suggest_loguniform(\"weight_decay\", 1e-4, 0.1)\n",
            "PyTorch: setting up devices\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n",
            "***** Running training *****\n",
            "  Num examples = 6091\n",
            "  Num Epochs = 10\n",
            "  Instantaneous batch size per device = 4\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
            "  Gradient Accumulation steps = 2\n",
            "  Total optimization steps = 7610\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting training...\n",
            "Step 0, Training Loss: 0.004737722221761942\n",
            "Step 0, Training Loss: 0.0036799595691263676\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='7610' max='7610' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [7610/7610 06:47, Epoch 9/10]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "      <th>F1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>0.018700</td>\n",
              "      <td>0.170980</td>\n",
              "      <td>0.838477</td>\n",
              "      <td>0.857524</td>\n",
              "      <td>0.840822</td>\n",
              "      <td>0.847033</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.007500</td>\n",
              "      <td>0.178946</td>\n",
              "      <td>0.839790</td>\n",
              "      <td>0.851861</td>\n",
              "      <td>0.851733</td>\n",
              "      <td>0.850164</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.018200</td>\n",
              "      <td>0.164790</td>\n",
              "      <td>0.852265</td>\n",
              "      <td>0.862484</td>\n",
              "      <td>0.859435</td>\n",
              "      <td>0.860379</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.014100</td>\n",
              "      <td>0.186607</td>\n",
              "      <td>0.841103</td>\n",
              "      <td>0.856408</td>\n",
              "      <td>0.849166</td>\n",
              "      <td>0.848764</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.005600</td>\n",
              "      <td>0.172035</td>\n",
              "      <td>0.850952</td>\n",
              "      <td>0.859681</td>\n",
              "      <td>0.858151</td>\n",
              "      <td>0.858054</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>0.027800</td>\n",
              "      <td>0.180164</td>\n",
              "      <td>0.837163</td>\n",
              "      <td>0.866296</td>\n",
              "      <td>0.847882</td>\n",
              "      <td>0.854979</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>0.015600</td>\n",
              "      <td>0.175553</td>\n",
              "      <td>0.857518</td>\n",
              "      <td>0.864797</td>\n",
              "      <td>0.863286</td>\n",
              "      <td>0.861881</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>0.008400</td>\n",
              "      <td>0.178505</td>\n",
              "      <td>0.839790</td>\n",
              "      <td>0.861304</td>\n",
              "      <td>0.853017</td>\n",
              "      <td>0.855828</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>0.016600</td>\n",
              "      <td>0.173842</td>\n",
              "      <td>0.859488</td>\n",
              "      <td>0.863889</td>\n",
              "      <td>0.866496</td>\n",
              "      <td>0.863873</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>0.010600</td>\n",
              "      <td>0.174469</td>\n",
              "      <td>0.854235</td>\n",
              "      <td>0.863189</td>\n",
              "      <td>0.865212</td>\n",
              "      <td>0.862901</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 10, Training Loss: 0.0030764376278966665\n",
            "Step 10, Training Loss: 0.0035428237169981003\n",
            "Step 20, Training Loss: 0.13129188120365143\n",
            "Step 20, Training Loss: 0.016814427450299263\n",
            "Step 30, Training Loss: 0.0037211552262306213\n",
            "Step 30, Training Loss: 0.03183289244771004\n",
            "Step 40, Training Loss: 0.004653475247323513\n",
            "Step 40, Training Loss: 0.004533357452601194\n",
            "Step 50, Training Loss: 0.005793791729956865\n",
            "Step 50, Training Loss: 0.007043514400720596\n",
            "Step 60, Training Loss: 0.005334828514605761\n",
            "Step 60, Training Loss: 0.0035735894925892353\n",
            "Step 70, Training Loss: 0.0058879973366856575\n",
            "Step 70, Training Loss: 0.02184722013771534\n",
            "Step 80, Training Loss: 0.01665329560637474\n",
            "Step 80, Training Loss: 0.017655475065112114\n",
            "Step 90, Training Loss: 0.00354962470009923\n",
            "Step 90, Training Loss: 0.010972580872476101\n",
            "Step 100, Training Loss: 0.0022808904759585857\n",
            "Step 100, Training Loss: 0.0031425587367266417\n",
            "Step 110, Training Loss: 0.005018388386815786\n",
            "Step 110, Training Loss: 0.004840399604290724\n",
            "Step 120, Training Loss: 0.022707808762788773\n",
            "Step 120, Training Loss: 0.0223905798047781\n",
            "Step 130, Training Loss: 0.020751357078552246\n",
            "Step 130, Training Loss: 0.0037750117480754852\n",
            "Step 140, Training Loss: 0.02401220239698887\n",
            "Step 140, Training Loss: 0.039224088191986084\n",
            "Step 150, Training Loss: 0.007696618791669607\n",
            "Step 150, Training Loss: 0.07302387058734894\n",
            "Step 160, Training Loss: 0.004792934283614159\n",
            "Step 160, Training Loss: 0.006701655685901642\n",
            "Step 170, Training Loss: 0.003695557126775384\n",
            "Step 170, Training Loss: 0.003792073577642441\n",
            "Step 180, Training Loss: 0.028659915551543236\n",
            "Step 180, Training Loss: 0.2577122151851654\n",
            "Step 190, Training Loss: 0.0253441222012043\n",
            "Step 190, Training Loss: 0.020870814099907875\n",
            "Step 200, Training Loss: 0.0034299148246645927\n",
            "Step 200, Training Loss: 0.003304016776382923\n",
            "Step 210, Training Loss: 0.06456165015697479\n",
            "Step 210, Training Loss: 0.03164779394865036\n",
            "Step 220, Training Loss: 0.015223006717860699\n",
            "Step 220, Training Loss: 0.07783046364784241\n",
            "Step 230, Training Loss: 0.0032959107775241137\n",
            "Step 230, Training Loss: 0.003200254635885358\n",
            "Step 240, Training Loss: 0.005595690105110407\n",
            "Step 240, Training Loss: 0.0030768851283937693\n",
            "Step 250, Training Loss: 0.007067339029163122\n",
            "Step 250, Training Loss: 0.013115589506924152\n",
            "Step 260, Training Loss: 0.002690903376787901\n",
            "Step 260, Training Loss: 0.0044921208173036575\n",
            "Step 270, Training Loss: 0.01321142353117466\n",
            "Step 270, Training Loss: 0.002765610348433256\n",
            "Step 280, Training Loss: 0.012057614512741566\n",
            "Step 280, Training Loss: 0.002860476030036807\n",
            "Step 290, Training Loss: 0.004507320933043957\n",
            "Step 290, Training Loss: 0.0040776487439870834\n",
            "Step 300, Training Loss: 0.017510097473859787\n",
            "Step 300, Training Loss: 0.003953887149691582\n",
            "Step 310, Training Loss: 0.0027178716845810413\n",
            "Step 310, Training Loss: 0.005408154334872961\n",
            "Step 320, Training Loss: 0.002196109388023615\n",
            "Step 320, Training Loss: 0.022588102146983147\n",
            "Step 330, Training Loss: 0.0032338490709662437\n",
            "Step 330, Training Loss: 0.0020140481647104025\n",
            "Step 340, Training Loss: 0.002507888711988926\n",
            "Step 340, Training Loss: 0.01042496133595705\n",
            "Step 350, Training Loss: 0.010383840650320053\n",
            "Step 350, Training Loss: 0.02655145898461342\n",
            "Step 360, Training Loss: 0.004101335536688566\n",
            "Step 360, Training Loss: 0.0045154523104429245\n",
            "Step 370, Training Loss: 0.0030309497378766537\n",
            "Step 370, Training Loss: 0.016369370743632317\n",
            "Step 380, Training Loss: 0.002448873594403267\n",
            "Step 380, Training Loss: 0.011272641830146313\n",
            "Step 390, Training Loss: 0.02084970846772194\n",
            "Step 390, Training Loss: 0.004191379528492689\n",
            "Step 400, Training Loss: 0.0025912390556186438\n",
            "Step 400, Training Loss: 0.0034672769252210855\n",
            "Step 410, Training Loss: 0.0018963785842061043\n",
            "Step 410, Training Loss: 0.019764292985200882\n",
            "Step 420, Training Loss: 0.0032324963249266148\n",
            "Step 420, Training Loss: 0.01579989865422249\n",
            "Step 430, Training Loss: 0.3630450963973999\n",
            "Step 430, Training Loss: 0.021570658311247826\n",
            "Step 440, Training Loss: 0.0048371716402471066\n",
            "Step 440, Training Loss: 0.021755822002887726\n",
            "Step 450, Training Loss: 0.005094461143016815\n",
            "Step 450, Training Loss: 0.01959417760372162\n",
            "Step 460, Training Loss: 0.00303284521214664\n",
            "Step 460, Training Loss: 0.05595770478248596\n",
            "Step 470, Training Loss: 0.01553382072597742\n",
            "Step 470, Training Loss: 0.0028804722242057323\n",
            "Step 480, Training Loss: 0.01637294515967369\n",
            "Step 480, Training Loss: 0.03103001043200493\n",
            "Step 490, Training Loss: 0.29749399423599243\n",
            "Step 490, Training Loss: 0.004823591094464064\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Saving model checkpoint to ./results/checkpoint-500\n",
            "Configuration saved in ./results/checkpoint-500/config.json\n",
            "Model weights saved in ./results/checkpoint-500/pytorch_model.bin\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 500, Training Loss: 0.018483920022845268\n",
            "Step 500, Training Loss: 0.0028924853540956974\n",
            "Step 510, Training Loss: 0.024163655936717987\n",
            "Step 510, Training Loss: 0.08573593199253082\n",
            "Step 520, Training Loss: 0.024349471554160118\n",
            "Step 520, Training Loss: 0.005032110959291458\n",
            "Step 530, Training Loss: 0.017159296199679375\n",
            "Step 530, Training Loss: 0.0027869371697306633\n",
            "Step 540, Training Loss: 0.0020653631072491407\n",
            "Step 540, Training Loss: 0.0034436164423823357\n",
            "Step 550, Training Loss: 0.0683588758111\n",
            "Step 550, Training Loss: 0.009745332412421703\n",
            "Step 560, Training Loss: 0.03107508085668087\n",
            "Step 560, Training Loss: 0.26110994815826416\n",
            "Step 570, Training Loss: 0.003442099317908287\n",
            "Step 570, Training Loss: 0.013182955794036388\n",
            "Step 580, Training Loss: 0.018045304343104362\n",
            "Step 580, Training Loss: 0.0037853559479117393\n",
            "Step 590, Training Loss: 0.0025566723197698593\n",
            "Step 590, Training Loss: 0.0034885306376963854\n",
            "Step 600, Training Loss: 0.18260547518730164\n",
            "Step 600, Training Loss: 0.021303346380591393\n",
            "Step 610, Training Loss: 0.015335564501583576\n",
            "Step 610, Training Loss: 0.35002490878105164\n",
            "Step 620, Training Loss: 0.10463575273752213\n",
            "Step 620, Training Loss: 0.0061201052740216255\n",
            "Step 630, Training Loss: 0.0024210705887526274\n",
            "Step 630, Training Loss: 0.03397558256983757\n",
            "Step 640, Training Loss: 0.0027499371208250523\n",
            "Step 640, Training Loss: 0.009830793365836143\n",
            "Step 650, Training Loss: 0.002692190697416663\n",
            "Step 650, Training Loss: 0.362428218126297\n",
            "Step 660, Training Loss: 0.012380575761198997\n",
            "Step 660, Training Loss: 0.00323112984187901\n",
            "Step 670, Training Loss: 0.0037167903501540422\n",
            "Step 670, Training Loss: 0.020448096096515656\n",
            "Step 680, Training Loss: 0.008102800697088242\n",
            "Step 680, Training Loss: 0.4187609553337097\n",
            "Step 690, Training Loss: 0.012941393069922924\n",
            "Step 690, Training Loss: 0.002404311206191778\n",
            "Step 700, Training Loss: 0.006574893835932016\n",
            "Step 700, Training Loss: 0.011988119222223759\n",
            "Step 710, Training Loss: 0.09209512174129486\n",
            "Step 710, Training Loss: 0.014387788251042366\n",
            "Step 720, Training Loss: 0.0031208747532218695\n",
            "Step 720, Training Loss: 0.15236732363700867\n",
            "Step 730, Training Loss: 0.0030716536566615105\n",
            "Step 730, Training Loss: 0.0025267810560762882\n",
            "Step 740, Training Loss: 0.0026113158091902733\n",
            "Step 740, Training Loss: 0.003375645959749818\n",
            "Step 750, Training Loss: 0.0026850684080272913\n",
            "Step 750, Training Loss: 0.021082963794469833\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 1523\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 760, Training Loss: 0.002421751618385315\n",
            "Step 760, Training Loss: 0.0017811127472668886\n",
            "Step 770, Training Loss: 0.0025771630462259054\n",
            "Step 770, Training Loss: 0.028719428926706314\n",
            "Step 780, Training Loss: 0.0029204143211245537\n",
            "Step 780, Training Loss: 0.0022931003477424383\n",
            "Step 790, Training Loss: 0.0024761115200817585\n",
            "Step 790, Training Loss: 0.0024509038776159286\n",
            "Step 800, Training Loss: 0.05794893205165863\n",
            "Step 800, Training Loss: 0.017302807420492172\n",
            "Step 810, Training Loss: 0.0028318699914962053\n",
            "Step 810, Training Loss: 0.004036380909383297\n",
            "Step 820, Training Loss: 0.005619129166007042\n",
            "Step 820, Training Loss: 0.003800899488851428\n",
            "Step 830, Training Loss: 0.05215109884738922\n",
            "Step 830, Training Loss: 0.0062513225711882114\n",
            "Step 840, Training Loss: 0.008726541884243488\n",
            "Step 840, Training Loss: 0.003392495447769761\n",
            "Step 850, Training Loss: 0.004000875633209944\n",
            "Step 850, Training Loss: 0.02108510583639145\n",
            "Step 860, Training Loss: 0.0024298185016959906\n",
            "Step 860, Training Loss: 0.020037509500980377\n",
            "Step 870, Training Loss: 0.01614425890147686\n",
            "Step 870, Training Loss: 0.04536403715610504\n",
            "Step 880, Training Loss: 0.003377800341695547\n",
            "Step 880, Training Loss: 0.014447787776589394\n",
            "Step 890, Training Loss: 0.004821296781301498\n",
            "Step 890, Training Loss: 0.1262454390525818\n",
            "Step 900, Training Loss: 0.3140980005264282\n",
            "Step 900, Training Loss: 0.002460305579006672\n",
            "Step 910, Training Loss: 0.046286456286907196\n",
            "Step 910, Training Loss: 0.013217157684266567\n",
            "Step 920, Training Loss: 0.026734093204140663\n",
            "Step 920, Training Loss: 0.0031568394042551517\n",
            "Step 930, Training Loss: 0.00291636330075562\n",
            "Step 930, Training Loss: 0.0034136823378503323\n",
            "Step 940, Training Loss: 0.002132133347913623\n",
            "Step 940, Training Loss: 0.06767084449529648\n",
            "Step 950, Training Loss: 0.03369767963886261\n",
            "Step 950, Training Loss: 0.01183517649769783\n",
            "Step 960, Training Loss: 0.022300314158201218\n",
            "Step 960, Training Loss: 0.0025075350422412157\n",
            "Step 970, Training Loss: 0.001831689034588635\n",
            "Step 970, Training Loss: 0.002894004574045539\n",
            "Step 980, Training Loss: 0.3362881541252136\n",
            "Step 980, Training Loss: 0.005370945204049349\n",
            "Step 990, Training Loss: 0.192294642329216\n",
            "Step 990, Training Loss: 0.016886252909898758\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Saving model checkpoint to ./results/checkpoint-1000\n",
            "Configuration saved in ./results/checkpoint-1000/config.json\n",
            "Model weights saved in ./results/checkpoint-1000/pytorch_model.bin\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 1000, Training Loss: 0.0027393370401114225\n",
            "Step 1000, Training Loss: 0.0034458015579730272\n",
            "Step 1010, Training Loss: 0.014921638183295727\n",
            "Step 1010, Training Loss: 0.0020588289480656385\n",
            "Step 1020, Training Loss: 0.022470857948064804\n",
            "Step 1020, Training Loss: 0.0027551616076380014\n",
            "Step 1030, Training Loss: 0.009772306308150291\n",
            "Step 1030, Training Loss: 0.020355992019176483\n",
            "Step 1040, Training Loss: 0.0018209557747468352\n",
            "Step 1040, Training Loss: 0.009147515520453453\n",
            "Step 1050, Training Loss: 0.007255095988512039\n",
            "Step 1050, Training Loss: 0.012057492509484291\n",
            "Step 1060, Training Loss: 0.008287869393825531\n",
            "Step 1060, Training Loss: 0.0016213861526921391\n",
            "Step 1070, Training Loss: 0.003242206061258912\n",
            "Step 1070, Training Loss: 0.012306805700063705\n",
            "Step 1080, Training Loss: 0.0034744124859571457\n",
            "Step 1080, Training Loss: 0.010967687703669071\n",
            "Step 1090, Training Loss: 0.0029280181042850018\n",
            "Step 1090, Training Loss: 0.0017597554251551628\n",
            "Step 1100, Training Loss: 0.0033663432113826275\n",
            "Step 1100, Training Loss: 0.002560564549639821\n",
            "Step 1110, Training Loss: 0.003072361694648862\n",
            "Step 1110, Training Loss: 0.0030708564445376396\n",
            "Step 1120, Training Loss: 0.003792642615735531\n",
            "Step 1120, Training Loss: 0.05355416610836983\n",
            "Step 1130, Training Loss: 0.003197574755176902\n",
            "Step 1130, Training Loss: 0.0054732211865484715\n",
            "Step 1140, Training Loss: 0.0020717522129416466\n",
            "Step 1140, Training Loss: 0.0026647828053683043\n",
            "Step 1150, Training Loss: 0.005522893276065588\n",
            "Step 1150, Training Loss: 0.4039170444011688\n",
            "Step 1160, Training Loss: 0.015654556453227997\n",
            "Step 1160, Training Loss: 0.0016844073543325067\n",
            "Step 1170, Training Loss: 0.23974399268627167\n",
            "Step 1170, Training Loss: 0.007485563866794109\n",
            "Step 1180, Training Loss: 0.004727435298264027\n",
            "Step 1180, Training Loss: 0.013650575652718544\n",
            "Step 1190, Training Loss: 0.07079219818115234\n",
            "Step 1190, Training Loss: 0.10339586436748505\n",
            "Step 1200, Training Loss: 0.007795797195285559\n",
            "Step 1200, Training Loss: 0.006566855125129223\n",
            "Step 1210, Training Loss: 0.01027013175189495\n",
            "Step 1210, Training Loss: 0.005369871389120817\n",
            "Step 1220, Training Loss: 0.14935988187789917\n",
            "Step 1220, Training Loss: 0.20681753754615784\n",
            "Step 1230, Training Loss: 0.12083832174539566\n",
            "Step 1230, Training Loss: 0.01034766435623169\n",
            "Step 1240, Training Loss: 0.015902169048786163\n",
            "Step 1240, Training Loss: 0.015521058812737465\n",
            "Step 1250, Training Loss: 0.002006879076361656\n",
            "Step 1250, Training Loss: 0.6377543807029724\n",
            "Step 1260, Training Loss: 0.005737680476158857\n",
            "Step 1260, Training Loss: 0.003111601108685136\n",
            "Step 1270, Training Loss: 0.16919103264808655\n",
            "Step 1270, Training Loss: 0.0018234607996419072\n",
            "Step 1280, Training Loss: 0.3242875039577484\n",
            "Step 1280, Training Loss: 0.00441785017028451\n",
            "Step 1290, Training Loss: 0.0028954006265848875\n",
            "Step 1290, Training Loss: 0.00900511909276247\n",
            "Step 1300, Training Loss: 0.0030774518381804228\n",
            "Step 1300, Training Loss: 0.0042306603863835335\n",
            "Step 1310, Training Loss: 0.046397823840379715\n",
            "Step 1310, Training Loss: 0.3740641176700592\n",
            "Step 1320, Training Loss: 0.0038824635557830334\n",
            "Step 1320, Training Loss: 0.005049969535320997\n",
            "Step 1330, Training Loss: 0.04231208935379982\n",
            "Step 1330, Training Loss: 0.00841568037867546\n",
            "Step 1340, Training Loss: 0.00218393187969923\n",
            "Step 1340, Training Loss: 0.0016029664548113942\n",
            "Step 1350, Training Loss: 0.0034767419565469027\n",
            "Step 1350, Training Loss: 0.0038731240201741457\n",
            "Step 1360, Training Loss: 0.019553977996110916\n",
            "Step 1360, Training Loss: 0.10041822493076324\n",
            "Step 1370, Training Loss: 0.05709264799952507\n",
            "Step 1370, Training Loss: 0.009248155169188976\n",
            "Step 1380, Training Loss: 0.1294332593679428\n",
            "Step 1380, Training Loss: 0.036015089601278305\n",
            "Step 1390, Training Loss: 0.536190390586853\n",
            "Step 1390, Training Loss: 0.009636685252189636\n",
            "Step 1400, Training Loss: 0.007556619122624397\n",
            "Step 1400, Training Loss: 0.008211091160774231\n",
            "Step 1410, Training Loss: 0.002205872442573309\n",
            "Step 1410, Training Loss: 0.0030662997160106897\n",
            "Step 1420, Training Loss: 0.0023485568817704916\n",
            "Step 1420, Training Loss: 0.002342723309993744\n",
            "Step 1430, Training Loss: 0.01640554890036583\n",
            "Step 1430, Training Loss: 0.009980205446481705\n",
            "Step 1440, Training Loss: 0.005466375034302473\n",
            "Step 1440, Training Loss: 0.0019962801598012447\n",
            "Step 1450, Training Loss: 0.0024420861154794693\n",
            "Step 1450, Training Loss: 0.0020939125679433346\n",
            "Step 1460, Training Loss: 0.002569362288340926\n",
            "Step 1460, Training Loss: 0.001871927990578115\n",
            "Step 1470, Training Loss: 0.0031363400630652905\n",
            "Step 1470, Training Loss: 0.03464585542678833\n",
            "Step 1480, Training Loss: 0.0018906313925981522\n",
            "Step 1480, Training Loss: 0.010031444951891899\n",
            "Step 1490, Training Loss: 0.16052883863449097\n",
            "Step 1490, Training Loss: 0.10462389886379242\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Saving model checkpoint to ./results/checkpoint-1500\n",
            "Configuration saved in ./results/checkpoint-1500/config.json\n",
            "Model weights saved in ./results/checkpoint-1500/pytorch_model.bin\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 1500, Training Loss: 0.011263257823884487\n",
            "Step 1500, Training Loss: 0.13849663734436035\n",
            "Step 1510, Training Loss: 0.0029762224294245243\n",
            "Step 1510, Training Loss: 0.002599685685709119\n",
            "Step 1520, Training Loss: 0.01003805547952652\n",
            "Step 1520, Training Loss: 0.00363039318472147\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 1523\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 1530, Training Loss: 0.0022900959011167288\n",
            "Step 1530, Training Loss: 0.013459335081279278\n",
            "Step 1540, Training Loss: 0.0026667381171137094\n",
            "Step 1540, Training Loss: 0.004171451553702354\n",
            "Step 1550, Training Loss: 0.0027002969291061163\n",
            "Step 1550, Training Loss: 0.14955741167068481\n",
            "Step 1560, Training Loss: 0.003346316749230027\n",
            "Step 1560, Training Loss: 0.01176544837653637\n",
            "Step 1570, Training Loss: 0.002005153801292181\n",
            "Step 1570, Training Loss: 0.0016669052420184016\n",
            "Step 1580, Training Loss: 0.037266504019498825\n",
            "Step 1580, Training Loss: 0.002803081413730979\n",
            "Step 1590, Training Loss: 0.004784265533089638\n",
            "Step 1590, Training Loss: 0.002032993361353874\n",
            "Step 1600, Training Loss: 0.004890412092208862\n",
            "Step 1600, Training Loss: 0.0076272934675216675\n",
            "Step 1610, Training Loss: 0.1195344552397728\n",
            "Step 1610, Training Loss: 0.008313051424920559\n",
            "Step 1620, Training Loss: 0.008549007587134838\n",
            "Step 1620, Training Loss: 0.0077760713174939156\n",
            "Step 1630, Training Loss: 0.003156329970806837\n",
            "Step 1630, Training Loss: 0.030813314020633698\n",
            "Step 1640, Training Loss: 0.002075004391372204\n",
            "Step 1640, Training Loss: 0.0042627169750630856\n",
            "Step 1650, Training Loss: 0.002293907105922699\n",
            "Step 1650, Training Loss: 0.09342224895954132\n",
            "Step 1660, Training Loss: 0.003325472818687558\n",
            "Step 1660, Training Loss: 0.0020955034997314215\n",
            "Step 1670, Training Loss: 0.005023933947086334\n",
            "Step 1670, Training Loss: 0.010727866552770138\n",
            "Step 1680, Training Loss: 0.0024831099435687065\n",
            "Step 1680, Training Loss: 0.0023930142633616924\n",
            "Step 1690, Training Loss: 0.0021039543207734823\n",
            "Step 1690, Training Loss: 0.018007565289735794\n",
            "Step 1700, Training Loss: 0.002317632082849741\n",
            "Step 1700, Training Loss: 0.002712403889745474\n",
            "Step 1710, Training Loss: 0.0017776403110474348\n",
            "Step 1710, Training Loss: 0.0020181862637400627\n",
            "Step 1720, Training Loss: 0.004808671306818724\n",
            "Step 1720, Training Loss: 0.022092152386903763\n",
            "Step 1730, Training Loss: 0.01116424985229969\n",
            "Step 1730, Training Loss: 0.014240729622542858\n",
            "Step 1740, Training Loss: 0.002989494940266013\n",
            "Step 1740, Training Loss: 0.0019191931933164597\n",
            "Step 1750, Training Loss: 0.1609703004360199\n",
            "Step 1750, Training Loss: 0.010955543257296085\n",
            "Step 1760, Training Loss: 0.019270718097686768\n",
            "Step 1760, Training Loss: 0.032325830310583115\n",
            "Step 1770, Training Loss: 0.007070005871355534\n",
            "Step 1770, Training Loss: 0.021507464349269867\n",
            "Step 1780, Training Loss: 0.09022180736064911\n",
            "Step 1780, Training Loss: 0.0018124716589227319\n",
            "Step 1790, Training Loss: 0.0018775371136143804\n",
            "Step 1790, Training Loss: 0.18128015100955963\n",
            "Step 1800, Training Loss: 0.23138703405857086\n",
            "Step 1800, Training Loss: 0.002205097582191229\n",
            "Step 1810, Training Loss: 0.16378208994865417\n",
            "Step 1810, Training Loss: 0.0027805930003523827\n",
            "Step 1820, Training Loss: 0.002097074408084154\n",
            "Step 1820, Training Loss: 0.0037676666397601366\n",
            "Step 1830, Training Loss: 0.01747826486825943\n",
            "Step 1830, Training Loss: 0.003392648184671998\n",
            "Step 1840, Training Loss: 0.003641407936811447\n",
            "Step 1840, Training Loss: 0.009948133490979671\n",
            "Step 1850, Training Loss: 0.002574202371761203\n",
            "Step 1850, Training Loss: 0.0017129566986113787\n",
            "Step 1860, Training Loss: 0.01394904125481844\n",
            "Step 1860, Training Loss: 0.03228159621357918\n",
            "Step 1870, Training Loss: 0.0018524331972002983\n",
            "Step 1870, Training Loss: 0.012812669388949871\n",
            "Step 1880, Training Loss: 0.0023307972587645054\n",
            "Step 1880, Training Loss: 0.0065682861022651196\n",
            "Step 1890, Training Loss: 0.07876741886138916\n",
            "Step 1890, Training Loss: 0.0024763490073382854\n",
            "Step 1900, Training Loss: 0.008823476731777191\n",
            "Step 1900, Training Loss: 0.013576677069067955\n",
            "Step 1910, Training Loss: 0.0029886106494814157\n",
            "Step 1910, Training Loss: 0.013410459272563457\n",
            "Step 1920, Training Loss: 0.0020291153341531754\n",
            "Step 1920, Training Loss: 0.0021421608980745077\n",
            "Step 1930, Training Loss: 0.0016083401860669255\n",
            "Step 1930, Training Loss: 0.0022300381679087877\n",
            "Step 1940, Training Loss: 0.001624882803298533\n",
            "Step 1940, Training Loss: 0.0024812063202261925\n",
            "Step 1950, Training Loss: 0.5231688022613525\n",
            "Step 1950, Training Loss: 0.005435300525277853\n",
            "Step 1960, Training Loss: 0.0016063092043623328\n",
            "Step 1960, Training Loss: 0.012328458949923515\n",
            "Step 1970, Training Loss: 0.013921045698225498\n",
            "Step 1970, Training Loss: 0.004436190705746412\n",
            "Step 1980, Training Loss: 0.00221958477050066\n",
            "Step 1980, Training Loss: 0.03506476432085037\n",
            "Step 1990, Training Loss: 0.00213161064311862\n",
            "Step 1990, Training Loss: 0.01774093136191368\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Saving model checkpoint to ./results/checkpoint-2000\n",
            "Configuration saved in ./results/checkpoint-2000/config.json\n",
            "Model weights saved in ./results/checkpoint-2000/pytorch_model.bin\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 2000, Training Loss: 0.07861457020044327\n",
            "Step 2000, Training Loss: 0.028503011912107468\n",
            "Step 2010, Training Loss: 0.0040141139179468155\n",
            "Step 2010, Training Loss: 0.04146837443113327\n",
            "Step 2020, Training Loss: 0.0016124938847497106\n",
            "Step 2020, Training Loss: 0.001905253273434937\n",
            "Step 2030, Training Loss: 0.22722215950489044\n",
            "Step 2030, Training Loss: 0.23336827754974365\n",
            "Step 2040, Training Loss: 0.0025727965403348207\n",
            "Step 2040, Training Loss: 0.00709864916279912\n",
            "Step 2050, Training Loss: 0.0024609691463410854\n",
            "Step 2050, Training Loss: 0.0016698880353942513\n",
            "Step 2060, Training Loss: 0.017727132886648178\n",
            "Step 2060, Training Loss: 0.0032000644132494926\n",
            "Step 2070, Training Loss: 0.0026195275131613016\n",
            "Step 2070, Training Loss: 0.007316037081182003\n",
            "Step 2080, Training Loss: 0.2313346266746521\n",
            "Step 2080, Training Loss: 0.3731118440628052\n",
            "Step 2090, Training Loss: 0.48038771748542786\n",
            "Step 2090, Training Loss: 0.0031582589726895094\n",
            "Step 2100, Training Loss: 0.010433768853545189\n",
            "Step 2100, Training Loss: 0.002681995276361704\n",
            "Step 2110, Training Loss: 0.0028794105164706707\n",
            "Step 2110, Training Loss: 0.0035520396195352077\n",
            "Step 2120, Training Loss: 0.0025715851224958897\n",
            "Step 2120, Training Loss: 0.002616604557260871\n",
            "Step 2130, Training Loss: 0.15562677383422852\n",
            "Step 2130, Training Loss: 0.0027375328354537487\n",
            "Step 2140, Training Loss: 0.0019053915748372674\n",
            "Step 2140, Training Loss: 0.002207651501521468\n",
            "Step 2150, Training Loss: 0.0020882333628833294\n",
            "Step 2150, Training Loss: 0.002181359101086855\n",
            "Step 2160, Training Loss: 0.00905129685997963\n",
            "Step 2160, Training Loss: 0.09242130070924759\n",
            "Step 2170, Training Loss: 0.021540991961956024\n",
            "Step 2170, Training Loss: 0.002712239744141698\n",
            "Step 2180, Training Loss: 0.010323463939130306\n",
            "Step 2180, Training Loss: 0.002249846002086997\n",
            "Step 2190, Training Loss: 0.002468686318024993\n",
            "Step 2190, Training Loss: 0.0021495416294783354\n",
            "Step 2200, Training Loss: 0.002581720007583499\n",
            "Step 2200, Training Loss: 0.0018382525304332376\n",
            "Step 2210, Training Loss: 0.007061114069074392\n",
            "Step 2210, Training Loss: 0.008411179296672344\n",
            "Step 2220, Training Loss: 0.0753762349486351\n",
            "Step 2220, Training Loss: 0.052644360810518265\n",
            "Step 2230, Training Loss: 0.0023222677409648895\n",
            "Step 2230, Training Loss: 0.002996267983689904\n",
            "Step 2240, Training Loss: 0.0026658878196030855\n",
            "Step 2240, Training Loss: 0.019559230655431747\n",
            "Step 2250, Training Loss: 0.007357413414865732\n",
            "Step 2250, Training Loss: 0.003698810003697872\n",
            "Step 2260, Training Loss: 0.0021595971193164587\n",
            "Step 2260, Training Loss: 0.00324694043956697\n",
            "Step 2270, Training Loss: 0.20226414501667023\n",
            "Step 2270, Training Loss: 0.002753018168732524\n",
            "Step 2280, Training Loss: 0.00679458724334836\n",
            "Step 2280, Training Loss: 0.010210550390183926\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 1523\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 2290, Training Loss: 0.0024020597338676453\n",
            "Step 2290, Training Loss: 0.006160437129437923\n",
            "Step 2300, Training Loss: 0.0015098482836037874\n",
            "Step 2300, Training Loss: 0.002091852482408285\n",
            "Step 2310, Training Loss: 0.0026561683043837547\n",
            "Step 2310, Training Loss: 0.0023210495710372925\n",
            "Step 2320, Training Loss: 0.0028574063908308744\n",
            "Step 2320, Training Loss: 0.007766801863908768\n",
            "Step 2330, Training Loss: 0.0014659393345937133\n",
            "Step 2330, Training Loss: 0.003976861014962196\n",
            "Step 2340, Training Loss: 0.0023139617405831814\n",
            "Step 2340, Training Loss: 0.00811729021370411\n",
            "Step 2350, Training Loss: 0.001961473375558853\n",
            "Step 2350, Training Loss: 0.0018706333357840776\n",
            "Step 2360, Training Loss: 0.0019648331217467785\n",
            "Step 2360, Training Loss: 0.0022748177871108055\n",
            "Step 2370, Training Loss: 0.16096530854701996\n",
            "Step 2370, Training Loss: 0.04105020686984062\n",
            "Step 2380, Training Loss: 0.010316582396626472\n",
            "Step 2380, Training Loss: 0.001364553696475923\n",
            "Step 2390, Training Loss: 0.006099511869251728\n",
            "Step 2390, Training Loss: 0.003348850877955556\n",
            "Step 2400, Training Loss: 0.010191722773015499\n",
            "Step 2400, Training Loss: 0.0016221960540860891\n",
            "Step 2410, Training Loss: 0.012338015250861645\n",
            "Step 2410, Training Loss: 0.08777748048305511\n",
            "Step 2420, Training Loss: 0.004085625987499952\n",
            "Step 2420, Training Loss: 0.004925451707094908\n",
            "Step 2430, Training Loss: 0.0020574654918164015\n",
            "Step 2430, Training Loss: 0.002665821695700288\n",
            "Step 2440, Training Loss: 0.0049281069077551365\n",
            "Step 2440, Training Loss: 0.0018696471815928817\n",
            "Step 2450, Training Loss: 0.0017498598899692297\n",
            "Step 2450, Training Loss: 0.0034574796445667744\n",
            "Step 2460, Training Loss: 0.007988459430634975\n",
            "Step 2460, Training Loss: 0.007064471486955881\n",
            "Step 2470, Training Loss: 0.3499012291431427\n",
            "Step 2470, Training Loss: 0.002126030856743455\n",
            "Step 2480, Training Loss: 0.003946359734982252\n",
            "Step 2480, Training Loss: 0.002183395205065608\n",
            "Step 2490, Training Loss: 0.387804239988327\n",
            "Step 2490, Training Loss: 0.0018731276504695415\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Saving model checkpoint to ./results/checkpoint-2500\n",
            "Configuration saved in ./results/checkpoint-2500/config.json\n",
            "Model weights saved in ./results/checkpoint-2500/pytorch_model.bin\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 2500, Training Loss: 0.0018758939113467932\n",
            "Step 2500, Training Loss: 0.19345085322856903\n",
            "Step 2510, Training Loss: 0.0020950171165168285\n",
            "Step 2510, Training Loss: 0.002576089696958661\n",
            "Step 2520, Training Loss: 0.0019617201760411263\n",
            "Step 2520, Training Loss: 0.04459551349282265\n",
            "Step 2530, Training Loss: 0.046835143119096756\n",
            "Step 2530, Training Loss: 0.0024114081170409918\n",
            "Step 2540, Training Loss: 0.0035991808399558067\n",
            "Step 2540, Training Loss: 0.001711147022433579\n",
            "Step 2550, Training Loss: 0.011720198206603527\n",
            "Step 2550, Training Loss: 0.009003875777125359\n",
            "Step 2560, Training Loss: 0.013694625347852707\n",
            "Step 2560, Training Loss: 0.004295372404158115\n",
            "Step 2570, Training Loss: 0.010355910286307335\n",
            "Step 2570, Training Loss: 0.004388127010315657\n",
            "Step 2580, Training Loss: 0.1835414469242096\n",
            "Step 2580, Training Loss: 0.002404263708740473\n",
            "Step 2590, Training Loss: 0.004351024981588125\n",
            "Step 2590, Training Loss: 0.01180109940469265\n",
            "Step 2600, Training Loss: 0.06003838777542114\n",
            "Step 2600, Training Loss: 0.0049438681453466415\n",
            "Step 2610, Training Loss: 0.0016797245480120182\n",
            "Step 2610, Training Loss: 0.2178681194782257\n",
            "Step 2620, Training Loss: 0.010695458389818668\n",
            "Step 2620, Training Loss: 0.032743681222200394\n",
            "Step 2630, Training Loss: 0.002237822161987424\n",
            "Step 2630, Training Loss: 0.003196901176124811\n",
            "Step 2640, Training Loss: 0.06708814203739166\n",
            "Step 2640, Training Loss: 0.010402766056358814\n",
            "Step 2650, Training Loss: 0.013156593777239323\n",
            "Step 2650, Training Loss: 0.0013203765265643597\n",
            "Step 2660, Training Loss: 0.002862722845748067\n",
            "Step 2660, Training Loss: 0.0017180582508444786\n",
            "Step 2670, Training Loss: 0.00265511660836637\n",
            "Step 2670, Training Loss: 0.001343129901215434\n",
            "Step 2680, Training Loss: 0.01691942848265171\n",
            "Step 2680, Training Loss: 0.0022666871082037687\n",
            "Step 2690, Training Loss: 0.0017736454028636217\n",
            "Step 2690, Training Loss: 0.017948726192116737\n",
            "Step 2700, Training Loss: 0.0029596358072012663\n",
            "Step 2700, Training Loss: 0.0014878136571496725\n",
            "Step 2710, Training Loss: 0.008384479209780693\n",
            "Step 2710, Training Loss: 0.12495049834251404\n",
            "Step 2720, Training Loss: 0.001670034253038466\n",
            "Step 2720, Training Loss: 0.002725414466112852\n",
            "Step 2730, Training Loss: 0.0030798926018178463\n",
            "Step 2730, Training Loss: 0.0028179476503282785\n",
            "Step 2740, Training Loss: 0.012695946730673313\n",
            "Step 2740, Training Loss: 0.00195670360699296\n",
            "Step 2750, Training Loss: 0.17207394540309906\n",
            "Step 2750, Training Loss: 0.30739253759384155\n",
            "Step 2760, Training Loss: 0.002681644866243005\n",
            "Step 2760, Training Loss: 0.003088057739660144\n",
            "Step 2770, Training Loss: 0.0027324198745191097\n",
            "Step 2770, Training Loss: 0.0023760765325278044\n",
            "Step 2780, Training Loss: 0.01774437725543976\n",
            "Step 2780, Training Loss: 0.005993591621518135\n",
            "Step 2790, Training Loss: 0.004460684489458799\n",
            "Step 2790, Training Loss: 0.005350762512534857\n",
            "Step 2800, Training Loss: 0.0025557540357112885\n",
            "Step 2800, Training Loss: 0.0035719883162528276\n",
            "Step 2810, Training Loss: 0.04150308296084404\n",
            "Step 2810, Training Loss: 0.002698003314435482\n",
            "Step 2820, Training Loss: 0.01753009296953678\n",
            "Step 2820, Training Loss: 0.0014201562153175473\n",
            "Step 2830, Training Loss: 0.08424831926822662\n",
            "Step 2830, Training Loss: 0.0015120935859158635\n",
            "Step 2840, Training Loss: 0.002008155221119523\n",
            "Step 2840, Training Loss: 0.15493519604206085\n",
            "Step 2850, Training Loss: 0.002336581004783511\n",
            "Step 2850, Training Loss: 0.005255513824522495\n",
            "Step 2860, Training Loss: 0.05995895341038704\n",
            "Step 2860, Training Loss: 0.0024442963767796755\n",
            "Step 2870, Training Loss: 0.0021396810188889503\n",
            "Step 2870, Training Loss: 0.00213145324960351\n",
            "Step 2880, Training Loss: 0.00659161526709795\n",
            "Step 2880, Training Loss: 0.002733097178861499\n",
            "Step 2890, Training Loss: 0.008057454600930214\n",
            "Step 2890, Training Loss: 0.009322388097643852\n",
            "Step 2900, Training Loss: 0.0020312615670263767\n",
            "Step 2900, Training Loss: 0.0022925520315766335\n",
            "Step 2910, Training Loss: 0.0026716990396380424\n",
            "Step 2910, Training Loss: 0.03795107081532478\n",
            "Step 2920, Training Loss: 0.001879719435237348\n",
            "Step 2920, Training Loss: 0.004675410222262144\n",
            "Step 2930, Training Loss: 0.004451053217053413\n",
            "Step 2930, Training Loss: 0.0064273495227098465\n",
            "Step 2940, Training Loss: 0.0017771540442481637\n",
            "Step 2940, Training Loss: 0.17859023809432983\n",
            "Step 2950, Training Loss: 0.0021390062756836414\n",
            "Step 2950, Training Loss: 0.11881789565086365\n",
            "Step 2960, Training Loss: 0.012717041186988354\n",
            "Step 2960, Training Loss: 0.27528175711631775\n",
            "Step 2970, Training Loss: 0.002445194637402892\n",
            "Step 2970, Training Loss: 0.007363153621554375\n",
            "Step 2980, Training Loss: 0.06698507815599442\n",
            "Step 2980, Training Loss: 0.0039492882788181305\n",
            "Step 2990, Training Loss: 0.020747704431414604\n",
            "Step 2990, Training Loss: 0.002215801505371928\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Saving model checkpoint to ./results/checkpoint-3000\n",
            "Configuration saved in ./results/checkpoint-3000/config.json\n",
            "Model weights saved in ./results/checkpoint-3000/pytorch_model.bin\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 3000, Training Loss: 0.008580701425671577\n",
            "Step 3000, Training Loss: 0.003102268325164914\n",
            "Step 3010, Training Loss: 0.004059776198118925\n",
            "Step 3010, Training Loss: 0.00850764662027359\n",
            "Step 3020, Training Loss: 0.3429986834526062\n",
            "Step 3020, Training Loss: 0.007879786193370819\n",
            "Step 3030, Training Loss: 0.0026934153866022825\n",
            "Step 3030, Training Loss: 0.003169348929077387\n",
            "Step 3040, Training Loss: 0.002005236689001322\n",
            "Step 3040, Training Loss: 0.012058784253895283\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 1523\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 3050, Training Loss: 0.002703241538256407\n",
            "Step 3050, Training Loss: 0.0021070116199553013\n",
            "Step 3060, Training Loss: 0.004236237611621618\n",
            "Step 3060, Training Loss: 0.0023650594521313906\n",
            "Step 3070, Training Loss: 0.0017542389687150717\n",
            "Step 3070, Training Loss: 0.026776114478707314\n",
            "Step 3080, Training Loss: 0.12558835744857788\n",
            "Step 3080, Training Loss: 0.0032581689301878214\n",
            "Step 3090, Training Loss: 0.004741770680993795\n",
            "Step 3090, Training Loss: 0.002489183098077774\n",
            "Step 3100, Training Loss: 0.004813107196241617\n",
            "Step 3100, Training Loss: 0.04123745113611221\n",
            "Step 3110, Training Loss: 0.007271127309650183\n",
            "Step 3110, Training Loss: 0.002149126958101988\n",
            "Step 3120, Training Loss: 0.001507214386947453\n",
            "Step 3120, Training Loss: 0.03049803338944912\n",
            "Step 3130, Training Loss: 0.002991044195368886\n",
            "Step 3130, Training Loss: 0.003422500565648079\n",
            "Step 3140, Training Loss: 0.002163489116355777\n",
            "Step 3140, Training Loss: 0.005364981945604086\n",
            "Step 3150, Training Loss: 0.0013847342925146222\n",
            "Step 3150, Training Loss: 0.009397268295288086\n",
            "Step 3160, Training Loss: 0.025645745918154716\n",
            "Step 3160, Training Loss: 0.001965787960216403\n",
            "Step 3170, Training Loss: 0.0062516918405890465\n",
            "Step 3170, Training Loss: 0.037491001188755035\n",
            "Step 3180, Training Loss: 0.001826620427891612\n",
            "Step 3180, Training Loss: 0.0013347077183425426\n",
            "Step 3190, Training Loss: 0.004384524654597044\n",
            "Step 3190, Training Loss: 0.0015165017684921622\n",
            "Step 3200, Training Loss: 0.36558106541633606\n",
            "Step 3200, Training Loss: 0.21563281118869781\n",
            "Step 3210, Training Loss: 0.18139611184597015\n",
            "Step 3210, Training Loss: 0.011465386487543583\n",
            "Step 3220, Training Loss: 0.0034426383208483458\n",
            "Step 3220, Training Loss: 0.007815171033143997\n",
            "Step 3230, Training Loss: 0.0015135452849790454\n",
            "Step 3230, Training Loss: 0.00203876243904233\n",
            "Step 3240, Training Loss: 0.0164848193526268\n",
            "Step 3240, Training Loss: 0.002971366746351123\n",
            "Step 3250, Training Loss: 0.0026096974033862352\n",
            "Step 3250, Training Loss: 0.20004311203956604\n",
            "Step 3260, Training Loss: 0.001945676514878869\n",
            "Step 3260, Training Loss: 0.006387907546013594\n",
            "Step 3270, Training Loss: 0.36886394023895264\n",
            "Step 3270, Training Loss: 0.008031708188354969\n",
            "Step 3280, Training Loss: 0.001681086141616106\n",
            "Step 3280, Training Loss: 0.045106418430805206\n",
            "Step 3290, Training Loss: 0.009213952347636223\n",
            "Step 3290, Training Loss: 0.0021871875505894423\n",
            "Step 3300, Training Loss: 0.004124239087104797\n",
            "Step 3300, Training Loss: 0.006814419291913509\n",
            "Step 3310, Training Loss: 0.04109923169016838\n",
            "Step 3310, Training Loss: 0.0020434032194316387\n",
            "Step 3320, Training Loss: 0.0036366924177855253\n",
            "Step 3320, Training Loss: 0.002047442365437746\n",
            "Step 3330, Training Loss: 0.2982350289821625\n",
            "Step 3330, Training Loss: 0.001347171375527978\n",
            "Step 3340, Training Loss: 0.009076684713363647\n",
            "Step 3340, Training Loss: 0.003933578263968229\n",
            "Step 3350, Training Loss: 0.003728821175172925\n",
            "Step 3350, Training Loss: 0.18819448351860046\n",
            "Step 3360, Training Loss: 0.002200561575591564\n",
            "Step 3360, Training Loss: 0.2461867481470108\n",
            "Step 3370, Training Loss: 0.21248485147953033\n",
            "Step 3370, Training Loss: 0.1119517907500267\n",
            "Step 3380, Training Loss: 0.004644358996301889\n",
            "Step 3380, Training Loss: 0.0046354029327631\n",
            "Step 3390, Training Loss: 0.050089720636606216\n",
            "Step 3390, Training Loss: 0.002603275002911687\n",
            "Step 3400, Training Loss: 0.019909173250198364\n",
            "Step 3400, Training Loss: 0.3702440559864044\n",
            "Step 3410, Training Loss: 0.001351017039269209\n",
            "Step 3410, Training Loss: 0.0019282801076769829\n",
            "Step 3420, Training Loss: 0.004632753320038319\n",
            "Step 3420, Training Loss: 0.001633028732612729\n",
            "Step 3430, Training Loss: 0.01565566658973694\n",
            "Step 3430, Training Loss: 0.0027871234342455864\n",
            "Step 3440, Training Loss: 0.002113242167979479\n",
            "Step 3440, Training Loss: 0.014918806962668896\n",
            "Step 3450, Training Loss: 0.11662355810403824\n",
            "Step 3450, Training Loss: 0.029851147904992104\n",
            "Step 3460, Training Loss: 0.007065820507705212\n",
            "Step 3460, Training Loss: 0.001856853486970067\n",
            "Step 3470, Training Loss: 0.002701407764106989\n",
            "Step 3470, Training Loss: 0.006162572186440229\n",
            "Step 3480, Training Loss: 0.0018393397331237793\n",
            "Step 3480, Training Loss: 0.00572745967656374\n",
            "Step 3490, Training Loss: 0.010818836279213428\n",
            "Step 3490, Training Loss: 0.0015954500995576382\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Saving model checkpoint to ./results/checkpoint-3500\n",
            "Configuration saved in ./results/checkpoint-3500/config.json\n",
            "Model weights saved in ./results/checkpoint-3500/pytorch_model.bin\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 3500, Training Loss: 0.002154258079826832\n",
            "Step 3500, Training Loss: 0.0033124417532235384\n",
            "Step 3510, Training Loss: 0.023793770000338554\n",
            "Step 3510, Training Loss: 0.01627320609986782\n",
            "Step 3520, Training Loss: 0.2505178153514862\n",
            "Step 3520, Training Loss: 0.001702904119156301\n",
            "Step 3530, Training Loss: 0.3977132737636566\n",
            "Step 3530, Training Loss: 0.0021412409842014313\n",
            "Step 3540, Training Loss: 0.0076427035965025425\n",
            "Step 3540, Training Loss: 0.002199128968641162\n",
            "Step 3550, Training Loss: 0.0020560016855597496\n",
            "Step 3550, Training Loss: 0.011844756081700325\n",
            "Step 3560, Training Loss: 0.004813315346837044\n",
            "Step 3560, Training Loss: 0.0020213215611875057\n",
            "Step 3570, Training Loss: 0.0022557105403393507\n",
            "Step 3570, Training Loss: 0.002439505886286497\n",
            "Step 3580, Training Loss: 0.001695039914920926\n",
            "Step 3580, Training Loss: 0.001461282023228705\n",
            "Step 3590, Training Loss: 0.0021862525027245283\n",
            "Step 3590, Training Loss: 0.011593502014875412\n",
            "Step 3600, Training Loss: 0.001664982526563108\n",
            "Step 3600, Training Loss: 0.06138897314667702\n",
            "Step 3610, Training Loss: 0.0071129826828837395\n",
            "Step 3610, Training Loss: 0.0033821023534983397\n",
            "Step 3620, Training Loss: 0.02294888347387314\n",
            "Step 3620, Training Loss: 0.003037803340703249\n",
            "Step 3630, Training Loss: 0.016611788421869278\n",
            "Step 3630, Training Loss: 0.1585730016231537\n",
            "Step 3640, Training Loss: 0.014923774637281895\n",
            "Step 3640, Training Loss: 0.0025481050834059715\n",
            "Step 3650, Training Loss: 0.2075243592262268\n",
            "Step 3650, Training Loss: 0.03870781511068344\n",
            "Step 3660, Training Loss: 0.0016579150687903166\n",
            "Step 3660, Training Loss: 0.0036981634330004454\n",
            "Step 3670, Training Loss: 0.0026568991597741842\n",
            "Step 3670, Training Loss: 0.09453641623258591\n",
            "Step 3680, Training Loss: 0.0019162383396178484\n",
            "Step 3680, Training Loss: 0.1014200821518898\n",
            "Step 3690, Training Loss: 0.0021321610547602177\n",
            "Step 3690, Training Loss: 0.3697492182254791\n",
            "Step 3700, Training Loss: 0.004654503893107176\n",
            "Step 3700, Training Loss: 0.03259781002998352\n",
            "Step 3710, Training Loss: 0.1757548600435257\n",
            "Step 3710, Training Loss: 0.0016568644205108285\n",
            "Step 3720, Training Loss: 0.0016397427534684539\n",
            "Step 3720, Training Loss: 0.0064348140731453896\n",
            "Step 3730, Training Loss: 0.0014461633982136846\n",
            "Step 3730, Training Loss: 0.007935643196105957\n",
            "Step 3740, Training Loss: 0.02787243202328682\n",
            "Step 3740, Training Loss: 0.0017901246901601553\n",
            "Step 3750, Training Loss: 0.0022444711066782475\n",
            "Step 3750, Training Loss: 0.001615058397874236\n",
            "Step 3760, Training Loss: 0.0017346832901239395\n",
            "Step 3760, Training Loss: 0.0016550869913771749\n",
            "Step 3770, Training Loss: 0.004740267060697079\n",
            "Step 3770, Training Loss: 0.0017457895446568727\n",
            "Step 3780, Training Loss: 0.0014930557226762176\n",
            "Step 3780, Training Loss: 0.001750550465658307\n",
            "Step 3790, Training Loss: 0.002362658269703388\n",
            "Step 3790, Training Loss: 0.002086270833387971\n",
            "Step 3800, Training Loss: 0.0031249059829860926\n",
            "Step 3800, Training Loss: 0.006641591899096966\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 1523\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 3810, Training Loss: 0.002192431129515171\n",
            "Step 3810, Training Loss: 0.0029578872490674257\n",
            "Step 3820, Training Loss: 0.0018999282037839293\n",
            "Step 3820, Training Loss: 0.002049280097708106\n",
            "Step 3830, Training Loss: 0.011373060755431652\n",
            "Step 3830, Training Loss: 0.12640130519866943\n",
            "Step 3840, Training Loss: 0.10323743522167206\n",
            "Step 3840, Training Loss: 0.0016772036906331778\n",
            "Step 3850, Training Loss: 0.007469079457223415\n",
            "Step 3850, Training Loss: 0.09489985555410385\n",
            "Step 3860, Training Loss: 0.03404979035258293\n",
            "Step 3860, Training Loss: 0.002007607137784362\n",
            "Step 3870, Training Loss: 0.0016475104494020343\n",
            "Step 3870, Training Loss: 0.003657518420368433\n",
            "Step 3880, Training Loss: 0.01804434135556221\n",
            "Step 3880, Training Loss: 0.0018885715398937464\n",
            "Step 3890, Training Loss: 0.004021801054477692\n",
            "Step 3890, Training Loss: 0.0019626894500106573\n",
            "Step 3900, Training Loss: 0.0045935651287436485\n",
            "Step 3900, Training Loss: 0.0015883116284385324\n",
            "Step 3910, Training Loss: 0.0023963837884366512\n",
            "Step 3910, Training Loss: 0.010064184665679932\n",
            "Step 3920, Training Loss: 0.0015784265706315637\n",
            "Step 3920, Training Loss: 0.003352689789608121\n",
            "Step 3930, Training Loss: 0.0022273901849985123\n",
            "Step 3930, Training Loss: 0.0065541877411305904\n",
            "Step 3940, Training Loss: 0.011291977018117905\n",
            "Step 3940, Training Loss: 0.023837078362703323\n",
            "Step 3950, Training Loss: 0.1449410319328308\n",
            "Step 3950, Training Loss: 0.0021730789449065924\n",
            "Step 3960, Training Loss: 0.0017763174837455153\n",
            "Step 3960, Training Loss: 0.0030507335904985666\n",
            "Step 3970, Training Loss: 0.003640225389972329\n",
            "Step 3970, Training Loss: 0.05052689090371132\n",
            "Step 3980, Training Loss: 0.0015863715671002865\n",
            "Step 3980, Training Loss: 0.005279005039483309\n",
            "Step 3990, Training Loss: 0.007047774735838175\n",
            "Step 3990, Training Loss: 0.0018645080272108316\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Saving model checkpoint to ./results/checkpoint-4000\n",
            "Configuration saved in ./results/checkpoint-4000/config.json\n",
            "Model weights saved in ./results/checkpoint-4000/pytorch_model.bin\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 4000, Training Loss: 0.0021849689073860645\n",
            "Step 4000, Training Loss: 0.0017505093710497022\n",
            "Step 4010, Training Loss: 0.002122570527717471\n",
            "Step 4010, Training Loss: 0.0024935579858720303\n",
            "Step 4020, Training Loss: 0.0020009735599160194\n",
            "Step 4020, Training Loss: 0.006946324370801449\n",
            "Step 4030, Training Loss: 0.011564711108803749\n",
            "Step 4030, Training Loss: 0.0017275437712669373\n",
            "Step 4040, Training Loss: 0.0067648133262991905\n",
            "Step 4040, Training Loss: 0.0013667704770341516\n",
            "Step 4050, Training Loss: 0.00530110951513052\n",
            "Step 4050, Training Loss: 0.0017178868874907494\n",
            "Step 4060, Training Loss: 0.0018233030568808317\n",
            "Step 4060, Training Loss: 0.08796927332878113\n",
            "Step 4070, Training Loss: 0.099602609872818\n",
            "Step 4070, Training Loss: 0.004446151200681925\n",
            "Step 4080, Training Loss: 0.24234697222709656\n",
            "Step 4080, Training Loss: 0.007941110990941525\n",
            "Step 4090, Training Loss: 0.009412519633769989\n",
            "Step 4090, Training Loss: 0.0023121570702642202\n",
            "Step 4100, Training Loss: 0.0028186063282191753\n",
            "Step 4100, Training Loss: 0.0015817335806787014\n",
            "Step 4110, Training Loss: 0.001708039315417409\n",
            "Step 4110, Training Loss: 0.004173515364527702\n",
            "Step 4120, Training Loss: 0.006707579363137484\n",
            "Step 4120, Training Loss: 0.1773381382226944\n",
            "Step 4130, Training Loss: 0.17913299798965454\n",
            "Step 4130, Training Loss: 0.004829842131584883\n",
            "Step 4140, Training Loss: 0.009216301143169403\n",
            "Step 4140, Training Loss: 0.002585414797067642\n",
            "Step 4150, Training Loss: 0.0018150985706597567\n",
            "Step 4150, Training Loss: 0.1460573673248291\n",
            "Step 4160, Training Loss: 0.012422647327184677\n",
            "Step 4160, Training Loss: 0.0022380794398486614\n",
            "Step 4170, Training Loss: 0.004126164596527815\n",
            "Step 4170, Training Loss: 0.38579973578453064\n",
            "Step 4180, Training Loss: 0.004135573282837868\n",
            "Step 4180, Training Loss: 0.004374789539724588\n",
            "Step 4190, Training Loss: 0.004423466511070728\n",
            "Step 4190, Training Loss: 0.004619435872882605\n",
            "Step 4200, Training Loss: 0.002673486014828086\n",
            "Step 4200, Training Loss: 0.00667206896468997\n",
            "Step 4210, Training Loss: 0.008306515403091908\n",
            "Step 4210, Training Loss: 0.003957823384553194\n",
            "Step 4220, Training Loss: 0.012786482460796833\n",
            "Step 4220, Training Loss: 0.0014010461745783687\n",
            "Step 4230, Training Loss: 0.002054888289421797\n",
            "Step 4230, Training Loss: 0.004491937346756458\n",
            "Step 4240, Training Loss: 0.0033719579223543406\n",
            "Step 4240, Training Loss: 0.03971412032842636\n",
            "Step 4250, Training Loss: 0.0015398272080346942\n",
            "Step 4250, Training Loss: 0.0055038281716406345\n",
            "Step 4260, Training Loss: 0.0043655261397361755\n",
            "Step 4260, Training Loss: 0.0029382009524852037\n",
            "Step 4270, Training Loss: 0.0015435235109180212\n",
            "Step 4270, Training Loss: 0.0013297090772539377\n",
            "Step 4280, Training Loss: 0.001954641891643405\n",
            "Step 4280, Training Loss: 0.011596058495342731\n",
            "Step 4290, Training Loss: 0.06908028572797775\n",
            "Step 4290, Training Loss: 0.005666536279022694\n",
            "Step 4300, Training Loss: 0.004732928238809109\n",
            "Step 4300, Training Loss: 0.008229530416429043\n",
            "Step 4310, Training Loss: 0.009123115800321102\n",
            "Step 4310, Training Loss: 0.004033501725643873\n",
            "Step 4320, Training Loss: 0.001517514348961413\n",
            "Step 4320, Training Loss: 0.0013248325558379292\n",
            "Step 4330, Training Loss: 0.002773736836388707\n",
            "Step 4330, Training Loss: 0.21397455036640167\n",
            "Step 4340, Training Loss: 0.0033829836174845695\n",
            "Step 4340, Training Loss: 0.008640719577670097\n",
            "Step 4350, Training Loss: 0.0030429880134761333\n",
            "Step 4350, Training Loss: 0.003941692411899567\n",
            "Step 4360, Training Loss: 0.008927051909267902\n",
            "Step 4360, Training Loss: 0.010965568944811821\n",
            "Step 4370, Training Loss: 0.047196581959724426\n",
            "Step 4370, Training Loss: 0.007138275075703859\n",
            "Step 4380, Training Loss: 0.0012796175433322787\n",
            "Step 4380, Training Loss: 0.006475334987044334\n",
            "Step 4390, Training Loss: 0.0013931745197623968\n",
            "Step 4390, Training Loss: 0.00404852069914341\n",
            "Step 4400, Training Loss: 0.0014466479187831283\n",
            "Step 4400, Training Loss: 0.01015985757112503\n",
            "Step 4410, Training Loss: 0.0021759846713393927\n",
            "Step 4410, Training Loss: 0.0018118718871846795\n",
            "Step 4420, Training Loss: 0.0030993379186838865\n",
            "Step 4420, Training Loss: 0.003760947147384286\n",
            "Step 4430, Training Loss: 0.0060685924254357815\n",
            "Step 4430, Training Loss: 0.006651417817920446\n",
            "Step 4440, Training Loss: 0.0064139892347157\n",
            "Step 4440, Training Loss: 0.008715173229575157\n",
            "Step 4450, Training Loss: 0.046850938349962234\n",
            "Step 4450, Training Loss: 0.010766533203423023\n",
            "Step 4460, Training Loss: 0.0012906768824905157\n",
            "Step 4460, Training Loss: 0.0016641758847981691\n",
            "Step 4470, Training Loss: 0.0028827814385294914\n",
            "Step 4470, Training Loss: 0.08047045767307281\n",
            "Step 4480, Training Loss: 0.002028869232162833\n",
            "Step 4480, Training Loss: 0.0021922977175563574\n",
            "Step 4490, Training Loss: 0.007401361130177975\n",
            "Step 4490, Training Loss: 0.0020371410064399242\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Saving model checkpoint to ./results/checkpoint-4500\n",
            "Configuration saved in ./results/checkpoint-4500/config.json\n",
            "Model weights saved in ./results/checkpoint-4500/pytorch_model.bin\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 4500, Training Loss: 0.004273065831512213\n",
            "Step 4500, Training Loss: 0.042006466537714005\n",
            "Step 4510, Training Loss: 0.030349187552928925\n",
            "Step 4510, Training Loss: 0.001214094227179885\n",
            "Step 4520, Training Loss: 0.016775641590356827\n",
            "Step 4520, Training Loss: 0.2547822594642639\n",
            "Step 4530, Training Loss: 0.008168241009116173\n",
            "Step 4530, Training Loss: 0.0014023458352312446\n",
            "Step 4540, Training Loss: 0.0065293810330331326\n",
            "Step 4540, Training Loss: 0.0016205988358706236\n",
            "Step 4550, Training Loss: 0.008767664432525635\n",
            "Step 4550, Training Loss: 0.0024054774548858404\n",
            "Step 4560, Training Loss: 0.0013709404738619924\n",
            "Step 4560, Training Loss: 0.0037649190053343773\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 1523\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 4570, Training Loss: 0.006752345710992813\n",
            "Step 4570, Training Loss: 0.0010904371738433838\n",
            "Step 4580, Training Loss: 0.008348535746335983\n",
            "Step 4580, Training Loss: 0.06383690237998962\n",
            "Step 4590, Training Loss: 0.17628484964370728\n",
            "Step 4590, Training Loss: 0.003911392763257027\n",
            "Step 4600, Training Loss: 0.004147791303694248\n",
            "Step 4600, Training Loss: 0.0035819984041154385\n",
            "Step 4610, Training Loss: 0.002182986121624708\n",
            "Step 4610, Training Loss: 0.002346011810004711\n",
            "Step 4620, Training Loss: 0.0036604241468012333\n",
            "Step 4620, Training Loss: 0.029978588223457336\n",
            "Step 4630, Training Loss: 0.00566041748970747\n",
            "Step 4630, Training Loss: 0.0011657783761620522\n",
            "Step 4640, Training Loss: 0.0035792787093669176\n",
            "Step 4640, Training Loss: 0.009396761655807495\n",
            "Step 4650, Training Loss: 0.0018152589909732342\n",
            "Step 4650, Training Loss: 0.0013868733076378703\n",
            "Step 4660, Training Loss: 0.004674202296882868\n",
            "Step 4660, Training Loss: 0.06726855039596558\n",
            "Step 4670, Training Loss: 0.002992832800373435\n",
            "Step 4670, Training Loss: 0.004359873477369547\n",
            "Step 4680, Training Loss: 0.0020781157072633505\n",
            "Step 4680, Training Loss: 0.004331719130277634\n",
            "Step 4690, Training Loss: 0.009004068560898304\n",
            "Step 4690, Training Loss: 0.0012466416228562593\n",
            "Step 4700, Training Loss: 0.0022412706166505814\n",
            "Step 4700, Training Loss: 0.005926841404289007\n",
            "Step 4710, Training Loss: 0.01978789083659649\n",
            "Step 4710, Training Loss: 0.0016074596205726266\n",
            "Step 4720, Training Loss: 0.007834970019757748\n",
            "Step 4720, Training Loss: 0.009139928966760635\n",
            "Step 4730, Training Loss: 0.0020935379434376955\n",
            "Step 4730, Training Loss: 0.0019630833994597197\n",
            "Step 4740, Training Loss: 0.0017375204479321837\n",
            "Step 4740, Training Loss: 0.0035385515075176954\n",
            "Step 4750, Training Loss: 0.0017164568416774273\n",
            "Step 4750, Training Loss: 0.030530722811818123\n",
            "Step 4760, Training Loss: 0.0013864042703062296\n",
            "Step 4760, Training Loss: 0.0015023252926766872\n",
            "Step 4770, Training Loss: 0.0016407419461756945\n",
            "Step 4770, Training Loss: 0.010641162283718586\n",
            "Step 4780, Training Loss: 0.0040354048833251\n",
            "Step 4780, Training Loss: 0.001872392138466239\n",
            "Step 4790, Training Loss: 0.0019483586074784398\n",
            "Step 4790, Training Loss: 0.006128148641437292\n",
            "Step 4800, Training Loss: 0.001380545669235289\n",
            "Step 4800, Training Loss: 0.0011679292656481266\n",
            "Step 4810, Training Loss: 0.005885397549718618\n",
            "Step 4810, Training Loss: 0.00201291311532259\n",
            "Step 4820, Training Loss: 0.0543963685631752\n",
            "Step 4820, Training Loss: 0.01701616868376732\n",
            "Step 4830, Training Loss: 0.0028680760879069567\n",
            "Step 4830, Training Loss: 0.0027678990736603737\n",
            "Step 4840, Training Loss: 0.00597014045342803\n",
            "Step 4840, Training Loss: 0.001803162507712841\n",
            "Step 4850, Training Loss: 0.0016538447234779596\n",
            "Step 4850, Training Loss: 0.006478147115558386\n",
            "Step 4860, Training Loss: 0.027209322899580002\n",
            "Step 4860, Training Loss: 0.0019137434428557754\n",
            "Step 4870, Training Loss: 0.002703946316614747\n",
            "Step 4870, Training Loss: 0.0017773682484403253\n",
            "Step 4880, Training Loss: 0.004314231686294079\n",
            "Step 4880, Training Loss: 0.00264300056733191\n",
            "Step 4890, Training Loss: 0.01092125941067934\n",
            "Step 4890, Training Loss: 0.008630327880382538\n",
            "Step 4900, Training Loss: 0.0022356666158884764\n",
            "Step 4900, Training Loss: 0.002217665547505021\n",
            "Step 4910, Training Loss: 0.0023680084850639105\n",
            "Step 4910, Training Loss: 0.002158793620765209\n",
            "Step 4920, Training Loss: 0.0012547909282147884\n",
            "Step 4920, Training Loss: 0.003094956511631608\n",
            "Step 4930, Training Loss: 0.0019649392925202847\n",
            "Step 4930, Training Loss: 0.012044539675116539\n",
            "Step 4940, Training Loss: 0.005777562502771616\n",
            "Step 4940, Training Loss: 0.0030033481307327747\n",
            "Step 4950, Training Loss: 0.003343816613778472\n",
            "Step 4950, Training Loss: 0.001691794372163713\n",
            "Step 4960, Training Loss: 0.0038683167658746243\n",
            "Step 4960, Training Loss: 0.15999695658683777\n",
            "Step 4970, Training Loss: 0.0013076282339170575\n",
            "Step 4970, Training Loss: 0.0023502681870013475\n",
            "Step 4980, Training Loss: 0.003879202762618661\n",
            "Step 4980, Training Loss: 0.010329295881092548\n",
            "Step 4990, Training Loss: 0.0061233374290168285\n",
            "Step 4990, Training Loss: 0.00173288956284523\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Saving model checkpoint to ./results/checkpoint-5000\n",
            "Configuration saved in ./results/checkpoint-5000/config.json\n",
            "Model weights saved in ./results/checkpoint-5000/pytorch_model.bin\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 5000, Training Loss: 0.0020047416910529137\n",
            "Step 5000, Training Loss: 0.001345723052509129\n",
            "Step 5010, Training Loss: 0.0021632665302604437\n",
            "Step 5010, Training Loss: 0.004938803147524595\n",
            "Step 5020, Training Loss: 0.0014789894921705127\n",
            "Step 5020, Training Loss: 0.004172705113887787\n",
            "Step 5030, Training Loss: 0.004793576896190643\n",
            "Step 5030, Training Loss: 0.001992621459066868\n",
            "Step 5040, Training Loss: 0.01706613227725029\n",
            "Step 5040, Training Loss: 0.0012571689439937472\n",
            "Step 5050, Training Loss: 0.0020942918490618467\n",
            "Step 5050, Training Loss: 0.0017073798226192594\n",
            "Step 5060, Training Loss: 0.03482963889837265\n",
            "Step 5060, Training Loss: 0.2657160460948944\n",
            "Step 5070, Training Loss: 0.020895643159747124\n",
            "Step 5070, Training Loss: 0.0013085894752293825\n",
            "Step 5080, Training Loss: 0.002336598699912429\n",
            "Step 5080, Training Loss: 0.005848373286426067\n",
            "Step 5090, Training Loss: 0.0019671216141432524\n",
            "Step 5090, Training Loss: 0.0018756019417196512\n",
            "Step 5100, Training Loss: 0.0021686996333301067\n",
            "Step 5100, Training Loss: 0.002567039569839835\n",
            "Step 5110, Training Loss: 0.0013186129508540034\n",
            "Step 5110, Training Loss: 0.0018765091663226485\n",
            "Step 5120, Training Loss: 0.3111037015914917\n",
            "Step 5120, Training Loss: 0.005751092918217182\n",
            "Step 5130, Training Loss: 0.0022118561901152134\n",
            "Step 5130, Training Loss: 0.0018604568904265761\n",
            "Step 5140, Training Loss: 0.0011988370679318905\n",
            "Step 5140, Training Loss: 0.026171008124947548\n",
            "Step 5150, Training Loss: 0.0018787096487358212\n",
            "Step 5150, Training Loss: 0.001905877492390573\n",
            "Step 5160, Training Loss: 0.0019013395067304373\n",
            "Step 5160, Training Loss: 0.0014159573474898934\n",
            "Step 5170, Training Loss: 0.0049092429690063\n",
            "Step 5170, Training Loss: 0.0035027842968702316\n",
            "Step 5180, Training Loss: 0.0021056560799479485\n",
            "Step 5180, Training Loss: 0.021006468683481216\n",
            "Step 5190, Training Loss: 0.0033691092394292355\n",
            "Step 5190, Training Loss: 0.0016057726461440325\n",
            "Step 5200, Training Loss: 0.0049880798906087875\n",
            "Step 5200, Training Loss: 0.0022526842076331377\n",
            "Step 5210, Training Loss: 0.04942886903882027\n",
            "Step 5210, Training Loss: 0.007585784420371056\n",
            "Step 5220, Training Loss: 0.0019539992790669203\n",
            "Step 5220, Training Loss: 0.014210921712219715\n",
            "Step 5230, Training Loss: 0.0014106757007539272\n",
            "Step 5230, Training Loss: 0.02432052232325077\n",
            "Step 5240, Training Loss: 0.002092285081744194\n",
            "Step 5240, Training Loss: 0.0013147465651854873\n",
            "Step 5250, Training Loss: 0.003784275148063898\n",
            "Step 5250, Training Loss: 0.002107498236000538\n",
            "Step 5260, Training Loss: 0.0021277465857565403\n",
            "Step 5260, Training Loss: 0.005917667411267757\n",
            "Step 5270, Training Loss: 0.0011151242069900036\n",
            "Step 5270, Training Loss: 0.0013979821233078837\n",
            "Step 5280, Training Loss: 0.001738357008434832\n",
            "Step 5280, Training Loss: 0.001963938819244504\n",
            "Step 5290, Training Loss: 0.0014509190805256367\n",
            "Step 5290, Training Loss: 0.006323654670268297\n",
            "Step 5300, Training Loss: 0.001720462110824883\n",
            "Step 5300, Training Loss: 0.0034228190779685974\n",
            "Step 5310, Training Loss: 0.0017693346599116921\n",
            "Step 5310, Training Loss: 0.0017578315455466509\n",
            "Step 5320, Training Loss: 0.07536571472883224\n",
            "Step 5320, Training Loss: 0.0016041194321587682\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 1523\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 5330, Training Loss: 0.0014922647969797254\n",
            "Step 5330, Training Loss: 0.0031711228657513857\n",
            "Step 5340, Training Loss: 0.005783719941973686\n",
            "Step 5340, Training Loss: 0.0016438079765066504\n",
            "Step 5350, Training Loss: 0.001980682834982872\n",
            "Step 5350, Training Loss: 0.0015181399649009109\n",
            "Step 5360, Training Loss: 0.0014969881158322096\n",
            "Step 5360, Training Loss: 0.009683694690465927\n",
            "Step 5370, Training Loss: 0.0013172895414754748\n",
            "Step 5370, Training Loss: 0.3811895251274109\n",
            "Step 5380, Training Loss: 0.0010826062643900514\n",
            "Step 5380, Training Loss: 0.006135696079581976\n",
            "Step 5390, Training Loss: 0.0023377418983727694\n",
            "Step 5390, Training Loss: 0.0017095727380365133\n",
            "Step 5400, Training Loss: 0.0019889594987034798\n",
            "Step 5400, Training Loss: 0.014120598323643208\n",
            "Step 5410, Training Loss: 0.0037535768933594227\n",
            "Step 5410, Training Loss: 0.10295157879590988\n",
            "Step 5420, Training Loss: 0.008835460059344769\n",
            "Step 5420, Training Loss: 0.0018154591089114547\n",
            "Step 5430, Training Loss: 0.007804876193404198\n",
            "Step 5430, Training Loss: 0.002290698466822505\n",
            "Step 5440, Training Loss: 0.004106748849153519\n",
            "Step 5440, Training Loss: 0.016184870153665543\n",
            "Step 5450, Training Loss: 0.0013681285781785846\n",
            "Step 5450, Training Loss: 0.006044494453817606\n",
            "Step 5460, Training Loss: 0.008353114128112793\n",
            "Step 5460, Training Loss: 0.009086689911782742\n",
            "Step 5470, Training Loss: 0.002209483413025737\n",
            "Step 5470, Training Loss: 0.06775934994220734\n",
            "Step 5480, Training Loss: 0.004041455220431089\n",
            "Step 5480, Training Loss: 0.013728080317378044\n",
            "Step 5490, Training Loss: 0.014308517798781395\n",
            "Step 5490, Training Loss: 0.004370916169136763\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Saving model checkpoint to ./results/checkpoint-5500\n",
            "Configuration saved in ./results/checkpoint-5500/config.json\n",
            "Model weights saved in ./results/checkpoint-5500/pytorch_model.bin\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 5500, Training Loss: 0.03231359273195267\n",
            "Step 5500, Training Loss: 0.0052434103563427925\n",
            "Step 5510, Training Loss: 0.0023045975249260664\n",
            "Step 5510, Training Loss: 0.0034591462463140488\n",
            "Step 5520, Training Loss: 0.001516024931333959\n",
            "Step 5520, Training Loss: 0.00418839231133461\n",
            "Step 5530, Training Loss: 0.004042671527713537\n",
            "Step 5530, Training Loss: 0.0017460588132962584\n",
            "Step 5540, Training Loss: 0.0044919573701918125\n",
            "Step 5540, Training Loss: 0.0013040247140452266\n",
            "Step 5550, Training Loss: 0.0020056988578289747\n",
            "Step 5550, Training Loss: 0.0014757913304492831\n",
            "Step 5560, Training Loss: 0.002194396685808897\n",
            "Step 5560, Training Loss: 0.00148797279689461\n",
            "Step 5570, Training Loss: 0.032791927456855774\n",
            "Step 5570, Training Loss: 0.016011768952012062\n",
            "Step 5580, Training Loss: 0.003402977017685771\n",
            "Step 5580, Training Loss: 0.0015351392794400454\n",
            "Step 5590, Training Loss: 0.40191489458084106\n",
            "Step 5590, Training Loss: 0.001835688017308712\n",
            "Step 5600, Training Loss: 0.00262869312427938\n",
            "Step 5600, Training Loss: 0.010059162974357605\n",
            "Step 5610, Training Loss: 0.00833423063158989\n",
            "Step 5610, Training Loss: 0.002459763316437602\n",
            "Step 5620, Training Loss: 0.020589344203472137\n",
            "Step 5620, Training Loss: 0.0022051329724490643\n",
            "Step 5630, Training Loss: 0.0013243784196674824\n",
            "Step 5630, Training Loss: 0.0038334382697939873\n",
            "Step 5640, Training Loss: 0.005880305077880621\n",
            "Step 5640, Training Loss: 0.01013274397701025\n",
            "Step 5650, Training Loss: 0.0020966911688447\n",
            "Step 5650, Training Loss: 0.0027757438365370035\n",
            "Step 5660, Training Loss: 0.0017555218655616045\n",
            "Step 5660, Training Loss: 0.014665293507277966\n",
            "Step 5670, Training Loss: 0.0031108392868191004\n",
            "Step 5670, Training Loss: 0.44174447655677795\n",
            "Step 5680, Training Loss: 0.12693363428115845\n",
            "Step 5680, Training Loss: 0.035930853337049484\n",
            "Step 5690, Training Loss: 0.0017456235364079475\n",
            "Step 5690, Training Loss: 0.0018473347881808877\n",
            "Step 5700, Training Loss: 0.001470712828449905\n",
            "Step 5700, Training Loss: 0.0013442306080833077\n",
            "Step 5710, Training Loss: 0.006110955961048603\n",
            "Step 5710, Training Loss: 0.004093972034752369\n",
            "Step 5720, Training Loss: 0.12102349102497101\n",
            "Step 5720, Training Loss: 0.012874010019004345\n",
            "Step 5730, Training Loss: 0.004068837035447359\n",
            "Step 5730, Training Loss: 0.004859941080212593\n",
            "Step 5740, Training Loss: 0.0031536302994936705\n",
            "Step 5740, Training Loss: 0.0047835963778197765\n",
            "Step 5750, Training Loss: 0.04602491110563278\n",
            "Step 5750, Training Loss: 0.0069368272088468075\n",
            "Step 5760, Training Loss: 0.001570552820339799\n",
            "Step 5760, Training Loss: 0.0020157727412879467\n",
            "Step 5770, Training Loss: 0.004707566928118467\n",
            "Step 5770, Training Loss: 0.0019576032646000385\n",
            "Step 5780, Training Loss: 0.18010373413562775\n",
            "Step 5780, Training Loss: 0.004974019713699818\n",
            "Step 5790, Training Loss: 0.004804445430636406\n",
            "Step 5790, Training Loss: 0.003348585916683078\n",
            "Step 5800, Training Loss: 0.0013101829681545496\n",
            "Step 5800, Training Loss: 0.0019197392975911498\n",
            "Step 5810, Training Loss: 0.003646757686510682\n",
            "Step 5810, Training Loss: 0.0032386898528784513\n",
            "Step 5820, Training Loss: 0.010745275765657425\n",
            "Step 5820, Training Loss: 0.0018831981578841805\n",
            "Step 5830, Training Loss: 0.0016401034081354737\n",
            "Step 5830, Training Loss: 0.007524351589381695\n",
            "Step 5840, Training Loss: 0.0013616696232929826\n",
            "Step 5840, Training Loss: 0.012018231675028801\n",
            "Step 5850, Training Loss: 0.0013977367198094726\n",
            "Step 5850, Training Loss: 0.0017987714381888509\n",
            "Step 5860, Training Loss: 0.00306340423412621\n",
            "Step 5860, Training Loss: 0.0019079549238085747\n",
            "Step 5870, Training Loss: 0.0015866489848122\n",
            "Step 5870, Training Loss: 0.06862595677375793\n",
            "Step 5880, Training Loss: 0.0018916460685431957\n",
            "Step 5880, Training Loss: 0.007174217142164707\n",
            "Step 5890, Training Loss: 0.0038247897755354643\n",
            "Step 5890, Training Loss: 0.00526228966191411\n",
            "Step 5900, Training Loss: 0.008214712142944336\n",
            "Step 5900, Training Loss: 0.004123541992157698\n",
            "Step 5910, Training Loss: 0.007840033620595932\n",
            "Step 5910, Training Loss: 0.001898790942505002\n",
            "Step 5920, Training Loss: 0.0017132279463112354\n",
            "Step 5920, Training Loss: 0.0015510288067162037\n",
            "Step 5930, Training Loss: 0.006059060804545879\n",
            "Step 5930, Training Loss: 0.0015067727072164416\n",
            "Step 5940, Training Loss: 0.0015399494441226125\n",
            "Step 5940, Training Loss: 0.0018179318867623806\n",
            "Step 5950, Training Loss: 0.00669012451544404\n",
            "Step 5950, Training Loss: 0.011429931968450546\n",
            "Step 5960, Training Loss: 0.0023360615596175194\n",
            "Step 5960, Training Loss: 0.009973791427910328\n",
            "Step 5970, Training Loss: 0.0033992957323789597\n",
            "Step 5970, Training Loss: 0.00150863581802696\n",
            "Step 5980, Training Loss: 0.0014896459178999066\n",
            "Step 5980, Training Loss: 0.001202855259180069\n",
            "Step 5990, Training Loss: 0.0018050698563456535\n",
            "Step 5990, Training Loss: 0.029849309474229813\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Saving model checkpoint to ./results/checkpoint-6000\n",
            "Configuration saved in ./results/checkpoint-6000/config.json\n",
            "Model weights saved in ./results/checkpoint-6000/pytorch_model.bin\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 6000, Training Loss: 0.0011644430924206972\n",
            "Step 6000, Training Loss: 0.048522498458623886\n",
            "Step 6010, Training Loss: 0.008217683993279934\n",
            "Step 6010, Training Loss: 0.004019009880721569\n",
            "Step 6020, Training Loss: 0.0017117068637162447\n",
            "Step 6020, Training Loss: 0.21807676553726196\n",
            "Step 6030, Training Loss: 0.006559125147759914\n",
            "Step 6030, Training Loss: 0.0010939912172034383\n",
            "Step 6040, Training Loss: 0.001906076562590897\n",
            "Step 6040, Training Loss: 0.0012729056179523468\n",
            "Step 6050, Training Loss: 0.3876139521598816\n",
            "Step 6050, Training Loss: 0.005274138413369656\n",
            "Step 6060, Training Loss: 0.004678876139223576\n",
            "Step 6060, Training Loss: 0.0021231891587376595\n",
            "Step 6070, Training Loss: 0.09602537006139755\n",
            "Step 6070, Training Loss: 0.0013018606696277857\n",
            "Step 6080, Training Loss: 0.008809816092252731\n",
            "Step 6080, Training Loss: 0.0013337588170543313\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 1523\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 6090, Training Loss: 0.0012038957793265581\n",
            "Step 6090, Training Loss: 0.0011229499941691756\n",
            "Step 6100, Training Loss: 0.024349013343453407\n",
            "Step 6100, Training Loss: 0.005497835110872984\n",
            "Step 6110, Training Loss: 0.016101252287626266\n",
            "Step 6110, Training Loss: 0.001890134415589273\n",
            "Step 6120, Training Loss: 0.0014271860709413886\n",
            "Step 6120, Training Loss: 0.0015957823488861322\n",
            "Step 6130, Training Loss: 0.0038208081386983395\n",
            "Step 6130, Training Loss: 0.1105603575706482\n",
            "Step 6140, Training Loss: 0.11639368534088135\n",
            "Step 6140, Training Loss: 0.004202705807983875\n",
            "Step 6150, Training Loss: 0.001359662739560008\n",
            "Step 6150, Training Loss: 0.0016354165272787213\n",
            "Step 6160, Training Loss: 0.040310051292181015\n",
            "Step 6160, Training Loss: 0.0020486803259700537\n",
            "Step 6170, Training Loss: 0.0020739445462822914\n",
            "Step 6170, Training Loss: 0.0031832591630518436\n",
            "Step 6180, Training Loss: 0.002244918840005994\n",
            "Step 6180, Training Loss: 0.07278855890035629\n",
            "Step 6190, Training Loss: 0.0017951877089217305\n",
            "Step 6190, Training Loss: 0.003968304954469204\n",
            "Step 6200, Training Loss: 0.008172663860023022\n",
            "Step 6200, Training Loss: 0.6470540165901184\n",
            "Step 6210, Training Loss: 0.0041554938070476055\n",
            "Step 6210, Training Loss: 0.0013655935181304812\n",
            "Step 6220, Training Loss: 0.1729966253042221\n",
            "Step 6220, Training Loss: 0.018214251846075058\n",
            "Step 6230, Training Loss: 0.039906930178403854\n",
            "Step 6230, Training Loss: 0.0037333040963858366\n",
            "Step 6240, Training Loss: 0.0016164110274985433\n",
            "Step 6240, Training Loss: 0.0036451632622629404\n",
            "Step 6250, Training Loss: 0.0018518423894420266\n",
            "Step 6250, Training Loss: 0.0016207543667405844\n",
            "Step 6260, Training Loss: 0.05674395337700844\n",
            "Step 6260, Training Loss: 0.0030040263663977385\n",
            "Step 6270, Training Loss: 0.009232363663613796\n",
            "Step 6270, Training Loss: 0.0021449048072099686\n",
            "Step 6280, Training Loss: 0.0014528875472024083\n",
            "Step 6280, Training Loss: 0.009409268386662006\n",
            "Step 6290, Training Loss: 0.0014538305113092065\n",
            "Step 6290, Training Loss: 0.008545689284801483\n",
            "Step 6300, Training Loss: 0.0029152294155210257\n",
            "Step 6300, Training Loss: 0.0022939913906157017\n",
            "Step 6310, Training Loss: 0.006946418434381485\n",
            "Step 6310, Training Loss: 0.0037903697229921818\n",
            "Step 6320, Training Loss: 0.02719917520880699\n",
            "Step 6320, Training Loss: 0.001256719813682139\n",
            "Step 6330, Training Loss: 0.001238338416442275\n",
            "Step 6330, Training Loss: 0.0022968589328229427\n",
            "Step 6340, Training Loss: 0.0025864453054964542\n",
            "Step 6340, Training Loss: 0.0015585165238007903\n",
            "Step 6350, Training Loss: 0.0012016664259135723\n",
            "Step 6350, Training Loss: 0.003817165270447731\n",
            "Step 6360, Training Loss: 0.001406861818395555\n",
            "Step 6360, Training Loss: 0.004956416320055723\n",
            "Step 6370, Training Loss: 0.0034101682249456644\n",
            "Step 6370, Training Loss: 0.006886214483529329\n",
            "Step 6380, Training Loss: 0.0013255306985229254\n",
            "Step 6380, Training Loss: 0.001590630621649325\n",
            "Step 6390, Training Loss: 0.0071256463415920734\n",
            "Step 6390, Training Loss: 0.008516039699316025\n",
            "Step 6400, Training Loss: 0.001062339753843844\n",
            "Step 6400, Training Loss: 0.004598323255777359\n",
            "Step 6410, Training Loss: 0.004530890844762325\n",
            "Step 6410, Training Loss: 0.0018543524201959372\n",
            "Step 6420, Training Loss: 0.007628331892192364\n",
            "Step 6420, Training Loss: 0.005014594178646803\n",
            "Step 6430, Training Loss: 0.0068593514151871204\n",
            "Step 6430, Training Loss: 0.006612134166061878\n",
            "Step 6440, Training Loss: 0.005457733292132616\n",
            "Step 6440, Training Loss: 0.004053833894431591\n",
            "Step 6450, Training Loss: 0.012382636778056622\n",
            "Step 6450, Training Loss: 0.0017927899025380611\n",
            "Step 6460, Training Loss: 0.0014639042783528566\n",
            "Step 6460, Training Loss: 0.41531965136528015\n",
            "Step 6470, Training Loss: 0.0018599653849378228\n",
            "Step 6470, Training Loss: 0.013853875920176506\n",
            "Step 6480, Training Loss: 0.004581642337143421\n",
            "Step 6480, Training Loss: 0.006269074510782957\n",
            "Step 6490, Training Loss: 0.0026538746897131205\n",
            "Step 6490, Training Loss: 0.001163329929113388\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Saving model checkpoint to ./results/checkpoint-6500\n",
            "Configuration saved in ./results/checkpoint-6500/config.json\n",
            "Model weights saved in ./results/checkpoint-6500/pytorch_model.bin\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 6500, Training Loss: 0.0038977274671196938\n",
            "Step 6500, Training Loss: 0.004699231591075659\n",
            "Step 6510, Training Loss: 0.003006374929100275\n",
            "Step 6510, Training Loss: 0.005326616577804089\n",
            "Step 6520, Training Loss: 0.005991050507873297\n",
            "Step 6520, Training Loss: 0.01470585260540247\n",
            "Step 6530, Training Loss: 0.004862260073423386\n",
            "Step 6530, Training Loss: 0.014985747635364532\n",
            "Step 6540, Training Loss: 0.01181110180914402\n",
            "Step 6540, Training Loss: 0.07832469046115875\n",
            "Step 6550, Training Loss: 0.001507994020357728\n",
            "Step 6550, Training Loss: 0.001867542159743607\n",
            "Step 6560, Training Loss: 0.003369668498635292\n",
            "Step 6560, Training Loss: 0.007341916207224131\n",
            "Step 6570, Training Loss: 0.0019818260334432125\n",
            "Step 6570, Training Loss: 0.0016390210948884487\n",
            "Step 6580, Training Loss: 0.006120878737419844\n",
            "Step 6580, Training Loss: 0.006085922010242939\n",
            "Step 6590, Training Loss: 0.0013338604476302862\n",
            "Step 6590, Training Loss: 0.002373041119426489\n",
            "Step 6600, Training Loss: 0.007475100923329592\n",
            "Step 6600, Training Loss: 0.003639128291979432\n",
            "Step 6610, Training Loss: 0.017226526513695717\n",
            "Step 6610, Training Loss: 0.0016857043374329805\n",
            "Step 6620, Training Loss: 0.001475560013204813\n",
            "Step 6620, Training Loss: 0.0021412274800240993\n",
            "Step 6630, Training Loss: 0.00161535176448524\n",
            "Step 6630, Training Loss: 0.0014991567004472017\n",
            "Step 6640, Training Loss: 0.0017986439634114504\n",
            "Step 6640, Training Loss: 0.004014700651168823\n",
            "Step 6650, Training Loss: 0.0024478964041918516\n",
            "Step 6650, Training Loss: 0.004880449268966913\n",
            "Step 6660, Training Loss: 0.0018841603305190802\n",
            "Step 6660, Training Loss: 0.023098062723875046\n",
            "Step 6670, Training Loss: 0.005855892784893513\n",
            "Step 6670, Training Loss: 0.0016548450803384185\n",
            "Step 6680, Training Loss: 0.005426899529993534\n",
            "Step 6680, Training Loss: 0.01095622405409813\n",
            "Step 6690, Training Loss: 0.0015940183075144887\n",
            "Step 6690, Training Loss: 0.06904148310422897\n",
            "Step 6700, Training Loss: 0.003153958125039935\n",
            "Step 6700, Training Loss: 0.0038818970788270235\n",
            "Step 6710, Training Loss: 0.0030784320551902056\n",
            "Step 6710, Training Loss: 0.002906553680077195\n",
            "Step 6720, Training Loss: 0.002651293994858861\n",
            "Step 6720, Training Loss: 0.002076831180602312\n",
            "Step 6730, Training Loss: 0.007683829870074987\n",
            "Step 6730, Training Loss: 0.002047522459179163\n",
            "Step 6740, Training Loss: 0.002160935662686825\n",
            "Step 6740, Training Loss: 0.0038339137099683285\n",
            "Step 6750, Training Loss: 0.12705589830875397\n",
            "Step 6750, Training Loss: 0.00973287969827652\n",
            "Step 6760, Training Loss: 0.004636797122657299\n",
            "Step 6760, Training Loss: 0.008439010009169579\n",
            "Step 6770, Training Loss: 0.0013636525254696608\n",
            "Step 6770, Training Loss: 0.009190153330564499\n",
            "Step 6780, Training Loss: 0.003017697250470519\n",
            "Step 6780, Training Loss: 0.004477675072848797\n",
            "Step 6790, Training Loss: 0.002999534597620368\n",
            "Step 6790, Training Loss: 0.006289940793067217\n",
            "Step 6800, Training Loss: 0.006670826580375433\n",
            "Step 6800, Training Loss: 0.0017493576742708683\n",
            "Step 6810, Training Loss: 0.004163553938269615\n",
            "Step 6810, Training Loss: 0.001374493702314794\n",
            "Step 6820, Training Loss: 0.0077789477072656155\n",
            "Step 6820, Training Loss: 0.003406290663406253\n",
            "Step 6830, Training Loss: 0.004972818773239851\n",
            "Step 6830, Training Loss: 0.02518332377076149\n",
            "Step 6840, Training Loss: 0.061661310493946075\n",
            "Step 6840, Training Loss: 0.00187391578219831\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 1523\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 6850, Training Loss: 0.007913298904895782\n",
            "Step 6850, Training Loss: 0.009107032790780067\n",
            "Step 6860, Training Loss: 0.0011849042493849993\n",
            "Step 6860, Training Loss: 0.001924195559695363\n",
            "Step 6870, Training Loss: 0.00403402978554368\n",
            "Step 6870, Training Loss: 0.0030487505719065666\n",
            "Step 6880, Training Loss: 0.0013282635482028127\n",
            "Step 6880, Training Loss: 0.0011079729301854968\n",
            "Step 6890, Training Loss: 0.0032288571819663048\n",
            "Step 6890, Training Loss: 0.002796871354803443\n",
            "Step 6900, Training Loss: 0.0015288484282791615\n",
            "Step 6900, Training Loss: 0.02457750029861927\n",
            "Step 6910, Training Loss: 0.0018392812926322222\n",
            "Step 6910, Training Loss: 0.0014273246051743627\n",
            "Step 6920, Training Loss: 0.0017664723563939333\n",
            "Step 6920, Training Loss: 0.0153345363214612\n",
            "Step 6930, Training Loss: 0.012966233305633068\n",
            "Step 6930, Training Loss: 0.0012638135813176632\n",
            "Step 6940, Training Loss: 0.006746350787580013\n",
            "Step 6940, Training Loss: 0.012510957196354866\n",
            "Step 6950, Training Loss: 0.0012793813366442919\n",
            "Step 6950, Training Loss: 0.0017482974799349904\n",
            "Step 6960, Training Loss: 0.003954518120735884\n",
            "Step 6960, Training Loss: 0.001854768954217434\n",
            "Step 6970, Training Loss: 0.0014818391064181924\n",
            "Step 6970, Training Loss: 0.0040255384519696236\n",
            "Step 6980, Training Loss: 0.0029608469922095537\n",
            "Step 6980, Training Loss: 0.0074157556518912315\n",
            "Step 6990, Training Loss: 0.004078286234289408\n",
            "Step 6990, Training Loss: 0.0013247521128505468\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Saving model checkpoint to ./results/checkpoint-7000\n",
            "Configuration saved in ./results/checkpoint-7000/config.json\n",
            "Model weights saved in ./results/checkpoint-7000/pytorch_model.bin\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 7000, Training Loss: 0.010917016305029392\n",
            "Step 7000, Training Loss: 0.004004024900496006\n",
            "Step 7010, Training Loss: 0.0013530125143006444\n",
            "Step 7010, Training Loss: 0.027163302525877953\n",
            "Step 7020, Training Loss: 0.008750000968575478\n",
            "Step 7020, Training Loss: 0.0052458602003753185\n",
            "Step 7030, Training Loss: 0.004721842706203461\n",
            "Step 7030, Training Loss: 0.0033347250428050756\n",
            "Step 7040, Training Loss: 0.005086042918264866\n",
            "Step 7040, Training Loss: 0.21179519593715668\n",
            "Step 7050, Training Loss: 0.009506798349320889\n",
            "Step 7050, Training Loss: 0.10222978889942169\n",
            "Step 7060, Training Loss: 0.006435698829591274\n",
            "Step 7060, Training Loss: 0.0016412854893133044\n",
            "Step 7070, Training Loss: 0.0044735753908753395\n",
            "Step 7070, Training Loss: 0.008475720882415771\n",
            "Step 7080, Training Loss: 0.0016436876030638814\n",
            "Step 7080, Training Loss: 0.006540132686495781\n",
            "Step 7090, Training Loss: 0.0011714407010003924\n",
            "Step 7090, Training Loss: 0.004210589453577995\n",
            "Step 7100, Training Loss: 0.0019804134499281645\n",
            "Step 7100, Training Loss: 0.19056947529315948\n",
            "Step 7110, Training Loss: 0.0322139747440815\n",
            "Step 7110, Training Loss: 0.004622388165444136\n",
            "Step 7120, Training Loss: 0.0012944748159497976\n",
            "Step 7120, Training Loss: 0.007435421459376812\n",
            "Step 7130, Training Loss: 0.0015848394250497222\n",
            "Step 7130, Training Loss: 0.0070626153610646725\n",
            "Step 7140, Training Loss: 0.004505544435232878\n",
            "Step 7140, Training Loss: 0.0045354715548455715\n",
            "Step 7150, Training Loss: 0.003367458935827017\n",
            "Step 7150, Training Loss: 0.0015873683150857687\n",
            "Step 7160, Training Loss: 0.001694627571851015\n",
            "Step 7160, Training Loss: 0.0029519444797188044\n",
            "Step 7170, Training Loss: 0.0012051983503624797\n",
            "Step 7170, Training Loss: 0.0013260573614388704\n",
            "Step 7180, Training Loss: 0.002214588923379779\n",
            "Step 7180, Training Loss: 0.0015637672040611506\n",
            "Step 7190, Training Loss: 0.18043091893196106\n",
            "Step 7190, Training Loss: 0.002227791817858815\n",
            "Step 7200, Training Loss: 0.0035876904148608446\n",
            "Step 7200, Training Loss: 0.006961482111364603\n",
            "Step 7210, Training Loss: 0.05669224262237549\n",
            "Step 7210, Training Loss: 0.003311639418825507\n",
            "Step 7220, Training Loss: 0.0014136502286419272\n",
            "Step 7220, Training Loss: 0.0034780348651111126\n",
            "Step 7230, Training Loss: 0.0032832135912030935\n",
            "Step 7230, Training Loss: 0.004286161623895168\n",
            "Step 7240, Training Loss: 0.012116706930100918\n",
            "Step 7240, Training Loss: 0.0014543760335072875\n",
            "Step 7250, Training Loss: 0.004898715298622847\n",
            "Step 7250, Training Loss: 0.006128083448857069\n",
            "Step 7260, Training Loss: 0.0017385052051395178\n",
            "Step 7260, Training Loss: 0.23163974285125732\n",
            "Step 7270, Training Loss: 0.0014269027160480618\n",
            "Step 7270, Training Loss: 0.0024513632524758577\n",
            "Step 7280, Training Loss: 0.0018010652856901288\n",
            "Step 7280, Training Loss: 0.0020874543115496635\n",
            "Step 7290, Training Loss: 0.008736491203308105\n",
            "Step 7290, Training Loss: 0.025554554536938667\n",
            "Step 7300, Training Loss: 0.2032652348279953\n",
            "Step 7300, Training Loss: 0.001477380865253508\n",
            "Step 7310, Training Loss: 0.002324409084394574\n",
            "Step 7310, Training Loss: 0.0172162763774395\n",
            "Step 7320, Training Loss: 0.0024900613352656364\n",
            "Step 7320, Training Loss: 0.0021216736640781164\n",
            "Step 7330, Training Loss: 0.0026269599329680204\n",
            "Step 7330, Training Loss: 0.0045689973048865795\n",
            "Step 7340, Training Loss: 0.007667830213904381\n",
            "Step 7340, Training Loss: 0.004468040075153112\n",
            "Step 7350, Training Loss: 0.002762195188552141\n",
            "Step 7350, Training Loss: 0.0016189293237403035\n",
            "Step 7360, Training Loss: 0.0014411590527743101\n",
            "Step 7360, Training Loss: 0.007104555610567331\n",
            "Step 7370, Training Loss: 0.008425465784966946\n",
            "Step 7370, Training Loss: 0.005080376751720905\n",
            "Step 7380, Training Loss: 0.0069718137383461\n",
            "Step 7380, Training Loss: 0.007302603218704462\n",
            "Step 7390, Training Loss: 0.0013365413760766387\n",
            "Step 7390, Training Loss: 0.004703408107161522\n",
            "Step 7400, Training Loss: 0.001520949648693204\n",
            "Step 7400, Training Loss: 0.0013532781740650535\n",
            "Step 7410, Training Loss: 0.0017592308577150106\n",
            "Step 7410, Training Loss: 0.0055646407417953014\n",
            "Step 7420, Training Loss: 0.002673268551006913\n",
            "Step 7420, Training Loss: 0.004717101342976093\n",
            "Step 7430, Training Loss: 0.1432022601366043\n",
            "Step 7430, Training Loss: 0.0019367882050573826\n",
            "Step 7440, Training Loss: 0.004005401395261288\n",
            "Step 7440, Training Loss: 0.008256266824901104\n",
            "Step 7450, Training Loss: 0.0015988987870514393\n",
            "Step 7450, Training Loss: 0.1463337391614914\n",
            "Step 7460, Training Loss: 0.0022458562161773443\n",
            "Step 7460, Training Loss: 0.0017427550628781319\n",
            "Step 7470, Training Loss: 0.0038379246834665537\n",
            "Step 7470, Training Loss: 0.0012634757440537214\n",
            "Step 7480, Training Loss: 0.00538975652307272\n",
            "Step 7480, Training Loss: 0.005477394443005323\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Saving model checkpoint to ./results/checkpoint-7500\n",
            "Configuration saved in ./results/checkpoint-7500/config.json\n",
            "Model weights saved in ./results/checkpoint-7500/pytorch_model.bin\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 7500, Training Loss: 0.019513320177793503\n",
            "Step 7500, Training Loss: 0.001237576245330274\n",
            "Step 7510, Training Loss: 0.0023274116683751345\n",
            "Step 7510, Training Loss: 0.0017781236674636602\n",
            "Step 7520, Training Loss: 0.015098510310053825\n",
            "Step 7520, Training Loss: 0.0016975648468360305\n",
            "Step 7530, Training Loss: 0.0035721459425985813\n",
            "Step 7530, Training Loss: 0.0013524055248126388\n",
            "Step 7540, Training Loss: 0.0016099079512059689\n",
            "Step 7540, Training Loss: 0.004644722677767277\n",
            "Step 7550, Training Loss: 0.003265778301283717\n",
            "Step 7550, Training Loss: 0.0017514737555757165\n",
            "Step 7560, Training Loss: 0.042631205171346664\n",
            "Step 7560, Training Loss: 0.2382929027080536\n",
            "Step 7570, Training Loss: 0.0016365139745175838\n",
            "Step 7570, Training Loss: 0.3038298785686493\n",
            "Step 7580, Training Loss: 0.00203956151381135\n",
            "Step 7580, Training Loss: 0.0013035248266533017\n",
            "Step 7590, Training Loss: 0.003461407497525215\n",
            "Step 7590, Training Loss: 0.0075736818835139275\n",
            "Step 7600, Training Loss: 0.0012923538452014327\n",
            "Step 7600, Training Loss: 0.03674106299877167\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 1523\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 7610, Training Loss: 0.3552511930465698\n",
            "Step 7610, Training Loss: 0.47686871886253357\n",
            "Step 7610, Training Loss: 0.06700902432203293\n",
            "Step 7610, Training Loss: 0.00841838400810957\n",
            "Step 7610, Training Loss: 0.3608073592185974\n",
            "Step 7610, Training Loss: 0.5022972226142883\n",
            "Step 7610, Training Loss: 0.35146868228912354\n",
            "Step 7610, Training Loss: 0.176463320851326\n",
            "Step 7610, Training Loss: 0.010867771692574024\n",
            "Step 7610, Training Loss: 0.004802650772035122\n",
            "Step 7610, Training Loss: 0.2342839241027832\n",
            "Step 7610, Training Loss: 0.19724126160144806\n",
            "Step 7610, Training Loss: 0.22260023653507233\n",
            "Step 7610, Training Loss: 0.18225494027137756\n",
            "Step 7610, Training Loss: 0.19937878847122192\n",
            "Step 7610, Training Loss: 0.11776187270879745\n",
            "Step 7610, Training Loss: 0.22781188786029816\n",
            "Step 7610, Training Loss: 0.11971457302570343\n",
            "Step 7610, Training Loss: 0.26536378264427185\n",
            "Step 7610, Training Loss: 0.1204332485795021\n",
            "Step 7610, Training Loss: 0.22531473636627197\n",
            "Step 7610, Training Loss: 0.001597349182702601\n",
            "Step 7610, Training Loss: 0.3588293194770813\n",
            "Step 7610, Training Loss: 0.040019139647483826\n",
            "Step 7610, Training Loss: 0.0010920853819698095\n",
            "Step 7610, Training Loss: 0.22709843516349792\n",
            "Step 7610, Training Loss: 0.2213607281446457\n",
            "Step 7610, Training Loss: 0.0023422252852469683\n",
            "Step 7610, Training Loss: 0.0021146719809621572\n",
            "Step 7610, Training Loss: 0.30743661522865295\n",
            "Step 7610, Training Loss: 0.0016383685870096087\n",
            "Step 7610, Training Loss: 0.44508281350135803\n",
            "Step 7610, Training Loss: 0.01637452468276024\n",
            "Step 7610, Training Loss: 0.004052930511534214\n",
            "Step 7610, Training Loss: 0.0013485245872288942\n",
            "Step 7610, Training Loss: 0.3617050349712372\n",
            "Step 7610, Training Loss: 0.14924457669258118\n",
            "Step 7610, Training Loss: 0.024073293432593346\n",
            "Step 7610, Training Loss: 0.12531960010528564\n",
            "Step 7610, Training Loss: 0.27373456954956055\n",
            "Step 7610, Training Loss: 0.2273491621017456\n",
            "Step 7610, Training Loss: 0.0015260246582329273\n",
            "Step 7610, Training Loss: 0.0026051285676658154\n",
            "Step 7610, Training Loss: 0.001930156722664833\n",
            "Step 7610, Training Loss: 0.008220420219004154\n",
            "Step 7610, Training Loss: 0.13257476687431335\n",
            "Step 7610, Training Loss: 0.22667135298252106\n",
            "Step 7610, Training Loss: 0.37037429213523865\n",
            "Step 7610, Training Loss: 0.22094914317131042\n",
            "Step 7610, Training Loss: 0.3540179431438446\n",
            "Step 7610, Training Loss: 0.002769432496279478\n",
            "Step 7610, Training Loss: 0.4135747253894806\n",
            "Step 7610, Training Loss: 0.1301214098930359\n",
            "Step 7610, Training Loss: 0.0799209401011467\n",
            "Step 7610, Training Loss: 0.059217650443315506\n",
            "Step 7610, Training Loss: 0.002578879939392209\n",
            "Step 7610, Training Loss: 0.22965534031391144\n",
            "Step 7610, Training Loss: 0.0014011304592713714\n",
            "Step 7610, Training Loss: 0.04962942376732826\n",
            "Step 7610, Training Loss: 0.029113134369254112\n",
            "Step 7610, Training Loss: 0.06176205351948738\n",
            "Step 7610, Training Loss: 0.13871806859970093\n",
            "Step 7610, Training Loss: 0.2742198407649994\n",
            "Step 7610, Training Loss: 0.18931140005588531\n",
            "Step 7610, Training Loss: 0.4344123601913452\n",
            "Step 7610, Training Loss: 0.0019293880322948098\n",
            "Step 7610, Training Loss: 0.08647911250591278\n",
            "Step 7610, Training Loss: 0.17148862779140472\n",
            "Step 7610, Training Loss: 0.0016500756610184908\n",
            "Step 7610, Training Loss: 0.0020881658419966698\n",
            "Step 7610, Training Loss: 0.004735867027193308\n",
            "Step 7610, Training Loss: 0.16088561713695526\n",
            "Step 7610, Training Loss: 0.4226735830307007\n",
            "Step 7610, Training Loss: 0.09566731005907059\n",
            "Step 7610, Training Loss: 0.22472409904003143\n",
            "Step 7610, Training Loss: 0.10381945222616196\n",
            "Step 7610, Training Loss: 0.0030857014935463667\n",
            "Step 7610, Training Loss: 0.012804444879293442\n",
            "Step 7610, Training Loss: 0.23670534789562225\n",
            "Step 7610, Training Loss: 0.36931124329566956\n",
            "Step 7610, Training Loss: 0.32358962297439575\n",
            "Step 7610, Training Loss: 0.10763420164585114\n",
            "Step 7610, Training Loss: 0.49336564540863037\n",
            "Step 7610, Training Loss: 0.3257488012313843\n",
            "Step 7610, Training Loss: 0.2921692430973053\n",
            "Step 7610, Training Loss: 0.4573574364185333\n",
            "Step 7610, Training Loss: 0.002691284054890275\n",
            "Step 7610, Training Loss: 0.15025478601455688\n",
            "Step 7610, Training Loss: 0.22201496362686157\n",
            "Step 7610, Training Loss: 0.00788920745253563\n",
            "Step 7610, Training Loss: 0.003016970120370388\n",
            "Step 7610, Training Loss: 0.23432867228984833\n",
            "Step 7610, Training Loss: 0.37008050084114075\n",
            "Step 7610, Training Loss: 0.0031496414449065924\n",
            "Step 7610, Training Loss: 0.09699574112892151\n",
            "Step 7610, Training Loss: 0.22211022675037384\n",
            "Step 7610, Training Loss: 0.2508983612060547\n",
            "Step 7610, Training Loss: 0.22017736732959747\n",
            "Step 7610, Training Loss: 0.694570004940033\n",
            "Step 7610, Training Loss: 0.33818355202674866\n",
            "Step 7610, Training Loss: 0.35851842164993286\n",
            "Step 7610, Training Loss: 0.2997286915779114\n",
            "Step 7610, Training Loss: 0.06677712500095367\n",
            "Step 7610, Training Loss: 0.5888283252716064\n",
            "Step 7610, Training Loss: 0.22867242991924286\n",
            "Step 7610, Training Loss: 0.0025459169410169125\n",
            "Step 7610, Training Loss: 0.009346669539809227\n",
            "Step 7610, Training Loss: 0.12342173606157303\n",
            "Step 7610, Training Loss: 0.005229549948126078\n",
            "Step 7610, Training Loss: 0.2343267798423767\n",
            "Step 7610, Training Loss: 0.5129740834236145\n",
            "Step 7610, Training Loss: 0.11280008405447006\n",
            "Step 7610, Training Loss: 0.008100591599941254\n",
            "Step 7610, Training Loss: 0.46050918102264404\n",
            "Step 7610, Training Loss: 0.4267614483833313\n",
            "Step 7610, Training Loss: 0.0031058157328516245\n",
            "Step 7610, Training Loss: 0.003115305444225669\n",
            "Step 7610, Training Loss: 0.10090342164039612\n",
            "Step 7610, Training Loss: 0.0011662178440019488\n",
            "Step 7610, Training Loss: 0.008085851557552814\n",
            "Step 7610, Training Loss: 0.45404309034347534\n",
            "Step 7610, Training Loss: 0.24284674227237701\n",
            "Step 7610, Training Loss: 0.006867260206490755\n",
            "Step 7610, Training Loss: 0.2149461954832077\n",
            "Step 7610, Training Loss: 0.08344271779060364\n",
            "Step 7610, Training Loss: 0.20213423669338226\n",
            "Step 7610, Training Loss: 0.005917021539062262\n",
            "Step 7610, Training Loss: 0.24764160811901093\n",
            "Step 7610, Training Loss: 0.4363560676574707\n",
            "Step 7610, Training Loss: 0.22685018181800842\n",
            "Step 7610, Training Loss: 0.09716247767210007\n",
            "Step 7610, Training Loss: 0.21913763880729675\n",
            "Step 7610, Training Loss: 0.22790801525115967\n",
            "Step 7610, Training Loss: 0.22414322197437286\n",
            "Step 7610, Training Loss: 0.001769326743669808\n",
            "Step 7610, Training Loss: 0.1547216773033142\n",
            "Step 7610, Training Loss: 0.001616037916392088\n",
            "Step 7610, Training Loss: 0.0021622113417834044\n",
            "Step 7610, Training Loss: 0.0014566507888957858\n",
            "Step 7610, Training Loss: 0.11074060201644897\n",
            "Step 7610, Training Loss: 0.24387042224407196\n",
            "Step 7610, Training Loss: 0.4807828962802887\n",
            "Step 7610, Training Loss: 0.29289254546165466\n",
            "Step 7610, Training Loss: 0.11497987061738968\n",
            "Step 7610, Training Loss: 0.19092467427253723\n",
            "Step 7610, Training Loss: 0.1008191630244255\n",
            "Step 7610, Training Loss: 0.2279742956161499\n",
            "Step 7610, Training Loss: 0.1252060830593109\n",
            "Step 7610, Training Loss: 0.03135569021105766\n",
            "Step 7610, Training Loss: 0.4516826570034027\n",
            "Step 7610, Training Loss: 0.004637250676751137\n",
            "Step 7610, Training Loss: 0.3098782002925873\n",
            "Step 7610, Training Loss: 0.0013913004659116268\n",
            "Step 7610, Training Loss: 0.45747703313827515\n",
            "Step 7610, Training Loss: 0.22676308453083038\n",
            "Step 7610, Training Loss: 0.004202206153422594\n",
            "Step 7610, Training Loss: 0.08846665918827057\n",
            "Step 7610, Training Loss: 0.07906480878591537\n",
            "Step 7610, Training Loss: 0.14330783486366272\n",
            "Step 7610, Training Loss: 0.07273750752210617\n",
            "Step 7610, Training Loss: 0.14378826320171356\n",
            "Step 7610, Training Loss: 0.5882503986358643\n",
            "Step 7610, Training Loss: 0.22984537482261658\n",
            "Step 7610, Training Loss: 0.24693071842193604\n",
            "Step 7610, Training Loss: 0.55478835105896\n",
            "Step 7610, Training Loss: 0.1673867553472519\n",
            "Step 7610, Training Loss: 0.22739680111408234\n",
            "Step 7610, Training Loss: 0.17543283104896545\n",
            "Step 7610, Training Loss: 0.05500289797782898\n",
            "Step 7610, Training Loss: 0.0018464947352185845\n",
            "Step 7610, Training Loss: 0.21673469245433807\n",
            "Step 7610, Training Loss: 0.001934971660375595\n",
            "Step 7610, Training Loss: 0.34629663825035095\n",
            "Step 7610, Training Loss: 0.0028886308427900076\n",
            "Step 7610, Training Loss: 0.02451510913670063\n",
            "Step 7610, Training Loss: 0.002341232728213072\n",
            "Step 7610, Training Loss: 0.28088149428367615\n",
            "Step 7610, Training Loss: 0.3006401062011719\n",
            "Step 7610, Training Loss: 0.4358142614364624\n",
            "Step 7610, Training Loss: 0.13198207318782806\n",
            "Step 7610, Training Loss: 0.0016263050492852926\n",
            "Step 7610, Training Loss: 0.13840167224407196\n",
            "Step 7610, Training Loss: 0.12824063003063202\n",
            "Step 7610, Training Loss: 0.11119125038385391\n",
            "Step 7610, Training Loss: 0.0018083472969010472\n",
            "Step 7610, Training Loss: 0.009982964023947716\n",
            "Step 7610, Training Loss: 0.1519474983215332\n",
            "Step 7610, Training Loss: 0.35431578755378723\n",
            "Step 7610, Training Loss: 0.19741274416446686\n",
            "Step 7610, Training Loss: 0.14693182706832886\n",
            "Step 7610, Training Loss: 0.33342766761779785\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training completed in 407.46 seconds."
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 1523\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Step 7610, Training Loss: 0.3552511930465698\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='191' max='191' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [191/191 00:03]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 7610, Training Loss: 0.47686871886253357\n",
            "Step 7610, Training Loss: 0.06700902432203293\n",
            "Step 7610, Training Loss: 0.00841838400810957\n",
            "Step 7610, Training Loss: 0.3608073592185974\n",
            "Step 7610, Training Loss: 0.5022972226142883\n",
            "Step 7610, Training Loss: 0.35146868228912354\n",
            "Step 7610, Training Loss: 0.176463320851326\n",
            "Step 7610, Training Loss: 0.010867771692574024\n",
            "Step 7610, Training Loss: 0.004802650772035122\n",
            "Step 7610, Training Loss: 0.2342839241027832\n",
            "Step 7610, Training Loss: 0.19724126160144806\n",
            "Step 7610, Training Loss: 0.22260023653507233\n",
            "Step 7610, Training Loss: 0.18225494027137756\n",
            "Step 7610, Training Loss: 0.19937878847122192\n",
            "Step 7610, Training Loss: 0.11776187270879745\n",
            "Step 7610, Training Loss: 0.22781188786029816\n",
            "Step 7610, Training Loss: 0.11971457302570343\n",
            "Step 7610, Training Loss: 0.26536378264427185\n",
            "Step 7610, Training Loss: 0.1204332485795021\n",
            "Step 7610, Training Loss: 0.22531473636627197\n",
            "Step 7610, Training Loss: 0.001597349182702601\n",
            "Step 7610, Training Loss: 0.3588293194770813\n",
            "Step 7610, Training Loss: 0.040019139647483826\n",
            "Step 7610, Training Loss: 0.0010920853819698095\n",
            "Step 7610, Training Loss: 0.22709843516349792\n",
            "Step 7610, Training Loss: 0.2213607281446457\n",
            "Step 7610, Training Loss: 0.0023422252852469683\n",
            "Step 7610, Training Loss: 0.0021146719809621572\n",
            "Step 7610, Training Loss: 0.30743661522865295\n",
            "Step 7610, Training Loss: 0.0016383685870096087\n",
            "Step 7610, Training Loss: 0.44508281350135803\n",
            "Step 7610, Training Loss: 0.01637452468276024\n",
            "Step 7610, Training Loss: 0.004052930511534214\n",
            "Step 7610, Training Loss: 0.0013485245872288942\n",
            "Step 7610, Training Loss: 0.3617050349712372\n",
            "Step 7610, Training Loss: 0.14924457669258118\n",
            "Step 7610, Training Loss: 0.024073293432593346\n",
            "Step 7610, Training Loss: 0.12531960010528564\n",
            "Step 7610, Training Loss: 0.27373456954956055\n",
            "Step 7610, Training Loss: 0.2273491621017456\n",
            "Step 7610, Training Loss: 0.0015260246582329273\n",
            "Step 7610, Training Loss: 0.0026051285676658154\n",
            "Step 7610, Training Loss: 0.001930156722664833\n",
            "Step 7610, Training Loss: 0.008220420219004154\n",
            "Step 7610, Training Loss: 0.13257476687431335\n",
            "Step 7610, Training Loss: 0.22667135298252106\n",
            "Step 7610, Training Loss: 0.37037429213523865\n",
            "Step 7610, Training Loss: 0.22094914317131042\n",
            "Step 7610, Training Loss: 0.3540179431438446\n",
            "Step 7610, Training Loss: 0.002769432496279478\n",
            "Step 7610, Training Loss: 0.4135747253894806\n",
            "Step 7610, Training Loss: 0.1301214098930359\n",
            "Step 7610, Training Loss: 0.0799209401011467\n",
            "Step 7610, Training Loss: 0.059217650443315506\n",
            "Step 7610, Training Loss: 0.002578879939392209\n",
            "Step 7610, Training Loss: 0.22965534031391144\n",
            "Step 7610, Training Loss: 0.0014011304592713714\n",
            "Step 7610, Training Loss: 0.04962942376732826\n",
            "Step 7610, Training Loss: 0.029113134369254112\n",
            "Step 7610, Training Loss: 0.06176205351948738\n",
            "Step 7610, Training Loss: 0.13871806859970093\n",
            "Step 7610, Training Loss: 0.2742198407649994\n",
            "Step 7610, Training Loss: 0.18931140005588531\n",
            "Step 7610, Training Loss: 0.4344123601913452\n",
            "Step 7610, Training Loss: 0.0019293880322948098\n",
            "Step 7610, Training Loss: 0.08647911250591278\n",
            "Step 7610, Training Loss: 0.17148862779140472\n",
            "Step 7610, Training Loss: 0.0016500756610184908\n",
            "Step 7610, Training Loss: 0.0020881658419966698\n",
            "Step 7610, Training Loss: 0.004735867027193308\n",
            "Step 7610, Training Loss: 0.16088561713695526\n",
            "Step 7610, Training Loss: 0.4226735830307007\n",
            "Step 7610, Training Loss: 0.09566731005907059\n",
            "Step 7610, Training Loss: 0.22472409904003143\n",
            "Step 7610, Training Loss: 0.10381945222616196\n",
            "Step 7610, Training Loss: 0.0030857014935463667\n",
            "Step 7610, Training Loss: 0.012804444879293442\n",
            "Step 7610, Training Loss: 0.23670534789562225\n",
            "Step 7610, Training Loss: 0.36931124329566956\n",
            "Step 7610, Training Loss: 0.32358962297439575\n",
            "Step 7610, Training Loss: 0.10763420164585114\n",
            "Step 7610, Training Loss: 0.49336564540863037\n",
            "Step 7610, Training Loss: 0.3257488012313843\n",
            "Step 7610, Training Loss: 0.2921692430973053\n",
            "Step 7610, Training Loss: 0.4573574364185333\n",
            "Step 7610, Training Loss: 0.002691284054890275\n",
            "Step 7610, Training Loss: 0.15025478601455688\n",
            "Step 7610, Training Loss: 0.22201496362686157\n",
            "Step 7610, Training Loss: 0.00788920745253563\n",
            "Step 7610, Training Loss: 0.003016970120370388\n",
            "Step 7610, Training Loss: 0.23432867228984833\n",
            "Step 7610, Training Loss: 0.37008050084114075\n",
            "Step 7610, Training Loss: 0.0031496414449065924\n",
            "Step 7610, Training Loss: 0.09699574112892151\n",
            "Step 7610, Training Loss: 0.22211022675037384\n",
            "Step 7610, Training Loss: 0.2508983612060547\n",
            "Step 7610, Training Loss: 0.22017736732959747\n",
            "Step 7610, Training Loss: 0.694570004940033\n",
            "Step 7610, Training Loss: 0.33818355202674866\n",
            "Step 7610, Training Loss: 0.35851842164993286\n",
            "Step 7610, Training Loss: 0.2997286915779114\n",
            "Step 7610, Training Loss: 0.06677712500095367\n",
            "Step 7610, Training Loss: 0.5888283252716064\n",
            "Step 7610, Training Loss: 0.22867242991924286\n",
            "Step 7610, Training Loss: 0.0025459169410169125\n",
            "Step 7610, Training Loss: 0.009346669539809227\n",
            "Step 7610, Training Loss: 0.12342173606157303\n",
            "Step 7610, Training Loss: 0.005229549948126078\n",
            "Step 7610, Training Loss: 0.2343267798423767\n",
            "Step 7610, Training Loss: 0.5129740834236145\n",
            "Step 7610, Training Loss: 0.11280008405447006\n",
            "Step 7610, Training Loss: 0.008100591599941254\n",
            "Step 7610, Training Loss: 0.46050918102264404\n",
            "Step 7610, Training Loss: 0.4267614483833313\n",
            "Step 7610, Training Loss: 0.0031058157328516245\n",
            "Step 7610, Training Loss: 0.003115305444225669\n",
            "Step 7610, Training Loss: 0.10090342164039612\n",
            "Step 7610, Training Loss: 0.0011662178440019488\n",
            "Step 7610, Training Loss: 0.008085851557552814\n",
            "Step 7610, Training Loss: 0.45404309034347534\n",
            "Step 7610, Training Loss: 0.24284674227237701\n",
            "Step 7610, Training Loss: 0.006867260206490755\n",
            "Step 7610, Training Loss: 0.2149461954832077\n",
            "Step 7610, Training Loss: 0.08344271779060364\n",
            "Step 7610, Training Loss: 0.20213423669338226\n",
            "Step 7610, Training Loss: 0.005917021539062262\n",
            "Step 7610, Training Loss: 0.24764160811901093\n",
            "Step 7610, Training Loss: 0.4363560676574707\n",
            "Step 7610, Training Loss: 0.22685018181800842\n",
            "Step 7610, Training Loss: 0.09716247767210007\n",
            "Step 7610, Training Loss: 0.21913763880729675\n",
            "Step 7610, Training Loss: 0.22790801525115967\n",
            "Step 7610, Training Loss: 0.22414322197437286\n",
            "Step 7610, Training Loss: 0.001769326743669808\n",
            "Step 7610, Training Loss: 0.1547216773033142\n",
            "Step 7610, Training Loss: 0.001616037916392088\n",
            "Step 7610, Training Loss: 0.0021622113417834044\n",
            "Step 7610, Training Loss: 0.0014566507888957858\n",
            "Step 7610, Training Loss: 0.11074060201644897\n",
            "Step 7610, Training Loss: 0.24387042224407196\n",
            "Step 7610, Training Loss: 0.4807828962802887\n",
            "Step 7610, Training Loss: 0.29289254546165466\n",
            "Step 7610, Training Loss: 0.11497987061738968\n",
            "Step 7610, Training Loss: 0.19092467427253723\n",
            "Step 7610, Training Loss: 0.1008191630244255\n",
            "Step 7610, Training Loss: 0.2279742956161499\n",
            "Step 7610, Training Loss: 0.1252060830593109\n",
            "Step 7610, Training Loss: 0.03135569021105766\n",
            "Step 7610, Training Loss: 0.4516826570034027\n",
            "Step 7610, Training Loss: 0.004637250676751137\n",
            "Step 7610, Training Loss: 0.3098782002925873\n",
            "Step 7610, Training Loss: 0.0013913004659116268\n",
            "Step 7610, Training Loss: 0.45747703313827515\n",
            "Step 7610, Training Loss: 0.22676308453083038\n",
            "Step 7610, Training Loss: 0.004202206153422594\n",
            "Step 7610, Training Loss: 0.08846665918827057\n",
            "Step 7610, Training Loss: 0.07906480878591537\n",
            "Step 7610, Training Loss: 0.14330783486366272\n",
            "Step 7610, Training Loss: 0.07273750752210617\n",
            "Step 7610, Training Loss: 0.14378826320171356\n",
            "Step 7610, Training Loss: 0.5882503986358643\n",
            "Step 7610, Training Loss: 0.22984537482261658\n",
            "Step 7610, Training Loss: 0.24693071842193604\n",
            "Step 7610, Training Loss: 0.55478835105896\n",
            "Step 7610, Training Loss: 0.1673867553472519\n",
            "Step 7610, Training Loss: 0.22739680111408234\n",
            "Step 7610, Training Loss: 0.17543283104896545\n",
            "Step 7610, Training Loss: 0.05500289797782898\n",
            "Step 7610, Training Loss: 0.0018464947352185845\n",
            "Step 7610, Training Loss: 0.21673469245433807\n",
            "Step 7610, Training Loss: 0.001934971660375595\n",
            "Step 7610, Training Loss: 0.34629663825035095\n",
            "Step 7610, Training Loss: 0.0028886308427900076\n",
            "Step 7610, Training Loss: 0.02451510913670063\n",
            "Step 7610, Training Loss: 0.002341232728213072\n",
            "Step 7610, Training Loss: 0.28088149428367615\n",
            "Step 7610, Training Loss: 0.3006401062011719\n",
            "Step 7610, Training Loss: 0.4358142614364624\n",
            "Step 7610, Training Loss: 0.13198207318782806\n",
            "Step 7610, Training Loss: 0.0016263050492852926\n",
            "Step 7610, Training Loss: 0.13840167224407196\n",
            "Step 7610, Training Loss: 0.12824063003063202\n",
            "Step 7610, Training Loss: 0.11119125038385391\n",
            "Step 7610, Training Loss: 0.0018083472969010472\n",
            "Step 7610, Training Loss: 0.009982964023947716\n",
            "Step 7610, Training Loss: 0.1519474983215332\n",
            "Step 7610, Training Loss: 0.35431578755378723\n",
            "Step 7610, Training Loss: 0.19741274416446686\n",
            "Step 7610, Training Loss: 0.14693182706832886\n",
            "Step 7610, Training Loss: 0.33342766761779785\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-12-09 19:36:32,385] Trial 9 finished with value: 0.8629014393184927 and parameters: {'learning_rate': 1.37587077953449e-05, 'batch_size': 4, 'num_train_epochs': 10, 'weight_decay': 0.00017332012008881235}. Best is trial 9 with value: 0.8629014393184927.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<<=================Best trial=========================>>\n",
            "FrozenTrial(number=9, state=TrialState.COMPLETE, values=[0.8629014393184927], datetime_start=datetime.datetime(2024, 12, 9, 19, 29, 41, 256485), datetime_complete=datetime.datetime(2024, 12, 9, 19, 36, 32, 385568), params={'learning_rate': 1.37587077953449e-05, 'batch_size': 4, 'num_train_epochs': 10, 'weight_decay': 0.00017332012008881235}, user_attrs={}, system_attrs={}, intermediate_values={}, distributions={'learning_rate': FloatDistribution(high=5e-05, log=True, low=1e-06, step=None), 'batch_size': CategoricalDistribution(choices=(4, 8, 16)), 'num_train_epochs': IntDistribution(high=10, log=False, low=3, step=1), 'weight_decay': FloatDistribution(high=0.1, log=True, low=0.0001, step=None)}, trial_id=9, value=None)\n",
            "<<==========================================>>\n"
          ]
        }
      ],
      "source": [
        "study = optuna.create_study(direction=\"maximize\")\n",
        "study.optimize(objective, n_trials=10)\n",
        "\n",
        "print(\"<<=================Best trial=========================>>\")\n",
        "print(study.best_trial)\n",
        "print(\"<<==========================================>>\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "YJceIAKW02z7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "0c444b18-4b1e-4284-9ee9-348c1c871d82"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "PyTorch: setting up devices\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n",
            "***** Running training *****\n",
            "  Num examples = 6091\n",
            "  Num Epochs = 10\n",
            "  Instantaneous batch size per device = 4\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
            "  Gradient Accumulation steps = 2\n",
            "  Total optimization steps = 7610\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting training...\n",
            "Step 0, Training Loss: 0.001971847377717495\n",
            "Step 0, Training Loss: 0.0015596641460433602\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='2' max='7610' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [   2/7610 : < :, Epoch 0.00/10]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='7610' max='7610' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [7610/7610 06:35, Epoch 9/10]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "      <th>F1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>0.014600</td>\n",
              "      <td>0.190941</td>\n",
              "      <td>0.856861</td>\n",
              "      <td>0.867664</td>\n",
              "      <td>0.860719</td>\n",
              "      <td>0.859752</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.016000</td>\n",
              "      <td>0.191745</td>\n",
              "      <td>0.848982</td>\n",
              "      <td>0.861417</td>\n",
              "      <td>0.858151</td>\n",
              "      <td>0.856890</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.012900</td>\n",
              "      <td>0.191247</td>\n",
              "      <td>0.839133</td>\n",
              "      <td>0.860688</td>\n",
              "      <td>0.858793</td>\n",
              "      <td>0.859468</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.013000</td>\n",
              "      <td>0.202272</td>\n",
              "      <td>0.854235</td>\n",
              "      <td>0.859486</td>\n",
              "      <td>0.870347</td>\n",
              "      <td>0.862239</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.009100</td>\n",
              "      <td>0.200534</td>\n",
              "      <td>0.845699</td>\n",
              "      <td>0.862561</td>\n",
              "      <td>0.856226</td>\n",
              "      <td>0.857833</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>0.009500</td>\n",
              "      <td>0.204698</td>\n",
              "      <td>0.855548</td>\n",
              "      <td>0.867875</td>\n",
              "      <td>0.868421</td>\n",
              "      <td>0.866589</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>0.012200</td>\n",
              "      <td>0.204138</td>\n",
              "      <td>0.856861</td>\n",
              "      <td>0.861963</td>\n",
              "      <td>0.869705</td>\n",
              "      <td>0.863818</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>0.009500</td>\n",
              "      <td>0.211037</td>\n",
              "      <td>0.847669</td>\n",
              "      <td>0.860111</td>\n",
              "      <td>0.862003</td>\n",
              "      <td>0.859989</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>0.009500</td>\n",
              "      <td>0.203029</td>\n",
              "      <td>0.861458</td>\n",
              "      <td>0.865921</td>\n",
              "      <td>0.875481</td>\n",
              "      <td>0.869614</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>0.006900</td>\n",
              "      <td>0.205522</td>\n",
              "      <td>0.857518</td>\n",
              "      <td>0.863561</td>\n",
              "      <td>0.874198</td>\n",
              "      <td>0.867643</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Saving model checkpoint to ./best_results/checkpoint-500\n",
            "Configuration saved in ./best_results/checkpoint-500/config.json\n",
            "Model weights saved in ./best_results/checkpoint-500/pytorch_model.bin\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 500, Training Loss: 0.005326523911207914\n",
            "Step 500, Training Loss: 0.0011863857507705688\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 1523\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to ./best_results/checkpoint-1000\n",
            "Configuration saved in ./best_results/checkpoint-1000/config.json\n",
            "Model weights saved in ./best_results/checkpoint-1000/pytorch_model.bin\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 1000, Training Loss: 0.001245805760845542\n",
            "Step 1000, Training Loss: 0.0015758307417854667\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Saving model checkpoint to ./best_results/checkpoint-1500\n",
            "Configuration saved in ./best_results/checkpoint-1500/config.json\n",
            "Model weights saved in ./best_results/checkpoint-1500/pytorch_model.bin\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 1500, Training Loss: 0.02158554084599018\n",
            "Step 1500, Training Loss: 0.0037173163145780563\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 1523\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to ./best_results/checkpoint-2000\n",
            "Configuration saved in ./best_results/checkpoint-2000/config.json\n",
            "Model weights saved in ./best_results/checkpoint-2000/pytorch_model.bin\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 2000, Training Loss: 0.00874790083616972\n",
            "Step 2000, Training Loss: 0.00202847714535892\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 1523\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to ./best_results/checkpoint-2500\n",
            "Configuration saved in ./best_results/checkpoint-2500/config.json\n",
            "Model weights saved in ./best_results/checkpoint-2500/pytorch_model.bin\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 2500, Training Loss: 0.0008377445046789944\n",
            "Step 2500, Training Loss: 0.17048101127147675\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Saving model checkpoint to ./best_results/checkpoint-3000\n",
            "Configuration saved in ./best_results/checkpoint-3000/config.json\n",
            "Model weights saved in ./best_results/checkpoint-3000/pytorch_model.bin\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 3000, Training Loss: 0.0030588763765990734\n",
            "Step 3000, Training Loss: 0.0012346584117040038\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 1523\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to ./best_results/checkpoint-3500\n",
            "Configuration saved in ./best_results/checkpoint-3500/config.json\n",
            "Model weights saved in ./best_results/checkpoint-3500/pytorch_model.bin\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 3500, Training Loss: 0.001010849024169147\n",
            "Step 3500, Training Loss: 0.0014813115121796727\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 1523\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to ./best_results/checkpoint-4000\n",
            "Configuration saved in ./best_results/checkpoint-4000/config.json\n",
            "Model weights saved in ./best_results/checkpoint-4000/pytorch_model.bin\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 4000, Training Loss: 0.0009165541850961745\n",
            "Step 4000, Training Loss: 0.0007469648844562471\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Saving model checkpoint to ./best_results/checkpoint-4500\n",
            "Configuration saved in ./best_results/checkpoint-4500/config.json\n",
            "Model weights saved in ./best_results/checkpoint-4500/pytorch_model.bin\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 4500, Training Loss: 0.0011452529579401016\n",
            "Step 4500, Training Loss: 0.005040346644818783\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 1523\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to ./best_results/checkpoint-5000\n",
            "Configuration saved in ./best_results/checkpoint-5000/config.json\n",
            "Model weights saved in ./best_results/checkpoint-5000/pytorch_model.bin\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 5000, Training Loss: 0.0008081878186203539\n",
            "Step 5000, Training Loss: 0.0005480027757585049\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 1523\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to ./best_results/checkpoint-5500\n",
            "Configuration saved in ./best_results/checkpoint-5500/config.json\n",
            "Model weights saved in ./best_results/checkpoint-5500/pytorch_model.bin\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 5500, Training Loss: 0.005599067080765963\n",
            "Step 5500, Training Loss: 0.0018454784294590354\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Saving model checkpoint to ./best_results/checkpoint-6000\n",
            "Configuration saved in ./best_results/checkpoint-6000/config.json\n",
            "Model weights saved in ./best_results/checkpoint-6000/pytorch_model.bin\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 6000, Training Loss: 0.0004786637728102505\n",
            "Step 6000, Training Loss: 0.026094460859894753\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 1523\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to ./best_results/checkpoint-6500\n",
            "Configuration saved in ./best_results/checkpoint-6500/config.json\n",
            "Model weights saved in ./best_results/checkpoint-6500/pytorch_model.bin\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 6500, Training Loss: 0.0017816289328038692\n",
            "Step 6500, Training Loss: 0.0016339686699211597\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 1523\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to ./best_results/checkpoint-7000\n",
            "Configuration saved in ./best_results/checkpoint-7000/config.json\n",
            "Model weights saved in ./best_results/checkpoint-7000/pytorch_model.bin\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 7000, Training Loss: 0.013997846283018589\n",
            "Step 7000, Training Loss: 0.0014883126132190228\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Saving model checkpoint to ./best_results/checkpoint-7500\n",
            "Configuration saved in ./best_results/checkpoint-7500/config.json\n",
            "Model weights saved in ./best_results/checkpoint-7500/pytorch_model.bin\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 7500, Training Loss: 0.001515594543889165\n",
            "Step 7500, Training Loss: 0.0004796756256837398\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 1523\n",
            "  Batch size = 8\n",
            "\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=====================Training Time=====================\n",
            "Training completed in 395.38 seconds.\n",
            "=======================================================\n"
          ]
        }
      ],
      "source": [
        "best_hyperparams = study.best_trial.params\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./best_results\",\n",
        "    num_train_epochs=best_hyperparams[\"num_train_epochs\"],\n",
        "    per_device_train_batch_size=best_hyperparams[\"batch_size\"],\n",
        "    per_device_eval_batch_size=best_hyperparams[\"batch_size\"] * 2,\n",
        "    learning_rate=best_hyperparams[\"learning_rate\"],\n",
        "    weight_decay=best_hyperparams[\"weight_decay\"],\n",
        "    warmup_steps=10,\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    gradient_accumulation_steps=2,\n",
        "    report_to=\"none\",\n",
        ")\n",
        "\n",
        "trainer = OneHotTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=eval_dataset,\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "\n",
        "# Measure training time\n",
        "print(\"Starting training...\")\n",
        "start_time = time.time()  # Start the timer\n",
        "\n",
        "trainer.train()\n",
        "\n",
        "end_time = time.time()  # End the timer\n",
        "training_duration = end_time - start_time  # Calculate the duration\n",
        "print(\"=====================Training Time=====================\")\n",
        "print(f\"Training completed in {training_duration:.2f} seconds.\")\n",
        "print(\"=======================================================\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gmbA8nKW1IAq"
      },
      "outputs": [],
      "source": [
        "eval_results = trainer.evaluate()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nlNRlUlh-7y0"
      },
      "source": [
        "### Model Saving"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "maZXl_QZ-Psa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fd0eb506-e26f-444a-dfb2-9282eb8f26e7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Saving model checkpoint to ./best_model_huawei-noah_TinyBERT_General_4L_312D_java\n",
            "Configuration saved in ./best_model_huawei-noah_TinyBERT_General_4L_312D_java/config.json\n",
            "Model weights saved in ./best_model_huawei-noah_TinyBERT_General_4L_312D_java/pytorch_model.bin\n",
            "tokenizer config file saved in ./best_model_huawei-noah_TinyBERT_General_4L_312D_java_tokenizer/tokenizer_config.json\n",
            "Special tokens file saved in ./best_model_huawei-noah_TinyBERT_General_4L_312D_java_tokenizer/special_tokens_map.json\n",
            "loading configuration file ./best_model_huawei-noah_TinyBERT_General_4L_312D_java/config.json\n",
            "Model config BertConfig {\n",
            "  \"_name_or_path\": \"./best_model_huawei-noah_TinyBERT_General_4L_312D_java\",\n",
            "  \"architectures\": [\n",
            "    \"BertForSequenceClassification\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"cell\": {},\n",
            "  \"classifier_dropout\": null,\n",
            "  \"emb_size\": 312,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 312,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\",\n",
            "    \"2\": \"LABEL_2\",\n",
            "    \"3\": \"LABEL_3\",\n",
            "    \"4\": \"LABEL_4\",\n",
            "    \"5\": \"LABEL_5\",\n",
            "    \"6\": \"LABEL_6\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 1200,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1,\n",
            "    \"LABEL_2\": 2,\n",
            "    \"LABEL_3\": 3,\n",
            "    \"LABEL_4\": 4,\n",
            "    \"LABEL_5\": 5,\n",
            "    \"LABEL_6\": 6\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 4,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"pre_trained\": \"\",\n",
            "  \"structure\": [],\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.17.0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "loading weights file ./best_model_huawei-noah_TinyBERT_General_4L_312D_java/pytorch_model.bin\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py:1439: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  state_dict = torch.load(resolved_archive_file, map_location=\"cpu\")\n",
            "All model checkpoint weights were used when initializing BertForSequenceClassification.\n",
            "\n",
            "All the weights of BertForSequenceClassification were initialized from the model checkpoint at ./best_model_huawei-noah_TinyBERT_General_4L_312D_java.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForSequenceClassification for predictions without further training.\n",
            "Didn't find file ./best_model_huawei-noah_TinyBERT_General_4L_312D_java_tokenizer/added_tokens.json. We won't load it.\n",
            "loading file ./best_model_huawei-noah_TinyBERT_General_4L_312D_java_tokenizer/vocab.txt\n",
            "loading file ./best_model_huawei-noah_TinyBERT_General_4L_312D_java_tokenizer/tokenizer.json\n",
            "loading file None\n",
            "loading file ./best_model_huawei-noah_TinyBERT_General_4L_312D_java_tokenizer/special_tokens_map.json\n",
            "loading file ./best_model_huawei-noah_TinyBERT_General_4L_312D_java_tokenizer/tokenizer_config.json\n"
          ]
        }
      ],
      "source": [
        "!mkdir 'best_model_huawei-noah_TinyBERT_General_4L_312D_java'\n",
        "!mkdir 'best_model_huawei-noah_TinyBERT_General_4L_312D_java_tokenizer'\n",
        "\n",
        "# Save model and tokenizer\n",
        "trainer.save_model('./best_model_huawei-noah_TinyBERT_General_4L_312D_java')\n",
        "tokenizer.save_pretrained('./best_model_huawei-noah_TinyBERT_General_4L_312D_java_tokenizer')\n",
        "\n",
        "# Load model and tokenizer\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained('./best_model_huawei-noah_TinyBERT_General_4L_312D_java')\n",
        "tokenizer = AutoTokenizer.from_pretrained('./best_model_huawei-noah_TinyBERT_General_4L_312D_java_tokenizer')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ejkhNznm77uR"
      },
      "source": [
        "### Model Testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "EMvNuBr37lzY"
      },
      "outputs": [],
      "source": [
        "X_test = list(test_df['combo'])\n",
        "y_test = list(test_df['labels'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "collapsed": true,
        "id": "Tut5zEUc790m",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "552c8f25-b8b8-4ad1-85b0-0440ce04e35d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[array([0, 0, 1, 0, 0, 0, 0]) array([0, 0, 1, 0, 0, 0, 0])\n",
            " array([0, 0, 1, 0, 0, 0, 0]) ... array([0, 1, 0, 0, 0, 0, 0])\n",
            " array([0, 1, 0, 0, 0, 0, 0]) array([0, 1, 0, 0, 0, 0, 0])]\n"
          ]
        }
      ],
      "source": [
        "df_test = pd.DataFrame({\"combo\":X_test,\"labels\":y_test})\n",
        "test_text = df_test.combo.values\n",
        "test_label = df_test.labels.values\n",
        "\n",
        "print(test_label)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "v43dDI8E8AFv",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 193
        },
        "outputId": "49376e14-3463-43b2-98d5-3c690f139c45"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "***** Running Prediction *****\n",
            "  Num examples = 1725\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='216' max='216' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [216/216 00:03]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=================Evaluation Metrics================\n",
            "Evaluation Metrics: {'accuracy': 0.768695652173913, 'precision': 0.7916460568726307, 'recall': 0.7829591249280369, 'f1': 0.7822260983692482}\n",
            "===================================================\n",
            "=================Inference Time================\n",
            "Inference Time: 3.43 seconds\n",
            "===================================================\n"
          ]
        }
      ],
      "source": [
        "test_dataset = TextClassificationDataset(test_text, test_label, tokenizer)\n",
        "\n",
        "# Measure inference time\n",
        "start_time = time.time()\n",
        "\n",
        "# Predict on the test dataset\n",
        "predictions = trainer.predict(test_dataset)\n",
        "\n",
        "# Calculate elapsed time\n",
        "end_time = time.time()\n",
        "inference_time = end_time - start_time\n",
        "\n",
        "# Compute metrics using the `compute_metrics` function\n",
        "metrics = compute_metrics(predictions)\n",
        "\n",
        "print(\"=================Evaluation Metrics================\")\n",
        "print(\"Evaluation Metrics:\", metrics)\n",
        "print(\"===================================================\")\n",
        "\n",
        "print(\"=================Inference Time================\")\n",
        "print(f\"Inference Time: {inference_time:.2f} seconds\")\n",
        "print(\"===================================================\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "hdd0lVlvALIk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b9371053-1e8a-4dc8-81cf-4d7d4f90141d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  adding: best_model_huawei-noah_TinyBERT_General_4L_312D_java/ (stored 0%)\n",
            "  adding: best_model_huawei-noah_TinyBERT_General_4L_312D_java/training_args.bin (deflated 51%)\n",
            "  adding: best_model_huawei-noah_TinyBERT_General_4L_312D_java/pytorch_model.bin (deflated 7%)\n",
            "  adding: best_model_huawei-noah_TinyBERT_General_4L_312D_java/config.json (deflated 54%)\n",
            "  adding: best_model_huawei-noah_TinyBERT_General_4L_312D_java_tokenizer/ (stored 0%)\n",
            "  adding: best_model_huawei-noah_TinyBERT_General_4L_312D_java_tokenizer/tokenizer_config.json (deflated 41%)\n",
            "  adding: best_model_huawei-noah_TinyBERT_General_4L_312D_java_tokenizer/vocab.txt (deflated 53%)\n",
            "  adding: best_model_huawei-noah_TinyBERT_General_4L_312D_java_tokenizer/special_tokens_map.json (deflated 40%)\n",
            "  adding: best_model_huawei-noah_TinyBERT_General_4L_312D_java_tokenizer/tokenizer.json (deflated 71%)\n"
          ]
        }
      ],
      "source": [
        "!zip -r best_model_huawei-noah_TinyBERT_General_4L_312D_java.zip './best_model_huawei-noah_TinyBERT_General_4L_312D_java'\n",
        "!zip -r best_model_huawei-noah_TinyBERT_General_4L_312D_java_tokenizer.zip './best_model_huawei-noah_TinyBERT_General_4L_312D_java_tokenizer'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "WI4xntMmASPi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aea800c3-98af-4409-de63-9779cce65376"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# Transferring the model to Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "!cp best_model_huawei-noah_TinyBERT_General_4L_312D_java.zip \"/content/drive/MyDrive/FYP/Revised/Models/huawei-noah_TinyBERT_General_4L_312D_java/\"\n",
        "!cp best_model_huawei-noah_TinyBERT_General_4L_312D_java_tokenizer.zip \"/content/drive/MyDrive/FYP/Revised/Models/huawei-noah_TinyBERT_General_4L_312D_java_Tokenizer/\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Le2HUk3TBuT1"
      },
      "source": [
        "### Load and Test Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "ei49o7unAVBx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a25e0b5e-5478-4e81-edac-88facf2c0618"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "6Dskeu5MBz_j",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "92083848-d833-4d28-d842-7e5108b9c76c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/drive/MyDrive/FYP/Revised/Models/huawei-noah_TinyBERT_General_4L_312D_java/best_model_huawei-noah_TinyBERT_General_4L_312D_java.zip\n",
            "replace ./best_model_huawei-noah_TinyBERT_General_4L_312D_java/training_args.bin? [y]es, [n]o, [A]ll, [N]one, [r]ename: A\n",
            "  inflating: ./best_model_huawei-noah_TinyBERT_General_4L_312D_java/training_args.bin  \n",
            "  inflating: ./best_model_huawei-noah_TinyBERT_General_4L_312D_java/pytorch_model.bin  \n",
            "  inflating: ./best_model_huawei-noah_TinyBERT_General_4L_312D_java/config.json  \n",
            "Archive:  /content/drive/MyDrive/FYP/Revised/Models/huawei-noah_TinyBERT_General_4L_312D_java_Tokenizer/best_model_huawei-noah_TinyBERT_General_4L_312D_java_tokenizer.zip\n",
            "replace ./best_model_huawei-noah_TinyBERT_General_4L_312D_java_tokenizer/tokenizer_config.json? [y]es, [n]o, [A]ll, [N]one, [r]ename: A\n",
            "  inflating: ./best_model_huawei-noah_TinyBERT_General_4L_312D_java_tokenizer/tokenizer_config.json  \n",
            "  inflating: ./best_model_huawei-noah_TinyBERT_General_4L_312D_java_tokenizer/vocab.txt  \n",
            "  inflating: ./best_model_huawei-noah_TinyBERT_General_4L_312D_java_tokenizer/special_tokens_map.json  \n",
            "  inflating: ./best_model_huawei-noah_TinyBERT_General_4L_312D_java_tokenizer/tokenizer.json  \n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "\n",
        "# Replace 'path/to/checkpoint-folder' with the actual path to your checkpoint folder.\n",
        "model_folder = '/content/drive/MyDrive/FYP/Revised/Models/huawei-noah_TinyBERT_General_4L_312D_java/'\n",
        "tokenizer_folder = '/content/drive/MyDrive/FYP/Revised/Models/huawei-noah_TinyBERT_General_4L_312D_java_Tokenizer/'\n",
        "\n",
        "!unzip '/content/drive/MyDrive/FYP/Revised/Models/huawei-noah_TinyBERT_General_4L_312D_java/best_model_huawei-noah_TinyBERT_General_4L_312D_java.zip' -d './'\n",
        "!unzip '/content/drive/MyDrive/FYP/Revised/Models/huawei-noah_TinyBERT_General_4L_312D_java_Tokenizer/best_model_huawei-noah_TinyBERT_General_4L_312D_java_tokenizer.zip' -d './'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "SzPbirKSCa8B"
      },
      "outputs": [],
      "source": [
        "best_model_huawei_noah_TinyBERT_General_4L_312D_java = 'best_model_huawei-noah_TinyBERT_General_4L_312D_java'\n",
        "best_model_huawei_noah_TinyBERT_General_4L_312D_java_tokenizer = 'best_model_huawei-noah_TinyBERT_General_4L_312D_java_tokenizer'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "TVduIUYeCbc1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0e64c522-080c-4e75-9c3a-3989821c243b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loading configuration file best_model_huawei-noah_TinyBERT_General_4L_312D_java/config.json\n",
            "Model config BertConfig {\n",
            "  \"_name_or_path\": \"best_model_huawei-noah_TinyBERT_General_4L_312D_java\",\n",
            "  \"architectures\": [\n",
            "    \"BertForSequenceClassification\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"cell\": {},\n",
            "  \"classifier_dropout\": null,\n",
            "  \"emb_size\": 312,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 312,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\",\n",
            "    \"2\": \"LABEL_2\",\n",
            "    \"3\": \"LABEL_3\",\n",
            "    \"4\": \"LABEL_4\",\n",
            "    \"5\": \"LABEL_5\",\n",
            "    \"6\": \"LABEL_6\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 1200,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1,\n",
            "    \"LABEL_2\": 2,\n",
            "    \"LABEL_3\": 3,\n",
            "    \"LABEL_4\": 4,\n",
            "    \"LABEL_5\": 5,\n",
            "    \"LABEL_6\": 6\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 4,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"pre_trained\": \"\",\n",
            "  \"structure\": [],\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.17.0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "loading weights file best_model_huawei-noah_TinyBERT_General_4L_312D_java/pytorch_model.bin\n",
            "All model checkpoint weights were used when initializing BertForSequenceClassification.\n",
            "\n",
            "All the weights of BertForSequenceClassification were initialized from the model checkpoint at best_model_huawei-noah_TinyBERT_General_4L_312D_java.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForSequenceClassification for predictions without further training.\n",
            "Didn't find file best_model_huawei-noah_TinyBERT_General_4L_312D_java_tokenizer/added_tokens.json. We won't load it.\n",
            "loading file best_model_huawei-noah_TinyBERT_General_4L_312D_java_tokenizer/vocab.txt\n",
            "loading file best_model_huawei-noah_TinyBERT_General_4L_312D_java_tokenizer/tokenizer.json\n",
            "loading file None\n",
            "loading file best_model_huawei-noah_TinyBERT_General_4L_312D_java_tokenizer/special_tokens_map.json\n",
            "loading file best_model_huawei-noah_TinyBERT_General_4L_312D_java_tokenizer/tokenizer_config.json\n"
          ]
        }
      ],
      "source": [
        "# Load the model and tokenizer from the checkpoint\n",
        "model = AutoModelForSequenceClassification.from_pretrained(best_model_huawei_noah_TinyBERT_General_4L_312D_java)\n",
        "tokenizer = AutoTokenizer.from_pretrained(best_model_huawei_noah_TinyBERT_General_4L_312D_java_tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "mraiRFgv8MG6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6122ba72-eb46-4a81-f4c3-00706ef412ec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=============Prediction Time==============\n",
            "0.023012399673461914\n",
            "==========================================\n"
          ]
        }
      ],
      "source": [
        "text = \"equals the @code match field. | AbstractContra....\"\n",
        "encoded_input = tokenizer(text, return_tensors='pt')\n",
        "start_time = time.time()\n",
        "output = model(**encoded_input)\n",
        "end_time = time.time()\n",
        "prediction_time = end_time - start_time\n",
        "output\n",
        "\n",
        "print(\"=============Prediction Time==============\")\n",
        "print(prediction_time)\n",
        "print(\"==========================================\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "WNGL38Kk9Vua",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "428a0817-ec78-465f-a9d7-15c696611b13"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted class/es: 3\n"
          ]
        }
      ],
      "source": [
        "logits = output.logits\n",
        "\n",
        "# Apply sigmoid to convert logits to probabilities\n",
        "probabilities = torch.sigmoid(logits)\n",
        "\n",
        "# Define a threshold to determine if a class is positive\n",
        "threshold = 0.5\n",
        "predicted_indices = (probabilities > threshold).nonzero(as_tuple=True)[1]\n",
        "\n",
        "# Convert indices to a list for output\n",
        "predicted_classes = (predicted_indices + 1).tolist()\n",
        "\n",
        "# Format the output to display classes\n",
        "if len(predicted_classes) > 0:\n",
        "    predicted_classes_str = \", \".join(map(str, predicted_classes))\n",
        "    print(f\"Predicted class/es: {predicted_classes_str}\")\n",
        "else:\n",
        "    print(\"No positive classes predicted.\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "f305934bad7346c39e2138434499c165": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_5e030adc0dd34eb4abc9494603bcc33f",
              "IPY_MODEL_755d27a89f6b43938cef965498724f67",
              "IPY_MODEL_3261f69aa9134467a6c7be0e54c69ec0"
            ],
            "layout": "IPY_MODEL_d52c30006e51406bb49456455dc8f6df"
          }
        },
        "5e030adc0dd34eb4abc9494603bcc33f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e4534745d1244405b68878b0f6f7cc61",
            "placeholder": "​",
            "style": "IPY_MODEL_fe589315eff74fb7a46d52c9629832c2",
            "value": "Downloading: 100%"
          }
        },
        "755d27a89f6b43938cef965498724f67": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9370a04a7d5a42eba5a0b569722fb619",
            "max": 409,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_81266e3c51434d1398db5a46b58add1c",
            "value": 409
          }
        },
        "3261f69aa9134467a6c7be0e54c69ec0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0c2d690fcf4f4fbb857c008bae7f1b12",
            "placeholder": "​",
            "style": "IPY_MODEL_94b0b7204fdc41109c19cd27ee66a039",
            "value": " 409/409 [00:00&lt;00:00, 16.0kB/s]"
          }
        },
        "d52c30006e51406bb49456455dc8f6df": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e4534745d1244405b68878b0f6f7cc61": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fe589315eff74fb7a46d52c9629832c2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9370a04a7d5a42eba5a0b569722fb619": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "81266e3c51434d1398db5a46b58add1c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "0c2d690fcf4f4fbb857c008bae7f1b12": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "94b0b7204fdc41109c19cd27ee66a039": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "342df5dbd71841aebbfbe42791eafbfb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_5b17febaf1974bda8628583799b61045",
              "IPY_MODEL_f51835395eb24fcba89a3eed46218123",
              "IPY_MODEL_97825d75548e40a1881fb258c2e7beeb"
            ],
            "layout": "IPY_MODEL_1bd46438f777449db23f9a815334a70f"
          }
        },
        "5b17febaf1974bda8628583799b61045": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5a09384224e94a688d48f754b2d38551",
            "placeholder": "​",
            "style": "IPY_MODEL_32fb0d854fa04e598aaacc92c95ac2c3",
            "value": "Downloading: 100%"
          }
        },
        "f51835395eb24fcba89a3eed46218123": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b8083aa880be4c4fa925b3e0ad0d5878",
            "max": 231508,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8a1e30fc96744f1c9e219f2b7bab44e4",
            "value": 231508
          }
        },
        "97825d75548e40a1881fb258c2e7beeb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ec137894b50846739d4d2aa569ee0190",
            "placeholder": "​",
            "style": "IPY_MODEL_efa25a580b8643368a2b1773e4296c0d",
            "value": " 226k/226k [00:00&lt;00:00, 4.92MB/s]"
          }
        },
        "1bd46438f777449db23f9a815334a70f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5a09384224e94a688d48f754b2d38551": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "32fb0d854fa04e598aaacc92c95ac2c3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b8083aa880be4c4fa925b3e0ad0d5878": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8a1e30fc96744f1c9e219f2b7bab44e4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ec137894b50846739d4d2aa569ee0190": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "efa25a580b8643368a2b1773e4296c0d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7e015dca2f194f74886615592a729c21": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c5772d7780e044b798969f3bc660fa5c",
              "IPY_MODEL_9373e22e12a4467ca233c8894754e73e",
              "IPY_MODEL_6eee226a54384f12a80fb25a2629b875"
            ],
            "layout": "IPY_MODEL_154203c10176432786aefa00a5ca651b"
          }
        },
        "c5772d7780e044b798969f3bc660fa5c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e7f1a49d201c41c8a59da116b34caa04",
            "placeholder": "​",
            "style": "IPY_MODEL_e99df93b272545ff900e6fbeb9b3dd77",
            "value": "Downloading: 100%"
          }
        },
        "9373e22e12a4467ca233c8894754e73e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_221e8e1847c44dfbb4ec61bbde7ca28e",
            "max": 62747391,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_203b69099d9c4775b944d9d066a07ac5",
            "value": 62747391
          }
        },
        "6eee226a54384f12a80fb25a2629b875": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f24c949b6a4744b18460151d93d6fa41",
            "placeholder": "​",
            "style": "IPY_MODEL_5b7404e4775445279facc4ba1e0b2b45",
            "value": " 59.8M/59.8M [00:00&lt;00:00, 73.9MB/s]"
          }
        },
        "154203c10176432786aefa00a5ca651b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e7f1a49d201c41c8a59da116b34caa04": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e99df93b272545ff900e6fbeb9b3dd77": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "221e8e1847c44dfbb4ec61bbde7ca28e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "203b69099d9c4775b944d9d066a07ac5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f24c949b6a4744b18460151d93d6fa41": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5b7404e4775445279facc4ba1e0b2b45": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}